2024-10-08 19:19:02,004 INFO [train.py:1230] Training started
2024-10-08 19:19:02,005 INFO [train.py:1240] Device: cuda:0
2024-10-08 19:19:02,007 INFO [train.py:1271] Using dtype=torch.float32
2024-10-08 19:19:02,007 INFO [train.py:1272] Use AMP=False
2024-10-08 19:19:02,007 INFO [train.py:1274] {'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 50, 'reset_interval': 200, 'valid_interval': 3000, 'feature_dim': 80, 'subsampling_factor': 4, 'ignore_id': -1, 'label_smoothing': 0.1, 'warm_step': 2000, 'env_info': {'k2-version': '1.24.3', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': 'e400fa3b456faf8afe0ee5bfe572946b4921a3db', 'k2-git-date': 'Sat Jul 15 04:21:50 2023', 'lhotse-version': '1.25.0', 'torch-version': '2.0.1+cu118', 'torch-cuda-available': True, 'torch-cuda-version': '11.8', 'python-version': '3.9', 'icefall-git-branch': 'master', 'icefall-git-sha1': 'cabeaf7f-dirty', 'icefall-git-date': 'Thu Oct 3 12:53:52 2024', 'icefall-path': '/mnt/ahogpu_ldisk2/adriang/icefall', 'k2-path': '/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/k2/__init__.py', 'lhotse-path': '/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/lhotse/__init__.py', 'hostname': 'ahogpu', 'IP address': '192.168.1.130'}, 'world_size': 1, 'master_port': 12354, 'tensorboard': True, 'num_epochs': 30, 'start_epoch': 1, 'start_batch': 0, 'exp_dir': PosixPath('zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_True'), 'bpe_model': '/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/data/lang_bpe_256/bpe.model', 'base_lr': 0.045, 'lr_batches': 7500, 'lr_epochs': 3.5, 'ref_duration': 600, 'context_size': 2, 'prune_range': 5, 'lm_scale': 0.25, 'am_scale': 0.0, 'simple_loss_scale': 0.5, 'ctc_loss_scale': 0.2, 'attention_decoder_loss_scale': 0.8, 'seed': 42, 'print_diagnostics': False, 'inf_check': False, 'save_every_n': 4000, 'keep_last_k': 30, 'average_period': 200, 'use_bf16': False, 'use_fp16': False, 'num_encoder_layers': '2,2,3,4,3,2', 'downsampling_factor': '1,2,4,8,4,2', 'feedforward_dim': '512,768,1024,1536,1024,768', 'num_heads': '4,4,4,8,4,4', 'encoder_dim': '192,256,384,512,384,256', 'query_head_dim': '32', 'value_head_dim': '12', 'pos_head_dim': '4', 'pos_dim': 48, 'encoder_unmasked_dim': '192,192,256,256,256,192', 'cnn_module_kernel': '31,31,15,15,15,31', 'decoder_dim': 512, 'joiner_dim': 512, 'attention_decoder_dim': 512, 'attention_decoder_num_layers': 6, 'attention_decoder_attention_dim': 512, 'attention_decoder_num_heads': 8, 'attention_decoder_feedforward_dim': 2048, 'causal': True, 'chunk_size': '16,32,64,-1', 'left_context_frames': '64,128,256,-1', 'use_transducer': False, 'use_ctc': True, 'use_attention_decoder': True, 'manifest_dir': PosixPath('/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/data/fbank'), 'max_duration': 200.0, 'bucketing_sampler': True, 'num_buckets': 30, 'concatenate_cuts': False, 'duration_factor': 1.0, 'gap': 1.0, 'on_the_fly_feats': False, 'shuffle': True, 'drop_last': True, 'return_cuts': True, 'num_workers': 2, 'enable_spec_aug': True, 'spec_aug_time_warp_factor': 80, 'enable_musan': True, 'input_strategy': 'PrecomputedFeatures', 'blank_id': 0, 'sos_id': 1, 'eos_id': 1, 'vocab_size': 256, 'dtype': torch.float32, 'use_autocast': False}
2024-10-08 19:19:02,007 INFO [train.py:1276] About to create model
2024-10-08 19:19:02,571 INFO [train.py:1280] Number of model parameters: 90173943
2024-10-08 19:19:04,220 INFO [train.py:1298] Using single GPU
2024-10-08 19:19:04,237 INFO [custom_asr_data_module.py:394] About to load train cuts
2024-10-08 19:19:04,238 INFO [custom_asr_data_module.py:204] Enable MUSAN
2024-10-08 19:19:04,238 INFO [custom_asr_data_module.py:205] About to get Musan cuts
2024-10-08 19:19:06,021 INFO [custom_asr_data_module.py:234] Enable SpecAugment
2024-10-08 19:19:06,021 INFO [custom_asr_data_module.py:235] Time warp factor: 80
2024-10-08 19:19:06,022 INFO [custom_asr_data_module.py:245] Num frame mask: 10
2024-10-08 19:19:06,022 INFO [custom_asr_data_module.py:260] About to create train dataset
2024-10-08 19:19:06,022 INFO [custom_asr_data_module.py:287] Using DynamicBucketingSampler.
2024-10-08 19:19:06,406 INFO [custom_asr_data_module.py:304] About to create train dataloader
2024-10-08 19:19:06,406 INFO [custom_asr_data_module.py:402] About to load valid cuts
2024-10-08 19:19:06,407 INFO [custom_asr_data_module.py:338] About to create dev dataset
2024-10-08 19:19:06,428 INFO [custom_asr_data_module.py:355] About to create dev dataloader
2024-10-08 19:19:06,428 INFO [train.py:1490] Sanity check -- see if any of the batches in epoch 1 would cause OOM.
/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
2024-10-08 19:19:39,048 INFO [train.py:1518] Maximum memory allocated so far is 5407MB
2024-10-08 19:19:39,976 INFO [train.py:1518] Maximum memory allocated so far is 5407MB
2024-10-08 19:19:40,953 INFO [train.py:1518] Maximum memory allocated so far is 5407MB
2024-10-08 19:19:42,019 INFO [train.py:1518] Maximum memory allocated so far is 5407MB
2024-10-08 19:19:43,032 INFO [train.py:1518] Maximum memory allocated so far is 5433MB
2024-10-08 19:19:43,793 INFO [scaling.py:1024] Whitening: name=None, num_groups=1, num_channels=384, metric=84.32 vs. limit=5.0
2024-10-08 19:19:44,094 INFO [train.py:1518] Maximum memory allocated so far is 5434MB
2024-10-08 19:19:56,902 INFO [train.py:1152] Epoch 1, batch 0, loss[loss=7.586, ctc_loss=4.271, attn_decoder_loss=8.415, over 4853.00 frames. ], tot_loss[loss=7.586, ctc_loss=4.271, attn_decoder_loss=8.415, over 4853.00 frames. ], batch size: 19, lr: 2.25e-02,
2024-10-08 19:19:56,902 INFO [train.py:1175] Computing validation loss
2024-10-08 19:20:02,494 INFO [train.py:1184] Epoch 1, validation: loss=7.444, ctc_loss=3.983, attn_decoder_loss=8.31, over 90464.00 frames.
2024-10-08 19:20:02,494 INFO [train.py:1185] Maximum memory allocated so far is 5457MB
2024-10-08 19:20:05,654 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=0.0, ans=0.5
2024-10-08 19:20:08,545 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module2.whiten, num_groups=1, num_channels=192, metric=12.30 vs. limit=7.5
2024-10-08 19:20:10,623 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=23.36 vs. limit=7.5
2024-10-08 19:20:10,651 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=17.31 vs. limit=7.5
2024-10-08 19:20:13,271 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=21.37 vs. limit=7.50125
2024-10-08 19:20:13,663 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.023e+02 9.062e+02 9.619e+02 1.017e+03 1.459e+03, threshold=3.848e+03, percent-clipped=0.0
2024-10-08 19:20:19,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=18.48 vs. limit=5.0008333333333335
2024-10-08 19:20:26,274 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.844e+02 8.023e+02 8.837e+02 9.706e+02 1.630e+03, threshold=3.535e+03, percent-clipped=0.0
2024-10-08 19:20:28,943 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=6.666666666666667, ans=0.19975
2024-10-08 19:20:29,709 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=11.77 vs. limit=5.001666666666667
2024-10-08 19:20:34,013 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=6.666666666666667, ans=0.4996875
2024-10-08 19:20:37,159 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=26.94 vs. limit=5.001666666666667
2024-10-08 19:20:44,906 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=40.36 vs. limit=5.005
2024-10-08 19:20:49,549 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=10.0, ans=0.49953125
2024-10-08 19:20:51,914 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.415e+02 6.326e+02 7.335e+02 8.837e+02 1.630e+03, threshold=2.934e+03, percent-clipped=0.0
2024-10-08 19:20:53,498 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=13.333333333333334, ans=0.29986666666666667
2024-10-08 19:20:56,403 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=46.57 vs. limit=7.51
2024-10-08 19:21:04,771 INFO [train.py:1152] Epoch 1, batch 50, loss[loss=2.012, ctc_loss=1.11, attn_decoder_loss=2.238, over 4912.00 frames. ], tot_loss[loss=3.46, ctc_loss=1.415, attn_decoder_loss=3.971, over 217782.93 frames. ], batch size: 19, lr: 2.48e-02,
2024-10-08 19:21:04,878 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=16.666666666666668, ans=0.20025
2024-10-08 19:21:17,850 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=20.0, ans=0.4990625
2024-10-08 19:21:19,432 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=26.46 vs. limit=7.515
2024-10-08 19:21:21,640 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=20.0, ans=0.8993
2024-10-08 19:21:29,496 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=23.333333333333332, ans=0.09985416666666667
2024-10-08 19:21:29,866 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.86 vs. limit=7.5175
2024-10-08 19:21:31,994 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=23.333333333333332, ans=0.49890625
2024-10-08 19:21:39,077 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=74.95 vs. limit=5.011666666666667
2024-10-08 19:21:40,460 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=34.46 vs. limit=7.50875
2024-10-08 19:21:41,898 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=15.21 vs. limit=7.5175
2024-10-08 19:21:42,937 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=19.83 vs. limit=5.006666666666667
2024-10-08 19:21:55,457 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=33.92 vs. limit=4.012
2024-10-08 19:21:59,408 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=24.29 vs. limit=7.51125
2024-10-08 19:22:02,712 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=30.0, ans=0.09932500000000001
2024-10-08 19:22:08,858 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.182e+02 4.100e+02 4.945e+02 6.769e+02 1.630e+03, threshold=9.890e+02, percent-clipped=0.0
2024-10-08 19:22:08,913 INFO [train.py:1152] Epoch 1, batch 100, loss[loss=1.391, ctc_loss=1.025, attn_decoder_loss=1.483, over 4752.00 frames. ], tot_loss[loss=2.518, ctc_loss=1.257, attn_decoder_loss=2.833, over 383572.91 frames. ], batch size: 19, lr: 2.70e-02,
2024-10-08 19:22:15,757 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=33.333333333333336, ans=0.09925
2024-10-08 19:22:16,469 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=80.63 vs. limit=7.5125
2024-10-08 19:22:20,251 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=53.68 vs. limit=7.5125
2024-10-08 19:22:21,228 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=115.70 vs. limit=7.51375
2024-10-08 19:22:30,559 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=40.19 vs. limit=7.5275
2024-10-08 19:22:40,943 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=90.89 vs. limit=7.53
2024-10-08 19:22:41,934 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=153.21 vs. limit=7.53
2024-10-08 19:22:44,658 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=34.03 vs. limit=7.515
2024-10-08 19:22:46,470 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.01 vs. limit=3.006
2024-10-08 19:22:48,922 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.79 vs. limit=3.0065
2024-10-08 19:22:52,244 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=43.333333333333336, ans=0.29956666666666665
2024-10-08 19:22:57,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=43.333333333333336, ans=0.19837500000000002
2024-10-08 19:22:57,793 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=160.27 vs. limit=7.5325
2024-10-08 19:22:58,704 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=43.333333333333336, ans=5.027083333333334
2024-10-08 19:22:59,259 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=22.54 vs. limit=5.010833333333333
2024-10-08 19:23:00,731 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=28.09 vs. limit=7.5175
2024-10-08 19:23:01,699 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=142.78 vs. limit=7.5175
2024-10-08 19:23:11,142 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=34.96 vs. limit=7.535
2024-10-08 19:23:11,727 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=46.666666666666664, ans=0.4978125
2024-10-08 19:23:11,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=46.666666666666664, ans=0.2995333333333333
2024-10-08 19:23:14,465 INFO [train.py:1152] Epoch 1, batch 150, loss[loss=1.216, ctc_loss=1.05, attn_decoder_loss=1.258, over 4910.00 frames. ], tot_loss[loss=2.023, ctc_loss=1.192, attn_decoder_loss=2.231, over 513480.80 frames. ], batch size: 19, lr: 2.93e-02,
2024-10-08 19:23:15,841 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=50.0, ans=0.49765625
2024-10-08 19:23:19,296 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=51.61 vs. limit=5.025
2024-10-08 19:23:20,548 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=95.01 vs. limit=7.5375
2024-10-08 19:23:24,136 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=38.32 vs. limit=7.5375
2024-10-08 19:23:32,375 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=53.333333333333336, ans=0.198
2024-10-08 19:23:40,941 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=31.76 vs. limit=5.028333333333333
2024-10-08 19:23:41,948 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=155.77 vs. limit=7.5425
2024-10-08 19:24:04,826 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=63.333333333333336, ans=0.49703125
2024-10-08 19:24:08,685 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=63.333333333333336, ans=5.039583333333334
2024-10-08 19:24:09,159 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=36.27 vs. limit=7.52375
2024-10-08 19:24:10,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=40.09 vs. limit=7.52375
2024-10-08 19:24:14,737 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=50.26 vs. limit=7.5475
2024-10-08 19:24:15,288 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=63.333333333333336, ans=0.09857500000000001
2024-10-08 19:24:19,456 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.610e+02 1.908e+02 2.260e+02 2.745e+02 3.410e+02, threshold=4.520e+02, percent-clipped=0.0
2024-10-08 19:24:19,511 INFO [train.py:1152] Epoch 1, batch 200, loss[loss=1.129, ctc_loss=1.155, attn_decoder_loss=1.122, over 4760.00 frames. ], tot_loss[loss=1.717, ctc_loss=1.162, attn_decoder_loss=1.856, over 613838.26 frames. ], batch size: 45, lr: 3.15e-02,
2024-10-08 19:24:20,557 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=88.03 vs. limit=7.525
2024-10-08 19:24:22,871 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=22.88 vs. limit=5.016666666666667
2024-10-08 19:24:27,424 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=66.66666666666667, ans=0.09958333333333334
2024-10-08 19:24:33,897 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=70.0, ans=0.49125
2024-10-08 19:24:39,650 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.28 vs. limit=3.0105
2024-10-08 19:24:53,405 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=73.33333333333333, ans=0.2992666666666667
2024-10-08 19:24:54,117 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=12.70 vs. limit=4.029333333333334
2024-10-08 19:24:57,785 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=51.15 vs. limit=7.52875
2024-10-08 19:24:58,786 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=180.90 vs. limit=7.52875
2024-10-08 19:25:03,698 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=76.66666666666667, ans=0.49640625
2024-10-08 19:25:14,046 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=80.0, ans=0.197
2024-10-08 19:25:14,614 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=13.53 vs. limit=7.53
2024-10-08 19:25:24,424 INFO [train.py:1152] Epoch 1, batch 250, loss[loss=1.066, ctc_loss=1.166, attn_decoder_loss=1.041, over 4819.00 frames. ], tot_loss[loss=1.503, ctc_loss=1.14, attn_decoder_loss=1.594, over 692578.36 frames. ], batch size: 38, lr: 3.38e-02,
2024-10-08 19:25:27,500 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=79.28 vs. limit=7.53125
2024-10-08 19:25:28,448 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=83.33333333333333, ans=5.052083333333333
2024-10-08 19:25:31,389 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=29.27 vs. limit=7.5625
2024-10-08 19:25:32,822 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=31.75 vs. limit=7.5625
2024-10-08 19:25:37,846 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=9.36 vs. limit=4.034666666666666
2024-10-08 19:25:39,324 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=42.37 vs. limit=7.5325
2024-10-08 19:25:43,908 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=86.66666666666667, ans=0.4959375
2024-10-08 19:25:47,215 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module2.whiten, num_groups=1, num_channels=192, metric=9.96 vs. limit=7.5325
2024-10-08 19:25:47,595 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=86.66666666666667, ans=0.4959375
2024-10-08 19:26:06,935 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=93.33333333333333, ans=0.495625
2024-10-08 19:26:11,422 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.whiten.whitening_limit, batch_count=93.33333333333333, ans=4.037333333333334
2024-10-08 19:26:12,835 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=34.50 vs. limit=5.046666666666667
2024-10-08 19:26:16,878 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=20.47 vs. limit=7.5725
2024-10-08 19:26:17,429 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=96.66666666666667, ans=0.2445625
2024-10-08 19:26:18,771 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=96.66666666666667, ans=0.49546875
2024-10-08 19:26:21,324 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=96.66666666666667, ans=0.19637500000000002
2024-10-08 19:26:24,315 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=86.70 vs. limit=7.53625
2024-10-08 19:26:29,004 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.448e+01 1.043e+02 1.176e+02 1.364e+02 1.627e+02, threshold=2.353e+02, percent-clipped=0.0
2024-10-08 19:26:29,056 INFO [train.py:1152] Epoch 1, batch 300, loss[loss=1.038, ctc_loss=1.196, attn_decoder_loss=0.9979, over 4773.00 frames. ], tot_loss[loss=1.352, ctc_loss=1.125, attn_decoder_loss=1.409, over 753007.62 frames. ], batch size: 32, lr: 3.60e-02,
2024-10-08 19:26:30,149 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=15.12 vs. limit=5.0
2024-10-08 19:26:31,165 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=188.19 vs. limit=5.05
2024-10-08 19:26:31,728 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=100.0, ans=0.4953125
2024-10-08 19:26:34,985 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=38.03 vs. limit=7.575
2024-10-08 19:26:50,132 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=103.33333333333333, ans=0.09935416666666667
2024-10-08 19:26:51,968 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=74.15 vs. limit=7.53875
2024-10-08 19:26:52,660 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=4.127e-02
2024-10-08 19:26:56,370 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=106.66666666666667, ans=0.244
2024-10-08 19:27:01,376 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=14.10 vs. limit=5.026666666666666
2024-10-08 19:27:10,597 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=12.32 vs. limit=5.0275
2024-10-08 19:27:13,914 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=110.0, ans=0.049656250000000006
2024-10-08 19:27:19,715 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=4.68 vs. limit=4.044
2024-10-08 19:27:23,046 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=113.33333333333333, ans=0.19575
2024-10-08 19:27:24,923 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=27.97 vs. limit=7.585
2024-10-08 19:27:28,570 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=137.68 vs. limit=7.5425
2024-10-08 19:27:35,496 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=126.81 vs. limit=7.54375
2024-10-08 19:27:36,093 INFO [train.py:1152] Epoch 1, batch 350, loss[loss=0.9318, ctc_loss=1.052, attn_decoder_loss=0.9018, over 4883.00 frames. ], tot_loss[loss=1.24, ctc_loss=1.116, attn_decoder_loss=1.271, over 800374.59 frames. ], batch size: 19, lr: 3.83e-02,
2024-10-08 19:27:37,075 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=93.96 vs. limit=7.54375
2024-10-08 19:27:46,949 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=85.00 vs. limit=7.54375
2024-10-08 19:27:48,035 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=128.31 vs. limit=7.59
2024-10-08 19:27:50,773 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=16.12 vs. limit=7.545
2024-10-08 19:27:50,825 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=6.80 vs. limit=4.048
2024-10-08 19:27:53,006 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=120.0, ans=0.494375
2024-10-08 19:27:55,176 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=13.28 vs. limit=7.545
2024-10-08 19:27:57,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module2.whiten, num_groups=1, num_channels=192, metric=7.76 vs. limit=7.545
2024-10-08 19:28:07,479 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=10.63 vs. limit=4.049333333333333
2024-10-08 19:28:09,144 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.91 vs. limit=7.5925
2024-10-08 19:28:32,993 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=130.0, ans=0.19512500000000002
2024-10-08 19:28:34,667 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=119.81 vs. limit=7.54875
2024-10-08 19:28:40,998 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.794e+01 6.880e+01 7.634e+01 8.147e+01 1.003e+02, threshold=1.527e+02, percent-clipped=0.0
2024-10-08 19:28:41,049 INFO [train.py:1152] Epoch 1, batch 400, loss[loss=0.9013, ctc_loss=1.099, attn_decoder_loss=0.8518, over 4874.00 frames. ], tot_loss[loss=1.156, ctc_loss=1.112, attn_decoder_loss=1.168, over 837078.96 frames. ], batch size: 22, lr: 4.05e-02,
2024-10-08 19:28:49,496 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=8.67 vs. limit=4.053333333333334
2024-10-08 19:28:57,244 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten.whitening_limit, batch_count=136.66666666666666, ans=7.55125
2024-10-08 19:29:07,344 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.14 vs. limit=5.035
2024-10-08 19:29:17,641 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=6.18 vs. limit=4.056
2024-10-08 19:29:18,439 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.min_positive, batch_count=143.33333333333334, ans=0.09910416666666667
2024-10-08 19:29:20,588 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=117.16 vs. limit=7.55375
2024-10-08 19:29:29,488 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=17.60 vs. limit=7.6075
2024-10-08 19:29:34,992 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=146.66666666666666, ans=0.2985333333333333
2024-10-08 19:29:42,559 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=146.66666666666666, ans=5.091666666666667
2024-10-08 19:29:44,965 INFO [train.py:1152] Epoch 1, batch 450, loss[loss=0.8829, ctc_loss=1.12, attn_decoder_loss=0.8235, over 4873.00 frames. ], tot_loss[loss=1.087, ctc_loss=1.105, attn_decoder_loss=1.083, over 865679.54 frames. ], batch size: 23, lr: 4.28e-02,
2024-10-08 19:29:45,558 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=92.85 vs. limit=5.075
2024-10-08 19:29:47,660 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=150.0, ans=0.49296875
2024-10-08 19:29:57,459 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=19.55 vs. limit=7.5575
2024-10-08 19:30:01,298 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.59 vs. limit=5.038333333333333
2024-10-08 19:30:02,424 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.53 vs. limit=5.038333333333333
2024-10-08 19:30:06,418 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=5.33 vs. limit=4.061333333333334
2024-10-08 19:30:07,103 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=153.33333333333334, ans=0.04952083333333333
2024-10-08 19:30:10,943 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=156.66666666666666, ans=0.49265625
2024-10-08 19:30:16,448 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=31.90 vs. limit=7.55875
2024-10-08 19:30:26,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=160.0, ans=0.8944
2024-10-08 19:30:33,319 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=125.52 vs. limit=7.56
2024-10-08 19:30:43,946 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=21.56 vs. limit=5.081666666666667
2024-10-08 19:30:44,622 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=163.33333333333334, ans=0.19387500000000002
2024-10-08 19:30:45,099 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=27.56 vs. limit=7.6225
2024-10-08 19:30:47,103 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=163.33333333333334, ans=0.8942833333333333
2024-10-08 19:30:49,143 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=16.82 vs. limit=7.5625
2024-10-08 19:30:49,523 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.512e+01 5.611e+01 6.101e+01 6.871e+01 1.248e+02, threshold=1.220e+02, percent-clipped=0.0
2024-10-08 19:30:49,574 INFO [train.py:1152] Epoch 1, batch 500, loss[loss=0.8548, ctc_loss=1.085, attn_decoder_loss=0.7973, over 4806.00 frames. ], tot_loss[loss=1.036, ctc_loss=1.101, attn_decoder_loss=1.02, over 888220.59 frames. ], batch size: 34, lr: 4.49e-02,
2024-10-08 19:30:50,119 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=15.92 vs. limit=7.5625
2024-10-08 19:30:53,594 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=166.66666666666666, ans=0.4921875
2024-10-08 19:30:56,421 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=166.66666666666666, ans=0.5
2024-10-08 19:30:58,183 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=8.13 vs. limit=7.5625
2024-10-08 19:31:07,172 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=111.72 vs. limit=7.56375
2024-10-08 19:31:13,477 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=24.26 vs. limit=7.6275
2024-10-08 19:31:18,503 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=112.37 vs. limit=7.565
2024-10-08 19:31:25,618 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=173.33333333333334, ans=0.491875
2024-10-08 19:31:26,204 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=66.59 vs. limit=7.565
2024-10-08 19:31:33,189 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=176.66666666666666, ans=0.09889583333333334
2024-10-08 19:31:33,773 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=13.75 vs. limit=5.0441666666666665
2024-10-08 19:31:33,881 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=29.72 vs. limit=7.56625
2024-10-08 19:31:35,785 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=176.66666666666666, ans=0.49171875
2024-10-08 19:31:38,255 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=176.66666666666666, ans=0.19337500000000002
2024-10-08 19:31:50,416 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.80 vs. limit=4.072
2024-10-08 19:31:53,687 INFO [train.py:1152] Epoch 1, batch 550, loss[loss=0.8786, ctc_loss=1.123, attn_decoder_loss=0.8176, over 4821.00 frames. ], tot_loss[loss=0.9935, ctc_loss=1.097, attn_decoder_loss=0.9678, over 905711.34 frames. ], batch size: 40, lr: 4.49e-02,
2024-10-08 19:31:53,805 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=183.33333333333334, ans=0.49140625
2024-10-08 19:32:03,986 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=183.33333333333334, ans=0.49140625
2024-10-08 19:32:08,658 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=21.43 vs. limit=7.64
2024-10-08 19:32:09,325 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=186.66666666666666, ans=0.04941666666666667
2024-10-08 19:32:09,393 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=186.66666666666666, ans=0.193
2024-10-08 19:32:13,821 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.59 vs. limit=7.64
2024-10-08 19:32:15,804 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=186.66666666666666, ans=0.49125
2024-10-08 19:32:17,627 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=512, metric=9.53 vs. limit=7.57
2024-10-08 19:32:28,004 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.91 vs. limit=5.0475
2024-10-08 19:32:38,497 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=18.50 vs. limit=7.5725
2024-10-08 19:32:38,934 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=193.33333333333334, ans=0.29806666666666665
2024-10-08 19:32:45,778 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=49.05 vs. limit=7.6475
2024-10-08 19:32:48,551 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=17.42 vs. limit=7.57375
2024-10-08 19:32:58,244 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.89 vs. limit=5.1
2024-10-08 19:32:58,603 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.112e+01 5.052e+01 5.494e+01 6.183e+01 8.527e+01, threshold=1.099e+02, percent-clipped=0.0
2024-10-08 19:32:58,655 INFO [train.py:1152] Epoch 1, batch 600, loss[loss=0.8902, ctc_loss=1.141, attn_decoder_loss=0.8275, over 4834.00 frames. ], tot_loss[loss=0.9583, ctc_loss=1.092, attn_decoder_loss=0.9249, over 919511.50 frames. ], batch size: 38, lr: 4.49e-02,
2024-10-08 19:33:02,034 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.14 vs. limit=7.65
2024-10-08 19:33:06,867 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=23.05 vs. limit=5.1
2024-10-08 19:33:22,376 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=9.38 vs. limit=4.081333333333333
2024-10-08 19:33:27,844 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.32 vs. limit=5.1033333333333335
2024-10-08 19:33:31,641 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=40.29 vs. limit=7.655
2024-10-08 19:33:32,463 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=206.66666666666666, ans=0.29793333333333333
2024-10-08 19:33:46,000 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=110.31 vs. limit=7.57875
2024-10-08 19:33:51,301 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=14.52 vs. limit=5.1066666666666665
2024-10-08 19:33:52,409 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.29 vs. limit=5.053333333333334
2024-10-08 19:34:03,283 INFO [train.py:1152] Epoch 1, batch 650, loss[loss=0.8259, ctc_loss=1.055, attn_decoder_loss=0.7686, over 4844.00 frames. ], tot_loss[loss=0.9317, ctc_loss=1.09, attn_decoder_loss=0.892, over 930350.51 frames. ], batch size: 21, lr: 4.49e-02,
2024-10-08 19:34:04,719 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=216.66666666666666, ans=0.8924166666666666
2024-10-08 19:34:05,027 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=102.04 vs. limit=7.58125
2024-10-08 19:34:18,634 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=11.02 vs. limit=7.5825
2024-10-08 19:34:24,400 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=220.0, ans=0.2978
2024-10-08 19:34:25,763 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=220.0, ans=0.4896875
2024-10-08 19:34:27,431 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=12.77 vs. limit=7.5825
2024-10-08 19:34:35,041 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=20.39 vs. limit=7.58375
2024-10-08 19:34:35,952 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=223.33333333333334, ans=0.48953125
2024-10-08 19:34:37,515 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=17.90 vs. limit=7.58375
2024-10-08 19:34:40,228 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=132.73 vs. limit=7.58375
2024-10-08 19:34:44,319 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=2.09 vs. limit=3.034
2024-10-08 19:34:47,495 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=226.66666666666666, ans=0.489375
2024-10-08 19:34:51,704 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=26.07 vs. limit=7.67
2024-10-08 19:34:53,218 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.47 vs. limit=5.056666666666667
2024-10-08 19:34:56,339 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=230.0, ans=0.89195
2024-10-08 19:35:00,683 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.43 vs. limit=4.092
2024-10-08 19:35:07,289 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.025e+01 4.755e+01 5.314e+01 6.341e+01 5.331e+02, threshold=1.063e+02, percent-clipped=1.0
2024-10-08 19:35:07,340 INFO [train.py:1152] Epoch 1, batch 700, loss[loss=0.7938, ctc_loss=1.003, attn_decoder_loss=0.7415, over 4758.00 frames. ], tot_loss[loss=0.9106, ctc_loss=1.09, attn_decoder_loss=0.8659, over 938206.07 frames. ], batch size: 19, lr: 4.49e-02,
2024-10-08 19:35:16,626 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=108.49 vs. limit=7.5875
2024-10-08 19:35:16,746 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.01 vs. limit=5.058333333333334
2024-10-08 19:35:21,364 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=236.66666666666666, ans=0.2976333333333333
2024-10-08 19:35:24,602 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.07 vs. limit=7.6775
2024-10-08 19:35:27,108 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=17.77 vs. limit=7.58875
2024-10-08 19:35:44,662 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=117.08 vs. limit=7.59125
2024-10-08 19:35:56,121 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=12.32 vs. limit=7.59125
2024-10-08 19:36:11,166 INFO [train.py:1152] Epoch 1, batch 750, loss[loss=0.7469, ctc_loss=1.008, attn_decoder_loss=0.6817, over 4880.00 frames. ], tot_loss[loss=0.8891, ctc_loss=1.084, attn_decoder_loss=0.8403, over 944974.19 frames. ], batch size: 22, lr: 4.49e-02,
2024-10-08 19:36:18,869 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=250.0, ans=0.48828125
2024-10-08 19:36:39,659 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=140.83 vs. limit=7.59625
2024-10-08 19:36:45,995 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.80 vs. limit=5.128333333333333
2024-10-08 19:36:52,418 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=11.16 vs. limit=7.5975
2024-10-08 19:36:54,953 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=35.49 vs. limit=7.695
2024-10-08 19:37:01,393 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=2.71 vs. limit=3.0395
2024-10-08 19:37:08,616 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=25.90 vs. limit=7.59875
2024-10-08 19:37:09,522 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=118.07 vs. limit=7.59875
2024-10-08 19:37:14,832 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=22.28 vs. limit=7.59875
2024-10-08 19:37:17,030 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.624e+01 5.005e+01 5.983e+01 8.469e+01 6.188e+02, threshold=1.197e+02, percent-clipped=16.0
2024-10-08 19:37:17,085 INFO [train.py:1152] Epoch 1, batch 800, loss[loss=0.7685, ctc_loss=0.9994, attn_decoder_loss=0.7107, over 4855.00 frames. ], tot_loss[loss=0.8734, ctc_loss=1.083, attn_decoder_loss=0.8211, over 949819.77 frames. ], batch size: 19, lr: 4.49e-02,
2024-10-08 19:37:19,729 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=266.6666666666667, ans=0.4875
2024-10-08 19:37:24,871 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=266.6666666666667, ans=0.4875
2024-10-08 19:37:46,064 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.84 vs. limit=5.068333333333333
2024-10-08 19:37:47,302 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=8.38 vs. limit=7.6025
2024-10-08 19:37:47,413 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=19.60 vs. limit=7.6025
2024-10-08 19:37:50,859 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=22.65 vs. limit=7.6025
2024-10-08 19:37:54,302 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=276.6666666666667, ans=0.8903166666666666
2024-10-08 19:38:05,640 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.min_positive, batch_count=276.6666666666667, ans=0.24723333333333333
2024-10-08 19:38:07,285 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=25.15 vs. limit=7.71
2024-10-08 19:38:08,538 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=61.33 vs. limit=7.605
2024-10-08 19:38:08,986 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=7.77 vs. limit=7.71
2024-10-08 19:38:10,008 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=38.01 vs. limit=7.71
2024-10-08 19:38:17,706 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.64 vs. limit=7.71
2024-10-08 19:38:20,732 INFO [train.py:1152] Epoch 1, batch 850, loss[loss=0.7489, ctc_loss=1.002, attn_decoder_loss=0.6856, over 4793.00 frames. ], tot_loss[loss=0.8612, ctc_loss=1.082, attn_decoder_loss=0.8061, over 954090.52 frames. ], batch size: 29, lr: 4.49e-02,
2024-10-08 19:38:24,609 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=283.3333333333333, ans=0.48671875
2024-10-08 19:38:28,092 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.85 vs. limit=7.7125
2024-10-08 19:38:39,229 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.13 vs. limit=5.1433333333333335
2024-10-08 19:38:40,511 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=8.67 vs. limit=7.6075
2024-10-08 19:38:46,760 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=5.15 vs. limit=5.145
2024-10-08 19:39:04,403 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=87.14 vs. limit=7.61
2024-10-08 19:39:17,369 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.79 vs. limit=5.074166666666667
2024-10-08 19:39:18,912 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=15.39 vs. limit=7.61125
2024-10-08 19:39:24,270 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.000e+01 8.076e+01 1.115e+02 2.112e+02 1.094e+03, threshold=2.230e+02, percent-clipped=45.0
2024-10-08 19:39:24,324 INFO [train.py:1152] Epoch 1, batch 900, loss[loss=0.7841, ctc_loss=1.041, attn_decoder_loss=0.7198, over 4854.00 frames. ], tot_loss[loss=0.8514, ctc_loss=1.08, attn_decoder_loss=0.7942, over 956897.83 frames. ], batch size: 19, lr: 4.48e-02,
2024-10-08 19:39:24,449 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=300.0, ans=0.4859375
2024-10-08 19:39:34,102 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=120.25 vs. limit=7.6125
2024-10-08 19:39:40,512 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=80.15 vs. limit=7.61375
2024-10-08 19:39:42,144 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.40 vs. limit=7.7275
2024-10-08 19:39:51,270 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=306.6666666666667, ans=0.485625
2024-10-08 19:39:57,321 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.max_positive, batch_count=306.6666666666667, ans=0.7530666666666667
2024-10-08 19:40:02,371 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=310.0, ans=0.48546875
2024-10-08 19:40:07,960 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=77.77 vs. limit=7.61625
2024-10-08 19:40:19,317 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=14.68 vs. limit=5.156666666666666
2024-10-08 19:40:20,911 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=3.99 vs. limit=5.156666666666666
2024-10-08 19:40:27,575 INFO [train.py:1152] Epoch 1, batch 950, loss[loss=0.8434, ctc_loss=1.089, attn_decoder_loss=0.7819, over 4817.00 frames. ], tot_loss[loss=0.845, ctc_loss=1.081, attn_decoder_loss=0.7859, over 958663.64 frames. ], batch size: 19, lr: 4.48e-02,
2024-10-08 19:40:28,457 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.16 vs. limit=7.7375
2024-10-08 19:41:00,392 WARNING [optim.py:503] Scaling gradients by 0.04317683354020119, model_norm_threshold=222.98683166503906
2024-10-08 19:41:00,546 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.205e+06, grad_sumsq=9.875e+09, orig_rms_sq=6.284e-04
2024-10-08 19:41:08,395 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=326.6666666666667, ans=0.09265000000000001
2024-10-08 19:41:21,089 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=330.0, ans=0.8884500000000001
2024-10-08 19:41:21,462 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=28.29 vs. limit=7.7475
2024-10-08 19:41:21,712 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=27.60 vs. limit=7.7475
2024-10-08 19:41:28,868 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=330.0, ans=0.48453125
2024-10-08 19:41:31,570 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.277e+01 1.147e+02 1.898e+02 3.849e+02 5.165e+03, threshold=3.797e+02, percent-clipped=38.0
2024-10-08 19:41:31,625 INFO [train.py:1152] Epoch 1, batch 1000, loss[loss=0.7598, ctc_loss=1.006, attn_decoder_loss=0.6982, over 4940.00 frames. ], tot_loss[loss=0.8412, ctc_loss=1.083, attn_decoder_loss=0.7807, over 960466.08 frames. ], batch size: 20, lr: 4.48e-02,
2024-10-08 19:41:33,834 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=23.12 vs. limit=7.625
2024-10-08 19:41:35,719 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=333.3333333333333, ans=0.8883333333333333
2024-10-08 19:41:56,112 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=16.02 vs. limit=7.6275
2024-10-08 19:42:15,003 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=343.3333333333333, ans=0.29656666666666665
2024-10-08 19:42:18,705 WARNING [optim.py:503] Scaling gradients by 0.08190065622329712, model_norm_threshold=379.6682434082031
2024-10-08 19:42:18,861 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.532e+06, grad_sumsq=1.861e+07, orig_rms_sq=4.584e-01
2024-10-08 19:42:20,285 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=343.3333333333333, ans=0.20515
2024-10-08 19:42:25,962 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=16.63 vs. limit=7.63
2024-10-08 19:42:26,434 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=346.6666666666667, ans=0.48375
2024-10-08 19:42:28,279 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=16.64 vs. limit=7.63
2024-10-08 19:42:33,772 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.63 vs. limit=5.173333333333334
2024-10-08 19:42:35,637 INFO [train.py:1152] Epoch 1, batch 1050, loss[loss=0.7865, ctc_loss=1.015, attn_decoder_loss=0.7294, over 4792.00 frames. ], tot_loss[loss=0.8345, ctc_loss=1.08, attn_decoder_loss=0.7732, over 962579.73 frames. ], batch size: 25, lr: 4.48e-02,
2024-10-08 19:42:44,838 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.40 vs. limit=4.14
2024-10-08 19:42:44,968 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.90 vs. limit=7.7625
2024-10-08 19:42:48,786 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.15 vs. limit=5.176666666666667
2024-10-08 19:42:49,885 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=37.10 vs. limit=7.765
2024-10-08 19:42:53,629 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=24.31 vs. limit=7.765
2024-10-08 19:42:55,046 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=46.53 vs. limit=7.6325
2024-10-08 19:42:57,247 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=353.3333333333333, ans=0.4834375
2024-10-08 19:43:00,410 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.95 vs. limit=7.7675
2024-10-08 19:43:06,381 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=63.06 vs. limit=7.63375
2024-10-08 19:43:07,613 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=34.88 vs. limit=7.7675
2024-10-08 19:43:09,668 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=356.6666666666667, ans=0.186625
2024-10-08 19:43:10,899 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=356.6666666666667, ans=0.04888541666666667
2024-10-08 19:43:12,204 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=360.0, ans=0.483125
2024-10-08 19:43:12,662 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=59.69 vs. limit=7.635
2024-10-08 19:43:13,490 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=360.0, ans=0.2964
2024-10-08 19:43:25,863 WARNING [optim.py:503] Scaling gradients by 0.03164682164788246, model_norm_threshold=379.6682434082031
2024-10-08 19:43:26,020 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.709e+07, grad_sumsq=3.209e+09, orig_rms_sq=2.091e-02
2024-10-08 19:43:38,729 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.131e+01 1.281e+02 2.364e+02 4.507e+02 1.200e+04, threshold=4.729e+02, percent-clipped=36.0
2024-10-08 19:43:38,781 INFO [train.py:1152] Epoch 1, batch 1100, loss[loss=0.7729, ctc_loss=0.9946, attn_decoder_loss=0.7174, over 4863.00 frames. ], tot_loss[loss=0.829, ctc_loss=1.076, attn_decoder_loss=0.7671, over 964036.58 frames. ], batch size: 20, lr: 4.48e-02,
2024-10-08 19:43:38,916 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=366.6666666666667, ans=0.4828125
2024-10-08 19:43:39,713 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=4.67 vs. limit=5.091666666666667
2024-10-08 19:43:53,161 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=23.53 vs. limit=7.7775
2024-10-08 19:43:54,956 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=5.26 vs. limit=4.074
2024-10-08 19:43:58,375 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys.whitening_limit, batch_count=370.0, ans=3.0555
2024-10-08 19:44:06,640 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=373.3333333333333, ans=0.4825
2024-10-08 19:44:14,746 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=5.96 vs. limit=5.1866666666666665
2024-10-08 19:44:15,933 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.38 vs. limit=5.094166666666666
2024-10-08 19:44:17,292 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=23.95 vs. limit=7.64125
2024-10-08 19:44:22,133 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=103.81 vs. limit=7.64125
2024-10-08 19:44:24,371 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=22.82 vs. limit=7.64125
2024-10-08 19:44:32,038 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=18.33 vs. limit=7.6425
2024-10-08 19:44:41,455 INFO [train.py:1152] Epoch 1, batch 1150, loss[loss=0.837, ctc_loss=1.114, attn_decoder_loss=0.7676, over 4850.00 frames. ], tot_loss[loss=0.826, ctc_loss=1.075, attn_decoder_loss=0.7637, over 964294.11 frames. ], batch size: 20, lr: 4.47e-02,
2024-10-08 19:44:47,394 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten.whitening_limit, batch_count=383.3333333333333, ans=7.64375
2024-10-08 19:44:57,211 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=15.84 vs. limit=7.645
2024-10-08 19:44:58,089 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=386.6666666666667, ans=0.481875
2024-10-08 19:45:01,695 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=386.6666666666667, ans=0.2058
2024-10-08 19:45:09,197 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=390.0, ans=0.48171875
2024-10-08 19:45:18,677 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=9.05 vs. limit=7.6475
2024-10-08 19:45:19,744 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=98.91 vs. limit=7.6475
2024-10-08 19:45:20,703 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=393.3333333333333, ans=0.2059
2024-10-08 19:45:21,786 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=393.3333333333333, ans=0.4815625
2024-10-08 19:45:24,869 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=28.73 vs. limit=7.795
2024-10-08 19:45:30,254 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=2.98 vs. limit=3.059
2024-10-08 19:45:33,408 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=396.6666666666667, ans=0.48140625
2024-10-08 19:45:35,206 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=40.21 vs. limit=7.64875
2024-10-08 19:45:39,869 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=396.6666666666667, ans=0.8861166666666667
2024-10-08 19:45:42,235 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=396.6666666666667, ans=0.8861166666666667
2024-10-08 19:45:45,273 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.775e+01 1.798e+02 3.036e+02 5.870e+02 4.316e+03, threshold=6.073e+02, percent-clipped=35.0
2024-10-08 19:45:45,325 INFO [train.py:1152] Epoch 1, batch 1200, loss[loss=0.8018, ctc_loss=1.107, attn_decoder_loss=0.7255, over 4810.00 frames. ], tot_loss[loss=0.8219, ctc_loss=1.073, attn_decoder_loss=0.759, over 964164.94 frames. ], batch size: 25, lr: 4.47e-02,
2024-10-08 19:45:48,056 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=400.0, ans=0.45
2024-10-08 19:46:20,494 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=406.6666666666667, ans=0.4809375
2024-10-08 19:46:24,245 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=410.0, ans=0.48078125
2024-10-08 19:46:24,253 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=410.0, ans=0.5
2024-10-08 19:46:25,547 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=410.0, ans=0.184625
2024-10-08 19:46:32,227 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=88.53 vs. limit=7.65375
2024-10-08 19:46:40,801 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=413.3333333333333, ans=0.7541333333333333
2024-10-08 19:46:50,891 INFO [train.py:1152] Epoch 1, batch 1250, loss[loss=0.7811, ctc_loss=1.018, attn_decoder_loss=0.7219, over 4754.00 frames. ], tot_loss[loss=0.8194, ctc_loss=1.073, attn_decoder_loss=0.7561, over 964201.34 frames. ], batch size: 32, lr: 4.47e-02,
2024-10-08 19:46:53,951 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=31.58 vs. limit=7.65625
2024-10-08 19:47:00,034 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=416.6666666666667, ans=0.48046875
2024-10-08 19:47:10,948 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=10.46 vs. limit=7.6575
2024-10-08 19:47:24,596 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=29.86 vs. limit=7.8175
2024-10-08 19:47:28,058 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=426.6666666666667, ans=0.184
2024-10-08 19:47:28,508 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=52.15 vs. limit=7.66
2024-10-08 19:47:28,580 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=25.94 vs. limit=7.82
2024-10-08 19:47:29,695 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=13.55 vs. limit=7.66
2024-10-08 19:47:30,095 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.16 vs. limit=7.82
2024-10-08 19:47:42,996 WARNING [optim.py:503] Scaling gradients by 0.0811239704489708, model_norm_threshold=607.2689819335938
2024-10-08 19:47:43,154 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.57, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.193e+07, grad_sumsq=1.810e+09, orig_rms_sq=1.764e-02
2024-10-08 19:47:44,752 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=430.0, ans=0.47984375
2024-10-08 19:47:48,909 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=94.64 vs. limit=7.66125
2024-10-08 19:47:54,814 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.864e+01 2.119e+02 4.374e+02 8.873e+02 7.486e+03, threshold=8.748e+02, percent-clipped=36.0
2024-10-08 19:47:54,865 INFO [train.py:1152] Epoch 1, batch 1300, loss[loss=0.8554, ctc_loss=1.112, attn_decoder_loss=0.7914, over 4845.00 frames. ], tot_loss[loss=0.814, ctc_loss=1.068, attn_decoder_loss=0.7505, over 965501.64 frames. ], batch size: 43, lr: 4.47e-02,
2024-10-08 19:48:01,175 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=433.3333333333333, ans=0.4796875
2024-10-08 19:48:08,265 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=13.24 vs. limit=7.66375
2024-10-08 19:48:10,039 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=436.6666666666667, ans=0.47953125
2024-10-08 19:48:17,178 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.53 vs. limit=7.66375
2024-10-08 19:48:17,699 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=436.6666666666667, ans=0.2956333333333333
2024-10-08 19:48:32,488 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.29 vs. limit=7.8325
2024-10-08 19:48:38,804 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=25.40 vs. limit=7.8325
2024-10-08 19:48:45,667 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=4.91 vs. limit=5.111666666666666
2024-10-08 19:48:58,494 INFO [train.py:1152] Epoch 1, batch 1350, loss[loss=0.8065, ctc_loss=1.041, attn_decoder_loss=0.7477, over 4829.00 frames. ], tot_loss[loss=0.8094, ctc_loss=1.063, attn_decoder_loss=0.7459, over 966236.50 frames. ], batch size: 21, lr: 4.46e-02,
2024-10-08 19:48:58,597 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=450.0, ans=0.47890625
2024-10-08 19:49:01,167 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=450.0, ans=0.88425
2024-10-08 19:49:11,622 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=453.3333333333333, ans=0.47875
2024-10-08 19:49:12,579 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=5.38 vs. limit=5.0
2024-10-08 19:49:19,769 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=512, metric=12.69 vs. limit=7.67
2024-10-08 19:49:24,300 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=456.6666666666667, ans=0.47859375
2024-10-08 19:49:35,406 WARNING [optim.py:503] Scaling gradients by 0.04104931652545929, model_norm_threshold=874.7957763671875
2024-10-08 19:49:35,562 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.088e+08, grad_sumsq=1.183e+10, orig_rms_sq=1.764e-02
2024-10-08 19:49:37,274 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=10.89 vs. limit=4.184
2024-10-08 19:49:38,888 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.91 vs. limit=7.845
2024-10-08 19:49:54,804 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=463.3333333333333, ans=0.44208333333333333
2024-10-08 19:49:56,091 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=463.3333333333333, ans=0.09710416666666667
2024-10-08 19:49:57,416 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=463.3333333333333, ans=0.47828125
2024-10-08 19:50:02,578 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.355e+01 2.812e+02 8.346e+02 1.954e+03 2.131e+04, threshold=1.669e+03, percent-clipped=47.0
2024-10-08 19:50:02,632 INFO [train.py:1152] Epoch 1, batch 1400, loss[loss=0.7938, ctc_loss=1.044, attn_decoder_loss=0.7312, over 4940.00 frames. ], tot_loss[loss=0.8082, ctc_loss=1.062, attn_decoder_loss=0.7448, over 966729.80 frames. ], batch size: 19, lr: 4.46e-02,
2024-10-08 19:50:02,799 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=466.6666666666667, ans=0.22375
2024-10-08 19:50:03,418 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=26.29 vs. limit=7.675
2024-10-08 19:50:03,985 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=466.6666666666667, ans=0.04854166666666667
2024-10-08 19:50:05,996 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=5.32 vs. limit=5.233333333333333
2024-10-08 19:50:10,842 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=53.12 vs. limit=7.675
2024-10-08 19:50:19,670 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=25.64 vs. limit=7.8525
2024-10-08 19:50:22,636 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=8.18 vs. limit=7.67625
2024-10-08 19:50:27,913 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.86 vs. limit=7.855
2024-10-08 19:50:28,416 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=2.255e+01
2024-10-08 19:50:30,072 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.89 vs. limit=7.855
2024-10-08 19:50:33,325 WARNING [optim.py:503] Scaling gradients by 0.06251970678567886, model_norm_threshold=1669.138671875
2024-10-08 19:50:33,483 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.739e+08, grad_sumsq=3.822e+08, orig_rms_sq=7.167e-01
2024-10-08 19:50:37,820 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=55.39 vs. limit=7.6775
2024-10-08 19:50:39,364 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=32.45 vs. limit=7.6775
2024-10-08 19:50:43,738 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=476.6666666666667, ans=0.182125
2024-10-08 19:50:46,536 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=52.28 vs. limit=7.67875
2024-10-08 19:50:50,085 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=476.6666666666667, ans=0.5
2024-10-08 19:50:57,909 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=480.0, ans=0.20720000000000002
2024-10-08 19:51:00,364 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=480.0, ans=0.223
2024-10-08 19:51:02,921 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=480.0, ans=0.8832
2024-10-08 19:51:06,488 INFO [train.py:1152] Epoch 1, batch 1450, loss[loss=0.7743, ctc_loss=1.04, attn_decoder_loss=0.7078, over 4798.00 frames. ], tot_loss[loss=0.8062, ctc_loss=1.059, attn_decoder_loss=0.7429, over 966641.08 frames. ], batch size: 34, lr: 4.46e-02,
2024-10-08 19:51:06,671 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=483.3333333333333, ans=0.47734375
2024-10-08 19:51:06,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=12.77 vs. limit=7.68125
2024-10-08 19:51:20,901 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=21.99 vs. limit=7.6825
2024-10-08 19:51:21,825 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=486.6666666666667, ans=0.4771875
2024-10-08 19:51:22,135 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=18.72 vs. limit=7.6825
2024-10-08 19:51:30,404 WARNING [optim.py:503] Scaling gradients by 0.09506546705961227, model_norm_threshold=1669.138671875
2024-10-08 19:51:30,564 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.389e+07, grad_sumsq=4.253e+09, orig_rms_sq=1.502e-02
2024-10-08 19:51:32,671 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten.whitening_limit, batch_count=490.0, ans=7.8675
2024-10-08 19:51:37,167 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=490.0, ans=0.09693750000000001
2024-10-08 19:51:37,660 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=61.92 vs. limit=7.68375
2024-10-08 19:51:39,738 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=490.0, ans=0.47703125
2024-10-08 19:51:44,001 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=70.96 vs. limit=7.685
2024-10-08 19:51:44,657 WARNING [optim.py:503] Scaling gradients by 0.03778291121125221, model_norm_threshold=1669.138671875
2024-10-08 19:51:44,812 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.657e+08, grad_sumsq=9.491e+09, orig_rms_sq=4.907e-02
2024-10-08 19:51:48,651 WARNING [optim.py:503] Scaling gradients by 0.0768987387418747, model_norm_threshold=1669.138671875
2024-10-08 19:51:48,807 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.1.nonlin_attention.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.053e+07, grad_sumsq=1.694e+09, orig_rms_sq=4.753e-02
2024-10-08 19:51:50,727 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=16.99 vs. limit=7.685
2024-10-08 19:51:57,839 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=496.6666666666667, ans=0.2950333333333333
2024-10-08 19:52:02,120 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten.whitening_limit, batch_count=496.6666666666667, ans=7.68625
2024-10-08 19:52:06,365 WARNING [optim.py:503] Scaling gradients by 0.0373043417930603, model_norm_threshold=1669.138671875
2024-10-08 19:52:06,522 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.192e+08, grad_sumsq=3.110e+11, orig_rms_sq=1.348e-03
2024-10-08 19:52:10,419 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.194e+02 4.006e+02 1.021e+03 2.527e+03 4.474e+04, threshold=2.042e+03, percent-clipped=36.0
2024-10-08 19:52:10,473 INFO [train.py:1152] Epoch 1, batch 1500, loss[loss=0.7637, ctc_loss=1.044, attn_decoder_loss=0.6936, over 4743.00 frames. ], tot_loss[loss=0.8069, ctc_loss=1.06, attn_decoder_loss=0.7436, over 966360.75 frames. ], batch size: 26, lr: 4.46e-02,
2024-10-08 19:52:22,555 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=7.85 vs. limit=7.68875
2024-10-08 19:52:23,904 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=37.54 vs. limit=7.68875
2024-10-08 19:52:24,500 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=503.3333333333333, ans=0.8823833333333334
2024-10-08 19:52:30,597 WARNING [optim.py:503] Scaling gradients by 0.04857207462191582, model_norm_threshold=2042.449462890625
2024-10-08 19:52:30,753 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.490e+08, grad_sumsq=2.591e+10, orig_rms_sq=1.347e-02
2024-10-08 19:52:37,016 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=506.6666666666667, ans=0.47625
2024-10-08 19:52:38,364 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=506.6666666666667, ans=0.2949333333333333
2024-10-08 19:52:38,402 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=506.6666666666667, ans=0.0886
2024-10-08 19:52:40,183 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=31.41 vs. limit=7.88
2024-10-08 19:52:40,458 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=35.05 vs. limit=7.88
2024-10-08 19:52:40,524 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.03 vs. limit=7.88
2024-10-08 19:52:44,243 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=49.42 vs. limit=7.69
2024-10-08 19:52:47,278 WARNING [optim.py:503] Scaling gradients by 0.07307969778776169, model_norm_threshold=2042.449462890625
2024-10-08 19:52:47,435 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.297e+08, grad_sumsq=3.022e+08, orig_rms_sq=7.601e-01
2024-10-08 19:52:48,658 WARNING [optim.py:503] Scaling gradients by 0.05768301337957382, model_norm_threshold=2042.449462890625
2024-10-08 19:52:48,814 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.70, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.798e+08, grad_sumsq=6.750e+10, orig_rms_sq=1.303e-02
2024-10-08 19:52:55,203 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=510.0, ans=0.2949
2024-10-08 19:52:56,239 WARNING [optim.py:503] Scaling gradients by 0.01499723456799984, model_norm_threshold=2042.449462890625
2024-10-08 19:52:56,392 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.838e+09, grad_sumsq=1.149e+10, orig_rms_sq=7.690e-01
2024-10-08 19:53:00,482 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=513.3333333333334, ans=0.5
2024-10-08 19:53:06,260 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=46.93 vs. limit=7.6925
2024-10-08 19:53:09,086 WARNING [optim.py:503] Scaling gradients by 0.08402307331562042, model_norm_threshold=2042.449462890625
2024-10-08 19:53:09,241 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.584e+08, grad_sumsq=1.148e+10, orig_rms_sq=1.380e-02
2024-10-08 19:53:09,373 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=513.3333333333334, ans=0.18075000000000002
2024-10-08 19:53:14,339 INFO [train.py:1152] Epoch 1, batch 1550, loss[loss=0.777, ctc_loss=1.023, attn_decoder_loss=0.7155, over 4857.00 frames. ], tot_loss[loss=0.8047, ctc_loss=1.058, attn_decoder_loss=0.7414, over 966180.64 frames. ], batch size: 31, lr: 4.45e-02,
2024-10-08 19:53:19,414 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=516.6666666666666, ans=0.47578125
2024-10-08 19:53:21,754 WARNING [optim.py:503] Scaling gradients by 0.07485979050397873, model_norm_threshold=2042.449462890625
2024-10-08 19:53:21,911 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.285e+08, grad_sumsq=1.536e+10, orig_rms_sq=1.488e-02
2024-10-08 19:53:23,121 WARNING [optim.py:503] Scaling gradients by 0.07030965387821198, model_norm_threshold=2042.449462890625
2024-10-08 19:53:23,277 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.581e+08, grad_sumsq=9.416e+10, orig_rms_sq=1.679e-03
2024-10-08 19:53:23,440 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=516.6666666666666, ans=0.8819166666666667
2024-10-08 19:53:28,946 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=512, metric=7.95 vs. limit=7.695
2024-10-08 19:53:34,192 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.60 vs. limit=7.695
2024-10-08 19:53:42,271 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=523.3333333333334, ans=0.088225
2024-10-08 19:53:47,090 WARNING [optim.py:503] Scaling gradients by 0.009029105305671692, model_norm_threshold=2042.449462890625
2024-10-08 19:53:47,247 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.032e+10, grad_sumsq=6.778e+11, orig_rms_sq=1.523e-02
2024-10-08 19:53:48,504 WARNING [optim.py:503] Scaling gradients by 0.0533517561852932, model_norm_threshold=2042.449462890625
2024-10-08 19:53:48,662 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.386e+08, grad_sumsq=2.879e+10, orig_rms_sq=1.523e-02
2024-10-08 19:53:49,409 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=25.69 vs. limit=7.69625
2024-10-08 19:53:51,690 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=16.11 vs. limit=5.131666666666667
2024-10-08 19:53:57,450 WARNING [optim.py:503] Scaling gradients by 0.047099098563194275, model_norm_threshold=2042.449462890625
2024-10-08 19:53:57,600 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.797e+08, grad_sumsq=4.507e+10, orig_rms_sq=1.508e-02
2024-10-08 19:53:59,035 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=526.6666666666666, ans=0.29473333333333335
2024-10-08 19:54:01,459 WARNING [optim.py:503] Scaling gradients by 0.09404165297746658, model_norm_threshold=2042.449462890625
2024-10-08 19:54:01,616 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.654e+08, grad_sumsq=1.097e+10, orig_rms_sq=1.508e-02
2024-10-08 19:54:02,300 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=67.59 vs. limit=7.6975
2024-10-08 19:54:08,732 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=15.98 vs. limit=4.212
2024-10-08 19:54:11,857 WARNING [optim.py:503] Scaling gradients by 0.06556586176156998, model_norm_threshold=2042.449462890625
2024-10-08 19:54:12,013 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.542e+08, grad_sumsq=2.498e+10, orig_rms_sq=1.418e-02
2024-10-08 19:54:13,048 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=14.13 vs. limit=7.69875
2024-10-08 19:54:15,303 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.65 vs. limit=5.265
2024-10-08 19:54:16,612 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=40.76 vs. limit=7.69875
2024-10-08 19:54:19,969 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.329e+02 2.308e+03 5.482e+03 1.195e+04 2.262e+05, threshold=1.096e+04, percent-clipped=75.0
2024-10-08 19:54:20,020 INFO [train.py:1152] Epoch 1, batch 1600, loss[loss=0.8217, ctc_loss=1.085, attn_decoder_loss=0.7559, over 4810.00 frames. ], tot_loss[loss=0.8018, ctc_loss=1.054, attn_decoder_loss=0.7387, over 966378.52 frames. ], batch size: 25, lr: 4.45e-02,
2024-10-08 19:54:20,620 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=29.17 vs. limit=7.9
2024-10-08 19:54:23,913 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=533.3333333333334, ans=0.8813333333333333
2024-10-08 19:54:28,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.min_abs, batch_count=533.3333333333334, ans=0.20800000000000002
2024-10-08 19:54:31,929 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=9.52 vs. limit=7.70125
2024-10-08 19:54:37,017 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=17.91 vs. limit=7.70125
2024-10-08 19:54:40,964 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=41.03 vs. limit=7.70125
2024-10-08 19:54:50,580 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=540.0, ans=0.08785
2024-10-08 19:54:54,435 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.max_abs, batch_count=540.0, ans=5.3375
2024-10-08 19:54:55,565 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=540.0, ans=0.4746875
2024-10-08 19:55:03,152 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=543.3333333333334, ans=0.29456666666666664
2024-10-08 19:55:03,231 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=543.3333333333334, ans=0.47453125
2024-10-08 19:55:05,836 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=543.3333333333334, ans=0.048302083333333336
2024-10-08 19:55:20,282 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.27 vs. limit=5.136666666666667
2024-10-08 19:55:23,388 INFO [train.py:1152] Epoch 1, batch 1650, loss[loss=0.8394, ctc_loss=1.135, attn_decoder_loss=0.7654, over 4792.00 frames. ], tot_loss[loss=0.8013, ctc_loss=1.053, attn_decoder_loss=0.7385, over 966732.12 frames. ], batch size: 29, lr: 4.45e-02,
2024-10-08 19:55:24,433 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.24 vs. limit=7.9125
2024-10-08 19:55:27,963 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=53.93 vs. limit=7.9125
2024-10-08 19:55:38,115 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=15.76 vs. limit=4.221333333333333
2024-10-08 19:55:39,695 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.53 vs. limit=5.276666666666666
2024-10-08 19:55:41,659 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=553.3333333333334, ans=0.4740625
2024-10-08 19:55:44,849 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.12 vs. limit=3.083
2024-10-08 19:55:49,557 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=556.6666666666666, ans=0.8805166666666667
2024-10-08 19:55:54,059 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.44 vs. limit=5.139166666666666
2024-10-08 19:56:00,101 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=29.13 vs. limit=7.9175
2024-10-08 19:56:00,318 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.78 vs. limit=7.9175
2024-10-08 19:56:00,899 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=560.0, ans=0.179
2024-10-08 19:56:00,911 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=560.0, ans=0.2084
2024-10-08 19:56:01,556 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=93.15 vs. limit=7.71
2024-10-08 19:56:05,876 WARNING [optim.py:503] Scaling gradients by 0.05512794852256775, model_norm_threshold=10964.4794921875
2024-10-08 19:56:06,033 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.266e+09, grad_sumsq=6.173e+11, orig_rms_sq=1.177e-02
2024-10-08 19:56:14,251 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=125.54 vs. limit=7.71125
2024-10-08 19:56:15,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=563.3333333333334, ans=0.8802833333333333
2024-10-08 19:56:16,314 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=563.3333333333334, ans=0.5
2024-10-08 19:56:22,114 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.20 vs. limit=7.9225
2024-10-08 19:56:24,124 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=16.68 vs. limit=7.71125
2024-10-08 19:56:25,560 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=8.69 vs. limit=7.71125
2024-10-08 19:56:27,149 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=10.27 vs. limit=7.7125
2024-10-08 19:56:27,204 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=9.08 vs. limit=7.925
2024-10-08 19:56:27,398 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.868e+02 2.760e+03 5.963e+03 1.387e+04 1.989e+05, threshold=1.193e+04, percent-clipped=30.0
2024-10-08 19:56:27,399 WARNING [optim.py:503] Scaling gradients by 0.08956985175609589, model_norm_threshold=11925.466796875
2024-10-08 19:56:27,556 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.914e+09, grad_sumsq=8.726e+09, orig_rms_sq=7.923e-01
2024-10-08 19:56:27,614 INFO [train.py:1152] Epoch 1, batch 1700, loss[loss=0.7622, ctc_loss=1.014, attn_decoder_loss=0.6993, over 4940.00 frames. ], tot_loss[loss=0.8019, ctc_loss=1.054, attn_decoder_loss=0.7389, over 966952.97 frames. ], batch size: 19, lr: 4.44e-02,
2024-10-08 19:56:28,407 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=2.94 vs. limit=4.226666666666667
2024-10-08 19:56:30,593 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1.whitening_limit, batch_count=566.6666666666666, ans=5.141666666666667
2024-10-08 19:56:30,912 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=33.58 vs. limit=7.7125
2024-10-08 19:56:31,285 WARNING [optim.py:503] Scaling gradients by 0.05576712265610695, model_norm_threshold=11925.466796875
2024-10-08 19:56:31,442 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.838e+10, grad_sumsq=1.518e+12, orig_rms_sq=1.211e-02
2024-10-08 19:56:43,836 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.38 vs. limit=5.1425
2024-10-08 19:56:48,854 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=18.17 vs. limit=7.9275
2024-10-08 19:56:55,668 WARNING [optim.py:503] Scaling gradients by 0.0654154047369957, model_norm_threshold=11925.466796875
2024-10-08 19:56:55,824 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.437e+09, grad_sumsq=6.121e+11, orig_rms_sq=1.215e-02
2024-10-08 19:57:00,672 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=28.45 vs. limit=7.715
2024-10-08 19:57:05,223 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=38.76 vs. limit=7.71625
2024-10-08 19:57:07,191 WARNING [optim.py:503] Scaling gradients by 0.06144506484270096, model_norm_threshold=11925.466796875
2024-10-08 19:57:07,346 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.586e+10, grad_sumsq=1.247e+12, orig_rms_sq=1.271e-02
2024-10-08 19:57:16,250 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=576.6666666666666, ans=0.178375
2024-10-08 19:57:31,521 INFO [train.py:1152] Epoch 1, batch 1750, loss[loss=0.7502, ctc_loss=0.9464, attn_decoder_loss=0.7011, over 4959.00 frames. ], tot_loss[loss=0.7978, ctc_loss=1.05, attn_decoder_loss=0.7349, over 967116.99 frames. ], batch size: 19, lr: 4.44e-02,
2024-10-08 19:57:34,660 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=30.49 vs. limit=7.71875
2024-10-08 19:57:51,328 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.52 vs. limit=7.94
2024-10-08 19:57:51,714 WARNING [optim.py:503] Scaling gradients by 0.08227072656154633, model_norm_threshold=11925.466796875
2024-10-08 19:57:51,871 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.357e+09, grad_sumsq=9.429e+09, orig_rms_sq=8.864e-01
2024-10-08 19:57:52,038 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=586.6666666666666, ans=5.366666666666666
2024-10-08 19:57:53,427 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=586.6666666666666, ans=0.4725
2024-10-08 19:57:56,133 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=590.0, ans=0.177875
2024-10-08 19:58:08,579 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=593.3333333333334, ans=0.2089
2024-10-08 19:58:11,523 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=20.01 vs. limit=5.296666666666667
2024-10-08 19:58:12,201 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=593.3333333333334, ans=0.4721875
2024-10-08 19:58:14,494 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.82 vs. limit=7.945
2024-10-08 19:58:15,229 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=87.63 vs. limit=7.7225
2024-10-08 19:58:15,873 WARNING [optim.py:503] Scaling gradients by 0.03219112381339073, model_norm_threshold=11925.466796875
2024-10-08 19:58:16,028 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.875e+10, grad_sumsq=4.258e+10, orig_rms_sq=9.100e-01
2024-10-08 19:58:16,874 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten.whitening_limit, batch_count=593.3333333333334, ans=7.7225
2024-10-08 19:58:23,723 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=596.6666666666666, ans=5.372916666666667
2024-10-08 19:58:30,231 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=38.59 vs. limit=7.9475
2024-10-08 19:58:35,003 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.957e+02 3.158e+03 8.995e+03 2.213e+04 3.705e+05, threshold=1.799e+04, percent-clipped=40.0
2024-10-08 19:58:35,054 INFO [train.py:1152] Epoch 1, batch 1800, loss[loss=0.8146, ctc_loss=1.1, attn_decoder_loss=0.7434, over 4864.00 frames. ], tot_loss[loss=0.7983, ctc_loss=1.049, attn_decoder_loss=0.7355, over 967907.35 frames. ], batch size: 23, lr: 4.44e-02,
2024-10-08 19:58:36,965 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=29.62 vs. limit=7.95
2024-10-08 19:58:42,066 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=4.91 vs. limit=5.3
2024-10-08 19:59:02,004 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=606.6666666666666, ans=0.17725000000000002
2024-10-08 19:59:10,676 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=19.30 vs. limit=7.7275
2024-10-08 19:59:11,766 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.81 vs. limit=5.151666666666666
2024-10-08 19:59:12,808 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=36.53 vs. limit=7.9575
2024-10-08 19:59:16,462 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=6.147e+01
2024-10-08 19:59:22,889 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=610.0, ans=0.2939
2024-10-08 19:59:27,233 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=13.19 vs. limit=7.73
2024-10-08 19:59:31,077 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=20.10 vs. limit=7.73
2024-10-08 19:59:32,150 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=16.48 vs. limit=7.73
2024-10-08 19:59:32,800 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=613.3333333333334, ans=0.17700000000000002
2024-10-08 19:59:38,882 INFO [train.py:1152] Epoch 1, batch 1850, loss[loss=0.772, ctc_loss=1.056, attn_decoder_loss=0.7009, over 4737.00 frames. ], tot_loss[loss=0.7964, ctc_loss=1.048, attn_decoder_loss=0.7335, over 968162.53 frames. ], batch size: 26, lr: 4.43e-02,
2024-10-08 19:59:39,032 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=616.6666666666666, ans=0.20925000000000002
2024-10-08 19:59:41,450 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=616.6666666666666, ans=0.08612500000000001
2024-10-08 19:59:41,919 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=75.94 vs. limit=7.73125
2024-10-08 19:59:48,375 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=27.68 vs. limit=7.73125
2024-10-08 19:59:50,247 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=620.0, ans=0.215125
2024-10-08 19:59:51,549 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=620.0, ans=0.08605
2024-10-08 19:59:55,892 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=24.99 vs. limit=7.965
2024-10-08 19:59:58,595 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=25.46 vs. limit=7.965
2024-10-08 19:59:59,533 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=37.52 vs. limit=7.965
2024-10-08 20:00:06,750 WARNING [optim.py:503] Scaling gradients by 0.09747865051031113, model_norm_threshold=17990.43359375
2024-10-08 20:00:06,905 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.65, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.200e+10, grad_sumsq=1.960e+12, orig_rms_sq=1.122e-02
2024-10-08 20:00:30,559 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=96.49 vs. limit=7.73625
2024-10-08 20:00:31,145 WARNING [optim.py:503] Scaling gradients by 0.005302532576024532, model_norm_threshold=17990.43359375
2024-10-08 20:00:31,300 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.53, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.066e+12, grad_sumsq=6.093e+12, orig_rms_sq=9.955e-01
2024-10-08 20:00:32,117 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.95 vs. limit=7.9725
2024-10-08 20:00:32,153 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.41 vs. limit=5.1575
2024-10-08 20:00:32,555 WARNING [optim.py:503] Scaling gradients by 0.0823824554681778, model_norm_threshold=17990.43359375
2024-10-08 20:00:32,713 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.319e+10, grad_sumsq=2.174e+12, orig_rms_sq=1.067e-02
2024-10-08 20:00:39,074 WARNING [optim.py:503] Scaling gradients by 0.05068554729223251, model_norm_threshold=17990.43359375
2024-10-08 20:00:39,236 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.71, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.989e+10, grad_sumsq=8.489e+12, orig_rms_sq=1.059e-02
2024-10-08 20:00:41,726 WARNING [optim.py:503] Scaling gradients by 0.012236496433615685, model_norm_threshold=17990.43359375
2024-10-08 20:00:41,893 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.532e+11, grad_sumsq=9.002e+13, orig_rms_sq=1.059e-02
2024-10-08 20:00:42,130 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=633.3333333333334, ans=0.09604166666666668
2024-10-08 20:00:43,121 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.106e+03 8.303e+03 1.896e+04 4.594e+04 3.393e+06, threshold=3.791e+04, percent-clipped=52.0
2024-10-08 20:00:43,176 INFO [train.py:1152] Epoch 1, batch 1900, loss[loss=0.8283, ctc_loss=1.119, attn_decoder_loss=0.7557, over 4773.00 frames. ], tot_loss[loss=0.7982, ctc_loss=1.053, attn_decoder_loss=0.7347, over 967894.55 frames. ], batch size: 29, lr: 4.43e-02,
2024-10-08 20:00:45,567 WARNING [optim.py:503] Scaling gradients by 0.042901769280433655, model_norm_threshold=37913.41796875
2024-10-08 20:00:45,726 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.902e+11, grad_sumsq=2.765e+13, orig_rms_sq=1.050e-02
2024-10-08 20:00:47,716 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.93 vs. limit=5.158333333333333
2024-10-08 20:00:48,312 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=633.3333333333334, ans=0.4703125
2024-10-08 20:00:48,391 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=6.227e+01
2024-10-08 20:00:50,865 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=633.3333333333334, ans=0.29366666666666663
2024-10-08 20:00:55,775 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.min_abs, batch_count=636.6666666666666, ans=0.20955000000000001
2024-10-08 20:01:00,980 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=636.6666666666666, ans=0.47015625
2024-10-08 20:01:10,524 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=20.18 vs. limit=7.74
2024-10-08 20:01:17,338 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=640.0, ans=0.42
2024-10-08 20:01:29,938 WARNING [optim.py:503] Scaling gradients by 0.039662860333919525, model_norm_threshold=37913.41796875
2024-10-08 20:01:30,093 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.834e+11, grad_sumsq=2.563e+13, orig_rms_sq=1.106e-02
2024-10-08 20:01:31,847 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=79.48 vs. limit=5.321666666666666
2024-10-08 20:01:31,982 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=34.65 vs. limit=7.74125
2024-10-08 20:01:33,991 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=646.6666666666666, ans=0.4696875
2024-10-08 20:01:46,246 INFO [train.py:1152] Epoch 1, batch 1950, loss[loss=0.759, ctc_loss=0.9868, attn_decoder_loss=0.7021, over 4863.00 frames. ], tot_loss[loss=0.7996, ctc_loss=1.055, attn_decoder_loss=0.7356, over 966894.31 frames. ], batch size: 20, lr: 4.43e-02,
2024-10-08 20:01:51,483 WARNING [optim.py:503] Scaling gradients by 0.06990321725606918, model_norm_threshold=37913.41796875
2024-10-08 20:01:51,637 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.597e+11, grad_sumsq=1.442e+13, orig_rms_sq=1.108e-02
2024-10-08 20:01:53,779 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=21.91 vs. limit=7.9875
2024-10-08 20:01:59,932 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=25.33 vs. limit=7.99
2024-10-08 20:02:07,650 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.41 vs. limit=7.745
2024-10-08 20:02:10,167 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=27.13 vs. limit=7.99
2024-10-08 20:02:11,266 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=35.76 vs. limit=7.745
2024-10-08 20:02:17,012 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=656.6666666666666, ans=0.46921875
2024-10-08 20:02:18,412 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.max_abs, batch_count=656.6666666666666, ans=5.410416666666666
2024-10-08 20:02:19,376 WARNING [optim.py:503] Scaling gradients by 0.037203606218099594, model_norm_threshold=37913.41796875
2024-10-08 20:02:19,531 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.837e+11, grad_sumsq=4.330e+13, orig_rms_sq=4.241e-03
2024-10-08 20:02:21,474 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.68 vs. limit=7.74625
2024-10-08 20:02:28,407 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=660.0, ans=0.4690625
2024-10-08 20:02:35,201 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.57 vs. limit=7.995
2024-10-08 20:02:35,785 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=660.0, ans=0.2934
2024-10-08 20:02:39,794 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=663.3333333333334, ans=0.41708333333333336
2024-10-08 20:02:44,129 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=22.22 vs. limit=7.74875
2024-10-08 20:02:48,280 WARNING [optim.py:503] Scaling gradients by 0.09666372090578079, model_norm_threshold=37913.41796875
2024-10-08 20:02:48,436 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.031e+10, grad_sumsq=3.829e+12, orig_rms_sq=1.053e-02
2024-10-08 20:02:49,684 WARNING [optim.py:503] Scaling gradients by 0.01314250286668539, model_norm_threshold=37913.41796875
2024-10-08 20:02:49,841 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.630e+12, grad_sumsq=2.498e+14, orig_rms_sq=1.053e-02
2024-10-08 20:02:51,662 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.215e+03 1.207e+04 3.415e+04 1.332e+05 2.885e+06, threshold=6.830e+04, percent-clipped=50.0
2024-10-08 20:02:51,714 INFO [train.py:1152] Epoch 1, batch 2000, loss[loss=0.7301, ctc_loss=0.9434, attn_decoder_loss=0.6768, over 4959.00 frames. ], tot_loss[loss=0.7994, ctc_loss=1.056, attn_decoder_loss=0.7352, over 966732.32 frames. ], batch size: 19, lr: 4.42e-02,
2024-10-08 20:02:52,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.99 vs. limit=7.75
2024-10-08 20:02:59,293 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=666.6666666666666, ans=0.17500000000000002
2024-10-08 20:03:02,559 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=12.12 vs. limit=7.75
2024-10-08 20:03:08,611 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.42 vs. limit=5.1675
2024-10-08 20:03:15,659 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.min_positive, batch_count=673.3333333333334, ans=0.04789583333333334
2024-10-08 20:03:28,148 WARNING [optim.py:503] Scaling gradients by 0.01753932796418667, model_norm_threshold=68301.0859375
2024-10-08 20:03:28,303 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.311e+12, grad_sumsq=9.531e+13, orig_rms_sq=5.573e-02
2024-10-08 20:03:29,897 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=676.6666666666666, ans=0.084775
2024-10-08 20:03:33,737 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=676.6666666666666, ans=0.8763166666666667
2024-10-08 20:03:36,138 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=676.6666666666666, ans=0.46828125
2024-10-08 20:03:42,200 WARNING [optim.py:503] Scaling gradients by 0.0971062034368515, model_norm_threshold=68301.0859375
2024-10-08 20:03:42,355 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.989e+11, grad_sumsq=1.773e+11, orig_rms_sq=1.122e+00
2024-10-08 20:03:44,972 WARNING [optim.py:503] Scaling gradients by 0.060293085873126984, model_norm_threshold=68301.0859375
2024-10-08 20:03:45,133 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.532e+11, grad_sumsq=4.041e+11, orig_rms_sq=1.122e+00
2024-10-08 20:03:45,693 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=35.71 vs. limit=8.01
2024-10-08 20:03:54,910 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=21.76 vs. limit=7.75625
2024-10-08 20:03:55,451 INFO [train.py:1152] Epoch 1, batch 2050, loss[loss=0.7207, ctc_loss=0.9504, attn_decoder_loss=0.6633, over 4914.00 frames. ], tot_loss[loss=0.798, ctc_loss=1.053, attn_decoder_loss=0.7341, over 967008.64 frames. ], batch size: 19, lr: 4.42e-02,
2024-10-08 20:04:03,726 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.39 vs. limit=7.75625
2024-10-08 20:04:06,712 WARNING [optim.py:503] Scaling gradients by 0.010728046298027039, model_norm_threshold=68301.0859375
2024-10-08 20:04:06,869 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.852e+13, grad_sumsq=1.765e+15, orig_rms_sq=1.049e-02
2024-10-08 20:04:07,737 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.26 vs. limit=8.015
2024-10-08 20:04:08,690 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.93 vs. limit=5.171666666666667
2024-10-08 20:04:09,281 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.39 vs. limit=5.171666666666667
2024-10-08 20:04:10,644 WARNING [optim.py:503] Scaling gradients by 0.04311726242303848, model_norm_threshold=68301.0859375
2024-10-08 20:04:10,798 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.71, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.783e+12, grad_sumsq=1.708e+14, orig_rms_sq=1.044e-02
2024-10-08 20:04:16,001 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=686.6666666666666, ans=0.211375
2024-10-08 20:04:18,275 WARNING [optim.py:503] Scaling gradients by 0.017326556146144867, model_norm_threshold=68301.0859375
2024-10-08 20:04:18,430 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.78, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.216e+13, grad_sumsq=1.211e+15, orig_rms_sq=1.004e-02
2024-10-08 20:04:19,318 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=18.15 vs. limit=7.7575
2024-10-08 20:04:20,513 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=20.47 vs. limit=7.75875
2024-10-08 20:04:21,718 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=14.82 vs. limit=7.75875
2024-10-08 20:04:24,759 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=690.0, ans=0.08447500000000001
2024-10-08 20:04:25,423 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=48.74 vs. limit=7.75875
2024-10-08 20:04:31,251 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=690.0, ans=0.047843750000000004
2024-10-08 20:04:37,789 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=42.90 vs. limit=7.76
2024-10-08 20:04:40,844 WARNING [optim.py:503] Scaling gradients by 0.022969182580709457, model_norm_threshold=68301.0859375
2024-10-08 20:04:40,998 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.77, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.851e+12, grad_sumsq=6.565e+14, orig_rms_sq=1.043e-02
2024-10-08 20:04:42,217 WARNING [optim.py:503] Scaling gradients by 0.03618744760751724, model_norm_threshold=68301.0859375
2024-10-08 20:04:42,372 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.780e+11, grad_sumsq=8.414e+13, orig_rms_sq=1.043e-02
2024-10-08 20:04:42,881 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=16.21 vs. limit=4.277333333333333
2024-10-08 20:04:45,545 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=16.00 vs. limit=7.76125
2024-10-08 20:04:48,037 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=37.91 vs. limit=7.76125
2024-10-08 20:04:49,495 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.66 vs. limit=5.348333333333334
2024-10-08 20:04:51,587 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer_na.min_abs, batch_count=696.6666666666666, ans=0.0067866666666666665
2024-10-08 20:04:53,370 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.49 vs. limit=5.174166666666666
2024-10-08 20:04:54,065 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=696.6666666666666, ans=0.46734375
2024-10-08 20:04:59,052 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.461e+03 1.752e+04 5.303e+04 1.882e+05 6.367e+06, threshold=1.061e+05, percent-clipped=38.0
2024-10-08 20:04:59,103 INFO [train.py:1152] Epoch 1, batch 2100, loss[loss=0.8151, ctc_loss=1.076, attn_decoder_loss=0.7498, over 4841.00 frames. ], tot_loss[loss=0.7952, ctc_loss=1.049, attn_decoder_loss=0.7317, over 967171.22 frames. ], batch size: 21, lr: 4.42e-02,
2024-10-08 20:05:01,735 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=700.0, ans=0.4671875
2024-10-08 20:05:02,459 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.83 vs. limit=7.7625
2024-10-08 20:05:06,501 WARNING [optim.py:503] Scaling gradients by 0.012950514443218708, model_norm_threshold=106060.2734375
2024-10-08 20:05:06,658 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.947e+13, grad_sumsq=1.781e+15, orig_rms_sq=1.093e-02
2024-10-08 20:05:07,510 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.57 vs. limit=7.7625
2024-10-08 20:05:07,580 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.00 vs. limit=8.025
2024-10-08 20:05:10,225 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.46 vs. limit=5.35
2024-10-08 20:05:17,169 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=703.3333333333334, ans=0.173625
2024-10-08 20:05:22,169 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=703.3333333333334, ans=0.047802083333333335
2024-10-08 20:05:29,619 WARNING [optim.py:503] Scaling gradients by 0.08906951546669006, model_norm_threshold=106060.2734375
2024-10-08 20:05:29,774 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.66, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.405e+11, grad_sumsq=8.625e+13, orig_rms_sq=1.090e-02
2024-10-08 20:05:35,160 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=706.6666666666666, ans=0.17350000000000002
2024-10-08 20:05:42,626 WARNING [optim.py:503] Scaling gradients by 0.0074577778577804565, model_norm_threshold=106060.2734375
2024-10-08 20:05:42,781 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.755e+13, grad_sumsq=5.556e+15, orig_rms_sq=1.036e-02
2024-10-08 20:05:54,934 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=713.3333333333334, ans=0.29286666666666666
2024-10-08 20:06:02,892 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.58 vs. limit=7.76875
2024-10-08 20:06:03,481 INFO [train.py:1152] Epoch 1, batch 2150, loss[loss=0.7385, ctc_loss=1.026, attn_decoder_loss=0.6667, over 4862.00 frames. ], tot_loss[loss=0.7939, ctc_loss=1.048, attn_decoder_loss=0.7303, over 968006.36 frames. ], batch size: 20, lr: 4.41e-02,
2024-10-08 20:06:14,467 WARNING [optim.py:503] Scaling gradients by 0.06090482324361801, model_norm_threshold=106060.2734375
2024-10-08 20:06:14,624 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.934e+11, grad_sumsq=9.504e+13, orig_rms_sq=9.400e-03
2024-10-08 20:06:15,527 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.66 vs. limit=4.288
2024-10-08 20:06:25,465 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=2.87 vs. limit=4.288
2024-10-08 20:06:42,505 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=726.6666666666666, ans=0.4659375
2024-10-08 20:06:52,357 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=42.78 vs. limit=7.7725
2024-10-08 20:06:54,103 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=730.0, ans=0.09543750000000001
2024-10-08 20:06:57,398 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.39 vs. limit=3.1095
2024-10-08 20:07:00,255 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=730.0, ans=0.40875
2024-10-08 20:07:04,858 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.52 vs. limit=5.1825
2024-10-08 20:07:06,795 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.722e+03 2.838e+04 8.049e+04 2.211e+05 1.422e+07, threshold=1.610e+05, percent-clipped=42.0
2024-10-08 20:07:06,846 INFO [train.py:1152] Epoch 1, batch 2200, loss[loss=0.811, ctc_loss=1.089, attn_decoder_loss=0.7415, over 4741.00 frames. ], tot_loss[loss=0.7922, ctc_loss=1.048, attn_decoder_loss=0.7282, over 967775.49 frames. ], batch size: 26, lr: 4.41e-02,
2024-10-08 20:07:13,324 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=733.3333333333334, ans=0.17250000000000001
2024-10-08 20:07:14,591 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=733.3333333333334, ans=0.0835
2024-10-08 20:07:16,980 WARNING [optim.py:503] Scaling gradients by 0.03477410972118378, model_norm_threshold=160987.8125
2024-10-08 20:07:17,136 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.063e+13, grad_sumsq=1.112e+15, orig_rms_sq=9.565e-03
2024-10-08 20:07:17,705 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=41.81 vs. limit=7.775
2024-10-08 20:07:17,812 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.95 vs. limit=5.183333333333334
2024-10-08 20:07:18,073 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=7.80 vs. limit=7.775
2024-10-08 20:07:18,457 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=736.6666666666666, ans=0.8742166666666666
2024-10-08 20:07:23,196 WARNING [optim.py:503] Scaling gradients by 0.022960709407925606, model_norm_threshold=160987.8125
2024-10-08 20:07:23,350 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.918e+13, grad_sumsq=2.043e+15, orig_rms_sq=9.385e-03
2024-10-08 20:07:40,017 WARNING [optim.py:503] Scaling gradients by 0.09475109726190567, model_norm_threshold=160987.8125
2024-10-08 20:07:40,173 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.533e+11, grad_sumsq=8.794e+11, orig_rms_sq=1.084e+00
2024-10-08 20:07:43,631 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.92 vs. limit=7.7775
2024-10-08 20:07:47,651 WARNING [optim.py:503] Scaling gradients by 0.08406873792409897, model_norm_threshold=160987.8125
2024-10-08 20:07:47,807 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.109e+12, grad_sumsq=1.054e+12, orig_rms_sq=1.052e+00
2024-10-08 20:07:49,270 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=743.3333333333334, ans=0.46515625
2024-10-08 20:07:59,283 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=746.6666666666666, ans=0.46499999999999997
2024-10-08 20:08:10,416 INFO [train.py:1152] Epoch 1, batch 2250, loss[loss=0.8482, ctc_loss=1.101, attn_decoder_loss=0.7849, over 4870.00 frames. ], tot_loss[loss=0.7915, ctc_loss=1.048, attn_decoder_loss=0.7275, over 967682.50 frames. ], batch size: 22, lr: 4.40e-02,
2024-10-08 20:08:11,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=750.0, ans=0.46484375
2024-10-08 20:08:19,965 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=25.61 vs. limit=8.0625
2024-10-08 20:08:24,955 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.15 vs. limit=5.1883333333333335
2024-10-08 20:08:29,114 WARNING [optim.py:503] Scaling gradients by 0.09665202349424362, model_norm_threshold=160987.8125
2024-10-08 20:08:29,271 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.359e+12, grad_sumsq=1.399e+14, orig_rms_sq=9.716e-03
2024-10-08 20:08:31,249 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.26 vs. limit=3.113
2024-10-08 20:08:33,665 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=26.19 vs. limit=7.7825
2024-10-08 20:08:37,988 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=756.6666666666666, ans=0.46453125
2024-10-08 20:08:41,755 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=756.6666666666666, ans=0.46453125
2024-10-08 20:08:42,448 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=14.46 vs. limit=7.78375
2024-10-08 20:08:49,237 WARNING [optim.py:503] Scaling gradients by 0.04036618769168854, model_norm_threshold=160987.8125
2024-10-08 20:08:49,395 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.116e+12, grad_sumsq=4.392e+12, orig_rms_sq=1.165e+00
2024-10-08 20:09:05,529 WARNING [optim.py:503] Scaling gradients by 0.042160749435424805, model_norm_threshold=160987.8125
2024-10-08 20:09:05,684 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.367e+12, grad_sumsq=4.409e+14, orig_rms_sq=9.905e-03
2024-10-08 20:09:09,054 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.63 vs. limit=3.1145
2024-10-08 20:09:10,053 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.90 vs. limit=5.381666666666667
2024-10-08 20:09:10,885 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=763.3333333333334, ans=0.46421875
2024-10-08 20:09:13,119 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.882e+03 4.715e+04 1.376e+05 3.814e+05 7.011e+06, threshold=2.752e+05, percent-clipped=44.0
2024-10-08 20:09:13,174 INFO [train.py:1152] Epoch 1, batch 2300, loss[loss=0.8196, ctc_loss=1.06, attn_decoder_loss=0.7596, over 4883.00 frames. ], tot_loss[loss=0.7889, ctc_loss=1.045, attn_decoder_loss=0.725, over 968174.42 frames. ], batch size: 19, lr: 4.40e-02,
2024-10-08 20:09:13,369 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=766.6666666666666, ans=5.479166666666667
2024-10-08 20:09:19,484 WARNING [optim.py:503] Scaling gradients by 0.07638275623321533, model_norm_threshold=275237.875
2024-10-08 20:09:19,638 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.692e+12, grad_sumsq=4.995e+12, orig_rms_sq=1.139e+00
2024-10-08 20:09:21,554 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=22.07 vs. limit=7.7875
2024-10-08 20:09:23,664 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=766.6666666666666, ans=0.4640625
2024-10-08 20:09:25,739 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.81 vs. limit=8.075
2024-10-08 20:09:27,920 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=31.80 vs. limit=7.78875
2024-10-08 20:09:28,528 WARNING [optim.py:503] Scaling gradients by 0.021564986556768417, model_norm_threshold=275237.875
2024-10-08 20:09:28,684 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.57, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.326e+13, grad_sumsq=8.244e+13, orig_rms_sq=1.131e+00
2024-10-08 20:09:31,127 WARNING [optim.py:503] Scaling gradients by 0.08511112630367279, model_norm_threshold=275237.875
2024-10-08 20:09:31,284 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.409e+12, grad_sumsq=4.594e+14, orig_rms_sq=9.599e-03
2024-10-08 20:09:32,487 WARNING [optim.py:503] Scaling gradients by 0.09832505136728287, model_norm_threshold=275237.875
2024-10-08 20:09:32,641 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.53, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.119e+12, grad_sumsq=4.291e+14, orig_rms_sq=9.599e-03
2024-10-08 20:09:36,181 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=8.12 vs. limit=7.78875
2024-10-08 20:09:40,156 WARNING [optim.py:503] Scaling gradients by 0.004678403493016958, model_norm_threshold=275237.875
2024-10-08 20:09:40,313 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.337e+14, grad_sumsq=6.536e+16, orig_rms_sq=1.123e-02
2024-10-08 20:09:41,779 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=773.3333333333334, ans=0.46375
2024-10-08 20:09:51,709 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=776.6666666666666, ans=0.170875
2024-10-08 20:09:56,152 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.15 vs. limit=5.388333333333334
2024-10-08 20:10:00,351 WARNING [optim.py:503] Scaling gradients by 0.046235401183366776, model_norm_threshold=275237.875
2024-10-08 20:10:00,508 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.738e+12, grad_sumsq=5.358e+12, orig_rms_sq=1.071e+00
2024-10-08 20:10:05,369 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.97 vs. limit=8.085
2024-10-08 20:10:09,089 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.03 vs. limit=5.195
2024-10-08 20:10:14,603 WARNING [optim.py:503] Scaling gradients by 0.04197409376502037, model_norm_threshold=275237.875
2024-10-08 20:10:14,757 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.837e+13, grad_sumsq=1.664e+13, orig_rms_sq=1.104e+00
2024-10-08 20:10:16,028 WARNING [optim.py:503] Scaling gradients by 0.026626769453287125, model_norm_threshold=275237.875
2024-10-08 20:10:16,185 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.900e+13, grad_sumsq=3.052e+15, orig_rms_sq=9.501e-03
2024-10-08 20:10:18,794 INFO [train.py:1152] Epoch 1, batch 2350, loss[loss=0.7685, ctc_loss=1.062, attn_decoder_loss=0.695, over 4865.00 frames. ], tot_loss[loss=0.7881, ctc_loss=1.045, attn_decoder_loss=0.7239, over 968126.48 frames. ], batch size: 23, lr: 4.40e-02,
2024-10-08 20:10:18,871 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=1.151e+04
2024-10-08 20:10:22,558 WARNING [optim.py:503] Scaling gradients by 0.028097346425056458, model_norm_threshold=275237.875
2024-10-08 20:10:22,713 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.932e+13, grad_sumsq=1.764e+13, orig_rms_sq=1.095e+00
2024-10-08 20:10:25,249 WARNING [optim.py:503] Scaling gradients by 0.015563376247882843, model_norm_threshold=275237.875
2024-10-08 20:10:25,408 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.024e+13, grad_sumsq=4.052e+15, orig_rms_sq=1.240e-02
2024-10-08 20:10:25,667 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=783.3333333333334, ans=0.2059375
2024-10-08 20:10:27,437 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=71.65 vs. limit=7.79375
2024-10-08 20:10:31,021 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=14.36 vs. limit=5.3933333333333335
2024-10-08 20:10:34,161 WARNING [optim.py:503] Scaling gradients by 0.06330667436122894, model_norm_threshold=275237.875
2024-10-08 20:10:34,316 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.61, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.151e+13, grad_sumsq=1.220e+15, orig_rms_sq=9.437e-03
2024-10-08 20:10:35,783 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=786.6666666666666, ans=0.1705
2024-10-08 20:10:37,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.01 vs. limit=3.118
2024-10-08 20:10:44,002 WARNING [optim.py:503] Scaling gradients by 0.005759929306805134, model_norm_threshold=275237.875
2024-10-08 20:10:44,158 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.52, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.197e+15, grad_sumsq=1.243e+17, orig_rms_sq=9.631e-03
2024-10-08 20:10:55,785 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=793.3333333333334, ans=0.8722333333333333
2024-10-08 20:10:59,441 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=793.3333333333334, ans=0.4628125
2024-10-08 20:11:00,758 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=5.180e+00
2024-10-08 20:11:01,347 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=29.85 vs. limit=7.7975
2024-10-08 20:11:06,438 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.35 vs. limit=5.198333333333333
2024-10-08 20:11:06,640 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.86 vs. limit=8.095
2024-10-08 20:11:14,631 WARNING [optim.py:503] Scaling gradients by 0.04655389487743378, model_norm_threshold=275237.875
2024-10-08 20:11:14,794 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.016e+12, grad_sumsq=6.787e+12, orig_rms_sq=1.181e+00
2024-10-08 20:11:17,227 WARNING [optim.py:503] Scaling gradients by 0.036730024963617325, model_norm_threshold=275237.875
2024-10-08 20:11:17,401 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.415e+13, grad_sumsq=2.504e+15, orig_rms_sq=9.643e-03
2024-10-08 20:11:19,810 WARNING [optim.py:503] Scaling gradients by 0.02584824338555336, model_norm_threshold=275237.875
2024-10-08 20:11:19,971 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.590e+13, grad_sumsq=3.066e+13, orig_rms_sq=1.171e+00
2024-10-08 20:11:22,876 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.574e+03 1.099e+05 2.570e+05 8.529e+05 5.883e+07, threshold=5.140e+05, percent-clipped=48.0
2024-10-08 20:11:22,927 INFO [train.py:1152] Epoch 1, batch 2400, loss[loss=0.8701, ctc_loss=1.121, attn_decoder_loss=0.8075, over 4749.00 frames. ], tot_loss[loss=0.7885, ctc_loss=1.046, attn_decoder_loss=0.7242, over 967387.13 frames. ], batch size: 19, lr: 4.39e-02,
2024-10-08 20:11:23,569 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.67 vs. limit=5.4
2024-10-08 20:11:25,242 WARNING [optim.py:503] Scaling gradients by 0.09004655480384827, model_norm_threshold=514023.21875
2024-10-08 20:11:25,401 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.199e+13, grad_sumsq=1.001e+13, orig_rms_sq=1.198e+00
2024-10-08 20:11:27,345 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=25.32 vs. limit=8.1
2024-10-08 20:11:35,916 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=26.11 vs. limit=7.80125
2024-10-08 20:11:39,737 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.62 vs. limit=7.80125
2024-10-08 20:11:41,976 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.22 vs. limit=5.200833333333334
2024-10-08 20:11:51,429 WARNING [optim.py:503] Scaling gradients by 0.004043670371174812, model_norm_threshold=514023.21875
2024-10-08 20:11:51,586 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.019e+15, grad_sumsq=4.065e+17, orig_rms_sq=9.887e-03
2024-10-08 20:11:52,327 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.04 vs. limit=4.322666666666667
2024-10-08 20:11:53,676 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.27 vs. limit=8.105
2024-10-08 20:11:54,074 WARNING [optim.py:503] Scaling gradients by 0.06187678128480911, model_norm_threshold=514023.21875
2024-10-08 20:11:54,230 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.546e+13, grad_sumsq=1.263e+13, orig_rms_sq=1.224e+00
2024-10-08 20:11:56,292 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.28 vs. limit=5.403333333333333
2024-10-08 20:11:56,317 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=22.16 vs. limit=7.8025
2024-10-08 20:11:56,741 WARNING [optim.py:503] Scaling gradients by 0.08315692842006683, model_norm_threshold=514023.21875
2024-10-08 20:11:56,896 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.67, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.543e+13, grad_sumsq=2.584e+15, orig_rms_sq=9.842e-03
2024-10-08 20:11:57,789 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=26.67 vs. limit=8.105
2024-10-08 20:12:00,734 WARNING [optim.py:503] Scaling gradients by 0.0029653480742126703, model_norm_threshold=514023.21875
2024-10-08 20:12:00,889 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.685e+15, grad_sumsq=6.301e+15, orig_rms_sq=1.220e+00
2024-10-08 20:12:04,703 WARNING [optim.py:503] Scaling gradients by 0.0022884770296514034, model_norm_threshold=514023.21875
2024-10-08 20:12:04,862 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.671e+16, grad_sumsq=1.721e+18, orig_rms_sq=9.710e-03
2024-10-08 20:12:09,810 WARNING [optim.py:503] Scaling gradients by 0.03301252797245979, model_norm_threshold=514023.21875
2024-10-08 20:12:09,966 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.430e+13, grad_sumsq=4.310e+13, orig_rms_sq=1.260e+00
2024-10-08 20:12:10,973 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=7.49 vs. limit=7.80375
2024-10-08 20:12:12,149 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.71 vs. limit=7.80375
2024-10-08 20:12:21,898 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=14.99 vs. limit=7.805
2024-10-08 20:12:22,297 WARNING [optim.py:503] Scaling gradients by 0.0207511056214571, model_norm_threshold=514023.21875
2024-10-08 20:12:22,455 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.524e+14, grad_sumsq=2.512e+16, orig_rms_sq=1.005e-02
2024-10-08 20:12:26,103 WARNING [optim.py:503] Scaling gradients by 0.030663490295410156, model_norm_threshold=514023.21875
2024-10-08 20:12:26,260 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.313e+13, grad_sumsq=8.238e+15, orig_rms_sq=1.009e-02
2024-10-08 20:12:26,318 INFO [train.py:1152] Epoch 1, batch 2450, loss[loss=0.7614, ctc_loss=1.027, attn_decoder_loss=0.6951, over 4865.00 frames. ], tot_loss[loss=0.7895, ctc_loss=1.047, attn_decoder_loss=0.725, over 966635.17 frames. ], batch size: 22, lr: 4.39e-02,
2024-10-08 20:12:27,445 WARNING [optim.py:503] Scaling gradients by 0.013505177572369576, model_norm_threshold=514023.21875
2024-10-08 20:12:27,602 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.285e+14, grad_sumsq=3.378e+14, orig_rms_sq=1.269e+00
2024-10-08 20:12:31,387 WARNING [optim.py:503] Scaling gradients by 0.060987573117017746, model_norm_threshold=514023.21875
2024-10-08 20:12:31,571 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.558e+13, grad_sumsq=2.481e+15, orig_rms_sq=1.031e-02
2024-10-08 20:12:40,233 WARNING [optim.py:503] Scaling gradients by 0.015023037791252136, model_norm_threshold=514023.21875
2024-10-08 20:12:40,388 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.820e+14, grad_sumsq=5.493e+16, orig_rms_sq=1.060e-02
2024-10-08 20:12:41,162 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=53.02 vs. limit=8.115
2024-10-08 20:12:42,262 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=14.40 vs. limit=7.8075
2024-10-08 20:12:42,829 WARNING [optim.py:503] Scaling gradients by 0.02563915029168129, model_norm_threshold=514023.21875
2024-10-08 20:12:42,986 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.037e+14, grad_sumsq=1.922e+16, orig_rms_sq=1.060e-02
2024-10-08 20:12:43,863 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.11 vs. limit=5.41
2024-10-08 20:12:46,305 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.24 vs. limit=8.115
2024-10-08 20:12:47,444 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.29 vs. limit=7.8075
2024-10-08 20:12:47,524 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=15.62 vs. limit=7.8075
2024-10-08 20:12:52,539 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=5.92 vs. limit=7.80875
2024-10-08 20:12:55,772 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=823.3333333333334, ans=0.39708333333333334
2024-10-08 20:12:56,895 WARNING [optim.py:503] Scaling gradients by 0.05447644367814064, model_norm_threshold=514023.21875
2024-10-08 20:12:57,052 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.56, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.983e+13, grad_sumsq=4.909e+15, orig_rms_sq=1.015e-02
2024-10-08 20:12:58,312 WARNING [optim.py:503] Scaling gradients by 0.011700502596795559, model_norm_threshold=514023.21875
2024-10-08 20:12:58,469 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.158e+14, grad_sumsq=3.305e+14, orig_rms_sq=1.258e+00
2024-10-08 20:12:59,410 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.93 vs. limit=8.1175
2024-10-08 20:13:01,378 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=40.59 vs. limit=8.1175
2024-10-08 20:13:03,321 WARNING [optim.py:503] Scaling gradients by 0.06523184478282928, model_norm_threshold=514023.21875
2024-10-08 20:13:03,477 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.767e+13, grad_sumsq=2.766e+15, orig_rms_sq=1.001e-02
2024-10-08 20:13:04,452 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=18.39 vs. limit=8.12
2024-10-08 20:13:07,295 WARNING [optim.py:503] Scaling gradients by 0.024633407592773438, model_norm_threshold=514023.21875
2024-10-08 20:13:07,451 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.661e+14, grad_sumsq=1.706e+16, orig_rms_sq=9.734e-03
2024-10-08 20:13:12,367 WARNING [optim.py:503] Scaling gradients by 0.05907752737402916, model_norm_threshold=514023.21875
2024-10-08 20:13:12,522 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.810e+13, grad_sumsq=2.907e+15, orig_rms_sq=9.667e-03
2024-10-08 20:13:27,767 WARNING [optim.py:503] Scaling gradients by 0.05908309668302536, model_norm_threshold=514023.21875
2024-10-08 20:13:27,924 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.090e+13, grad_sumsq=3.157e+15, orig_rms_sq=9.789e-03
2024-10-08 20:13:29,133 WARNING [optim.py:503] Scaling gradients by 0.06451588124036789, model_norm_threshold=514023.21875
2024-10-08 20:13:29,292 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.284e+13, grad_sumsq=1.000e+13, orig_rms_sq=1.283e+00
2024-10-08 20:13:30,549 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.136e+04 2.843e+05 7.941e+05 3.616e+06 2.246e+08, threshold=1.588e+06, percent-clipped=62.0
2024-10-08 20:13:30,600 INFO [train.py:1152] Epoch 1, batch 2500, loss[loss=0.8171, ctc_loss=1.103, attn_decoder_loss=0.7457, over 4740.00 frames. ], tot_loss[loss=0.789, ctc_loss=1.047, attn_decoder_loss=0.7245, over 966280.93 frames. ], batch size: 26, lr: 4.38e-02,
2024-10-08 20:13:40,576 WARNING [optim.py:503] Scaling gradients by 0.008660889230668545, model_norm_threshold=1588236.375
2024-10-08 20:13:40,731 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.165e+16, grad_sumsq=9.644e+15, orig_rms_sq=1.208e+00
2024-10-08 20:13:46,141 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=836.6666666666666, ans=0.46078125000000003
2024-10-08 20:13:48,591 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=836.6666666666666, ans=0.2029375
2024-10-08 20:13:52,206 WARNING [optim.py:503] Scaling gradients by 0.02391713857650757, model_norm_threshold=1588236.375
2024-10-08 20:13:52,362 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.868e+15, grad_sumsq=1.868e+17, orig_rms_sq=1.000e-02
2024-10-08 20:13:55,784 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.51 vs. limit=8.13
2024-10-08 20:13:58,923 WARNING [optim.py:503] Scaling gradients by 0.07978145033121109, model_norm_threshold=1588236.375
2024-10-08 20:13:59,079 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.57, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.249e+14, grad_sumsq=2.262e+16, orig_rms_sq=9.944e-03
2024-10-08 20:14:05,929 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=76.77 vs. limit=5.42
2024-10-08 20:14:06,066 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=20.10 vs. limit=7.815
2024-10-08 20:14:14,411 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=843.3333333333334, ans=0.168375
2024-10-08 20:14:23,578 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=846.6666666666666, ans=0.4603125
2024-10-08 20:14:29,231 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.03 vs. limit=5.211666666666667
2024-10-08 20:14:33,124 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=14.37 vs. limit=7.8175
2024-10-08 20:14:34,963 INFO [train.py:1152] Epoch 1, batch 2550, loss[loss=0.711, ctc_loss=0.9388, attn_decoder_loss=0.654, over 4959.00 frames. ], tot_loss[loss=0.7891, ctc_loss=1.047, attn_decoder_loss=0.7247, over 966882.73 frames. ], batch size: 19, lr: 4.38e-02,
2024-10-08 20:14:42,395 WARNING [optim.py:503] Scaling gradients by 0.04486759752035141, model_norm_threshold=1588236.375
2024-10-08 20:14:42,551 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.866e+14, grad_sumsq=2.996e+14, orig_rms_sq=1.291e+00
2024-10-08 20:14:48,447 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=15.15 vs. limit=5.426666666666667
2024-10-08 20:14:52,653 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=853.3333333333334, ans=0.46
2024-10-08 20:15:00,069 WARNING [optim.py:503] Scaling gradients by 0.01643660105764866, model_norm_threshold=1588236.375
2024-10-08 20:15:00,226 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.621e+15, grad_sumsq=3.708e+15, orig_rms_sq=1.246e+00
2024-10-08 20:15:01,776 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer_ff3.min_abs, batch_count=856.6666666666666, ans=0.042833333333333334
2024-10-08 20:15:04,103 WARNING [optim.py:503] Scaling gradients by 0.08842843025922775, model_norm_threshold=1588236.375
2024-10-08 20:15:04,260 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.084e+14, grad_sumsq=8.773e+13, orig_rms_sq=1.236e+00
2024-10-08 20:15:07,951 WARNING [optim.py:503] Scaling gradients by 0.051064860075712204, model_norm_threshold=1588236.375
2024-10-08 20:15:08,107 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.984e+14, grad_sumsq=2.405e+14, orig_rms_sq=1.241e+00
2024-10-08 20:15:12,560 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.89 vs. limit=5.215
2024-10-08 20:15:16,853 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.38 vs. limit=5.215
2024-10-08 20:15:18,466 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=860.0, ans=0.16775
2024-10-08 20:15:27,020 WARNING [optim.py:503] Scaling gradients by 0.05022437497973442, model_norm_threshold=1588236.375
2024-10-08 20:15:27,175 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.230e+14, grad_sumsq=3.512e+14, orig_rms_sq=1.205e+00
2024-10-08 20:15:28,420 WARNING [optim.py:503] Scaling gradients by 0.09789155423641205, model_norm_threshold=1588236.375
2024-10-08 20:15:28,576 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.169e+14, grad_sumsq=9.578e+13, orig_rms_sq=1.221e+00
2024-10-08 20:15:29,044 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=37.16 vs. limit=8.1475
2024-10-08 20:15:35,201 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=48.12 vs. limit=5.431666666666667
2024-10-08 20:15:38,886 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.961e+04 2.803e+05 1.309e+06 4.325e+06 1.834e+08, threshold=2.618e+06, percent-clipped=43.0
2024-10-08 20:15:38,937 INFO [train.py:1152] Epoch 1, batch 2600, loss[loss=0.8082, ctc_loss=1.045, attn_decoder_loss=0.749, over 4859.00 frames. ], tot_loss[loss=0.7889, ctc_loss=1.047, attn_decoder_loss=0.7245, over 966370.44 frames. ], batch size: 20, lr: 4.37e-02,
2024-10-08 20:15:42,880 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=866.6666666666666, ans=0.0805
2024-10-08 20:15:54,518 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.60 vs. limit=8.1525
2024-10-08 20:15:55,615 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=22.13 vs. limit=7.82625
2024-10-08 20:15:57,338 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.17 vs. limit=8.1525
2024-10-08 20:16:08,195 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=28.80 vs. limit=7.8275
2024-10-08 20:16:13,721 WARNING [optim.py:503] Scaling gradients by 0.03910868614912033, model_norm_threshold=2617739.5
2024-10-08 20:16:13,875 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.611e+15, grad_sumsq=1.671e+17, orig_rms_sq=9.643e-03
2024-10-08 20:16:21,463 WARNING [optim.py:503] Scaling gradients by 0.013749714940786362, model_norm_threshold=2617739.5
2024-10-08 20:16:21,619 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.52, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.889e+16, grad_sumsq=1.953e+18, orig_rms_sq=9.677e-03
2024-10-08 20:16:24,825 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=31.31 vs. limit=7.82875
2024-10-08 20:16:38,314 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=10.07 vs. limit=5.22
2024-10-08 20:16:42,797 INFO [train.py:1152] Epoch 1, batch 2650, loss[loss=0.786, ctc_loss=1.059, attn_decoder_loss=0.7177, over 4827.00 frames. ], tot_loss[loss=0.7918, ctc_loss=1.05, attn_decoder_loss=0.7272, over 966102.73 frames. ], batch size: 38, lr: 4.37e-02,
2024-10-08 20:16:55,325 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.61 vs. limit=7.8325
2024-10-08 20:16:59,491 WARNING [optim.py:503] Scaling gradients by 0.09082476049661636, model_norm_threshold=2617739.5
2024-10-08 20:16:59,647 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.53, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.428e+14, grad_sumsq=4.324e+16, orig_rms_sq=1.024e-02
2024-10-08 20:17:02,437 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=886.6666666666666, ans=0.8689666666666667
2024-10-08 20:17:06,140 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=886.6666666666666, ans=0.16675
2024-10-08 20:17:08,135 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.57 vs. limit=7.83375
2024-10-08 20:17:12,260 WARNING [optim.py:503] Scaling gradients by 0.08699490129947662, model_norm_threshold=2617739.5
2024-10-08 20:17:12,416 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.146e+14, grad_sumsq=1.755e+14, orig_rms_sq=1.223e+00
2024-10-08 20:17:13,619 WARNING [optim.py:503] Scaling gradients by 0.02445831522345543, model_norm_threshold=2617739.5
2024-10-08 20:17:13,777 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.171e+15, grad_sumsq=4.019e+17, orig_rms_sq=1.038e-02
2024-10-08 20:17:16,004 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.26 vs. limit=5.2225
2024-10-08 20:17:22,490 WARNING [optim.py:503] Scaling gradients by 0.07946759462356567, model_norm_threshold=2617739.5
2024-10-08 20:17:22,647 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.614e+14, grad_sumsq=3.613e+16, orig_rms_sq=1.000e-02
2024-10-08 20:17:24,118 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=893.3333333333334, ans=0.29106666666666664
2024-10-08 20:17:25,184 WARNING [optim.py:503] Scaling gradients by 0.048837319016456604, model_norm_threshold=2617739.5
2024-10-08 20:17:25,340 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.676e+14, grad_sumsq=1.634e+16, orig_rms_sq=5.309e-02
2024-10-08 20:17:28,677 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=24.20 vs. limit=8.17
2024-10-08 20:17:32,968 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.min_positive, batch_count=896.6666666666666, ans=0.24103333333333332
2024-10-08 20:17:43,497 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=13.93 vs. limit=7.83625
2024-10-08 20:17:46,740 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.462e+04 2.925e+05 7.062e+05 3.965e+06 1.904e+08, threshold=1.412e+06, percent-clipped=27.0
2024-10-08 20:17:46,791 INFO [train.py:1152] Epoch 1, batch 2700, loss[loss=0.7863, ctc_loss=1.078, attn_decoder_loss=0.7135, over 4854.00 frames. ], tot_loss[loss=0.7903, ctc_loss=1.049, attn_decoder_loss=0.7256, over 966383.78 frames. ], batch size: 28, lr: 4.36e-02,
2024-10-08 20:17:48,079 WARNING [optim.py:503] Scaling gradients by 0.0011016696225851774, model_norm_threshold=1412419.0
2024-10-08 20:17:48,236 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.461e+17, grad_sumsq=1.056e+19, orig_rms_sq=5.171e-02
2024-10-08 20:17:57,134 WARNING [optim.py:503] Scaling gradients by 0.007199347484856844, model_norm_threshold=1412419.0
2024-10-08 20:17:57,290 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.673e+16, grad_sumsq=1.839e+18, orig_rms_sq=9.094e-03
2024-10-08 20:17:57,822 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.68 vs. limit=3.135
2024-10-08 20:17:58,242 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn2.whiten.whitening_limit, batch_count=900.0, ans=8.175
2024-10-08 20:17:59,464 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.53 vs. limit=7.83875
2024-10-08 20:18:07,644 WARNING [optim.py:503] Scaling gradients by 0.04457003250718117, model_norm_threshold=1412419.0
2024-10-08 20:18:07,801 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.838e+14, grad_sumsq=1.975e+16, orig_rms_sq=9.310e-03
2024-10-08 20:18:08,050 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=903.3333333333334, ans=0.079675
2024-10-08 20:18:08,448 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=14.65 vs. limit=7.83875
2024-10-08 20:18:09,235 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=903.3333333333334, ans=0.079675
2024-10-08 20:18:15,622 WARNING [optim.py:503] Scaling gradients by 0.030840124934911728, model_norm_threshold=1412419.0
2024-10-08 20:18:15,778 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.824e+14, grad_sumsq=1.045e+17, orig_rms_sq=9.402e-03
2024-10-08 20:18:16,962 WARNING [optim.py:503] Scaling gradients by 0.025568533688783646, model_norm_threshold=1412419.0
2024-10-08 20:18:17,117 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.048e+14, grad_sumsq=4.476e+14, orig_rms_sq=1.128e+00
2024-10-08 20:18:17,797 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.70 vs. limit=8.18
2024-10-08 20:18:20,302 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=13.22 vs. limit=7.84
2024-10-08 20:18:21,216 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=906.6666666666666, ans=0.4575
2024-10-08 20:18:23,268 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=5.67 vs. limit=5.453333333333333
2024-10-08 20:18:24,045 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=15.69 vs. limit=7.84
2024-10-08 20:18:25,851 WARNING [optim.py:503] Scaling gradients by 0.047780927270650864, model_norm_threshold=1412419.0
2024-10-08 20:18:26,009 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.754e+14, grad_sumsq=3.516e+15, orig_rms_sq=4.987e-02
2024-10-08 20:18:32,434 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=910.0, ans=0.07952500000000001
2024-10-08 20:18:33,512 WARNING [optim.py:503] Scaling gradients by 0.01567072607576847, model_norm_threshold=1412419.0
2024-10-08 20:18:33,665 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.174e+15, grad_sumsq=4.431e+17, orig_rms_sq=9.421e-03
2024-10-08 20:18:38,602 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=3.98 vs. limit=4.365333333333333
2024-10-08 20:18:42,051 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.44 vs. limit=8.185
2024-10-08 20:18:45,109 WARNING [optim.py:503] Scaling gradients by 0.010384357534348965, model_norm_threshold=1412419.0
2024-10-08 20:18:45,266 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.406e+15, grad_sumsq=3.866e+15, orig_rms_sq=1.140e+00
2024-10-08 20:18:46,501 WARNING [optim.py:503] Scaling gradients by 0.051658812910318375, model_norm_threshold=1412419.0
2024-10-08 20:18:46,658 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.881e+14, grad_sumsq=3.730e+15, orig_rms_sq=5.043e-02
2024-10-08 20:18:51,733 INFO [train.py:1152] Epoch 1, batch 2750, loss[loss=0.8028, ctc_loss=1.053, attn_decoder_loss=0.7402, over 4799.00 frames. ], tot_loss[loss=0.7915, ctc_loss=1.05, attn_decoder_loss=0.727, over 967020.74 frames. ], batch size: 19, lr: 4.36e-02,
2024-10-08 20:18:52,920 WARNING [optim.py:503] Scaling gradients by 0.08877891302108765, model_norm_threshold=1412419.0
2024-10-08 20:18:53,080 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.964e+13, grad_sumsq=7.542e+15, orig_rms_sq=9.234e-03
2024-10-08 20:18:54,450 WARNING [optim.py:503] Scaling gradients by 0.04477211833000183, model_norm_threshold=1412419.0
2024-10-08 20:18:54,605 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.146e+14, grad_sumsq=6.149e+15, orig_rms_sq=5.116e-02
2024-10-08 20:19:00,561 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.81 vs. limit=4.366666666666666
2024-10-08 20:19:06,662 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=15.90 vs. limit=7.845
2024-10-08 20:19:06,878 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.26 vs. limit=7.845
2024-10-08 20:19:14,991 WARNING [optim.py:503] Scaling gradients by 0.08363083004951477, model_norm_threshold=1412419.0
2024-10-08 20:19:15,146 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.258e+13, grad_sumsq=6.621e+15, orig_rms_sq=9.452e-03
2024-10-08 20:19:20,234 WARNING [optim.py:503] Scaling gradients by 0.003347867401316762, model_norm_threshold=1412419.0
2024-10-08 20:19:20,389 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.448e+16, grad_sumsq=5.709e+18, orig_rms_sq=9.543e-03
2024-10-08 20:19:20,969 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=17.84 vs. limit=5.461666666666667
2024-10-08 20:19:22,763 WARNING [optim.py:503] Scaling gradients by 0.0056708334013819695, model_norm_threshold=1412419.0
2024-10-08 20:19:22,917 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.787e+16, grad_sumsq=2.920e+18, orig_rms_sq=9.543e-03
2024-10-08 20:19:23,499 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=25.19 vs. limit=7.84625
2024-10-08 20:19:24,753 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=10.22 vs. limit=7.84625
2024-10-08 20:19:27,962 WARNING [optim.py:503] Scaling gradients by 0.06864271312952042, model_norm_threshold=1412419.0
2024-10-08 20:19:28,119 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.965e+13, grad_sumsq=8.268e+13, orig_rms_sq=1.205e+00
2024-10-08 20:19:30,697 WARNING [optim.py:503] Scaling gradients by 0.02953367307782173, model_norm_threshold=1412419.0
2024-10-08 20:19:30,851 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.031e+15, grad_sumsq=1.054e+17, orig_rms_sq=9.783e-03
2024-10-08 20:19:33,329 WARNING [optim.py:503] Scaling gradients by 0.0258652213960886, model_norm_threshold=1412419.0
2024-10-08 20:19:33,487 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.311e+15, grad_sumsq=1.340e+17, orig_rms_sq=9.783e-03
2024-10-08 20:19:34,939 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=926.6666666666666, ans=0.2139
2024-10-08 20:19:37,233 WARNING [optim.py:503] Scaling gradients by 0.06657750904560089, model_norm_threshold=1412419.0
2024-10-08 20:19:37,393 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.972e+13, grad_sumsq=1.938e+15, orig_rms_sq=5.146e-02
2024-10-08 20:19:39,363 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.37 vs. limit=8.195
2024-10-08 20:19:40,122 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=926.6666666666666, ans=0.16525
2024-10-08 20:19:40,532 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=24.87 vs. limit=5.463333333333333
2024-10-08 20:19:42,528 WARNING [optim.py:503] Scaling gradients by 0.0874251276254654, model_norm_threshold=1412419.0
2024-10-08 20:19:42,684 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.217e+13, grad_sumsq=1.200e+15, orig_rms_sq=5.179e-02
2024-10-08 20:19:44,344 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=36.85 vs. limit=8.1975
2024-10-08 20:19:45,028 WARNING [optim.py:503] Scaling gradients by 0.011841153725981712, model_norm_threshold=1412419.0
2024-10-08 20:19:45,185 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.782e+15, grad_sumsq=9.235e+16, orig_rms_sq=5.179e-02
2024-10-08 20:19:50,347 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=930.0, ans=0.19768750000000002
2024-10-08 20:19:55,957 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=57.43 vs. limit=7.85
2024-10-08 20:19:56,565 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.787e+04 5.626e+05 2.327e+06 7.273e+06 1.282e+09, threshold=4.655e+06, percent-clipped=59.0
2024-10-08 20:19:56,619 INFO [train.py:1152] Epoch 1, batch 2800, loss[loss=0.8202, ctc_loss=1.094, attn_decoder_loss=0.7517, over 4771.00 frames. ], tot_loss[loss=0.7931, ctc_loss=1.052, attn_decoder_loss=0.7284, over 967098.21 frames. ], batch size: 53, lr: 4.36e-02,
2024-10-08 20:20:04,213 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=933.3333333333334, ans=0.29066666666666663
2024-10-08 20:20:14,944 WARNING [optim.py:503] Scaling gradients by 0.016644423827528954, model_norm_threshold=4654516.0
2024-10-08 20:20:15,101 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.706e+16, grad_sumsq=2.911e+18, orig_rms_sq=9.296e-03
2024-10-08 20:20:20,770 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten.whitening_limit, batch_count=940.0, ans=8.205
2024-10-08 20:20:21,994 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=31.30 vs. limit=8.205
2024-10-08 20:20:22,882 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=940.0, ans=0.8671
2024-10-08 20:20:25,099 WARNING [optim.py:503] Scaling gradients by 0.040235668420791626, model_norm_threshold=4654516.0
2024-10-08 20:20:25,254 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.868e+15, grad_sumsq=4.364e+17, orig_rms_sq=8.864e-03
2024-10-08 20:20:28,963 WARNING [optim.py:503] Scaling gradients by 0.045166365802288055, model_norm_threshold=4654516.0
2024-10-08 20:20:29,120 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.919e+15, grad_sumsq=3.266e+17, orig_rms_sq=8.938e-03
2024-10-08 20:20:38,068 WARNING [optim.py:503] Scaling gradients by 0.06729957461357117, model_norm_threshold=4654516.0
2024-10-08 20:20:38,224 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.357e+15, grad_sumsq=2.412e+16, orig_rms_sq=5.623e-02
2024-10-08 20:20:39,673 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=943.3333333333334, ans=0.45578125
2024-10-08 20:20:44,625 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=943.3333333333334, ans=0.078775
2024-10-08 20:20:46,573 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.32 vs. limit=3.142
2024-10-08 20:20:56,545 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=24.50 vs. limit=8.21
2024-10-08 20:20:57,425 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=946.6666666666666, ans=0.04704166666666667
2024-10-08 20:20:59,852 INFO [train.py:1152] Epoch 1, batch 2850, loss[loss=0.8089, ctc_loss=1.03, attn_decoder_loss=0.7537, over 4932.00 frames. ], tot_loss[loss=0.7936, ctc_loss=1.053, attn_decoder_loss=0.7288, over 966900.70 frames. ], batch size: 20, lr: 4.35e-02,
2024-10-08 20:21:02,517 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=2.601e+01
2024-10-08 20:21:03,476 WARNING [optim.py:503] Scaling gradients by 0.01890123076736927, model_norm_threshold=4654516.0
2024-10-08 20:21:03,668 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.952e+16, grad_sumsq=1.706e+16, orig_rms_sq=1.144e+00
2024-10-08 20:21:12,566 WARNING [optim.py:503] Scaling gradients by 0.03893003240227699, model_norm_threshold=4654516.0
2024-10-08 20:21:12,722 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.092e+15, grad_sumsq=2.713e+15, orig_rms_sq=1.139e+00
2024-10-08 20:21:20,290 WARNING [optim.py:503] Scaling gradients by 0.05472058430314064, model_norm_threshold=4654516.0
2024-10-08 20:21:20,445 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.205e+15, grad_sumsq=2.592e+17, orig_rms_sq=8.507e-03
2024-10-08 20:21:23,579 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=5.62 vs. limit=5.476666666666667
2024-10-08 20:21:26,405 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=4.21 vs. limit=7.85875
2024-10-08 20:21:34,731 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=956.6666666666666, ans=0.3804166666666667
2024-10-08 20:21:35,973 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=956.6666666666666, ans=5.597916666666666
2024-10-08 20:21:37,787 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=25.94 vs. limit=8.22
2024-10-08 20:21:37,907 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=58.59 vs. limit=7.86
2024-10-08 20:21:38,225 WARNING [optim.py:503] Scaling gradients by 0.047525789588689804, model_norm_threshold=4654516.0
2024-10-08 20:21:38,385 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.858e+15, grad_sumsq=3.867e+15, orig_rms_sq=1.256e+00
2024-10-08 20:21:43,004 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=24.60 vs. limit=7.86
2024-10-08 20:21:52,159 WARNING [optim.py:503] Scaling gradients by 0.07985759526491165, model_norm_threshold=4654516.0
2024-10-08 20:21:52,316 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.189e+14, grad_sumsq=4.226e+14, orig_rms_sq=1.228e+00
2024-10-08 20:21:55,946 WARNING [optim.py:503] Scaling gradients by 0.09330237656831741, model_norm_threshold=4654516.0
2024-10-08 20:21:56,101 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.877e+14, grad_sumsq=6.361e+16, orig_rms_sq=9.240e-03
2024-10-08 20:21:59,746 WARNING [optim.py:503] Scaling gradients by 0.08364800363779068, model_norm_threshold=4654516.0
2024-10-08 20:21:59,900 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.756e+14, grad_sumsq=5.493e+14, orig_rms_sq=1.230e+00
2024-10-08 20:22:01,831 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=13.48 vs. limit=7.86125
2024-10-08 20:22:01,969 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.26 vs. limit=7.86125
2024-10-08 20:22:02,422 WARNING [optim.py:503] Scaling gradients by 0.0963822528719902, model_norm_threshold=4654516.0
2024-10-08 20:22:02,578 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.902e+14, grad_sumsq=1.074e+17, orig_rms_sq=9.219e-03
2024-10-08 20:22:03,825 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.170e+04 1.212e+06 3.057e+06 1.739e+07 2.796e+08, threshold=6.114e+06, percent-clipped=47.0
2024-10-08 20:22:03,876 INFO [train.py:1152] Epoch 1, batch 2900, loss[loss=0.7623, ctc_loss=0.9947, attn_decoder_loss=0.7042, over 4746.00 frames. ], tot_loss[loss=0.7931, ctc_loss=1.054, attn_decoder_loss=0.728, over 965979.15 frames. ], batch size: 20, lr: 4.35e-02,
2024-10-08 20:22:09,755 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.01 vs. limit=8.225
2024-10-08 20:22:19,627 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=12.72 vs. limit=7.86375
2024-10-08 20:22:20,505 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=970.0, ans=0.45453125
2024-10-08 20:22:22,074 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=39.96 vs. limit=7.86375
2024-10-08 20:22:22,159 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2.whitening_limit, batch_count=970.0, ans=5.485
2024-10-08 20:22:25,465 WARNING [optim.py:503] Scaling gradients by 0.027575664222240448, model_norm_threshold=6113602.0
2024-10-08 20:22:25,621 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.662e+16, grad_sumsq=1.833e+18, orig_rms_sq=9.068e-03
2024-10-08 20:22:27,429 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=10.07 vs. limit=5.2425
2024-10-08 20:22:28,930 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=41.29 vs. limit=7.86375
2024-10-08 20:22:34,005 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.60 vs. limit=7.865
2024-10-08 20:22:34,372 WARNING [optim.py:503] Scaling gradients by 0.04399655759334564, model_norm_threshold=6113602.0
2024-10-08 20:22:34,527 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.777e+15, grad_sumsq=4.643e+15, orig_rms_sq=1.244e+00
2024-10-08 20:22:39,286 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.59 vs. limit=5.243333333333333
2024-10-08 20:22:39,783 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=973.3333333333334, ans=0.1635
2024-10-08 20:22:40,980 WARNING [optim.py:503] Scaling gradients by 0.07437789440155029, model_norm_threshold=6113602.0
2024-10-08 20:22:41,135 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.021e+15, grad_sumsq=2.434e+15, orig_rms_sq=1.241e+00
2024-10-08 20:22:41,352 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=973.3333333333334, ans=0.1635
2024-10-08 20:22:43,111 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=35.11 vs. limit=7.86625
2024-10-08 20:22:45,143 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=976.6666666666666, ans=0.45421875
2024-10-08 20:22:47,296 WARNING [optim.py:503] Scaling gradients by 0.07024940848350525, model_norm_threshold=6113602.0
2024-10-08 20:22:47,453 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.300e+15, grad_sumsq=2.668e+15, orig_rms_sq=1.237e+00
2024-10-08 20:22:48,981 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=976.6666666666666, ans=0.07802500000000001
2024-10-08 20:22:56,115 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.22 vs. limit=8.235
2024-10-08 20:22:59,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=980.0, ans=0.4540625
2024-10-08 20:23:09,063 INFO [train.py:1152] Epoch 1, batch 2950, loss[loss=0.7537, ctc_loss=1.018, attn_decoder_loss=0.6877, over 4798.00 frames. ], tot_loss[loss=0.7916, ctc_loss=1.051, attn_decoder_loss=0.7267, over 966555.58 frames. ], batch size: 19, lr: 4.34e-02,
2024-10-08 20:23:09,784 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=31.92 vs. limit=7.86875
2024-10-08 20:23:13,391 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=42.19 vs. limit=8.2375
2024-10-08 20:23:14,255 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=983.3333333333334, ans=0.45390625
2024-10-08 20:23:25,803 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=9.44 vs. limit=7.87
2024-10-08 20:23:34,036 WARNING [optim.py:503] Scaling gradients by 0.045400749891996384, model_norm_threshold=6113602.0
2024-10-08 20:23:34,192 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.928e+15, grad_sumsq=1.145e+17, orig_rms_sq=5.176e-02
2024-10-08 20:23:38,313 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=990.0, ans=0.45359375
2024-10-08 20:23:40,623 WARNING [optim.py:503] Scaling gradients by 0.030757974833250046, model_norm_threshold=6113602.0
2024-10-08 20:23:40,779 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.303e+16, grad_sumsq=9.582e+15, orig_rms_sq=1.360e+00
2024-10-08 20:23:42,323 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=990.0, ans=0.077725
2024-10-08 20:23:44,613 WARNING [optim.py:503] Scaling gradients by 0.034481480717659, model_norm_threshold=6113602.0
2024-10-08 20:23:44,768 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.213e+16, grad_sumsq=8.875e+15, orig_rms_sq=1.366e+00
2024-10-08 20:23:46,782 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.96 vs. limit=8.245
2024-10-08 20:23:49,134 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=28.30 vs. limit=8.245
2024-10-08 20:23:53,055 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.28 vs. limit=5.248333333333333
2024-10-08 20:23:57,668 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=993.3333333333334, ans=0.4534375
2024-10-08 20:23:59,988 WARNING [optim.py:503] Scaling gradients by 0.08565888553857803, model_norm_threshold=6113602.0
2024-10-08 20:24:00,142 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.024e+15, grad_sumsq=7.006e+14, orig_rms_sq=1.462e+00
2024-10-08 20:24:00,390 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=996.6666666666666, ans=0.077575
2024-10-08 20:24:06,348 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module1.whiten, num_groups=1, num_channels=192, metric=2.97 vs. limit=7.87375
2024-10-08 20:24:12,178 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=1000.0, ans=0.453125
2024-10-08 20:24:13,265 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.081e+05 1.458e+06 4.621e+06 1.224e+07 2.217e+08, threshold=9.242e+06, percent-clipped=42.0
2024-10-08 20:24:13,316 INFO [train.py:1152] Epoch 1, batch 3000, loss[loss=0.8086, ctc_loss=1.081, attn_decoder_loss=0.7404, over 4849.00 frames. ], tot_loss[loss=0.7905, ctc_loss=1.051, attn_decoder_loss=0.7254, over 967211.69 frames. ], batch size: 21, lr: 4.34e-02,
2024-10-08 20:24:13,317 INFO [train.py:1175] Computing validation loss
2024-10-08 20:24:19,526 INFO [train.py:1184] Epoch 1, validation: loss=0.8327, ctc_loss=1.091, attn_decoder_loss=0.768, over 90464.00 frames.
2024-10-08 20:24:19,527 INFO [train.py:1185] Maximum memory allocated so far is 6925MB
2024-10-08 20:24:22,671 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=42.61 vs. limit=7.875
2024-10-08 20:24:29,465 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=1000.0, ans=0.453125
2024-10-08 20:24:30,549 WARNING [optim.py:503] Scaling gradients by 0.08785200119018555, model_norm_threshold=9241684.0
2024-10-08 20:24:30,705 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.815e+15, grad_sumsq=2.787e+15, orig_rms_sq=1.369e+00
2024-10-08 20:24:35,517 WARNING [optim.py:503] Scaling gradients by 0.09990242123603821, model_norm_threshold=9241684.0
2024-10-08 20:24:35,675 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.863e+15, grad_sumsq=3.712e+16, orig_rms_sq=5.019e-02
2024-10-08 20:24:39,033 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=13.08 vs. limit=7.87625
2024-10-08 20:24:42,450 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=9.62 vs. limit=4.4013333333333335
2024-10-08 20:24:42,493 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=22.01 vs. limit=8.2525
2024-10-08 20:24:46,615 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=16.40 vs. limit=7.8775
2024-10-08 20:24:49,270 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=33.71 vs. limit=7.8775
2024-10-08 20:24:53,686 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=1006.6666666666666, ans=0.16225
2024-10-08 20:24:58,135 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.31 vs. limit=8.2575
2024-10-08 20:24:59,814 WARNING [optim.py:503] Scaling gradients by 0.034328360110521317, model_norm_threshold=9241684.0
2024-10-08 20:24:59,972 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.75, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.430e+16, grad_sumsq=6.303e+18, orig_rms_sq=8.616e-03
2024-10-08 20:25:06,630 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=1010.0, ans=0.45265625
2024-10-08 20:25:20,346 WARNING [optim.py:503] Scaling gradients by 0.011984349228441715, model_norm_threshold=9241684.0
2024-10-08 20:25:20,502 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.410e+17, grad_sumsq=1.683e+19, orig_rms_sq=8.382e-03
2024-10-08 20:25:20,974 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=11.45 vs. limit=4.405333333333333
2024-10-08 20:25:22,971 INFO [train.py:1152] Epoch 1, batch 3050, loss[loss=0.7994, ctc_loss=1.067, attn_decoder_loss=0.7324, over 4750.00 frames. ], tot_loss[loss=0.7917, ctc_loss=1.052, attn_decoder_loss=0.7267, over 966736.06 frames. ], batch size: 19, lr: 4.33e-02,
2024-10-08 20:25:23,455 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=11.77 vs. limit=7.88125
2024-10-08 20:25:27,230 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.32 vs. limit=7.88125
2024-10-08 20:25:27,789 WARNING [optim.py:503] Scaling gradients by 0.017501644790172577, model_norm_threshold=9241684.0
2024-10-08 20:25:27,946 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.530e+16, grad_sumsq=1.004e+19, orig_rms_sq=8.498e-03
2024-10-08 20:25:30,042 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=23.79 vs. limit=7.88125
2024-10-08 20:25:32,040 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=1016.6666666666666, ans=0.45234375
2024-10-08 20:25:32,616 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.87 vs. limit=5.508333333333333
2024-10-08 20:25:36,402 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=24.81 vs. limit=5.51
2024-10-08 20:25:40,801 WARNING [optim.py:503] Scaling gradients by 0.00996674969792366, model_norm_threshold=9241684.0
2024-10-08 20:25:40,960 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.69, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.971e+17, grad_sumsq=4.443e+17, orig_rms_sq=1.344e+00
2024-10-08 20:25:41,138 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer_na.min_abs, batch_count=1020.0, ans=0.00808
2024-10-08 20:25:42,982 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.81 vs. limit=4.408
2024-10-08 20:25:43,097 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=59.13 vs. limit=7.8825
2024-10-08 20:25:48,923 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=49.52 vs. limit=8.2675
2024-10-08 20:25:48,971 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=17.37 vs. limit=7.88375
2024-10-08 20:25:49,204 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.16 vs. limit=8.2675
2024-10-08 20:25:58,672 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1023.3333333333334, ans=0.2897666666666667
2024-10-08 20:26:01,156 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=1026.6666666666667, ans=0.451875
2024-10-08 20:26:02,399 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1026.6666666666667, ans=0.28973333333333334
2024-10-08 20:26:04,800 WARNING [optim.py:503] Scaling gradients by 0.08848299086093903, model_norm_threshold=9241684.0
2024-10-08 20:26:04,958 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.130e+15, grad_sumsq=2.362e+15, orig_rms_sq=1.325e+00
2024-10-08 20:26:15,167 WARNING [optim.py:503] Scaling gradients by 0.09878165274858475, model_norm_threshold=9241684.0
2024-10-08 20:26:15,326 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.045e+15, grad_sumsq=3.259e+17, orig_rms_sq=9.345e-03
2024-10-08 20:26:16,094 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=48.35 vs. limit=7.88625
2024-10-08 20:26:16,193 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.42 vs. limit=5.2575
2024-10-08 20:26:17,392 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=33.46 vs. limit=8.2725
2024-10-08 20:26:21,535 WARNING [optim.py:503] Scaling gradients by 0.0615573413670063, model_norm_threshold=9241684.0
2024-10-08 20:26:21,690 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.188e+15, grad_sumsq=9.803e+17, orig_rms_sq=9.373e-03
2024-10-08 20:26:24,977 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=14.21 vs. limit=7.88625
2024-10-08 20:26:26,560 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.992e+04 2.241e+06 5.766e+06 2.232e+07 9.273e+08, threshold=1.153e+07, percent-clipped=41.0
2024-10-08 20:26:26,612 INFO [train.py:1152] Epoch 1, batch 3100, loss[loss=0.7467, ctc_loss=0.9803, attn_decoder_loss=0.6883, over 4807.00 frames. ], tot_loss[loss=0.7901, ctc_loss=1.049, attn_decoder_loss=0.7254, over 966426.09 frames. ], batch size: 38, lr: 4.33e-02,
2024-10-08 20:26:35,207 WARNING [optim.py:503] Scaling gradients by 0.09053104370832443, model_norm_threshold=11532885.0
2024-10-08 20:26:35,364 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.201e+15, grad_sumsq=4.449e+17, orig_rms_sq=9.441e-03
2024-10-08 20:26:40,201 WARNING [optim.py:503] Scaling gradients by 0.021790191531181335, model_norm_threshold=11532885.0
2024-10-08 20:26:40,357 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.126e+17, grad_sumsq=1.198e+19, orig_rms_sq=9.400e-03
2024-10-08 20:26:50,440 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=1040.0, ans=0.45125
2024-10-08 20:26:55,103 WARNING [optim.py:503] Scaling gradients by 0.048887405544519424, model_norm_threshold=11532885.0
2024-10-08 20:26:55,262 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.835e+16, grad_sumsq=1.466e+16, orig_rms_sq=1.252e+00
2024-10-08 20:26:56,491 WARNING [optim.py:503] Scaling gradients by 0.030067674815654755, model_norm_threshold=11532885.0
2024-10-08 20:26:56,648 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.567e+16, grad_sumsq=3.698e+16, orig_rms_sq=1.235e+00
2024-10-08 20:27:00,283 WARNING [optim.py:503] Scaling gradients by 0.03347574919462204, model_norm_threshold=11532885.0
2024-10-08 20:27:00,441 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.795e+16, grad_sumsq=5.323e+18, orig_rms_sq=9.008e-03
2024-10-08 20:27:06,666 WARNING [optim.py:503] Scaling gradients by 0.025445017963647842, model_norm_threshold=11532885.0
2024-10-08 20:27:06,825 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.524e+16, grad_sumsq=1.104e+18, orig_rms_sq=5.004e-02
2024-10-08 20:27:09,133 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=14.73 vs. limit=7.89125
2024-10-08 20:27:10,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1043.3333333333333, ans=0.28956666666666664
2024-10-08 20:27:11,805 WARNING [optim.py:503] Scaling gradients by 0.0016360841691493988, model_norm_threshold=11532885.0
2024-10-08 20:27:11,962 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.659e+19, grad_sumsq=1.899e+21, orig_rms_sq=8.733e-03
2024-10-08 20:27:13,443 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1043.3333333333333, ans=0.28956666666666664
2024-10-08 20:27:14,462 WARNING [optim.py:503] Scaling gradients by 0.05900903791189194, model_norm_threshold=11532885.0
2024-10-08 20:27:14,619 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.183e+16, grad_sumsq=1.355e+18, orig_rms_sq=8.733e-03
2024-10-08 20:27:18,392 WARNING [optim.py:503] Scaling gradients by 0.09151618927717209, model_norm_threshold=11532885.0
2024-10-08 20:27:18,548 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.315e+15, grad_sumsq=3.400e+15, orig_rms_sq=1.269e+00
2024-10-08 20:27:20,246 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=8.64 vs. limit=4.418666666666667
2024-10-08 20:27:22,755 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=32.56 vs. limit=8.285
2024-10-08 20:27:23,161 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.03 vs. limit=5.523333333333333
2024-10-08 20:27:23,589 WARNING [optim.py:503] Scaling gradients by 0.07135853171348572, model_norm_threshold=11532885.0
2024-10-08 20:27:23,746 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.261e+16, grad_sumsq=1.347e+18, orig_rms_sq=9.358e-03
2024-10-08 20:27:26,008 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=5.34 vs. limit=5.523333333333333
2024-10-08 20:27:29,942 INFO [train.py:1152] Epoch 1, batch 3150, loss[loss=0.8063, ctc_loss=1.089, attn_decoder_loss=0.7357, over 4800.00 frames. ], tot_loss[loss=0.786, ctc_loss=1.045, attn_decoder_loss=0.7212, over 966760.36 frames. ], batch size: 40, lr: 4.32e-02,
2024-10-08 20:27:32,144 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.54 vs. limit=5.2625
2024-10-08 20:27:34,332 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.98 vs. limit=7.89375
2024-10-08 20:27:36,342 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=1050.0, ans=0.45078125
2024-10-08 20:27:36,968 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten.whitening_limit, batch_count=1050.0, ans=8.2875
2024-10-08 20:27:38,631 WARNING [optim.py:503] Scaling gradients by 0.0034973497968167067, model_norm_threshold=11532885.0
2024-10-08 20:27:38,787 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.764e+18, grad_sumsq=7.396e+19, orig_rms_sq=5.090e-02
2024-10-08 20:27:42,277 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=16.48 vs. limit=8.29
2024-10-08 20:27:46,452 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=1053.3333333333333, ans=0.450625
2024-10-08 20:27:47,381 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=5.20 vs. limit=4.210666666666667
2024-10-08 20:27:49,933 WARNING [optim.py:503] Scaling gradients by 0.0005107583128847182, model_norm_threshold=11532885.0
2024-10-08 20:27:50,090 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.994e+20, grad_sumsq=1.499e+20, orig_rms_sq=1.331e+00
2024-10-08 20:27:50,913 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=22.33 vs. limit=7.895
2024-10-08 20:27:51,597 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=1053.3333333333333, ans=0.8631333333333333
2024-10-08 20:28:01,999 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=45.01 vs. limit=8.2925
2024-10-08 20:28:02,966 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=24.15 vs. limit=7.89625
2024-10-08 20:28:07,830 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=1060.0, ans=0.4503125
2024-10-08 20:28:08,975 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=1060.0, ans=0.8629
2024-10-08 20:28:12,799 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=1060.0, ans=0.8629
2024-10-08 20:28:15,239 WARNING [optim.py:503] Scaling gradients by 0.06411391496658325, model_norm_threshold=11532885.0
2024-10-08 20:28:15,394 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.487e+16, grad_sumsq=1.651e+18, orig_rms_sq=9.005e-03
2024-10-08 20:28:19,452 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=1060.0, ans=0.5
2024-10-08 20:28:20,608 WARNING [optim.py:503] Scaling gradients by 0.09128597378730774, model_norm_threshold=11532885.0
2024-10-08 20:28:20,763 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.868e+15, grad_sumsq=4.189e+17, orig_rms_sq=9.234e-03
2024-10-08 20:28:29,615 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=1063.3333333333333, ans=0.09335416666666667
2024-10-08 20:28:34,844 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.983e+05 4.442e+06 1.816e+07 4.314e+07 2.258e+10, threshold=3.632e+07, percent-clipped=55.0
2024-10-08 20:28:34,896 INFO [train.py:1152] Epoch 1, batch 3200, loss[loss=0.7819, ctc_loss=1.007, attn_decoder_loss=0.7257, over 4743.00 frames. ], tot_loss[loss=0.7848, ctc_loss=1.044, attn_decoder_loss=0.72, over 967211.05 frames. ], batch size: 20, lr: 4.32e-02,
2024-10-08 20:28:35,608 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.68 vs. limit=7.9
2024-10-08 20:28:37,395 WARNING [optim.py:503] Scaling gradients by 0.08889702707529068, model_norm_threshold=36321548.0
2024-10-08 20:28:37,552 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.472e+16, grad_sumsq=1.819e+16, orig_rms_sq=1.359e+00
2024-10-08 20:28:38,166 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.42 vs. limit=3.16
2024-10-08 20:28:38,964 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=1066.6666666666667, ans=0.3666666666666667
2024-10-08 20:28:43,164 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.60 vs. limit=4.426666666666667
2024-10-08 20:28:43,173 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=9.56 vs. limit=7.9
2024-10-08 20:28:46,785 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=1070.0, ans=0.5
2024-10-08 20:28:47,858 WARNING [optim.py:503] Scaling gradients by 0.03448345139622688, model_norm_threshold=36321548.0
2024-10-08 20:28:48,017 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.034e+17, grad_sumsq=2.209e+19, orig_rms_sq=9.204e-03
2024-10-08 20:28:56,718 WARNING [optim.py:503] Scaling gradients by 0.0701650083065033, model_norm_threshold=36321548.0
2024-10-08 20:28:56,876 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.059e+16, grad_sumsq=8.613e+17, orig_rms_sq=5.874e-02
2024-10-08 20:28:58,200 WARNING [optim.py:503] Scaling gradients by 0.09767856448888779, model_norm_threshold=36321548.0
2024-10-08 20:28:58,356 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.58, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.985e+16, grad_sumsq=9.109e+18, orig_rms_sq=8.766e-03
2024-10-08 20:29:02,710 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=13.70 vs. limit=5.536666666666667
2024-10-08 20:29:06,455 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.37 vs. limit=7.9025
2024-10-08 20:29:08,279 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=1073.3333333333333, ans=0.15975
2024-10-08 20:29:12,469 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=49.11 vs. limit=7.9037500000000005
2024-10-08 20:29:13,276 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=1076.6666666666667, ans=0.44953125
2024-10-08 20:29:27,253 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=1080.0, ans=0.18925
2024-10-08 20:29:29,570 WARNING [optim.py:503] Scaling gradients by 0.07587186992168427, model_norm_threshold=36321548.0
2024-10-08 20:29:29,727 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.326e+16, grad_sumsq=7.281e+16, orig_rms_sq=1.281e+00
2024-10-08 20:29:35,866 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.91 vs. limit=5.54
2024-10-08 20:29:36,030 WARNING [optim.py:503] Scaling gradients by 0.022934094071388245, model_norm_threshold=36321548.0
2024-10-08 20:29:36,184 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.347e+18, grad_sumsq=1.662e+20, orig_rms_sq=8.106e-03
2024-10-08 20:29:36,767 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=46.29 vs. limit=7.905
2024-10-08 20:29:38,815 INFO [train.py:1152] Epoch 1, batch 3250, loss[loss=0.7687, ctc_loss=1.037, attn_decoder_loss=0.7016, over 4855.00 frames. ], tot_loss[loss=0.7858, ctc_loss=1.046, attn_decoder_loss=0.7209, over 967283.25 frames. ], batch size: 24, lr: 4.31e-02,
2024-10-08 20:29:41,301 WARNING [optim.py:503] Scaling gradients by 0.008019840344786644, model_norm_threshold=36321548.0
2024-10-08 20:29:41,457 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.262e+18, grad_sumsq=4.152e+18, orig_rms_sq=1.268e+00
2024-10-08 20:29:46,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=1083.3333333333333, ans=0.15937500000000002
2024-10-08 20:29:48,993 WARNING [optim.py:503] Scaling gradients by 0.06702355295419693, model_norm_threshold=36321548.0
2024-10-08 20:29:49,150 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.967e+16, grad_sumsq=1.175e+19, orig_rms_sq=8.482e-03
2024-10-08 20:29:54,901 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=14.91 vs. limit=7.9075
2024-10-08 20:29:59,641 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=1086.6666666666667, ans=0.2891333333333333
2024-10-08 20:30:00,822 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=1086.6666666666667, ans=0.8619666666666667
2024-10-08 20:30:04,460 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer2.prob, batch_count=1090.0, ans=0.44890625
2024-10-08 20:30:08,290 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=1090.0, ans=0.15912500000000002
2024-10-08 20:30:10,085 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.42 vs. limit=7.90875
2024-10-08 20:30:11,832 WARNING [optim.py:503] Scaling gradients by 0.05232900753617287, model_norm_threshold=36321548.0
2024-10-08 20:30:11,989 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.326e+17, grad_sumsq=2.691e+19, orig_rms_sq=8.642e-03
2024-10-08 20:30:15,331 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.99 vs. limit=8.3175
2024-10-08 20:30:34,036 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=37.66 vs. limit=7.91125
2024-10-08 20:30:36,039 WARNING [optim.py:503] Scaling gradients by 0.09672372043132782, model_norm_threshold=36321548.0
2024-10-08 20:30:36,195 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.212e+16, grad_sumsq=2.443e+16, orig_rms_sq=1.315e+00
2024-10-08 20:30:42,585 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.638e+05 6.947e+06 2.325e+07 5.200e+07 4.529e+09, threshold=4.650e+07, percent-clipped=38.0
2024-10-08 20:30:42,637 INFO [train.py:1152] Epoch 1, batch 3300, loss[loss=0.8387, ctc_loss=1.086, attn_decoder_loss=0.777, over 4840.00 frames. ], tot_loss[loss=0.7853, ctc_loss=1.044, attn_decoder_loss=0.7206, over 967719.24 frames. ], batch size: 43, lr: 4.31e-02,
2024-10-08 20:30:45,768 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=34.73 vs. limit=8.325
2024-10-08 20:31:06,536 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=1106.6666666666667, ans=0.1585
2024-10-08 20:31:06,967 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.85 vs. limit=5.553333333333334
2024-10-08 20:31:09,147 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=8.82 vs. limit=7.915
2024-10-08 20:31:18,764 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=1110.0, ans=0.44796875
2024-10-08 20:31:22,055 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.93 vs. limit=8.3325
2024-10-08 20:31:24,752 WARNING [optim.py:503] Scaling gradients by 0.041624367237091064, model_norm_threshold=46496264.0
2024-10-08 20:31:24,909 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.284e+17, grad_sumsq=2.589e+17, orig_rms_sq=1.269e+00
2024-10-08 20:31:29,869 WARNING [optim.py:503] Scaling gradients by 0.08381442725658417, model_norm_threshold=46496264.0
2024-10-08 20:31:30,025 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.138e+17, grad_sumsq=1.406e+19, orig_rms_sq=8.094e-03
2024-10-08 20:31:44,168 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=1116.6666666666667, ans=0.44765625
2024-10-08 20:31:45,493 INFO [train.py:1152] Epoch 1, batch 3350, loss[loss=0.8251, ctc_loss=1.111, attn_decoder_loss=0.7538, over 4802.00 frames. ], tot_loss[loss=0.7857, ctc_loss=1.046, attn_decoder_loss=0.7205, over 966981.25 frames. ], batch size: 40, lr: 4.30e-02,
2024-10-08 20:31:48,019 WARNING [optim.py:503] Scaling gradients by 0.0470576211810112, model_norm_threshold=46496264.0
2024-10-08 20:31:48,177 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.939e+17, grad_sumsq=3.617e+19, orig_rms_sq=8.126e-03
2024-10-08 20:31:49,745 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=1116.6666666666667, ans=0.44765625
2024-10-08 20:31:50,788 WARNING [optim.py:503] Scaling gradients by 0.08376891911029816, model_norm_threshold=46496264.0
2024-10-08 20:31:50,945 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.605e+16, grad_sumsq=6.588e+16, orig_rms_sq=1.306e+00
2024-10-08 20:31:58,035 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.78 vs. limit=8.34
2024-10-08 20:31:58,212 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.21 vs. limit=8.34
2024-10-08 20:32:03,660 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1120.0, ans=0.2888
2024-10-08 20:32:04,360 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.15 vs. limit=5.5600000000000005
2024-10-08 20:32:13,241 WARNING [optim.py:503] Scaling gradients by 0.013543453998863697, model_norm_threshold=46496264.0
2024-10-08 20:32:13,396 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.075e+18, grad_sumsq=2.265e+18, orig_rms_sq=1.357e+00
2024-10-08 20:32:13,592 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=1123.3333333333333, ans=0.44734375
2024-10-08 20:32:19,005 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.28 vs. limit=8.3425
2024-10-08 20:32:21,432 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=30.62 vs. limit=7.92125
2024-10-08 20:32:22,900 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.05 vs. limit=4.450666666666667
2024-10-08 20:32:25,573 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=29.06 vs. limit=8.345
2024-10-08 20:32:27,391 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=1126.6666666666667, ans=0.2169
2024-10-08 20:32:29,242 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.51 vs. limit=5.281666666666666
2024-10-08 20:32:33,696 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=1126.6666666666667, ans=0.35916666666666663
2024-10-08 20:32:35,150 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=1130.0, ans=0.0929375
2024-10-08 20:32:37,002 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.02 vs. limit=8.3475
2024-10-08 20:32:44,295 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=35.76 vs. limit=8.3475
2024-10-08 20:32:44,786 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.23 vs. limit=8.3475
2024-10-08 20:32:46,183 WARNING [optim.py:503] Scaling gradients by 0.040822990238666534, model_norm_threshold=46496264.0
2024-10-08 20:32:46,338 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.391e+17, grad_sumsq=3.922e+19, orig_rms_sq=8.645e-03
2024-10-08 20:32:49,323 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.013e+05 1.089e+07 1.897e+07 5.697e+07 3.433e+09, threshold=3.794e+07, percent-clipped=29.0
2024-10-08 20:32:49,378 INFO [train.py:1152] Epoch 1, batch 3400, loss[loss=0.7659, ctc_loss=0.9679, attn_decoder_loss=0.7154, over 4959.00 frames. ], tot_loss[loss=0.7851, ctc_loss=1.047, attn_decoder_loss=0.7198, over 966766.51 frames. ], batch size: 19, lr: 4.29e-02,
2024-10-08 20:32:54,354 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=1133.3333333333333, ans=0.446875
2024-10-08 20:32:56,097 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=1133.3333333333333, ans=8.35
2024-10-08 20:33:04,637 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.03 vs. limit=5.284166666666667
2024-10-08 20:33:12,546 WARNING [optim.py:503] Scaling gradients by 0.004630323965102434, model_norm_threshold=37938840.0
2024-10-08 20:33:12,705 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.808e+19, grad_sumsq=2.111e+21, orig_rms_sq=8.565e-03
2024-10-08 20:33:16,513 WARNING [optim.py:503] Scaling gradients by 0.06807907670736313, model_norm_threshold=37938840.0
2024-10-08 20:33:16,670 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.676e+16, grad_sumsq=1.690e+18, orig_rms_sq=4.542e-02
2024-10-08 20:33:16,842 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=1140.0, ans=0.8601
2024-10-08 20:33:18,860 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=13.73 vs. limit=7.9275
2024-10-08 20:33:19,203 WARNING [optim.py:503] Scaling gradients by 0.01262781210243702, model_norm_threshold=37938840.0
2024-10-08 20:33:19,362 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.703e+18, grad_sumsq=1.346e+18, orig_rms_sq=1.266e+00
2024-10-08 20:33:20,594 WARNING [optim.py:503] Scaling gradients by 0.0030952715314924717, model_norm_threshold=37938840.0
2024-10-08 20:33:20,750 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.550e+19, grad_sumsq=8.023e+20, orig_rms_sq=4.425e-02
2024-10-08 20:33:22,696 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.40 vs. limit=3.171
2024-10-08 20:33:22,790 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=25.99 vs. limit=8.355
2024-10-08 20:33:25,929 WARNING [optim.py:503] Scaling gradients by 0.08627557009458542, model_norm_threshold=37938840.0
2024-10-08 20:33:26,085 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.901e+16, grad_sumsq=9.624e+18, orig_rms_sq=8.210e-03
2024-10-08 20:33:29,067 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=8.73 vs. limit=7.92875
2024-10-08 20:33:32,986 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.63 vs. limit=5.571666666666666
2024-10-08 20:33:37,114 WARNING [optim.py:503] Scaling gradients by 0.005300097167491913, model_norm_threshold=37938840.0
2024-10-08 20:33:37,269 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.986e+18, grad_sumsq=8.198e+18, orig_rms_sq=1.218e+00
2024-10-08 20:33:37,454 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.min_abs, batch_count=1143.3333333333333, ans=0.21715
2024-10-08 20:33:39,646 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=20.67 vs. limit=7.93
2024-10-08 20:33:48,225 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=5.66 vs. limit=7.93
2024-10-08 20:33:49,416 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=63.14 vs. limit=8.36
2024-10-08 20:33:51,241 WARNING [optim.py:503] Scaling gradients by 0.07886397838592529, model_norm_threshold=37938840.0
2024-10-08 20:33:51,397 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.171e+17, grad_sumsq=1.458e+19, orig_rms_sq=8.030e-03
2024-10-08 20:33:52,676 INFO [train.py:1152] Epoch 1, batch 3450, loss[loss=0.8814, ctc_loss=1.161, attn_decoder_loss=0.8115, over 4854.00 frames. ], tot_loss[loss=0.785, ctc_loss=1.047, attn_decoder_loss=0.7194, over 967096.41 frames. ], batch size: 43, lr: 4.29e-02,
2024-10-08 20:33:55,107 WARNING [optim.py:503] Scaling gradients by 0.013928806409239769, model_norm_threshold=37938840.0
2024-10-08 20:33:55,264 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.954e+18, grad_sumsq=4.425e+19, orig_rms_sq=4.416e-02
2024-10-08 20:33:55,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=1150.0, ans=0.074125
2024-10-08 20:33:58,909 WARNING [optim.py:503] Scaling gradients by 0.005151642486453056, model_norm_threshold=37938840.0
2024-10-08 20:33:59,064 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.090e+19, grad_sumsq=8.897e+18, orig_rms_sq=1.225e+00
2024-10-08 20:34:00,444 WARNING [optim.py:503] Scaling gradients by 0.01834857650101185, model_norm_threshold=37938840.0
2024-10-08 20:34:00,603 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.400e+18, grad_sumsq=3.128e+19, orig_rms_sq=4.477e-02
2024-10-08 20:34:05,341 WARNING [optim.py:503] Scaling gradients by 0.01797746680676937, model_norm_threshold=37938840.0
2024-10-08 20:34:05,498 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.348e+18, grad_sumsq=1.696e+20, orig_rms_sq=7.944e-03
2024-10-08 20:34:07,894 WARNING [optim.py:503] Scaling gradients by 0.01981811411678791, model_norm_threshold=37938840.0
2024-10-08 20:34:08,049 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.214e+18, grad_sumsq=2.655e+19, orig_rms_sq=4.573e-02
2024-10-08 20:34:15,411 WARNING [optim.py:503] Scaling gradients by 0.05756805092096329, model_norm_threshold=37938840.0
2024-10-08 20:34:15,566 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.486e+17, grad_sumsq=1.911e+19, orig_rms_sq=7.775e-03
2024-10-08 20:34:18,229 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=1156.6666666666667, ans=0.44578125
2024-10-08 20:34:20,817 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1156.6666666666667, ans=0.2884333333333333
2024-10-08 20:34:32,964 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=27.13 vs. limit=8.37
2024-10-08 20:34:35,567 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.93 vs. limit=8.37
2024-10-08 20:34:42,231 WARNING [optim.py:503] Scaling gradients by 0.048809800297021866, model_norm_threshold=37938840.0
2024-10-08 20:34:42,388 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.970e+17, grad_sumsq=1.575e+17, orig_rms_sq=1.251e+00
2024-10-08 20:34:42,580 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1160.0, ans=0.2884
2024-10-08 20:34:44,851 WARNING [optim.py:503] Scaling gradients by 0.011015006341040134, model_norm_threshold=37938840.0
2024-10-08 20:34:45,008 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.725e+18, grad_sumsq=2.976e+18, orig_rms_sq=1.251e+00
2024-10-08 20:34:46,386 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=1163.3333333333333, ans=0.44546874999999997
2024-10-08 20:34:49,867 WARNING [optim.py:503] Scaling gradients by 0.030491238459944725, model_norm_threshold=37938840.0
2024-10-08 20:34:50,025 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.659e+17, grad_sumsq=1.015e+20, orig_rms_sq=7.550e-03
2024-10-08 20:34:51,208 WARNING [optim.py:503] Scaling gradients by 0.01800413988530636, model_norm_threshold=37938840.0
2024-10-08 20:34:51,365 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.100e+18, grad_sumsq=2.781e+20, orig_rms_sq=7.550e-03
2024-10-08 20:34:51,992 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.43 vs. limit=3.1745
2024-10-08 20:34:55,015 WARNING [optim.py:503] Scaling gradients by 0.02767782285809517, model_norm_threshold=37938840.0
2024-10-08 20:34:55,170 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.836e+17, grad_sumsq=1.134e+20, orig_rms_sq=7.790e-03
2024-10-08 20:34:55,797 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=25.56 vs. limit=8.3725
2024-10-08 20:34:57,722 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.765e+05 9.626e+06 6.219e+07 2.009e+08 1.226e+10, threshold=1.244e+08, percent-clipped=52.0
2024-10-08 20:34:57,774 INFO [train.py:1152] Epoch 1, batch 3500, loss[loss=0.7708, ctc_loss=1.013, attn_decoder_loss=0.7102, over 4883.00 frames. ], tot_loss[loss=0.7837, ctc_loss=1.045, attn_decoder_loss=0.7183, over 967403.75 frames. ], batch size: 19, lr: 4.28e-02,
2024-10-08 20:34:59,846 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.35 vs. limit=7.9375
2024-10-08 20:35:02,902 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=1166.6666666666667, ans=0.09270833333333334
2024-10-08 20:35:02,918 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1166.6666666666667, ans=0.28833333333333333
2024-10-08 20:35:03,923 WARNING [optim.py:503] Scaling gradients by 0.01918950304389, model_norm_threshold=124380880.0
2024-10-08 20:35:04,078 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.354e+19, grad_sumsq=1.021e+19, orig_rms_sq=1.327e+00
2024-10-08 20:35:05,119 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=10.41 vs. limit=7.9375
2024-10-08 20:35:08,188 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=1166.6666666666667, ans=0.4453125
2024-10-08 20:35:09,365 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=1170.0, ans=0.156125
2024-10-08 20:35:13,876 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.19 vs. limit=8.3775
2024-10-08 20:35:16,792 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=1170.0, ans=0.0926875
2024-10-08 20:35:24,910 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.65 vs. limit=5.586666666666667
2024-10-08 20:35:49,073 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=18.90 vs. limit=7.9425
2024-10-08 20:35:56,251 WARNING [optim.py:503] Scaling gradients by 0.006801178678870201, model_norm_threshold=124380880.0
2024-10-08 20:35:56,405 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.483e+20, grad_sumsq=1.839e+22, orig_rms_sq=8.064e-03
2024-10-08 20:35:57,102 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.33 vs. limit=3.177
2024-10-08 20:35:57,672 WARNING [optim.py:503] Scaling gradients by 0.033944789320230484, model_norm_threshold=124380880.0
2024-10-08 20:35:57,829 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.444e+18, grad_sumsq=4.685e+18, orig_rms_sq=1.376e+00
2024-10-08 20:36:01,588 INFO [train.py:1152] Epoch 1, batch 3550, loss[loss=0.7522, ctc_loss=1.003, attn_decoder_loss=0.6895, over 4792.00 frames. ], tot_loss[loss=0.7833, ctc_loss=1.045, attn_decoder_loss=0.7178, over 967338.54 frames. ], batch size: 29, lr: 4.28e-02,
2024-10-08 20:36:11,240 WARNING [optim.py:503] Scaling gradients by 0.06139509752392769, model_norm_threshold=124380880.0
2024-10-08 20:36:11,397 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.968e+17, grad_sumsq=6.896e+17, orig_rms_sq=1.300e+00
2024-10-08 20:36:13,271 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.49 vs. limit=5.296666666666667
2024-10-08 20:36:14,494 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=57.00 vs. limit=8.39
2024-10-08 20:36:21,167 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.23 vs. limit=5.296666666666667
2024-10-08 20:36:23,033 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=1186.6666666666667, ans=0.8584666666666667
2024-10-08 20:36:23,071 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=1186.6666666666667, ans=0.1555
2024-10-08 20:36:23,651 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.67 vs. limit=5.296666666666667
2024-10-08 20:36:31,216 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=42.23 vs. limit=7.94625
2024-10-08 20:36:40,681 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=1193.3333333333333, ans=0.35083333333333333
2024-10-08 20:36:44,489 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=4.616e+02
2024-10-08 20:36:45,359 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module2.whiten, num_groups=1, num_channels=192, metric=5.41 vs. limit=7.9475
2024-10-08 20:36:45,408 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=5.64 vs. limit=4.477333333333333
2024-10-08 20:36:47,959 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=7.10 vs. limit=5.298333333333333
2024-10-08 20:36:54,924 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=1196.6666666666667, ans=0.44390625
2024-10-08 20:36:58,007 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=57.43 vs. limit=8.3975
2024-10-08 20:36:58,166 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.60 vs. limit=7.94875
2024-10-08 20:37:05,357 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.846e+05 6.819e+06 4.469e+07 1.466e+08 1.829e+10, threshold=8.937e+07, percent-clipped=26.0
2024-10-08 20:37:05,409 INFO [train.py:1152] Epoch 1, batch 3600, loss[loss=0.7447, ctc_loss=0.9894, attn_decoder_loss=0.6836, over 4946.00 frames. ], tot_loss[loss=0.7855, ctc_loss=1.048, attn_decoder_loss=0.72, over 967405.82 frames. ], batch size: 20, lr: 4.27e-02,
2024-10-08 20:37:05,565 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.min_abs, batch_count=1200.0, ans=0.218
2024-10-08 20:37:08,152 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=1200.0, ans=0.44375
2024-10-08 20:37:08,592 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.26 vs. limit=7.95
2024-10-08 20:37:12,365 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=24.29 vs. limit=7.95
2024-10-08 20:37:17,550 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.whiten.whitening_limit, batch_count=1203.3333333333333, ans=4.481333333333334
2024-10-08 20:37:17,588 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=51.80 vs. limit=7.95125
2024-10-08 20:37:19,763 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=1203.3333333333333, ans=0.44359375
2024-10-08 20:37:21,525 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=17.93 vs. limit=7.95125
2024-10-08 20:37:29,702 WARNING [optim.py:503] Scaling gradients by 0.026469890028238297, model_norm_threshold=89370608.0
2024-10-08 20:37:29,860 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.768e+18, grad_sumsq=3.182e+18, orig_rms_sq=1.184e+00
2024-10-08 20:37:31,764 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=32.66 vs. limit=7.9525
2024-10-08 20:37:35,156 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=1206.6666666666667, ans=0.39938776934588294
2024-10-08 20:37:36,525 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=1206.6666666666667, ans=0.3491666666666666
2024-10-08 20:37:36,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=1206.6666666666667, ans=0.04622916666666667
2024-10-08 20:37:37,159 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.27 vs. limit=5.6033333333333335
2024-10-08 20:37:43,347 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=27.61 vs. limit=7.95375
2024-10-08 20:37:44,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=23.94 vs. limit=7.95375
2024-10-08 20:37:45,731 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=48.08 vs. limit=7.95375
2024-10-08 20:37:51,198 WARNING [optim.py:503] Scaling gradients by 0.05558686703443527, model_norm_threshold=89370608.0
2024-10-08 20:37:51,353 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.742e+17, grad_sumsq=4.574e+17, orig_rms_sq=1.474e+00
2024-10-08 20:37:55,466 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1213.3333333333333, ans=0.28786666666666666
2024-10-08 20:37:55,551 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=1213.3333333333333, ans=0.443125
2024-10-08 20:38:01,007 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.36 vs. limit=4.485333333333333
2024-10-08 20:38:03,809 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten.whitening_limit, batch_count=1213.3333333333333, ans=7.955
2024-10-08 20:38:09,454 INFO [train.py:1152] Epoch 1, batch 3650, loss[loss=0.7348, ctc_loss=1.011, attn_decoder_loss=0.6658, over 4842.00 frames. ], tot_loss[loss=0.7851, ctc_loss=1.048, attn_decoder_loss=0.7195, over 967903.82 frames. ], batch size: 31, lr: 4.27e-02,
2024-10-08 20:38:18,044 WARNING [optim.py:503] Scaling gradients by 0.09071581810712814, model_norm_threshold=89370608.0
2024-10-08 20:38:18,201 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.628e+17, grad_sumsq=3.213e+19, orig_rms_sq=8.178e-03
2024-10-08 20:38:22,621 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=13.77 vs. limit=5.61
2024-10-08 20:38:44,666 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=6.67 vs. limit=5.0
2024-10-08 20:38:45,516 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=44.21 vs. limit=7.95875
2024-10-08 20:38:48,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=1226.6666666666667, ans=0.28773333333333334
2024-10-08 20:38:48,724 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=1226.6666666666667, ans=5.766666666666667
2024-10-08 20:38:54,277 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=3.97 vs. limit=4.490666666666667
2024-10-08 20:38:59,545 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=36.70 vs. limit=8.4225
2024-10-08 20:39:02,535 WARNING [optim.py:503] Scaling gradients by 0.05478525906801224, model_norm_threshold=89370608.0
2024-10-08 20:39:02,691 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.289e+17, grad_sumsq=5.338e+17, orig_rms_sq=1.553e+00
2024-10-08 20:39:11,403 WARNING [optim.py:503] Scaling gradients by 0.03335518017411232, model_norm_threshold=89370608.0
2024-10-08 20:39:11,560 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.992e+18, grad_sumsq=1.302e+18, orig_rms_sq=1.530e+00
2024-10-08 20:39:12,794 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.176e+06 1.506e+07 6.392e+07 1.542e+08 3.376e+09, threshold=1.278e+08, percent-clipped=40.0
2024-10-08 20:39:12,845 INFO [train.py:1152] Epoch 1, batch 3700, loss[loss=0.8657, ctc_loss=1.151, attn_decoder_loss=0.7944, over 4827.00 frames. ], tot_loss[loss=0.7849, ctc_loss=1.049, attn_decoder_loss=0.7188, over 967394.99 frames. ], batch size: 24, lr: 4.26e-02,
2024-10-08 20:39:12,985 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=1233.3333333333333, ans=0.5
2024-10-08 20:39:14,886 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.16 vs. limit=5.616666666666666
2024-10-08 20:39:15,232 WARNING [optim.py:503] Scaling gradients by 0.005553863476961851, model_norm_threshold=127846928.0
2024-10-08 20:39:15,390 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.596e+20, grad_sumsq=1.073e+20, orig_rms_sq=1.487e+00
2024-10-08 20:39:18,141 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=1233.3333333333333, ans=5.770833333333333
2024-10-08 20:39:21,692 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1233.3333333333333, ans=0.2876666666666667
2024-10-08 20:39:27,843 WARNING [optim.py:503] Scaling gradients by 0.05653020367026329, model_norm_threshold=127846928.0
2024-10-08 20:39:27,999 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.162e+18, grad_sumsq=7.382e+17, orig_rms_sq=1.574e+00
2024-10-08 20:39:30,652 WARNING [optim.py:503] Scaling gradients by 0.05024972930550575, model_norm_threshold=127846928.0
2024-10-08 20:39:30,808 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.651e+18, grad_sumsq=1.049e+18, orig_rms_sq=1.574e+00
2024-10-08 20:39:32,058 WARNING [optim.py:503] Scaling gradients by 0.09413747489452362, model_norm_threshold=127846928.0
2024-10-08 20:39:32,217 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.889e+17, grad_sumsq=9.151e+18, orig_rms_sq=4.250e-02
2024-10-08 20:39:36,784 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=512, metric=11.25 vs. limit=7.96375
2024-10-08 20:39:38,781 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=1240.0, ans=0.046125
2024-10-08 20:39:39,748 WARNING [optim.py:503] Scaling gradients by 0.012381690554320812, model_norm_threshold=127846928.0
2024-10-08 20:39:39,905 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.747e+19, grad_sumsq=1.093e+19, orig_rms_sq=1.599e+00
2024-10-08 20:39:40,487 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=147.78 vs. limit=7.965
2024-10-08 20:39:41,381 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.min_positive, batch_count=1240.0, ans=0.2376
2024-10-08 20:39:41,454 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=1240.0, ans=0.09225
2024-10-08 20:39:42,396 WARNING [optim.py:503] Scaling gradients by 0.05117155238986015, model_norm_threshold=127846928.0
2024-10-08 20:39:42,552 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.224e+18, grad_sumsq=7.654e+17, orig_rms_sq=1.599e+00
2024-10-08 20:39:43,898 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=1240.0, ans=0.441875
2024-10-08 20:39:48,169 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=20.83 vs. limit=7.965
2024-10-08 20:39:48,377 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=7.32 vs. limit=5.31
2024-10-08 20:39:58,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=1243.3333333333333, ans=0.072025
2024-10-08 20:39:59,226 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=79.67 vs. limit=7.96625
2024-10-08 20:40:02,844 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=14.28 vs. limit=7.96625
2024-10-08 20:40:03,501 WARNING [optim.py:503] Scaling gradients by 0.06312155723571777, model_norm_threshold=127846928.0
2024-10-08 20:40:03,658 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.032e+18, grad_sumsq=2.534e+20, orig_rms_sq=8.018e-03
2024-10-08 20:40:05,375 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=18.20 vs. limit=7.9675
2024-10-08 20:40:05,649 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=37.08 vs. limit=8.435
2024-10-08 20:40:08,666 WARNING [optim.py:503] Scaling gradients by 0.032310452312231064, model_norm_threshold=127846928.0
2024-10-08 20:40:08,822 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.677e+18, grad_sumsq=4.592e+20, orig_rms_sq=8.007e-03
2024-10-08 20:40:17,572 WARNING [optim.py:503] Scaling gradients by 0.03539419174194336, model_norm_threshold=127846928.0
2024-10-08 20:40:17,729 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.291e+18, grad_sumsq=5.387e+20, orig_rms_sq=7.965e-03
2024-10-08 20:40:17,787 INFO [train.py:1152] Epoch 1, batch 3750, loss[loss=0.745, ctc_loss=0.9808, attn_decoder_loss=0.686, over 4959.00 frames. ], tot_loss[loss=0.7812, ctc_loss=1.046, attn_decoder_loss=0.7151, over 967724.70 frames. ], batch size: 19, lr: 4.26e-02,
2024-10-08 20:40:18,952 WARNING [optim.py:503] Scaling gradients by 0.07682429254055023, model_norm_threshold=127846928.0
2024-10-08 20:40:19,109 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.396e+18, grad_sumsq=1.753e+20, orig_rms_sq=7.965e-03
2024-10-08 20:40:22,483 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=27.79 vs. limit=7.96875
2024-10-08 20:40:34,341 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=21.35 vs. limit=7.97
2024-10-08 20:40:35,909 WARNING [optim.py:503] Scaling gradients by 0.014665523543953896, model_norm_threshold=127846928.0
2024-10-08 20:40:36,065 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.310e+19, grad_sumsq=1.515e+19, orig_rms_sq=1.524e+00
2024-10-08 20:40:41,877 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.49 vs. limit=3.188
2024-10-08 20:40:42,019 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=37.73 vs. limit=7.97
2024-10-08 20:40:42,635 WARNING [optim.py:503] Scaling gradients by 0.011128287762403488, model_norm_threshold=127846928.0
2024-10-08 20:40:42,791 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.660e+19, grad_sumsq=5.974e+21, orig_rms_sq=7.800e-03
2024-10-08 20:40:43,717 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=6.95 vs. limit=7.97125
2024-10-08 20:40:51,448 WARNING [optim.py:503] Scaling gradients by 0.06480333209037781, model_norm_threshold=127846928.0
2024-10-08 20:40:51,604 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.605e+17, grad_sumsq=1.275e+20, orig_rms_sq=7.530e-03
2024-10-08 20:40:59,108 WARNING [optim.py:503] Scaling gradients by 0.01581602357327938, model_norm_threshold=127846928.0
2024-10-08 20:40:59,265 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.524e+19, grad_sumsq=9.906e+18, orig_rms_sq=1.538e+00
2024-10-08 20:41:00,318 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.06 vs. limit=3.189
2024-10-08 20:41:02,913 WARNING [optim.py:503] Scaling gradients by 0.050565753132104874, model_norm_threshold=127846928.0
2024-10-08 20:41:03,069 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.654e+18, grad_sumsq=1.061e+18, orig_rms_sq=1.559e+00
2024-10-08 20:41:04,952 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=12.53 vs. limit=5.315
2024-10-08 20:41:04,980 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.60 vs. limit=5.63
2024-10-08 20:41:05,534 WARNING [optim.py:503] Scaling gradients by 0.0032139639370143414, model_norm_threshold=127846928.0
2024-10-08 20:41:05,691 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.870e+20, grad_sumsq=1.841e+20, orig_rms_sq=1.559e+00
2024-10-08 20:41:10,618 WARNING [optim.py:503] Scaling gradients by 0.09750925004482269, model_norm_threshold=127846928.0
2024-10-08 20:41:10,775 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.180e+17, grad_sumsq=2.049e+17, orig_rms_sq=1.552e+00
2024-10-08 20:41:13,275 WARNING [optim.py:503] Scaling gradients by 0.06947435438632965, model_norm_threshold=127846928.0
2024-10-08 20:41:13,433 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.414e+17, grad_sumsq=3.587e+17, orig_rms_sq=1.509e+00
2024-10-08 20:41:15,521 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.83 vs. limit=8.4475
2024-10-08 20:41:15,868 WARNING [optim.py:503] Scaling gradients by 0.006044269539415836, model_norm_threshold=127846928.0
2024-10-08 20:41:16,026 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.550e+19, grad_sumsq=5.665e+19, orig_rms_sq=1.509e+00
2024-10-08 20:41:16,219 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=1263.3333333333333, ans=0.152625
2024-10-08 20:41:22,252 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.07 vs. limit=5.633333333333334
2024-10-08 20:41:22,389 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.52 vs. limit=8.45
2024-10-08 20:41:22,805 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.185e+06 4.278e+07 1.379e+08 7.943e+08 3.978e+10, threshold=2.758e+08, percent-clipped=49.0
2024-10-08 20:41:22,859 INFO [train.py:1152] Epoch 1, batch 3800, loss[loss=0.792, ctc_loss=1.043, attn_decoder_loss=0.7293, over 4760.00 frames. ], tot_loss[loss=0.7821, ctc_loss=1.047, attn_decoder_loss=0.7158, over 967570.03 frames. ], batch size: 26, lr: 4.25e-02,
2024-10-08 20:41:22,963 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=1.152e+05
2024-10-08 20:41:27,994 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=1.265e+05
2024-10-08 20:41:28,027 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=1266.6666666666667, ans=0.440625
2024-10-08 20:41:33,225 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.min_positive, batch_count=1266.6666666666667, ans=0.23733333333333334
2024-10-08 20:41:34,426 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.max_abs, batch_count=1270.0, ans=5.79375
2024-10-08 20:41:35,470 WARNING [optim.py:503] Scaling gradients by 0.011224109679460526, model_norm_threshold=275763296.0
2024-10-08 20:41:35,629 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.260e+20, grad_sumsq=3.163e+21, orig_rms_sq=3.983e-02
2024-10-08 20:41:36,459 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.36 vs. limit=7.97625
2024-10-08 20:41:40,866 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=1270.0, ans=0.85555
2024-10-08 20:41:41,821 WARNING [optim.py:503] Scaling gradients by 0.02845621295273304, model_norm_threshold=275763296.0
2024-10-08 20:41:41,984 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.678e+19, grad_sumsq=9.927e+18, orig_rms_sq=1.690e+00
2024-10-08 20:41:57,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=25.63 vs. limit=7.9775
2024-10-08 20:41:59,011 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.69 vs. limit=8.455
2024-10-08 20:41:59,574 WARNING [optim.py:503] Scaling gradients by 0.03227509930729866, model_norm_threshold=275763296.0
2024-10-08 20:41:59,730 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.328e+19, grad_sumsq=1.334e+19, orig_rms_sq=1.745e+00
2024-10-08 20:42:01,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1276.6666666666667, ans=0.28723333333333334
2024-10-08 20:42:04,363 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.30 vs. limit=5.319166666666667
2024-10-08 20:42:08,238 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.95 vs. limit=5.638333333333334
2024-10-08 20:42:08,554 WARNING [optim.py:503] Scaling gradients by 0.017865873873233795, model_norm_threshold=275763296.0
2024-10-08 20:42:08,713 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.791e+19, grad_sumsq=2.832e+19, orig_rms_sq=1.691e+00
2024-10-08 20:42:12,288 WARNING [optim.py:503] Scaling gradients by 0.03435355797410011, model_norm_threshold=275763296.0
2024-10-08 20:42:12,444 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.158e+19, grad_sumsq=1.867e+19, orig_rms_sq=1.691e+00
2024-10-08 20:42:15,025 WARNING [optim.py:503] Scaling gradients by 0.002947121625766158, model_norm_threshold=275763296.0
2024-10-08 20:42:15,183 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.255e+21, grad_sumsq=1.347e+21, orig_rms_sq=1.673e+00
2024-10-08 20:42:15,335 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1280.0, ans=0.44
2024-10-08 20:42:16,731 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=1280.0, ans=0.0712
2024-10-08 20:42:17,425 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=7.89 vs. limit=7.98
2024-10-08 20:42:17,939 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=1280.0, ans=0.7628
2024-10-08 20:42:23,474 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.54 vs. limit=8.46
2024-10-08 20:42:26,689 INFO [train.py:1152] Epoch 1, batch 3850, loss[loss=0.8446, ctc_loss=1.126, attn_decoder_loss=0.7743, over 4830.00 frames. ], tot_loss[loss=0.7837, ctc_loss=1.049, attn_decoder_loss=0.7172, over 967529.49 frames. ], batch size: 38, lr: 4.24e-02,
2024-10-08 20:42:27,824 WARNING [optim.py:503] Scaling gradients by 0.05750660225749016, model_norm_threshold=275763296.0
2024-10-08 20:42:27,979 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.604e+18, grad_sumsq=2.730e+18, orig_rms_sq=1.686e+00
2024-10-08 20:42:28,209 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=1283.3333333333333, ans=0.7628333333333334
2024-10-08 20:42:28,857 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=28.42 vs. limit=7.98125
2024-10-08 20:42:39,670 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=1286.6666666666667, ans=0.8549666666666667
2024-10-08 20:42:47,082 WARNING [optim.py:503] Scaling gradients by 0.0957774817943573, model_norm_threshold=275763296.0
2024-10-08 20:42:47,238 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.219e+18, grad_sumsq=4.455e+20, orig_rms_sq=7.226e-03
2024-10-08 20:42:51,175 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.max_positive, batch_count=1290.0, ans=0.7629
2024-10-08 20:42:52,533 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=1290.0, ans=0.7629
2024-10-08 20:42:56,718 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.45 vs. limit=5.645
2024-10-08 20:42:57,865 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.46 vs. limit=8.4675
2024-10-08 20:43:00,264 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=60.14 vs. limit=8.4675
2024-10-08 20:43:02,858 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=17.92 vs. limit=7.98375
2024-10-08 20:43:03,463 WARNING [optim.py:503] Scaling gradients by 0.06143055483698845, model_norm_threshold=275763296.0
2024-10-08 20:43:03,617 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.590e+18, grad_sumsq=1.158e+20, orig_rms_sq=3.964e-02
2024-10-08 20:43:08,662 WARNING [optim.py:503] Scaling gradients by 0.0033661923371255398, model_norm_threshold=275763296.0
2024-10-08 20:43:08,818 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.456e+21, grad_sumsq=3.384e+23, orig_rms_sq=7.259e-03
2024-10-08 20:43:14,253 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=1293.3333333333333, ans=0.8547333333333333
2024-10-08 20:43:15,226 WARNING [optim.py:503] Scaling gradients by 0.07936928421258926, model_norm_threshold=275763296.0
2024-10-08 20:43:15,383 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.984e+18, grad_sumsq=3.945e+20, orig_rms_sq=7.565e-03
2024-10-08 20:43:19,159 WARNING [optim.py:503] Scaling gradients by 0.01398394163697958, model_norm_threshold=275763296.0
2024-10-08 20:43:19,316 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.963e+19, grad_sumsq=9.831e+20, orig_rms_sq=6.065e-02
2024-10-08 20:43:28,034 WARNING [optim.py:503] Scaling gradients by 0.018120603635907173, model_norm_threshold=275763296.0
2024-10-08 20:43:28,190 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.195e+19, grad_sumsq=2.884e+19, orig_rms_sq=1.801e+00
2024-10-08 20:43:30,603 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.834e+06 7.731e+07 2.235e+08 1.047e+09 9.357e+10, threshold=4.470e+08, percent-clipped=44.0
2024-10-08 20:43:30,603 WARNING [optim.py:503] Scaling gradients by 0.060602862387895584, model_norm_threshold=447007104.0
2024-10-08 20:43:30,760 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.099e+19, grad_sumsq=2.826e+21, orig_rms_sq=7.429e-03
2024-10-08 20:43:30,818 INFO [train.py:1152] Epoch 1, batch 3900, loss[loss=0.7799, ctc_loss=1.063, attn_decoder_loss=0.709, over 4743.00 frames. ], tot_loss[loss=0.7846, ctc_loss=1.05, attn_decoder_loss=0.7181, over 967015.45 frames. ], batch size: 26, lr: 4.24e-02,
2024-10-08 20:43:35,144 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.88 vs. limit=5.325
2024-10-08 20:43:35,669 WARNING [optim.py:503] Scaling gradients by 0.08981303125619888, model_norm_threshold=447007104.0
2024-10-08 20:43:35,825 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.568e+18, grad_sumsq=1.029e+21, orig_rms_sq=7.355e-03
2024-10-08 20:43:42,852 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=37.84 vs. limit=7.98875
2024-10-08 20:43:46,940 WARNING [optim.py:503] Scaling gradients by 0.03540605306625366, model_norm_threshold=447007104.0
2024-10-08 20:43:47,100 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.63, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.009e+20, grad_sumsq=1.401e+22, orig_rms_sq=7.200e-03
2024-10-08 20:43:50,131 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys.whitening_limit, batch_count=1303.3333333333333, ans=3.1955
2024-10-08 20:43:52,664 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten.whitening_limit, batch_count=1303.3333333333333, ans=7.98875
2024-10-08 20:43:53,533 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=1303.3333333333333, ans=0.151125
2024-10-08 20:43:57,213 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=1306.6666666666667, ans=0.33666666666666667
2024-10-08 20:44:00,692 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module1.whiten, num_groups=1, num_channels=192, metric=3.38 vs. limit=7.99
2024-10-08 20:44:02,347 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=1306.6666666666667, ans=0.151
2024-10-08 20:44:05,775 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.62 vs. limit=3.196
2024-10-08 20:44:05,923 WARNING [optim.py:503] Scaling gradients by 0.07783041149377823, model_norm_threshold=447007104.0
2024-10-08 20:44:06,080 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.342e+18, grad_sumsq=4.273e+18, orig_rms_sq=1.718e+00
2024-10-08 20:44:07,529 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=1310.0, ans=0.43859375
2024-10-08 20:44:08,597 WARNING [optim.py:503] Scaling gradients by 0.03287611901760101, model_norm_threshold=447007104.0
2024-10-08 20:44:08,754 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.648e+19, grad_sumsq=2.123e+19, orig_rms_sq=1.718e+00
2024-10-08 20:44:16,628 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=1310.0, ans=0.04590625
2024-10-08 20:44:19,277 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=10.79 vs. limit=5.3275
2024-10-08 20:44:21,618 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=1313.3333333333333, ans=0.15075
2024-10-08 20:44:31,329 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=1313.3333333333333, ans=0.15075
2024-10-08 20:44:33,672 INFO [train.py:1152] Epoch 1, batch 3950, loss[loss=0.7485, ctc_loss=1.014, attn_decoder_loss=0.6821, over 4839.00 frames. ], tot_loss[loss=0.7823, ctc_loss=1.047, attn_decoder_loss=0.716, over 967247.74 frames. ], batch size: 36, lr: 4.23e-02,
2024-10-08 20:44:39,776 WARNING [optim.py:503] Scaling gradients by 0.058263055980205536, model_norm_threshold=447007104.0
2024-10-08 20:44:39,933 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.563e+19, grad_sumsq=9.267e+18, orig_rms_sq=1.687e+00
2024-10-08 20:44:43,807 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=1316.6666666666667, ans=0.3354166666666667
2024-10-08 20:44:50,200 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=19.33 vs. limit=8.49
2024-10-08 20:44:52,226 WARNING [optim.py:503] Scaling gradients by 0.03853178024291992, model_norm_threshold=447007104.0
2024-10-08 20:44:52,382 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.62, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.409e+19, grad_sumsq=1.242e+22, orig_rms_sq=6.769e-03
2024-10-08 20:44:53,070 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=41.18 vs. limit=7.995
2024-10-08 20:44:56,116 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=1320.0, ans=0.438125
2024-10-08 20:44:57,354 WARNING [optim.py:503] Scaling gradients by 0.02382754348218441, model_norm_threshold=447007104.0
2024-10-08 20:44:57,520 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.735e+19, grad_sumsq=3.683e+19, orig_rms_sq=1.829e+00
2024-10-08 20:44:59,754 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=4.29 vs. limit=4.529333333333334
2024-10-08 20:45:03,687 WARNING [optim.py:503] Scaling gradients by 0.02631375938653946, model_norm_threshold=447007104.0
2024-10-08 20:45:03,844 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.388e+19, grad_sumsq=1.157e+22, orig_rms_sq=7.247e-03
2024-10-08 20:45:10,751 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=12.14 vs. limit=7.99625
2024-10-08 20:45:19,865 WARNING [optim.py:503] Scaling gradients by 0.08972381055355072, model_norm_threshold=447007104.0
2024-10-08 20:45:20,022 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.318e+18, grad_sumsq=3.463e+18, orig_rms_sq=1.824e+00
2024-10-08 20:45:21,199 WARNING [optim.py:503] Scaling gradients by 0.0031117936596274376, model_norm_threshold=447007104.0
2024-10-08 20:45:21,358 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.726e+21, grad_sumsq=1.120e+23, orig_rms_sq=4.221e-02
2024-10-08 20:45:23,374 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=105.14 vs. limit=7.9975
2024-10-08 20:45:25,712 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.59 vs. limit=7.99875
2024-10-08 20:45:30,441 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=1330.0, ans=0.43765624999999997
2024-10-08 20:45:31,519 WARNING [optim.py:503] Scaling gradients by 0.0025275431107729673, model_norm_threshold=447007104.0
2024-10-08 20:45:31,679 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.100e+21, grad_sumsq=1.944e+23, orig_rms_sq=4.167e-02
2024-10-08 20:45:32,855 WARNING [optim.py:503] Scaling gradients by 0.016133179888129234, model_norm_threshold=447007104.0
2024-10-08 20:45:33,012 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.062e+20, grad_sumsq=7.388e+21, orig_rms_sq=4.144e-02
2024-10-08 20:45:37,256 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_True/checkpoint-4000.pt
2024-10-08 20:45:40,016 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.845e+06 1.445e+08 4.201e+08 1.475e+09 1.769e+11, threshold=8.402e+08, percent-clipped=48.0
2024-10-08 20:45:40,071 INFO [train.py:1152] Epoch 1, batch 4000, loss[loss=0.7421, ctc_loss=0.9569, attn_decoder_loss=0.6884, over 4815.00 frames. ], tot_loss[loss=0.7828, ctc_loss=1.048, attn_decoder_loss=0.7165, over 967198.32 frames. ], batch size: 19, lr: 4.23e-02,
2024-10-08 20:45:43,982 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=1333.3333333333333, ans=0.4375
2024-10-08 20:45:47,658 WARNING [optim.py:503] Scaling gradients by 0.047830406576395035, model_norm_threshold=840234112.0
2024-10-08 20:45:47,815 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.901e+19, grad_sumsq=1.950e+21, orig_rms_sq=4.051e-02
2024-10-08 20:45:54,078 WARNING [optim.py:503] Scaling gradients by 0.0690692588686943, model_norm_threshold=840234112.0
2024-10-08 20:45:54,233 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.524e+19, grad_sumsq=8.639e+20, orig_rms_sq=4.079e-02
2024-10-08 20:46:00,577 WARNING [optim.py:503] Scaling gradients by 0.0743550956249237, model_norm_threshold=840234112.0
2024-10-08 20:46:00,733 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.178e+19, grad_sumsq=2.907e+19, orig_rms_sq=1.782e+00
2024-10-08 20:46:02,272 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=1336.6666666666667, ans=0.149875
2024-10-08 20:46:02,813 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.15 vs. limit=8.5025
2024-10-08 20:46:04,517 WARNING [optim.py:503] Scaling gradients by 0.09845481812953949, model_norm_threshold=840234112.0
2024-10-08 20:46:04,673 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.670e+19, grad_sumsq=4.183e+20, orig_rms_sq=3.994e-02
2024-10-08 20:46:09,255 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=7.77 vs. limit=8.0025
2024-10-08 20:46:11,278 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=1340.0, ans=0.4371875
2024-10-08 20:46:14,438 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=34.01 vs. limit=8.504999999999999
2024-10-08 20:46:14,725 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=9.07 vs. limit=8.0025
2024-10-08 20:46:20,739 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.91 vs. limit=5.335833333333333
2024-10-08 20:46:21,131 WARNING [optim.py:503] Scaling gradients by 0.08146916329860687, model_norm_threshold=840234112.0
2024-10-08 20:46:21,286 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.685e+19, grad_sumsq=3.872e+21, orig_rms_sq=6.935e-03
2024-10-08 20:46:27,801 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=77.04 vs. limit=8.00375
2024-10-08 20:46:32,280 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=1346.6666666666667, ans=0.436875
2024-10-08 20:46:36,576 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=9.99 vs. limit=8.005
2024-10-08 20:46:38,474 WARNING [optim.py:503] Scaling gradients by 0.03619720786809921, model_norm_threshold=840234112.0
2024-10-08 20:46:38,632 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.597e+20, grad_sumsq=2.289e+22, orig_rms_sq=6.979e-03
2024-10-08 20:46:43,635 INFO [train.py:1152] Epoch 1, batch 4050, loss[loss=0.9114, ctc_loss=1.221, attn_decoder_loss=0.8339, over 4777.00 frames. ], tot_loss[loss=0.7847, ctc_loss=1.05, attn_decoder_loss=0.7183, over 967549.96 frames. ], batch size: 53, lr: 4.22e-02,
2024-10-08 20:46:43,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1350.0, ans=0.43671875
2024-10-08 20:46:44,115 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=18.43 vs. limit=8.00625
2024-10-08 20:46:46,343 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=1350.0, ans=0.33125
2024-10-08 20:46:49,485 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=14.13 vs. limit=5.675
2024-10-08 20:46:51,423 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1350.0, ans=0.43671875
2024-10-08 20:46:55,451 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=9.14 vs. limit=4.541333333333333
2024-10-08 20:46:58,475 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.39 vs. limit=8.515
2024-10-08 20:46:59,337 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten.whitening_limit, batch_count=1353.3333333333333, ans=8.515
2024-10-08 20:47:03,336 WARNING [optim.py:503] Scaling gradients by 0.02892737276852131, model_norm_threshold=840234112.0
2024-10-08 20:47:03,493 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.559e+20, grad_sumsq=3.795e+22, orig_rms_sq=6.743e-03
2024-10-08 20:47:06,319 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=1353.3333333333333, ans=0.14925
2024-10-08 20:47:13,391 WARNING [optim.py:503] Scaling gradients by 0.02995435521006584, model_norm_threshold=840234112.0
2024-10-08 20:47:13,547 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.908e+20, grad_sumsq=1.042e+20, orig_rms_sq=1.831e+00
2024-10-08 20:47:24,972 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.attention_skip_rate, batch_count=1360.0, ans=0.14900000000000002
2024-10-08 20:47:25,490 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=7.19 vs. limit=4.5440000000000005
2024-10-08 20:47:29,183 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.96 vs. limit=4.5440000000000005
2024-10-08 20:47:29,832 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=1360.0, ans=0.43625
2024-10-08 20:47:32,838 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=28.03 vs. limit=8.01125
2024-10-08 20:47:37,295 WARNING [optim.py:503] Scaling gradients by 0.03430589661002159, model_norm_threshold=840234112.0
2024-10-08 20:47:37,453 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.245e+20, grad_sumsq=5.246e+21, orig_rms_sq=4.280e-02
2024-10-08 20:47:38,647 WARNING [optim.py:503] Scaling gradients by 0.008309677243232727, model_norm_threshold=840234112.0
2024-10-08 20:47:38,805 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.112e+21, grad_sumsq=5.049e+23, orig_rms_sq=6.164e-03
2024-10-08 20:47:40,014 WARNING [optim.py:503] Scaling gradients by 0.05560055747628212, model_norm_threshold=840234112.0
2024-10-08 20:47:40,171 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.212e+19, grad_sumsq=9.842e+20, orig_rms_sq=4.280e-02
2024-10-08 20:47:41,392 WARNING [optim.py:503] Scaling gradients by 0.00017313254647888243, model_norm_threshold=840234112.0
2024-10-08 20:47:41,548 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.606e+24, grad_sumsq=1.104e+26, orig_rms_sq=4.172e-02
2024-10-08 20:47:43,984 WARNING [optim.py:503] Scaling gradients by 0.072910837829113, model_norm_threshold=840234112.0
2024-10-08 20:47:44,140 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.356e+19, grad_sumsq=3.856e+21, orig_rms_sq=6.110e-03
2024-10-08 20:47:44,326 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=1363.3333333333333, ans=0.43609375
2024-10-08 20:47:46,567 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.032e+06 1.295e+08 6.130e+08 2.441e+09 4.853e+12, threshold=1.226e+09, percent-clipped=42.0
2024-10-08 20:47:46,622 INFO [train.py:1152] Epoch 1, batch 4100, loss[loss=0.7026, ctc_loss=0.989, attn_decoder_loss=0.631, over 4861.00 frames. ], tot_loss[loss=0.786, ctc_loss=1.052, attn_decoder_loss=0.7195, over 966936.24 frames. ], batch size: 31, lr: 4.22e-02,
2024-10-08 20:47:46,769 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1366.6666666666667, ans=0.28633333333333333
2024-10-08 20:47:47,464 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=16.04 vs. limit=8.0125
2024-10-08 20:47:49,044 WARNING [optim.py:503] Scaling gradients by 0.09764236956834793, model_norm_threshold=1225993728.0
2024-10-08 20:47:49,199 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.122e+19, grad_sumsq=1.177e+22, orig_rms_sq=6.049e-03
2024-10-08 20:47:51,699 WARNING [optim.py:503] Scaling gradients by 0.033972084522247314, model_norm_threshold=1225993728.0
2024-10-08 20:47:51,855 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.266e+20, grad_sumsq=1.035e+23, orig_rms_sq=6.052e-03
2024-10-08 20:47:52,038 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.min_positive, batch_count=1366.6666666666667, ans=0.23633333333333334
2024-10-08 20:48:00,121 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=32.74 vs. limit=8.5275
2024-10-08 20:48:00,554 WARNING [optim.py:503] Scaling gradients by 0.08007221668958664, model_norm_threshold=1225993728.0
2024-10-08 20:48:00,710 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.259e+19, grad_sumsq=1.077e+21, orig_rms_sq=3.954e-02
2024-10-08 20:48:03,829 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.24 vs. limit=3.2055
2024-10-08 20:48:08,742 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.95 vs. limit=5.3425
2024-10-08 20:48:10,079 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.98 vs. limit=4.548
2024-10-08 20:48:12,233 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=1373.3333333333333, ans=0.5
2024-10-08 20:48:13,980 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.27 vs. limit=8.53
2024-10-08 20:48:14,214 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.26 vs. limit=8.53
2024-10-08 20:48:23,533 WARNING [optim.py:503] Scaling gradients by 0.05778718367218971, model_norm_threshold=1225993728.0
2024-10-08 20:48:23,690 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.349e+20, grad_sumsq=6.868e+19, orig_rms_sq=1.964e+00
2024-10-08 20:48:25,486 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=24.03 vs. limit=8.01625
2024-10-08 20:48:35,192 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=1376.6666666666667, ans=0.43546875
2024-10-08 20:48:40,132 WARNING [optim.py:503] Scaling gradients by 0.007958449423313141, model_norm_threshold=1225993728.0
2024-10-08 20:48:40,289 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.669e+21, grad_sumsq=1.048e+23, orig_rms_sq=5.407e-02
2024-10-08 20:48:50,418 INFO [train.py:1152] Epoch 1, batch 4150, loss[loss=0.78, ctc_loss=1.071, attn_decoder_loss=0.7074, over 4749.00 frames. ], tot_loss[loss=0.7824, ctc_loss=1.049, attn_decoder_loss=0.7159, over 967084.83 frames. ], batch size: 20, lr: 4.21e-02,
2024-10-08 20:49:00,828 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1383.3333333333333, ans=0.43515625
2024-10-08 20:49:04,313 WARNING [optim.py:503] Scaling gradients by 0.07223761826753616, model_norm_threshold=1225993728.0
2024-10-08 20:49:04,471 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.882e+19, grad_sumsq=4.811e+19, orig_rms_sq=1.846e+00
2024-10-08 20:49:04,977 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=20.34 vs. limit=8.54
2024-10-08 20:49:05,999 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=1386.6666666666667, ans=0.435
2024-10-08 20:49:07,906 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.38 vs. limit=8.02
2024-10-08 20:49:08,301 WARNING [optim.py:503] Scaling gradients by 0.002053862903267145, model_norm_threshold=1225993728.0
2024-10-08 20:49:08,458 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.875e+22, grad_sumsq=1.820e+24, orig_rms_sq=4.328e-02
2024-10-08 20:49:09,640 WARNING [optim.py:503] Scaling gradients by 0.06033323332667351, model_norm_threshold=1225993728.0
2024-10-08 20:49:09,797 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.255e+20, grad_sumsq=6.965e+19, orig_rms_sq=1.802e+00
2024-10-08 20:49:11,339 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=1386.6666666666667, ans=0.14800000000000002
2024-10-08 20:49:16,729 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=33.02 vs. limit=8.02125
2024-10-08 20:49:22,147 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.69 vs. limit=5.695
2024-10-08 20:49:24,284 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.50 vs. limit=5.3475
2024-10-08 20:49:25,040 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1390.0, ans=0.28609999999999997
2024-10-08 20:49:27,358 WARNING [optim.py:503] Scaling gradients by 0.033084969967603683, model_norm_threshold=1225993728.0
2024-10-08 20:49:27,515 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.030e+20, grad_sumsq=5.111e+22, orig_rms_sq=5.928e-03
2024-10-08 20:49:28,036 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=20.16 vs. limit=8.545
2024-10-08 20:49:28,253 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.56 vs. limit=8.545
2024-10-08 20:49:30,219 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1393.3333333333333, ans=0.28606666666666664
2024-10-08 20:49:31,547 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=1393.3333333333333, ans=0.4346875
2024-10-08 20:49:37,413 WARNING [optim.py:503] Scaling gradients by 0.02787492610514164, model_norm_threshold=1225993728.0
2024-10-08 20:49:37,569 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.865e+20, grad_sumsq=1.317e+23, orig_rms_sq=5.972e-03
2024-10-08 20:49:40,224 WARNING [optim.py:503] Scaling gradients by 0.05581853911280632, model_norm_threshold=1225993728.0
2024-10-08 20:49:40,381 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.317e+20, grad_sumsq=2.165e+22, orig_rms_sq=6.083e-03
2024-10-08 20:49:41,065 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=12.28 vs. limit=5.349166666666667
2024-10-08 20:49:50,394 WARNING [optim.py:503] Scaling gradients by 0.018900465220212936, model_norm_threshold=1225993728.0
2024-10-08 20:49:50,552 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.134e+20, grad_sumsq=2.158e+22, orig_rms_sq=4.233e-02
2024-10-08 20:49:52,673 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=16.80 vs. limit=8.5475
2024-10-08 20:49:54,044 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=28.10 vs. limit=8.025
2024-10-08 20:49:54,604 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.149e+07 2.551e+08 1.077e+09 2.708e+09 5.969e+11, threshold=2.154e+09, percent-clipped=42.0
2024-10-08 20:49:54,655 INFO [train.py:1152] Epoch 1, batch 4200, loss[loss=0.7752, ctc_loss=1.028, attn_decoder_loss=0.712, over 4858.00 frames. ], tot_loss[loss=0.7828, ctc_loss=1.05, attn_decoder_loss=0.7159, over 967267.35 frames. ], batch size: 31, lr: 4.20e-02,
2024-10-08 20:49:54,836 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=1400.0, ans=0.434375
2024-10-08 20:50:02,095 WARNING [optim.py:503] Scaling gradients by 0.017766274511814117, model_norm_threshold=2153661184.0
2024-10-08 20:50:02,252 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.838e+21, grad_sumsq=8.306e+23, orig_rms_sq=5.825e-03
2024-10-08 20:50:03,299 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=8.52 vs. limit=8.025
2024-10-08 20:50:03,700 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=1400.0, ans=0.434375
2024-10-08 20:50:04,997 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=1400.0, ans=0.045625
2024-10-08 20:50:15,721 WARNING [optim.py:503] Scaling gradients by 0.008488613180816174, model_norm_threshold=2153661184.0
2024-10-08 20:50:15,879 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.874e+21, grad_sumsq=2.417e+23, orig_rms_sq=4.085e-02
2024-10-08 20:50:23,092 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=4.65 vs. limit=8.0275
2024-10-08 20:50:23,527 WARNING [optim.py:503] Scaling gradients by 0.02545175328850746, model_norm_threshold=2153661184.0
2024-10-08 20:50:23,686 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.002e+21, grad_sumsq=3.505e+23, orig_rms_sq=5.712e-03
2024-10-08 20:50:27,525 WARNING [optim.py:503] Scaling gradients by 0.005256852135062218, model_norm_threshold=2153661184.0
2024-10-08 20:50:27,681 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.792e+22, grad_sumsq=6.703e+24, orig_rms_sq=5.657e-03
2024-10-08 20:50:34,947 WARNING [optim.py:503] Scaling gradients by 0.09050597995519638, model_norm_threshold=2153661184.0
2024-10-08 20:50:35,105 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.080e+20, grad_sumsq=3.615e+22, orig_rms_sq=5.755e-03
2024-10-08 20:50:40,053 WARNING [optim.py:503] Scaling gradients by 0.04034202918410301, model_norm_threshold=2153661184.0
2024-10-08 20:50:40,209 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.120e+21, grad_sumsq=1.898e+23, orig_rms_sq=5.899e-03
2024-10-08 20:50:50,862 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=46.81 vs. limit=8.03
2024-10-08 20:50:51,571 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=1.086e+04
2024-10-08 20:50:54,131 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=1413.3333333333333, ans=0.17049999999999998
2024-10-08 20:50:58,831 WARNING [optim.py:503] Scaling gradients by 0.03445892035961151, model_norm_threshold=2153661184.0
2024-10-08 20:50:58,988 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.772e+20, grad_sumsq=4.151e+20, orig_rms_sq=1.872e+00
2024-10-08 20:50:59,047 INFO [train.py:1152] Epoch 1, batch 4250, loss[loss=0.7945, ctc_loss=1.079, attn_decoder_loss=0.7234, over 4756.00 frames. ], tot_loss[loss=0.7818, ctc_loss=1.05, attn_decoder_loss=0.7148, over 967210.84 frames. ], batch size: 19, lr: 4.20e-02,
2024-10-08 20:51:01,468 WARNING [optim.py:503] Scaling gradients by 0.010313011705875397, model_norm_threshold=2153661184.0
2024-10-08 20:51:01,622 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.677e+22, grad_sumsq=3.044e+24, orig_rms_sq=5.510e-03
2024-10-08 20:51:07,473 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.58 vs. limit=5.354166666666667
2024-10-08 20:51:30,710 WARNING [optim.py:503] Scaling gradients by 0.026578543707728386, model_norm_threshold=2153661184.0
2024-10-08 20:51:30,867 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.515e+21, grad_sumsq=8.161e+20, orig_rms_sq=1.856e+00
2024-10-08 20:51:31,739 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.98 vs. limit=5.355833333333333
2024-10-08 20:51:43,808 WARNING [optim.py:503] Scaling gradients by 0.0831994041800499, model_norm_threshold=2153661184.0
2024-10-08 20:51:43,965 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.622e+20, grad_sumsq=8.522e+19, orig_rms_sq=1.903e+00
2024-10-08 20:51:48,048 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=1426.6666666666667, ans=0.433125
2024-10-08 20:51:48,085 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=1426.6666666666667, ans=0.2214
2024-10-08 20:51:53,193 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=1430.0, ans=0.43296875
2024-10-08 20:51:57,472 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.22 vs. limit=3.2145
2024-10-08 20:52:01,850 WARNING [optim.py:503] Scaling gradients by 0.0037432825192809105, model_norm_threshold=2153661184.0
2024-10-08 20:52:02,006 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.799e+22, grad_sumsq=1.049e+25, orig_rms_sq=6.482e-03
2024-10-08 20:52:03,408 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.764e+07 2.641e+08 1.040e+09 7.578e+09 5.753e+11, threshold=2.080e+09, percent-clipped=37.0
2024-10-08 20:52:03,459 INFO [train.py:1152] Epoch 1, batch 4300, loss[loss=0.6917, ctc_loss=0.9937, attn_decoder_loss=0.6162, over 4849.00 frames. ], tot_loss[loss=0.7809, ctc_loss=1.051, attn_decoder_loss=0.7134, over 967391.96 frames. ], batch size: 21, lr: 4.19e-02,
2024-10-08 20:52:08,565 WARNING [optim.py:503] Scaling gradients by 0.09090165048837662, model_norm_threshold=2079517440.0
2024-10-08 20:52:08,722 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.960e+19, grad_sumsq=1.545e+22, orig_rms_sq=6.446e-03
2024-10-08 20:52:12,575 WARNING [optim.py:503] Scaling gradients by 0.04579516500234604, model_norm_threshold=2079517440.0
2024-10-08 20:52:12,734 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.981e+20, grad_sumsq=7.728e+22, orig_rms_sq=6.446e-03
2024-10-08 20:52:16,687 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=1436.6666666666667, ans=0.43265625
2024-10-08 20:52:20,100 WARNING [optim.py:503] Scaling gradients by 0.0856398269534111, model_norm_threshold=2079517440.0
2024-10-08 20:52:20,256 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.387e+20, grad_sumsq=2.183e+22, orig_rms_sq=6.353e-03
2024-10-08 20:52:20,974 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=5.91 vs. limit=5.718333333333334
2024-10-08 20:52:29,163 WARNING [optim.py:503] Scaling gradients by 0.04016157612204552, model_norm_threshold=2079517440.0
2024-10-08 20:52:29,320 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.981e+20, grad_sumsq=9.495e+22, orig_rms_sq=6.299e-03
2024-10-08 20:52:31,702 WARNING [optim.py:503] Scaling gradients by 0.07881037145853043, model_norm_threshold=2079517440.0
2024-10-08 20:52:31,858 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.296e+20, grad_sumsq=5.449e+21, orig_rms_sq=4.215e-02
2024-10-08 20:52:32,064 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=1440.0, ans=0.4325
2024-10-08 20:52:38,854 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=14.78 vs. limit=8.04
2024-10-08 20:52:39,479 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=1440.0, ans=0.4325
2024-10-08 20:52:39,767 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=23.35 vs. limit=8.58
2024-10-08 20:52:41,591 WARNING [optim.py:503] Scaling gradients by 0.021323474124073982, model_norm_threshold=2079517440.0
2024-10-08 20:52:41,746 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.778e+21, grad_sumsq=1.332e+21, orig_rms_sq=2.085e+00
2024-10-08 20:52:41,993 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=1443.3333333333333, ans=0.145875
2024-10-08 20:52:42,997 WARNING [optim.py:503] Scaling gradients by 0.031244883313775063, model_norm_threshold=2079517440.0
2024-10-08 20:52:43,156 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.893e+21, grad_sumsq=9.078e+20, orig_rms_sq=2.085e+00
2024-10-08 20:52:46,465 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=14.36 vs. limit=8.04125
2024-10-08 20:52:48,057 WARNING [optim.py:503] Scaling gradients by 0.002482950920239091, model_norm_threshold=2079517440.0
2024-10-08 20:52:48,214 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.426e+23, grad_sumsq=2.153e+25, orig_rms_sq=6.626e-03
2024-10-08 20:52:56,805 WARNING [optim.py:503] Scaling gradients by 0.06499330699443817, model_norm_threshold=2079517440.0
2024-10-08 20:52:56,961 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.267e+20, grad_sumsq=6.294e+22, orig_rms_sq=6.779e-03
2024-10-08 20:53:02,166 WARNING [optim.py:503] Scaling gradients by 0.08718147873878479, model_norm_threshold=2079517440.0
2024-10-08 20:53:02,324 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.398e+20, grad_sumsq=1.441e+20, orig_rms_sq=9.701e-01
2024-10-08 20:53:06,088 WARNING [optim.py:503] Scaling gradients by 0.06353764235973358, model_norm_threshold=2079517440.0
2024-10-08 20:53:06,243 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.101e+20, grad_sumsq=3.020e+22, orig_rms_sq=6.959e-03
2024-10-08 20:53:07,062 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.19 vs. limit=5.725
2024-10-08 20:53:07,526 INFO [train.py:1152] Epoch 1, batch 4350, loss[loss=0.7507, ctc_loss=1.045, attn_decoder_loss=0.6771, over 4851.00 frames. ], tot_loss[loss=0.7804, ctc_loss=1.05, attn_decoder_loss=0.7129, over 966230.05 frames. ], batch size: 21, lr: 4.19e-02,
2024-10-08 20:53:09,528 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.00 vs. limit=8.04375
2024-10-08 20:53:10,798 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=29.23 vs. limit=8.5875
2024-10-08 20:53:13,919 WARNING [optim.py:503] Scaling gradients by 0.013217615894973278, model_norm_threshold=2079517440.0
2024-10-08 20:53:14,076 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.492e+21, grad_sumsq=1.074e+24, orig_rms_sq=6.973e-03
2024-10-08 20:53:14,924 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=12.22 vs. limit=5.3625
2024-10-08 20:53:15,313 WARNING [optim.py:503] Scaling gradients by 0.02697763405740261, model_norm_threshold=2079517440.0
2024-10-08 20:53:15,470 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.724e+21, grad_sumsq=2.499e+23, orig_rms_sq=6.901e-03
2024-10-08 20:53:17,839 WARNING [optim.py:503] Scaling gradients by 0.035879891365766525, model_norm_threshold=2079517440.0
2024-10-08 20:53:17,997 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.618e+20, grad_sumsq=1.249e+23, orig_rms_sq=6.901e-03
2024-10-08 20:53:19,242 WARNING [optim.py:503] Scaling gradients by 0.0668109729886055, model_norm_threshold=2079517440.0
2024-10-08 20:53:19,398 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.110e+20, grad_sumsq=1.003e+20, orig_rms_sq=2.103e+00
2024-10-08 20:53:20,195 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=29.55 vs. limit=8.59
2024-10-08 20:53:20,650 WARNING [optim.py:503] Scaling gradients by 0.009283971041440964, model_norm_threshold=2079517440.0
2024-10-08 20:53:20,807 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.312e+22, grad_sumsq=5.396e+23, orig_rms_sq=4.285e-02
2024-10-08 20:53:28,453 WARNING [optim.py:503] Scaling gradients by 0.08391031622886658, model_norm_threshold=2079517440.0
2024-10-08 20:53:28,607 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.082e+20, grad_sumsq=5.411e+19, orig_rms_sq=2.000e+00
2024-10-08 20:53:32,750 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=1456.6666666666667, ans=0.145375
2024-10-08 20:53:34,982 WARNING [optim.py:503] Scaling gradients by 0.01743815280497074, model_norm_threshold=2079517440.0
2024-10-08 20:53:35,139 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.987e+21, grad_sumsq=8.980e+23, orig_rms_sq=6.667e-03
2024-10-08 20:53:36,307 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.04 vs. limit=5.7283333333333335
2024-10-08 20:53:40,093 WARNING [optim.py:503] Scaling gradients by 0.05505510792136192, model_norm_threshold=2079517440.0
2024-10-08 20:53:40,252 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.473e+20, grad_sumsq=5.267e+22, orig_rms_sq=6.593e-03
2024-10-08 20:53:44,350 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=7.488e+00
2024-10-08 20:53:45,259 WARNING [optim.py:503] Scaling gradients by 0.04843480885028839, model_norm_threshold=2079517440.0
2024-10-08 20:53:45,414 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.778e+20, grad_sumsq=1.887e+20, orig_rms_sq=2.002e+00
2024-10-08 20:53:46,123 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=8.75 vs. limit=8.0475
2024-10-08 20:53:50,159 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=9.60 vs. limit=8.595
2024-10-08 20:53:53,436 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=46.77 vs. limit=8.595
2024-10-08 20:53:55,194 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.47 vs. limit=8.595
2024-10-08 20:53:57,445 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=10.30 vs. limit=5.365
2024-10-08 20:53:57,996 WARNING [optim.py:503] Scaling gradients by 0.023742765188217163, model_norm_threshold=2079517440.0
2024-10-08 20:53:58,153 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.770e+21, grad_sumsq=6.536e+22, orig_rms_sq=4.239e-02
2024-10-08 20:54:00,632 WARNING [optim.py:503] Scaling gradients by 0.014912501908838749, model_norm_threshold=2079517440.0
2024-10-08 20:54:00,788 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.078e+21, grad_sumsq=9.620e+22, orig_rms_sq=4.239e-02
2024-10-08 20:54:01,580 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.67 vs. limit=5.365833333333334
2024-10-08 20:54:01,997 WARNING [optim.py:503] Scaling gradients by 0.044237952679395676, model_norm_threshold=2079517440.0
2024-10-08 20:54:02,155 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.534e+20, grad_sumsq=1.554e+22, orig_rms_sq=4.205e-02
2024-10-08 20:54:02,384 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=1463.3333333333333, ans=0.43140625
2024-10-08 20:54:05,417 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.59 vs. limit=5.7316666666666665
2024-10-08 20:54:12,092 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=49.03 vs. limit=8.05
2024-10-08 20:54:12,805 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.255e+07 9.525e+08 4.360e+09 1.528e+10 8.375e+11, threshold=8.720e+09, percent-clipped=60.0
2024-10-08 20:54:12,856 INFO [train.py:1152] Epoch 1, batch 4400, loss[loss=0.7779, ctc_loss=1.01, attn_decoder_loss=0.7199, over 4735.00 frames. ], tot_loss[loss=0.7813, ctc_loss=1.051, attn_decoder_loss=0.7138, over 965771.58 frames. ], batch size: 26, lr: 4.18e-02,
2024-10-08 20:54:13,867 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=8.73 vs. limit=8.6
2024-10-08 20:54:14,815 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=41.82 vs. limit=8.6
2024-10-08 20:54:15,256 WARNING [optim.py:503] Scaling gradients by 0.044140301644802094, model_norm_threshold=8719795200.0
2024-10-08 20:54:15,412 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.043e+22, grad_sumsq=2.513e+23, orig_rms_sq=4.149e-02
2024-10-08 20:54:17,612 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.89 vs. limit=5.366666666666667
2024-10-08 20:54:18,145 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=1466.6666666666667, ans=0.43125
2024-10-08 20:54:19,230 WARNING [optim.py:503] Scaling gradients by 0.09548909217119217, model_norm_threshold=8719795200.0
2024-10-08 20:54:19,385 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.992e+21, grad_sumsq=4.747e+22, orig_rms_sq=4.196e-02
2024-10-08 20:54:20,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten.whitening_limit, batch_count=1466.6666666666667, ans=8.05
2024-10-08 20:54:24,324 WARNING [optim.py:503] Scaling gradients by 0.08119211345911026, model_norm_threshold=8719795200.0
2024-10-08 20:54:24,485 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.859e+21, grad_sumsq=2.380e+21, orig_rms_sq=2.042e+00
2024-10-08 20:54:31,069 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=1470.0, ans=0.144875
2024-10-08 20:54:40,989 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=1473.3333333333333, ans=0.4309375
2024-10-08 20:54:57,922 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.whiten.whitening_limit, batch_count=1476.6666666666667, ans=4.5906666666666665
2024-10-08 20:55:02,033 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.47 vs. limit=5.738333333333333
2024-10-08 20:55:07,592 WARNING [optim.py:503] Scaling gradients by 0.00077935861190781, model_norm_threshold=8719795200.0
2024-10-08 20:55:07,748 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.846e+25, grad_sumsq=4.113e+27, orig_rms_sq=6.919e-03
2024-10-08 20:55:10,172 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=11.75 vs. limit=8.055
2024-10-08 20:55:14,441 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1480.0, ans=0.2852
2024-10-08 20:55:18,152 INFO [train.py:1152] Epoch 1, batch 4450, loss[loss=0.7566, ctc_loss=1.024, attn_decoder_loss=0.6899, over 4883.00 frames. ], tot_loss[loss=0.7812, ctc_loss=1.051, attn_decoder_loss=0.7137, over 966090.40 frames. ], batch size: 19, lr: 4.17e-02,
2024-10-08 20:55:21,359 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.34 vs. limit=3.2225
2024-10-08 20:55:22,431 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.37 vs. limit=5.741666666666666
2024-10-08 20:55:23,729 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.97 vs. limit=8.6125
2024-10-08 20:55:24,304 WARNING [optim.py:503] Scaling gradients by 0.03755172714591026, model_norm_threshold=8719795200.0
2024-10-08 20:55:24,459 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.803e+22, grad_sumsq=2.535e+24, orig_rms_sq=7.115e-03
2024-10-08 20:55:25,007 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=54.01 vs. limit=8.05625
2024-10-08 20:55:25,694 WARNING [optim.py:503] Scaling gradients by 0.053601060062646866, model_norm_threshold=8719795200.0
2024-10-08 20:55:25,851 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.828e+21, grad_sumsq=1.473e+23, orig_rms_sq=3.956e-02
2024-10-08 20:55:27,936 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.48 vs. limit=5.370833333333334
2024-10-08 20:55:30,550 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=30.68 vs. limit=8.0575
2024-10-08 20:55:31,687 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=23.53 vs. limit=8.0575
2024-10-08 20:55:40,620 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=39.21 vs. limit=8.0575
2024-10-08 20:55:41,173 WARNING [optim.py:503] Scaling gradients by 0.037843454629182816, model_norm_threshold=8719795200.0
2024-10-08 20:55:41,329 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.480e+22, grad_sumsq=3.689e+23, orig_rms_sq=4.013e-02
2024-10-08 20:55:42,121 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=40.62 vs. limit=8.0575
2024-10-08 20:55:43,970 WARNING [optim.py:503] Scaling gradients by 0.0593092143535614, model_norm_threshold=8719795200.0
2024-10-08 20:55:44,128 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.971e+21, grad_sumsq=1.238e+23, orig_rms_sq=4.013e-02
2024-10-08 20:55:46,513 WARNING [optim.py:503] Scaling gradients by 0.028556248173117638, model_norm_threshold=8719795200.0
2024-10-08 20:55:46,670 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.55, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.125e+22, grad_sumsq=6.903e+24, orig_rms_sq=7.425e-03
2024-10-08 20:55:48,633 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.18 vs. limit=8.05875
2024-10-08 20:55:51,050 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=33.20 vs. limit=5.745
2024-10-08 20:55:54,344 WARNING [optim.py:503] Scaling gradients by 0.01728675328195095, model_norm_threshold=8719795200.0
2024-10-08 20:55:54,501 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.558e+22, grad_sumsq=4.243e+22, orig_rms_sq=1.310e+00
2024-10-08 20:55:58,439 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1493.3333333333333, ans=0.28506666666666663
2024-10-08 20:56:00,529 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.46 vs. limit=5.373333333333333
2024-10-08 20:56:04,381 WARNING [optim.py:503] Scaling gradients by 0.0944189578294754, model_norm_threshold=8719795200.0
2024-10-08 20:56:04,539 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.877e+21, grad_sumsq=2.571e+23, orig_rms_sq=7.299e-03
2024-10-08 20:56:05,769 WARNING [optim.py:503] Scaling gradients by 0.0878089889883995, model_norm_threshold=8719795200.0
2024-10-08 20:56:05,929 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.806e+21, grad_sumsq=1.318e+21, orig_rms_sq=2.129e+00
2024-10-08 20:56:09,287 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.96 vs. limit=5.748333333333333
2024-10-08 20:56:09,417 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.08 vs. limit=3.2245
2024-10-08 20:56:09,771 WARNING [optim.py:503] Scaling gradients by 0.014800910837948322, model_norm_threshold=8719795200.0
2024-10-08 20:56:09,928 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.460e+23, grad_sumsq=6.891e+22, orig_rms_sq=2.119e+00
2024-10-08 20:56:12,247 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=16.52 vs. limit=8.06125
2024-10-08 20:56:14,082 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=1496.6666666666667, ans=0.066325
2024-10-08 20:56:14,539 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=28.51 vs. limit=8.06125
2024-10-08 20:56:15,287 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=1496.6666666666667, ans=5.935416666666667
2024-10-08 20:56:16,013 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=15.29 vs. limit=8.6225
2024-10-08 20:56:22,579 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.863e+07 9.798e+08 6.943e+09 3.167e+10 1.119e+13, threshold=1.389e+10, percent-clipped=44.0
2024-10-08 20:56:22,631 INFO [train.py:1152] Epoch 1, batch 4500, loss[loss=0.7236, ctc_loss=0.9981, attn_decoder_loss=0.6549, over 4861.00 frames. ], tot_loss[loss=0.7833, ctc_loss=1.054, attn_decoder_loss=0.7156, over 966208.89 frames. ], batch size: 28, lr: 4.17e-02,
2024-10-08 20:56:29,728 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.95 vs. limit=8.625
2024-10-08 20:56:30,597 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.52 vs. limit=5.75
2024-10-08 20:56:36,305 WARNING [optim.py:503] Scaling gradients by 0.09396446496248245, model_norm_threshold=13885245440.0
2024-10-08 20:56:36,462 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.463e+21, grad_sumsq=1.404e+23, orig_rms_sq=3.891e-02
2024-10-08 20:56:46,142 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=17.31 vs. limit=8.06375
2024-10-08 20:56:46,580 WARNING [optim.py:503] Scaling gradients by 0.027418406680226326, model_norm_threshold=13885245440.0
2024-10-08 20:56:46,736 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.161e+22, grad_sumsq=2.449e+22, orig_rms_sq=2.108e+00
2024-10-08 20:56:48,773 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=6.63 vs. limit=8.065
2024-10-08 20:56:53,154 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=1506.6666666666667, ans=0.06609999999999999
2024-10-08 20:56:56,528 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=6.86 vs. limit=8.065
2024-10-08 20:57:00,073 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=24.82 vs. limit=8.6325
2024-10-08 20:57:02,044 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=1510.0, ans=0.143375
2024-10-08 20:57:03,014 WARNING [optim.py:503] Scaling gradients by 0.016032449901103973, model_norm_threshold=13885245440.0
2024-10-08 20:57:03,171 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.274e+23, grad_sumsq=4.637e+25, orig_rms_sq=7.061e-03
2024-10-08 20:57:05,613 WARNING [optim.py:503] Scaling gradients by 0.009933107532560825, model_norm_threshold=13885245440.0
2024-10-08 20:57:05,769 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.635e+23, grad_sumsq=2.631e+23, orig_rms_sq=2.142e+00
2024-10-08 20:57:10,627 WARNING [optim.py:503] Scaling gradients by 0.03588631749153137, model_norm_threshold=13885245440.0
2024-10-08 20:57:10,784 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.488e+22, grad_sumsq=3.071e+22, orig_rms_sq=2.113e+00
2024-10-08 20:57:14,473 WARNING [optim.py:503] Scaling gradients by 0.08595751971006393, model_norm_threshold=13885245440.0
2024-10-08 20:57:14,630 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.69, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.793e+22, grad_sumsq=2.553e+24, orig_rms_sq=7.024e-03
2024-10-08 20:57:18,030 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.86 vs. limit=5.756666666666667
2024-10-08 20:57:18,377 WARNING [optim.py:503] Scaling gradients by 0.027821317315101624, model_norm_threshold=13885245440.0
2024-10-08 20:57:18,534 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.479e+22, grad_sumsq=3.591e+22, orig_rms_sq=2.083e+00
2024-10-08 20:57:18,782 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=1513.3333333333333, ans=0.4290625
2024-10-08 20:57:21,332 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=1513.3333333333333, ans=0.4290625
2024-10-08 20:57:21,849 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=28.37 vs. limit=8.635
2024-10-08 20:57:25,649 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=55.91 vs. limit=8.06875
2024-10-08 20:57:26,295 INFO [train.py:1152] Epoch 1, batch 4550, loss[loss=0.7632, ctc_loss=1.042, attn_decoder_loss=0.6936, over 4846.00 frames. ], tot_loss[loss=0.7815, ctc_loss=1.053, attn_decoder_loss=0.7137, over 966014.55 frames. ], batch size: 20, lr: 4.16e-02,
2024-10-08 20:57:33,958 WARNING [optim.py:503] Scaling gradients by 0.00014840623771306127, model_norm_threshold=13885245440.0
2024-10-08 20:57:34,114 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.151e+27, grad_sumsq=4.522e+29, orig_rms_sq=6.968e-03
2024-10-08 20:57:41,721 WARNING [optim.py:503] Scaling gradients by 0.0012742583639919758, model_norm_threshold=13885245440.0
2024-10-08 20:57:41,876 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.253e+25, grad_sumsq=2.128e+25, orig_rms_sq=1.999e+00
2024-10-08 20:57:43,102 WARNING [optim.py:503] Scaling gradients by 0.07408371567726135, model_norm_threshold=13885245440.0
2024-10-08 20:57:43,257 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.369e+22, grad_sumsq=6.851e+21, orig_rms_sq=1.999e+00
2024-10-08 20:57:46,312 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=17.98 vs. limit=8.07
2024-10-08 20:57:48,216 WARNING [optim.py:503] Scaling gradients by 0.088072769343853, model_norm_threshold=13885245440.0
2024-10-08 20:57:48,373 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.939e+21, grad_sumsq=1.788e+23, orig_rms_sq=3.880e-02
2024-10-08 20:57:49,596 WARNING [optim.py:503] Scaling gradients by 0.05161426216363907, model_norm_threshold=13885245440.0
2024-10-08 20:57:49,751 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.140e+22, grad_sumsq=5.674e+23, orig_rms_sq=3.770e-02
2024-10-08 20:57:50,493 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=26.58 vs. limit=8.07
2024-10-08 20:57:50,964 WARNING [optim.py:503] Scaling gradients by 0.0016804657643660903, model_norm_threshold=13885245440.0
2024-10-08 20:57:51,123 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.988e+25, grad_sumsq=1.017e+25, orig_rms_sq=1.955e+00
2024-10-08 20:57:57,549 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1523.3333333333333, ans=0.28476666666666667
2024-10-08 20:58:01,626 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.02 vs. limit=5.761666666666667
2024-10-08 20:58:04,905 WARNING [optim.py:503] Scaling gradients by 0.023871280252933502, model_norm_threshold=13885245440.0
2024-10-08 20:58:05,061 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.329e+22, grad_sumsq=7.814e+24, orig_rms_sq=6.819e-03
2024-10-08 20:58:11,830 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=9.60 vs. limit=5.381666666666667
2024-10-08 20:58:16,386 WARNING [optim.py:503] Scaling gradients by 0.04037366434931755, model_norm_threshold=13885245440.0
2024-10-08 20:58:16,540 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.903e+22, grad_sumsq=4.322e+24, orig_rms_sq=6.717e-03
2024-10-08 20:58:19,506 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=3.99 vs. limit=4.612
2024-10-08 20:58:20,681 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten.whitening_limit, batch_count=1530.0, ans=8.6475
2024-10-08 20:58:22,607 WARNING [optim.py:503] Scaling gradients by 0.09527470171451569, model_norm_threshold=13885245440.0
2024-10-08 20:58:22,765 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.514e+21, grad_sumsq=3.873e+21, orig_rms_sq=1.424e+00
2024-10-08 20:58:24,801 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.08 vs. limit=8.07375
2024-10-08 20:58:28,494 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.86 vs. limit=5.3825
2024-10-08 20:58:28,662 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.93 vs. limit=8.6475
2024-10-08 20:58:30,583 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.843e+07 1.803e+09 1.065e+10 5.441e+10 9.356e+13, threshold=2.130e+10, percent-clipped=45.0
2024-10-08 20:58:30,634 INFO [train.py:1152] Epoch 1, batch 4600, loss[loss=0.8466, ctc_loss=1.146, attn_decoder_loss=0.7719, over 4767.00 frames. ], tot_loss[loss=0.7815, ctc_loss=1.053, attn_decoder_loss=0.7138, over 966350.88 frames. ], batch size: 45, lr: 4.15e-02,
2024-10-08 20:58:35,964 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1533.3333333333333, ans=0.2846666666666667
2024-10-08 20:58:36,909 WARNING [optim.py:503] Scaling gradients by 0.02725047618150711, model_norm_threshold=21304117248.0
2024-10-08 20:58:37,066 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.233e+23, grad_sumsq=7.964e+22, orig_rms_sq=1.548e+00
2024-10-08 20:58:40,119 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=13.70 vs. limit=8.075
2024-10-08 20:58:46,462 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=10.88 vs. limit=5.384166666666666
2024-10-08 20:58:48,174 WARNING [optim.py:503] Scaling gradients by 0.020777298137545586, model_norm_threshold=21304117248.0
2024-10-08 20:58:48,332 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.083e+23, grad_sumsq=1.064e+23, orig_rms_sq=1.957e+00
2024-10-08 20:58:49,069 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.17 vs. limit=8.07625
2024-10-08 20:58:50,348 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=55.75 vs. limit=8.07625
2024-10-08 20:59:00,794 WARNING [optim.py:503] Scaling gradients by 0.017065545544028282, model_norm_threshold=21304117248.0
2024-10-08 20:59:00,952 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.308e+23, grad_sumsq=3.202e+23, orig_rms_sq=1.970e+00
2024-10-08 20:59:06,070 WARNING [optim.py:503] Scaling gradients by 0.084535151720047, model_norm_threshold=21304117248.0
2024-10-08 20:59:06,226 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.290e+22, grad_sumsq=3.628e+23, orig_rms_sq=3.555e-02
2024-10-08 20:59:06,809 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=73.64 vs. limit=8.0775
2024-10-08 20:59:11,229 WARNING [optim.py:503] Scaling gradients by 0.03026460111141205, model_norm_threshold=21304117248.0
2024-10-08 20:59:11,383 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.614e+22, grad_sumsq=4.084e+22, orig_rms_sq=2.109e+00
2024-10-08 20:59:13,418 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.85 vs. limit=5.7716666666666665
2024-10-08 20:59:22,535 WARNING [optim.py:503] Scaling gradients by 0.05746375769376755, model_norm_threshold=21304117248.0
2024-10-08 20:59:22,691 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.659e+22, grad_sumsq=1.011e+24, orig_rms_sq=3.619e-02
2024-10-08 20:59:31,085 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.53 vs. limit=8.66
2024-10-08 20:59:33,807 WARNING [optim.py:503] Scaling gradients by 0.016709011048078537, model_norm_threshold=21304117248.0
2024-10-08 20:59:33,963 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.375e+23, grad_sumsq=2.541e+23, orig_rms_sq=2.116e+00
2024-10-08 20:59:34,020 INFO [train.py:1152] Epoch 1, batch 4650, loss[loss=0.8266, ctc_loss=1.104, attn_decoder_loss=0.7572, over 4824.00 frames. ], tot_loss[loss=0.7825, ctc_loss=1.053, attn_decoder_loss=0.7148, over 965794.19 frames. ], batch size: 36, lr: 4.15e-02,
2024-10-08 20:59:36,114 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.84 vs. limit=8.6625
2024-10-08 20:59:52,986 WARNING [optim.py:503] Scaling gradients by 0.04422313719987869, model_norm_threshold=21304117248.0
2024-10-08 20:59:53,144 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.145e+22, grad_sumsq=2.264e+22, orig_rms_sq=2.273e+00
2024-10-08 20:59:55,067 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.04 vs. limit=5.776666666666666
2024-10-08 21:00:03,028 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=1556.6666666666667, ans=0.2844333333333333
2024-10-08 21:00:05,644 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1556.6666666666667, ans=0.2844333333333333
2024-10-08 21:00:13,927 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.71 vs. limit=8.67
2024-10-08 21:00:16,830 WARNING [optim.py:503] Scaling gradients by 0.006871504243463278, model_norm_threshold=21304117248.0
2024-10-08 21:00:16,989 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.128e+24, grad_sumsq=5.763e+25, orig_rms_sq=3.693e-02
2024-10-08 21:00:17,141 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=1560.0, ans=0.426875
2024-10-08 21:00:18,867 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.97 vs. limit=5.39
2024-10-08 21:00:21,627 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.63 vs. limit=8.67
2024-10-08 21:00:21,865 WARNING [optim.py:503] Scaling gradients by 0.006579835433512926, model_norm_threshold=21304117248.0
2024-10-08 21:00:22,022 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.122e+24, grad_sumsq=8.403e+25, orig_rms_sq=3.716e-02
2024-10-08 21:00:26,215 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=1563.3333333333333, ans=0.42671875
2024-10-08 21:00:26,231 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=1563.3333333333333, ans=0.42671875
2024-10-08 21:00:34,247 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=5.81 vs. limit=8.08625
2024-10-08 21:00:34,544 WARNING [optim.py:503] Scaling gradients by 0.08785577863454819, model_norm_threshold=21304117248.0
2024-10-08 21:00:34,700 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.482e+22, grad_sumsq=2.115e+24, orig_rms_sq=7.006e-03
2024-10-08 21:00:37,328 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.918e+07 3.522e+09 1.179e+10 9.721e+10 3.238e+12, threshold=2.358e+10, percent-clipped=40.0
2024-10-08 21:00:37,383 INFO [train.py:1152] Epoch 1, batch 4700, loss[loss=0.7708, ctc_loss=1.02, attn_decoder_loss=0.7086, over 4940.00 frames. ], tot_loss[loss=0.7829, ctc_loss=1.052, attn_decoder_loss=0.7157, over 965687.48 frames. ], batch size: 19, lr: 4.14e-02,
2024-10-08 21:00:39,417 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.03 vs. limit=8.0875
2024-10-08 21:00:44,599 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.06 vs. limit=5.391666666666667
2024-10-08 21:00:45,023 WARNING [optim.py:503] Scaling gradients by 0.06973209977149963, model_norm_threshold=23577307136.0
2024-10-08 21:00:45,180 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.075e+22, grad_sumsq=7.266e+24, orig_rms_sq=6.984e-03
2024-10-08 21:00:51,629 WARNING [optim.py:503] Scaling gradients by 0.029979286715388298, model_norm_threshold=23577307136.0
2024-10-08 21:00:51,786 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.742e+23, grad_sumsq=7.634e+22, orig_rms_sq=2.282e+00
2024-10-08 21:00:51,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=1570.0, ans=0.141125
2024-10-08 21:01:00,879 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=1570.0, ans=0.42640625
2024-10-08 21:01:01,930 WARNING [optim.py:503] Scaling gradients by 0.00093492167070508, model_norm_threshold=23577307136.0
2024-10-08 21:01:02,086 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.324e+26, grad_sumsq=4.908e+25, orig_rms_sq=2.697e+00
2024-10-08 21:01:03,358 WARNING [optim.py:503] Scaling gradients by 0.07307884842157364, model_norm_threshold=23577307136.0
2024-10-08 21:01:03,516 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.049e+22, grad_sumsq=5.343e+23, orig_rms_sq=3.835e-02
2024-10-08 21:01:04,744 WARNING [optim.py:503] Scaling gradients by 0.07052423804998398, model_norm_threshold=23577307136.0
2024-10-08 21:01:04,901 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.516e+22, grad_sumsq=9.168e+23, orig_rms_sq=3.835e-02
2024-10-08 21:01:07,321 WARNING [optim.py:503] Scaling gradients by 0.07809283584356308, model_norm_threshold=23577307136.0
2024-10-08 21:01:07,478 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.674e+22, grad_sumsq=9.580e+23, orig_rms_sq=3.835e-02
2024-10-08 21:01:10,324 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=1573.3333333333333, ans=0.14100000000000001
2024-10-08 21:01:18,686 WARNING [optim.py:503] Scaling gradients by 0.03849392756819725, model_norm_threshold=23577307136.0
2024-10-08 21:01:18,843 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.039e+23, grad_sumsq=1.706e+25, orig_rms_sq=6.091e-03
2024-10-08 21:01:20,196 WARNING [optim.py:503] Scaling gradients by 0.0924891009926796, model_norm_threshold=23577307136.0
2024-10-08 21:01:20,365 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.064e+22, grad_sumsq=1.747e+24, orig_rms_sq=6.091e-03
2024-10-08 21:01:21,544 WARNING [optim.py:503] Scaling gradients by 0.05199090391397476, model_norm_threshold=23577307136.0
2024-10-08 21:01:21,705 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.405e+22, grad_sumsq=2.062e+22, orig_rms_sq=2.137e+00
2024-10-08 21:01:36,184 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.87 vs. limit=4.632
2024-10-08 21:01:41,637 INFO [train.py:1152] Epoch 1, batch 4750, loss[loss=0.8119, ctc_loss=1.131, attn_decoder_loss=0.7322, over 4769.00 frames. ], tot_loss[loss=0.7831, ctc_loss=1.053, attn_decoder_loss=0.7156, over 965640.65 frames. ], batch size: 45, lr: 4.14e-02,
2024-10-08 21:01:42,910 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=1583.3333333333333, ans=0.8445833333333334
2024-10-08 21:01:43,350 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.17 vs. limit=3.2375
2024-10-08 21:01:45,178 WARNING [optim.py:503] Scaling gradients by 0.09911315143108368, model_norm_threshold=23577307136.0
2024-10-08 21:01:45,334 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.296e+22, grad_sumsq=2.191e+24, orig_rms_sq=5.916e-03
2024-10-08 21:01:46,533 WARNING [optim.py:503] Scaling gradients by 0.04348981752991676, model_norm_threshold=23577307136.0
2024-10-08 21:01:46,692 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.727e+22, grad_sumsq=1.137e+25, orig_rms_sq=5.916e-03
2024-10-08 21:01:48,718 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=22.83 vs. limit=8.09375
2024-10-08 21:01:49,963 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=20.39 vs. limit=8.09375
2024-10-08 21:01:50,295 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.64 vs. limit=8.6875
2024-10-08 21:01:55,396 WARNING [optim.py:503] Scaling gradients by 0.009605028666555882, model_norm_threshold=23577307136.0
2024-10-08 21:01:55,553 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.894e+24, grad_sumsq=8.078e+23, orig_rms_sq=2.345e+00
2024-10-08 21:01:58,857 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=33.13 vs. limit=8.69
2024-10-08 21:01:58,954 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=11.33 vs. limit=8.095
2024-10-08 21:01:59,360 WARNING [optim.py:503] Scaling gradients by 0.04591882601380348, model_norm_threshold=23577307136.0
2024-10-08 21:01:59,516 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.917e+22, grad_sumsq=3.670e+22, orig_rms_sq=2.429e+00
2024-10-08 21:02:13,655 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=1590.0, ans=0.0900625
2024-10-08 21:02:16,021 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=1590.0, ans=0.22385000000000002
2024-10-08 21:02:16,686 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.38 vs. limit=3.2385
2024-10-08 21:02:23,546 WARNING [optim.py:503] Scaling gradients by 0.030424240976572037, model_norm_threshold=23577307136.0
2024-10-08 21:02:23,700 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.999e+23, grad_sumsq=8.748e+22, orig_rms_sq=2.285e+00
2024-10-08 21:02:45,450 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.541e+08 6.544e+09 1.967e+10 9.388e+10 2.522e+13, threshold=3.934e+10, percent-clipped=47.0
2024-10-08 21:02:45,502 INFO [train.py:1152] Epoch 1, batch 4800, loss[loss=0.7521, ctc_loss=1.055, attn_decoder_loss=0.6763, over 4873.00 frames. ], tot_loss[loss=0.7807, ctc_loss=1.051, attn_decoder_loss=0.7131, over 965918.29 frames. ], batch size: 22, lr: 4.13e-02,
2024-10-08 21:02:45,602 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=1600.0, ans=0.064
2024-10-08 21:02:45,987 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=21.79 vs. limit=8.7
2024-10-08 21:02:47,662 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.04 vs. limit=5.4
2024-10-08 21:02:49,426 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=1600.0, ans=0.14
2024-10-08 21:02:50,481 WARNING [optim.py:503] Scaling gradients by 0.013976924121379852, model_norm_threshold=39343804416.0
2024-10-08 21:02:50,636 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.893e+24, grad_sumsq=3.188e+26, orig_rms_sq=5.937e-03
2024-10-08 21:02:51,794 WARNING [optim.py:503] Scaling gradients by 0.015640372410416603, model_norm_threshold=39343804416.0
2024-10-08 21:02:51,950 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.553e+24, grad_sumsq=4.301e+26, orig_rms_sq=5.937e-03
2024-10-08 21:02:55,692 WARNING [optim.py:503] Scaling gradients by 0.06422062963247299, model_norm_threshold=39343804416.0
2024-10-08 21:02:55,845 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.811e+23, grad_sumsq=2.986e+25, orig_rms_sq=6.065e-03
2024-10-08 21:02:57,834 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=4.49 vs. limit=4.641333333333334
2024-10-08 21:03:03,223 WARNING [optim.py:503] Scaling gradients by 0.007802488282322884, model_norm_threshold=39343804416.0
2024-10-08 21:03:03,380 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.645e+24, grad_sumsq=1.062e+27, orig_rms_sq=6.256e-03
2024-10-08 21:03:04,600 WARNING [optim.py:503] Scaling gradients by 0.041312165558338165, model_norm_threshold=39343804416.0
2024-10-08 21:03:04,757 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.900e+23, grad_sumsq=5.353e+24, orig_rms_sq=3.550e-02
2024-10-08 21:03:24,909 WARNING [optim.py:503] Scaling gradients by 0.0013850870309397578, model_norm_threshold=39343804416.0
2024-10-08 21:03:25,065 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.942e+26, grad_sumsq=1.527e+26, orig_rms_sq=2.581e+00
2024-10-08 21:03:31,418 WARNING [optim.py:503] Scaling gradients by 0.06367887556552887, model_norm_threshold=39343804416.0
2024-10-08 21:03:31,573 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.986e+22, grad_sumsq=2.584e+24, orig_rms_sq=3.477e-02
2024-10-08 21:03:32,300 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=31.49 vs. limit=8.7075
2024-10-08 21:03:32,513 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.88 vs. limit=8.7075
2024-10-08 21:03:35,275 WARNING [optim.py:503] Scaling gradients by 0.013548916205763817, model_norm_threshold=39343804416.0
2024-10-08 21:03:35,430 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.812e+24, grad_sumsq=1.078e+24, orig_rms_sq=2.608e+00
2024-10-08 21:03:39,171 WARNING [optim.py:503] Scaling gradients by 0.05195556953549385, model_norm_threshold=39343804416.0
2024-10-08 21:03:39,328 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.410e+23, grad_sumsq=4.082e+24, orig_rms_sq=3.455e-02
2024-10-08 21:03:41,385 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.23 vs. limit=5.806666666666667
2024-10-08 21:03:42,670 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.24 vs. limit=3.242
2024-10-08 21:03:43,784 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=17.13 vs. limit=8.105
2024-10-08 21:03:45,608 WARNING [optim.py:503] Scaling gradients by 0.06263844668865204, model_norm_threshold=39343804416.0
2024-10-08 21:03:45,763 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.788e+23, grad_sumsq=2.764e+25, orig_rms_sq=6.469e-03
2024-10-08 21:03:49,422 WARNING [optim.py:503] Scaling gradients by 0.049798090010881424, model_norm_threshold=39343804416.0
2024-10-08 21:03:49,578 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.681e+23, grad_sumsq=4.317e+25, orig_rms_sq=6.210e-03
2024-10-08 21:03:49,636 INFO [train.py:1152] Epoch 1, batch 4850, loss[loss=0.7439, ctc_loss=0.9721, attn_decoder_loss=0.6868, over 4848.00 frames. ], tot_loss[loss=0.7793, ctc_loss=1.049, attn_decoder_loss=0.7119, over 966613.53 frames. ], batch size: 28, lr: 4.12e-02,
2024-10-08 21:03:52,627 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=18.17 vs. limit=8.10625
2024-10-08 21:03:52,916 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.46 vs. limit=8.7125
2024-10-08 21:03:53,179 WARNING [optim.py:503] Scaling gradients by 0.06878261268138885, model_norm_threshold=39343804416.0
2024-10-08 21:03:53,336 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.167e+22, grad_sumsq=1.560e+25, orig_rms_sq=5.876e-03
2024-10-08 21:04:00,387 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.34 vs. limit=5.404166666666667
2024-10-08 21:04:00,988 WARNING [optim.py:503] Scaling gradients by 0.009917528368532658, model_norm_threshold=39343804416.0
2024-10-08 21:04:01,951 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.560e+24, grad_sumsq=2.759e+24, orig_rms_sq=2.378e+00
2024-10-08 21:04:05,512 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=16.27 vs. limit=8.715
2024-10-08 21:04:06,941 WARNING [optim.py:503] Scaling gradients by 0.08642357587814331, model_norm_threshold=39343804416.0
2024-10-08 21:04:07,096 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.768e+22, grad_sumsq=2.002e+22, orig_rms_sq=3.381e+00
2024-10-08 21:04:13,305 WARNING [optim.py:503] Scaling gradients by 0.018676938489079475, model_norm_threshold=39343804416.0
2024-10-08 21:04:13,460 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.010e+24, grad_sumsq=3.556e+26, orig_rms_sq=5.653e-03
2024-10-08 21:04:18,443 WARNING [optim.py:503] Scaling gradients by 0.0014550408814102411, model_norm_threshold=39343804416.0
2024-10-08 21:04:18,599 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.795e+26, grad_sumsq=1.232e+26, orig_rms_sq=2.269e+00
2024-10-08 21:04:28,478 WARNING [optim.py:503] Scaling gradients by 0.08944552391767502, model_norm_threshold=39343804416.0
2024-10-08 21:04:28,635 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.020e+22, grad_sumsq=1.523e+24, orig_rms_sq=3.297e-02
2024-10-08 21:04:29,898 WARNING [optim.py:503] Scaling gradients by 0.019039813429117203, model_norm_threshold=39343804416.0
2024-10-08 21:04:30,054 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.307e+24, grad_sumsq=1.017e+24, orig_rms_sq=2.268e+00
2024-10-08 21:04:32,023 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.66 vs. limit=4.650666666666667
2024-10-08 21:04:32,444 WARNING [optim.py:503] Scaling gradients by 0.07260927557945251, model_norm_threshold=39343804416.0
2024-10-08 21:04:32,601 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.430e+22, grad_sumsq=1.824e+22, orig_rms_sq=3.525e+00
2024-10-08 21:04:36,612 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=1626.6666666666667, ans=0.42375
2024-10-08 21:04:38,979 WARNING [optim.py:503] Scaling gradients by 0.008556285873055458, model_norm_threshold=39343804416.0
2024-10-08 21:04:39,135 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.234e+24, grad_sumsq=2.722e+24, orig_rms_sq=2.290e+00
2024-10-08 21:04:42,925 WARNING [optim.py:503] Scaling gradients by 0.03523138910531998, model_norm_threshold=39343804416.0
2024-10-08 21:04:43,081 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.500e+23, grad_sumsq=8.929e+25, orig_rms_sq=6.159e-03
2024-10-08 21:04:43,237 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=1630.0, ans=0.29625
2024-10-08 21:04:45,865 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1630.0, ans=0.2837
2024-10-08 21:04:46,826 WARNING [optim.py:503] Scaling gradients by 0.04565175995230675, model_norm_threshold=39343804416.0
2024-10-08 21:04:46,983 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.55, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.073e+23, grad_sumsq=1.755e+23, orig_rms_sq=2.321e+00
2024-10-08 21:04:50,341 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=28.82 vs. limit=8.11125
2024-10-08 21:04:54,765 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.067e+07 5.989e+09 4.730e+10 2.810e+11 2.841e+13, threshold=9.460e+10, percent-clipped=55.0
2024-10-08 21:04:54,816 INFO [train.py:1152] Epoch 1, batch 4900, loss[loss=0.7129, ctc_loss=1.001, attn_decoder_loss=0.6409, over 4854.00 frames. ], tot_loss[loss=0.7784, ctc_loss=1.049, attn_decoder_loss=0.7107, over 967195.10 frames. ], batch size: 21, lr: 4.12e-02,
2024-10-08 21:04:55,934 WARNING [optim.py:503] Scaling gradients by 0.036880869418382645, model_norm_threshold=94603526144.0
2024-10-08 21:04:56,090 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.904e+24, grad_sumsq=7.969e+23, orig_rms_sq=2.389e+00
2024-10-08 21:05:03,852 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=1633.3333333333333, ans=0.13875
2024-10-08 21:05:04,923 WARNING [optim.py:503] Scaling gradients by 0.08389966934919357, model_norm_threshold=94603526144.0
2024-10-08 21:05:05,082 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.230e+23, grad_sumsq=6.929e+25, orig_rms_sq=6.104e-03
2024-10-08 21:05:40,038 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.51 vs. limit=8.11625
2024-10-08 21:05:44,113 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.44 vs. limit=8.7325
2024-10-08 21:05:45,501 WARNING [optim.py:503] Scaling gradients by 0.0435255691409111, model_norm_threshold=94603526144.0
2024-10-08 21:05:45,657 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.669e+23, grad_sumsq=2.180e+25, orig_rms_sq=3.518e-02
2024-10-08 21:05:46,849 WARNING [optim.py:503] Scaling gradients by 0.08863171190023422, model_norm_threshold=94603526144.0
2024-10-08 21:05:47,006 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.231e+23, grad_sumsq=6.343e+24, orig_rms_sq=3.518e-02
2024-10-08 21:05:50,327 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.79 vs. limit=8.735
2024-10-08 21:05:50,643 WARNING [optim.py:503] Scaling gradients by 0.06631235778331757, model_norm_threshold=94603526144.0
2024-10-08 21:05:50,798 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.871e+23, grad_sumsq=1.362e+26, orig_rms_sq=5.778e-03
2024-10-08 21:05:51,987 WARNING [optim.py:503] Scaling gradients by 0.012659109197556973, model_norm_threshold=94603526144.0
2024-10-08 21:05:52,142 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.413e+25, grad_sumsq=2.445e+27, orig_rms_sq=5.778e-03
2024-10-08 21:05:54,521 WARNING [optim.py:503] Scaling gradients by 0.03678041324019432, model_norm_threshold=94603526144.0
2024-10-08 21:05:54,678 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.349e+24, grad_sumsq=5.795e+23, orig_rms_sq=2.327e+00
2024-10-08 21:05:56,780 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.70 vs. limit=5.823333333333333
2024-10-08 21:05:58,458 INFO [train.py:1152] Epoch 1, batch 4950, loss[loss=0.7911, ctc_loss=1.1, attn_decoder_loss=0.7138, over 4775.00 frames. ], tot_loss[loss=0.7802, ctc_loss=1.051, attn_decoder_loss=0.7125, over 966685.70 frames. ], batch size: 53, lr: 4.11e-02,
2024-10-08 21:06:02,207 WARNING [optim.py:503] Scaling gradients by 0.0033900877460837364, model_norm_threshold=94603526144.0
2024-10-08 21:06:02,363 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.881e+26, grad_sumsq=4.912e+28, orig_rms_sq=5.864e-03
2024-10-08 21:06:03,672 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=1650.0, ans=0.84225
2024-10-08 21:06:06,103 WARNING [optim.py:503] Scaling gradients by 0.04923753812909126, model_norm_threshold=94603526144.0
2024-10-08 21:06:06,259 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.134e+24, grad_sumsq=4.791e+23, orig_rms_sq=2.367e+00
2024-10-08 21:06:07,502 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=1650.0, ans=0.42265625
2024-10-08 21:06:09,901 WARNING [optim.py:503] Scaling gradients by 0.024340813979506493, model_norm_threshold=94603526144.0
2024-10-08 21:06:10,057 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.791e+24, grad_sumsq=2.869e+24, orig_rms_sq=2.367e+00
2024-10-08 21:06:11,346 WARNING [optim.py:503] Scaling gradients by 0.006452860310673714, model_norm_threshold=94603526144.0
2024-10-08 21:06:11,500 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.53, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.142e+26, grad_sumsq=4.800e+25, orig_rms_sq=2.379e+00
2024-10-08 21:06:15,122 WARNING [optim.py:503] Scaling gradients by 7.175726204877719e-05, model_norm_threshold=94603526144.0
2024-10-08 21:06:15,277 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.064e+29, grad_sumsq=2.548e+29, orig_rms_sq=2.379e+00
2024-10-08 21:06:16,566 WARNING [optim.py:503] Scaling gradients by 0.007420159410685301, model_norm_threshold=94603526144.0
2024-10-08 21:06:16,723 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.921e+25, grad_sumsq=1.006e+28, orig_rms_sq=5.887e-03
2024-10-08 21:06:30,200 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=23.93 vs. limit=8.12125
2024-10-08 21:06:30,877 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=1656.6666666666667, ans=0.42234375
2024-10-08 21:06:46,858 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten.whitening_limit, batch_count=1660.0, ans=8.1225
2024-10-08 21:06:59,370 WARNING [optim.py:503] Scaling gradients by 0.05672687664628029, model_norm_threshold=94603526144.0
2024-10-08 21:06:59,527 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.652e+23, grad_sumsq=1.527e+25, orig_rms_sq=3.701e-02
2024-10-08 21:07:00,560 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module1.whiten, num_groups=1, num_channels=192, metric=4.85 vs. limit=8.12375
2024-10-08 21:07:02,216 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.105e+08 5.190e+09 3.420e+10 3.232e+11 1.318e+15, threshold=6.839e+10, percent-clipped=35.0
2024-10-08 21:07:02,216 WARNING [optim.py:503] Scaling gradients by 0.08488616347312927, model_norm_threshold=68391084032.0
2024-10-08 21:07:02,373 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.533e+23, grad_sumsq=2.588e+25, orig_rms_sq=5.923e-03
2024-10-08 21:07:02,434 INFO [train.py:1152] Epoch 1, batch 5000, loss[loss=0.7924, ctc_loss=1.073, attn_decoder_loss=0.7222, over 4786.00 frames. ], tot_loss[loss=0.7774, ctc_loss=1.048, attn_decoder_loss=0.7098, over 967691.25 frames. ], batch size: 29, lr: 4.10e-02,
2024-10-08 21:07:04,880 WARNING [optim.py:503] Scaling gradients by 0.052030738443136215, model_norm_threshold=68391084032.0
2024-10-08 21:07:05,036 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.605e+23, grad_sumsq=1.790e+25, orig_rms_sq=3.690e-02
2024-10-08 21:07:08,209 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=36.89 vs. limit=8.125
2024-10-08 21:07:14,501 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=27.49 vs. limit=8.12625
2024-10-08 21:07:14,540 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=5.83 vs. limit=8.12625
2024-10-08 21:07:20,938 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.90 vs. limit=8.12625
2024-10-08 21:07:25,886 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=30.22 vs. limit=8.7525
2024-10-08 21:07:28,737 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=23.60 vs. limit=8.754999999999999
2024-10-08 21:07:29,504 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=2.030e+03
2024-10-08 21:07:35,656 WARNING [optim.py:503] Scaling gradients by 0.0358692966401577, model_norm_threshold=68391084032.0
2024-10-08 21:07:35,813 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.503e+24, grad_sumsq=2.526e+26, orig_rms_sq=5.952e-03
2024-10-08 21:07:44,235 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.81 vs. limit=8.7575
2024-10-08 21:07:53,577 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=5.90 vs. limit=5.84
2024-10-08 21:07:55,983 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=13.99 vs. limit=8.13
2024-10-08 21:07:56,888 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=37.10 vs. limit=8.76
2024-10-08 21:07:57,558 WARNING [optim.py:503] Scaling gradients by 0.04732087254524231, model_norm_threshold=68391084032.0
2024-10-08 21:07:57,715 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.442e+23, grad_sumsq=1.642e+26, orig_rms_sq=5.751e-03
2024-10-08 21:08:01,579 WARNING [optim.py:503] Scaling gradients by 0.055041249841451645, model_norm_threshold=68391084032.0
2024-10-08 21:08:01,736 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.745e+23, grad_sumsq=1.062e+25, orig_rms_sq=3.527e-02
2024-10-08 21:08:02,951 WARNING [optim.py:503] Scaling gradients by 0.041018128395080566, model_norm_threshold=68391084032.0
2024-10-08 21:08:03,106 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.834e+23, grad_sumsq=1.938e+25, orig_rms_sq=3.527e-02
2024-10-08 21:08:06,765 INFO [train.py:1152] Epoch 1, batch 5050, loss[loss=0.7556, ctc_loss=0.9979, attn_decoder_loss=0.695, over 4854.00 frames. ], tot_loss[loss=0.7758, ctc_loss=1.045, attn_decoder_loss=0.7084, over 968668.79 frames. ], batch size: 19, lr: 4.10e-02,
2024-10-08 21:08:06,902 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=1683.3333333333333, ans=0.42109375
2024-10-08 21:08:08,455 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=18.78 vs. limit=8.7625
2024-10-08 21:08:13,646 WARNING [optim.py:503] Scaling gradients by 0.02866683155298233, model_norm_threshold=68391084032.0
2024-10-08 21:08:13,802 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.107e+24, grad_sumsq=4.560e+23, orig_rms_sq=2.429e+00
2024-10-08 21:08:14,466 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.32 vs. limit=8.7625
2024-10-08 21:08:15,080 WARNING [optim.py:503] Scaling gradients by 0.0006919013685546815, model_norm_threshold=68391084032.0
2024-10-08 21:08:15,236 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.134e+27, grad_sumsq=8.744e+26, orig_rms_sq=2.440e+00
2024-10-08 21:08:17,425 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.01 vs. limit=5.420833333333333
2024-10-08 21:08:19,633 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=25.15 vs. limit=8.765
2024-10-08 21:08:20,199 WARNING [optim.py:503] Scaling gradients by 0.003559604287147522, model_norm_threshold=68391084032.0
2024-10-08 21:08:20,356 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.298e+25, grad_sumsq=2.577e+25, orig_rms_sq=2.443e+00
2024-10-08 21:08:23,006 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=1686.6666666666667, ans=0.4209375
2024-10-08 21:08:23,439 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.93 vs. limit=8.765
2024-10-08 21:08:28,821 WARNING [optim.py:503] Scaling gradients by 0.04288608953356743, model_norm_threshold=68391084032.0
2024-10-08 21:08:28,978 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.256e+23, grad_sumsq=1.084e+23, orig_rms_sq=4.850e+00
2024-10-08 21:08:29,984 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=7.60 vs. limit=8.1325
2024-10-08 21:08:32,826 WARNING [optim.py:503] Scaling gradients by 0.006739129777997732, model_norm_threshold=68391084032.0
2024-10-08 21:08:32,983 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.451e+25, grad_sumsq=6.374e+26, orig_rms_sq=3.845e-02
2024-10-08 21:08:35,450 WARNING [optim.py:503] Scaling gradients by 0.0018816569354385138, model_norm_threshold=68391084032.0
2024-10-08 21:08:35,606 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.985e+26, grad_sumsq=7.739e+27, orig_rms_sq=3.857e-02
2024-10-08 21:08:41,934 WARNING [optim.py:503] Scaling gradients by 0.02439265139400959, model_norm_threshold=68391084032.0
2024-10-08 21:08:42,094 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.509e+24, grad_sumsq=7.354e+24, orig_rms_sq=2.052e-01
2024-10-08 21:08:42,272 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=1690.0, ans=0.1549375
2024-10-08 21:08:44,653 WARNING [optim.py:503] Scaling gradients by 0.04506150633096695, model_norm_threshold=68391084032.0
2024-10-08 21:08:44,810 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.509e+23, grad_sumsq=1.166e+25, orig_rms_sq=3.866e-02
2024-10-08 21:08:48,507 WARNING [optim.py:503] Scaling gradients by 0.008786780759692192, model_norm_threshold=68391084032.0
2024-10-08 21:08:48,662 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.381e+25, grad_sumsq=3.563e+26, orig_rms_sq=3.876e-02
2024-10-08 21:08:49,640 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.00 vs. limit=5.846666666666667
2024-10-08 21:08:52,163 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.68 vs. limit=8.77
2024-10-08 21:08:53,966 WARNING [optim.py:503] Scaling gradients by 0.06359043717384338, model_norm_threshold=68391084032.0
2024-10-08 21:08:54,123 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.648e+23, grad_sumsq=1.170e+23, orig_rms_sq=2.264e+00
2024-10-08 21:08:56,416 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.80 vs. limit=5.423333333333333
2024-10-08 21:08:56,813 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=1693.3333333333333, ans=0.1365
2024-10-08 21:09:00,423 WARNING [optim.py:503] Scaling gradients by 0.03155466914176941, model_norm_threshold=68391084032.0
2024-10-08 21:09:00,579 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.821e+24, grad_sumsq=3.582e+26, orig_rms_sq=5.083e-03
2024-10-08 21:09:00,789 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=1696.6666666666667, ans=0.136375
2024-10-08 21:09:05,884 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=1696.6666666666667, ans=0.136375
2024-10-08 21:09:07,655 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=30.88 vs. limit=8.7725
2024-10-08 21:09:09,172 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=14.61 vs. limit=8.13625
2024-10-08 21:09:09,534 WARNING [optim.py:503] Scaling gradients by 0.00822136364877224, model_norm_threshold=68391084032.0
2024-10-08 21:09:09,691 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.65, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.525e+25, grad_sumsq=8.711e+27, orig_rms_sq=5.194e-03
2024-10-08 21:09:12,279 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.066e+08 1.027e+10 4.489e+10 4.258e+11 9.885e+13, threshold=8.978e+10, percent-clipped=45.0
2024-10-08 21:09:12,330 INFO [train.py:1152] Epoch 1, batch 5100, loss[loss=0.7643, ctc_loss=1.044, attn_decoder_loss=0.6944, over 4816.00 frames. ], tot_loss[loss=0.7799, ctc_loss=1.05, attn_decoder_loss=0.7124, over 967968.84 frames. ], batch size: 19, lr: 4.09e-02,
2024-10-08 21:09:13,795 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer_ff2.min_abs, batch_count=1700.0, ans=0.0425
2024-10-08 21:09:17,005 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=28.05 vs. limit=8.1375
2024-10-08 21:09:18,574 WARNING [optim.py:503] Scaling gradients by 0.047172822058200836, model_norm_threshold=89775251456.0
2024-10-08 21:09:18,730 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.808e+24, grad_sumsq=3.256e+26, orig_rms_sq=5.554e-03
2024-10-08 21:09:19,940 WARNING [optim.py:503] Scaling gradients by 0.06635750085115433, model_norm_threshold=89775251456.0
2024-10-08 21:09:20,093 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.608e+23, grad_sumsq=1.927e+23, orig_rms_sq=2.391e+00
2024-10-08 21:09:26,472 WARNING [optim.py:503] Scaling gradients by 0.09538912773132324, model_norm_threshold=89775251456.0
2024-10-08 21:09:26,628 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.762e+23, grad_sumsq=3.141e+25, orig_rms_sq=5.611e-03
2024-10-08 21:09:26,788 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1703.3333333333333, ans=0.28296666666666664
2024-10-08 21:09:42,899 WARNING [optim.py:503] Scaling gradients by 0.03226981684565544, model_norm_threshold=89775251456.0
2024-10-08 21:09:43,054 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.438e+24, grad_sumsq=1.060e+24, orig_rms_sq=2.299e+00
2024-10-08 21:09:50,402 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.88 vs. limit=3.2565
2024-10-08 21:09:51,686 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.71 vs. limit=5.4275
2024-10-08 21:09:53,921 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=8.75 vs. limit=4.684
2024-10-08 21:09:59,005 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=9.03 vs. limit=4.684
2024-10-08 21:09:59,722 WARNING [optim.py:503] Scaling gradients by 0.01792101189494133, model_norm_threshold=89775251456.0
2024-10-08 21:09:59,879 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.894e+24, grad_sumsq=2.692e+24, orig_rms_sq=2.189e+00
2024-10-08 21:10:01,576 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=22.94 vs. limit=8.14125
2024-10-08 21:10:03,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.77 vs. limit=5.8566666666666665
2024-10-08 21:10:04,353 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.82 vs. limit=3.257
2024-10-08 21:10:06,137 WARNING [optim.py:503] Scaling gradients by 0.0004150658205617219, model_norm_threshold=89775251456.0
2024-10-08 21:10:06,291 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.178e+28, grad_sumsq=3.064e+29, orig_rms_sq=3.843e-02
2024-10-08 21:10:07,948 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=1713.3333333333333, ans=0.4196875
2024-10-08 21:10:12,771 WARNING [optim.py:503] Scaling gradients by 0.03229817748069763, model_norm_threshold=89775251456.0
2024-10-08 21:10:12,928 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.747e+24, grad_sumsq=5.266e+26, orig_rms_sq=5.216e-03
2024-10-08 21:10:15,404 WARNING [optim.py:503] Scaling gradients by 0.009595919400453568, model_norm_threshold=89775251456.0
2024-10-08 21:10:15,560 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.206e+25, grad_sumsq=8.341e+26, orig_rms_sq=3.844e-02
2024-10-08 21:10:16,793 INFO [train.py:1152] Epoch 1, batch 5150, loss[loss=0.8212, ctc_loss=1.092, attn_decoder_loss=0.7534, over 4814.00 frames. ], tot_loss[loss=0.7792, ctc_loss=1.05, attn_decoder_loss=0.7113, over 968096.19 frames. ], batch size: 36, lr: 4.09e-02,
2024-10-08 21:10:19,201 WARNING [optim.py:503] Scaling gradients by 0.04297187179327011, model_norm_threshold=89775251456.0
2024-10-08 21:10:19,358 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.161e+24, grad_sumsq=3.057e+25, orig_rms_sq=3.799e-02
2024-10-08 21:10:20,611 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=1716.6666666666667, ans=0.41953125
2024-10-08 21:10:23,841 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=24.50 vs. limit=8.14375
2024-10-08 21:10:25,183 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=24.32 vs. limit=8.7875
2024-10-08 21:10:25,578 WARNING [optim.py:503] Scaling gradients by 0.0004373005067463964, model_norm_threshold=89775251456.0
2024-10-08 21:10:25,731 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.297e+28, grad_sumsq=5.717e+27, orig_rms_sq=2.269e+00
2024-10-08 21:10:32,845 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=6.82 vs. limit=4.688
2024-10-08 21:10:33,375 WARNING [optim.py:503] Scaling gradients by 0.04052054509520531, model_norm_threshold=89775251456.0
2024-10-08 21:10:33,530 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.220e+24, grad_sumsq=3.315e+25, orig_rms_sq=3.681e-02
2024-10-08 21:10:34,156 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=43.80 vs. limit=8.145
2024-10-08 21:10:36,008 WARNING [optim.py:503] Scaling gradients by 0.0778057724237442, model_norm_threshold=89775251456.0
2024-10-08 21:10:36,165 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.080e+23, grad_sumsq=8.496e+24, orig_rms_sq=3.625e-02
2024-10-08 21:10:39,913 WARNING [optim.py:503] Scaling gradients by 0.0033238641917705536, model_norm_threshold=89775251456.0
2024-10-08 21:10:40,069 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.928e+26, grad_sumsq=3.802e+28, orig_rms_sq=5.070e-03
2024-10-08 21:10:49,020 WARNING [optim.py:503] Scaling gradients by 0.03340080752968788, model_norm_threshold=89775251456.0
2024-10-08 21:10:49,175 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.454e+24, grad_sumsq=6.753e+25, orig_rms_sq=3.634e-02
2024-10-08 21:10:50,566 WARNING [optim.py:503] Scaling gradients by 0.012573649175465107, model_norm_threshold=89775251456.0
2024-10-08 21:10:50,721 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.344e+25, grad_sumsq=3.688e+26, orig_rms_sq=3.645e-02
2024-10-08 21:10:56,486 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=79.66 vs. limit=8.1475
2024-10-08 21:10:56,708 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=43.97 vs. limit=8.1475
2024-10-08 21:10:57,164 WARNING [optim.py:503] Scaling gradients by 0.058822426944971085, model_norm_threshold=89775251456.0
2024-10-08 21:10:57,320 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.867e+23, grad_sumsq=2.408e+25, orig_rms_sq=3.682e-02
2024-10-08 21:11:01,242 WARNING [optim.py:503] Scaling gradients by 0.0028205534908920527, model_norm_threshold=89775251456.0
2024-10-08 21:11:01,402 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.502e+26, grad_sumsq=1.206e+28, orig_rms_sq=3.734e-02
2024-10-08 21:11:03,357 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.14 vs. limit=5.431666666666667
2024-10-08 21:11:06,506 WARNING [optim.py:503] Scaling gradients by 0.023498402908444405, model_norm_threshold=89775251456.0
2024-10-08 21:11:06,662 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.309e+24, grad_sumsq=8.276e+26, orig_rms_sq=5.207e-03
2024-10-08 21:11:12,885 WARNING [optim.py:503] Scaling gradients by 0.053824812173843384, model_norm_threshold=89775251456.0
2024-10-08 21:11:13,042 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.796e+23, grad_sumsq=3.557e+23, orig_rms_sq=1.911e+00
2024-10-08 21:11:13,222 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1730.0, ans=0.2827
2024-10-08 21:11:15,501 WARNING [optim.py:503] Scaling gradients by 0.02001555636525154, model_norm_threshold=89775251456.0
2024-10-08 21:11:15,656 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.950e+24, grad_sumsq=1.582e+26, orig_rms_sq=3.761e-02
2024-10-08 21:11:22,182 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.911e+08 1.676e+10 8.336e+10 6.116e+11 2.163e+14, threshold=1.667e+11, percent-clipped=49.0
2024-10-08 21:11:22,233 INFO [train.py:1152] Epoch 1, batch 5200, loss[loss=0.7722, ctc_loss=1.065, attn_decoder_loss=0.6989, over 4793.00 frames. ], tot_loss[loss=0.7779, ctc_loss=1.049, attn_decoder_loss=0.7101, over 967730.18 frames. ], batch size: 29, lr: 4.08e-02,
2024-10-08 21:11:24,034 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.72 vs. limit=8.8
2024-10-08 21:11:24,627 WARNING [optim.py:503] Scaling gradients by 0.004765862599015236, model_norm_threshold=166711328768.0
2024-10-08 21:11:24,789 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.732e+26, grad_sumsq=1.462e+26, orig_rms_sq=1.869e+00
2024-10-08 21:11:25,973 WARNING [optim.py:503] Scaling gradients by 0.023679491132497787, model_norm_threshold=166711328768.0
2024-10-08 21:11:26,128 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.366e+24, grad_sumsq=2.285e+26, orig_rms_sq=3.661e-02
2024-10-08 21:11:28,117 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.21 vs. limit=8.15
2024-10-08 21:11:31,255 WARNING [optim.py:503] Scaling gradients by 0.09962572157382965, model_norm_threshold=166711328768.0
2024-10-08 21:11:31,413 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.622e+23, grad_sumsq=2.611e+25, orig_rms_sq=3.685e-02
2024-10-08 21:11:42,937 WARNING [optim.py:503] Scaling gradients by 0.002605052897706628, model_norm_threshold=166711328768.0
2024-10-08 21:11:43,092 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.015e+27, grad_sumsq=3.998e+26, orig_rms_sq=2.538e+00
2024-10-08 21:11:45,638 WARNING [optim.py:503] Scaling gradients by 0.013850504532456398, model_norm_threshold=166711328768.0
2024-10-08 21:11:45,793 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.221e+25, grad_sumsq=9.407e+25, orig_rms_sq=2.361e-01
2024-10-08 21:11:47,065 WARNING [optim.py:503] Scaling gradients by 0.06821908056735992, model_norm_threshold=166711328768.0
2024-10-08 21:11:47,222 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.463e+24, grad_sumsq=2.614e+26, orig_rms_sq=5.596e-03
2024-10-08 21:11:51,242 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=1740.0, ans=0.06085
2024-10-08 21:11:55,305 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=15.77 vs. limit=8.1525
2024-10-08 21:11:57,377 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=1740.0, ans=0.13474999999999998
2024-10-08 21:12:05,310 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=8.60 vs. limit=5.435833333333333
2024-10-08 21:12:05,664 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=25.07 vs. limit=8.807500000000001
2024-10-08 21:12:06,803 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=41.90 vs. limit=8.15375
2024-10-08 21:12:14,704 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=36.67 vs. limit=8.155
2024-10-08 21:12:17,829 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=29.96 vs. limit=8.81
2024-10-08 21:12:18,575 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=1746.6666666666667, ans=0.08908333333333333
2024-10-08 21:12:26,804 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.14 vs. limit=5.875
2024-10-08 21:12:27,154 INFO [train.py:1152] Epoch 1, batch 5250, loss[loss=0.8119, ctc_loss=1.077, attn_decoder_loss=0.7456, over 4857.00 frames. ], tot_loss[loss=0.7751, ctc_loss=1.045, attn_decoder_loss=0.7076, over 967808.47 frames. ], batch size: 20, lr: 4.07e-02,
2024-10-08 21:12:30,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.min_positive, batch_count=1750.0, ans=0.04453125
2024-10-08 21:12:37,157 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=1750.0, ans=0.22625
2024-10-08 21:12:39,416 WARNING [optim.py:503] Scaling gradients by 0.014024636708199978, model_norm_threshold=166711328768.0
2024-10-08 21:12:39,574 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.359e+25, grad_sumsq=7.055e+27, orig_rms_sq=4.762e-03
2024-10-08 21:12:41,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=25.83 vs. limit=8.815
2024-10-08 21:12:48,181 WARNING [optim.py:503] Scaling gradients by 0.09764011949300766, model_norm_threshold=166711328768.0
2024-10-08 21:12:48,337 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.238e+23, grad_sumsq=8.487e+22, orig_rms_sq=7.350e+00
2024-10-08 21:12:52,040 WARNING [optim.py:503] Scaling gradients by 0.056490544229745865, model_norm_threshold=166711328768.0
2024-10-08 21:12:52,197 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.295e+24, grad_sumsq=5.817e+25, orig_rms_sq=3.945e-02
2024-10-08 21:13:06,757 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=23.75 vs. limit=8.16
2024-10-08 21:13:11,023 WARNING [optim.py:503] Scaling gradients by 0.013327817432582378, model_norm_threshold=166711328768.0
2024-10-08 21:13:11,180 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.646e+25, grad_sumsq=3.644e+24, orig_rms_sq=7.262e+00
2024-10-08 21:13:15,002 WARNING [optim.py:503] Scaling gradients by 0.0030611450783908367, model_norm_threshold=166711328768.0
2024-10-08 21:13:15,158 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.050e+26, grad_sumsq=2.030e+28, orig_rms_sq=3.966e-02
2024-10-08 21:13:17,203 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.80 vs. limit=8.8225
2024-10-08 21:13:19,405 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=1763.3333333333333, ans=0.060325000000000004
2024-10-08 21:13:21,615 WARNING [optim.py:503] Scaling gradients by 0.003677906235679984, model_norm_threshold=166711328768.0
2024-10-08 21:13:21,773 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.931e+26, grad_sumsq=1.213e+29, orig_rms_sq=4.891e-03
2024-10-08 21:13:24,205 WARNING [optim.py:503] Scaling gradients by 0.0012885505566373467, model_norm_threshold=166711328768.0
2024-10-08 21:13:24,359 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.347e+27, grad_sumsq=1.350e+27, orig_rms_sq=2.479e+00
2024-10-08 21:13:25,542 WARNING [optim.py:503] Scaling gradients by 0.014740172773599625, model_norm_threshold=166711328768.0
2024-10-08 21:13:25,699 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.466e+25, grad_sumsq=8.746e+26, orig_rms_sq=3.963e-02
2024-10-08 21:13:30,806 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.563e+09 4.071e+10 1.147e+11 7.237e+11 1.294e+14, threshold=2.295e+11, percent-clipped=43.0
2024-10-08 21:13:30,857 INFO [train.py:1152] Epoch 1, batch 5300, loss[loss=0.7871, ctc_loss=1.04, attn_decoder_loss=0.7238, over 4825.00 frames. ], tot_loss[loss=0.7767, ctc_loss=1.047, attn_decoder_loss=0.7092, over 967887.53 frames. ], batch size: 38, lr: 4.07e-02,
2024-10-08 21:13:31,055 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=1766.6666666666667, ans=0.4171875
2024-10-08 21:13:32,813 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=25.67 vs. limit=8.825
2024-10-08 21:13:35,615 WARNING [optim.py:503] Scaling gradients by 0.049041301012039185, model_norm_threshold=229487378432.0
2024-10-08 21:13:35,770 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.451e+24, grad_sumsq=1.756e+27, orig_rms_sq=4.813e-03
2024-10-08 21:13:45,870 WARNING [optim.py:503] Scaling gradients by 0.013906704261898994, model_norm_threshold=229487378432.0
2024-10-08 21:13:46,025 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.213e+25, grad_sumsq=1.262e+28, orig_rms_sq=4.923e-03
2024-10-08 21:13:47,859 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten.whitening_limit, batch_count=1770.0, ans=8.16375
2024-10-08 21:13:51,096 WARNING [optim.py:503] Scaling gradients by 0.023397110402584076, model_norm_threshold=229487378432.0
2024-10-08 21:13:51,250 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.455e+25, grad_sumsq=6.093e+26, orig_rms_sq=4.029e-02
2024-10-08 21:13:52,490 WARNING [optim.py:503] Scaling gradients by 0.024898625910282135, model_norm_threshold=229487378432.0
2024-10-08 21:13:52,648 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.275e+25, grad_sumsq=1.303e+25, orig_rms_sq=1.745e+00
2024-10-08 21:13:53,528 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.17 vs. limit=5.885
2024-10-08 21:13:53,532 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=13.64 vs. limit=8.16375
2024-10-08 21:14:03,952 WARNING [optim.py:503] Scaling gradients by 0.012596936896443367, model_norm_threshold=229487378432.0
2024-10-08 21:14:04,110 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.963e+25, grad_sumsq=3.042e+25, orig_rms_sq=2.289e+00
2024-10-08 21:14:05,331 WARNING [optim.py:503] Scaling gradients by 0.06949133425951004, model_norm_threshold=229487378432.0
2024-10-08 21:14:05,488 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.569e+24, grad_sumsq=7.684e+26, orig_rms_sq=4.645e-03
2024-10-08 21:14:06,713 WARNING [optim.py:503] Scaling gradients by 0.042522747069597244, model_norm_threshold=229487378432.0
2024-10-08 21:14:06,870 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.600e+24, grad_sumsq=1.590e+26, orig_rms_sq=4.150e-02
2024-10-08 21:14:08,043 WARNING [optim.py:503] Scaling gradients by 0.09545616805553436, model_norm_threshold=229487378432.0
2024-10-08 21:14:08,201 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.338e+24, grad_sumsq=3.223e+25, orig_rms_sq=4.150e-02
2024-10-08 21:14:08,791 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=12.62 vs. limit=5.888333333333334
2024-10-08 21:14:13,127 WARNING [optim.py:503] Scaling gradients by 0.07250343263149261, model_norm_threshold=229487378432.0
2024-10-08 21:14:13,282 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.637e+24, grad_sumsq=8.671e+25, orig_rms_sq=4.194e-02
2024-10-08 21:14:28,514 WARNING [optim.py:503] Scaling gradients by 0.014132771641016006, model_norm_threshold=229487378432.0
2024-10-08 21:14:28,669 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.555e+25, grad_sumsq=1.545e+27, orig_rms_sq=4.244e-02
2024-10-08 21:14:28,826 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=1780.0, ans=0.4165625
2024-10-08 21:14:32,528 WARNING [optim.py:503] Scaling gradients by 0.07988939434289932, model_norm_threshold=229487378432.0
2024-10-08 21:14:32,684 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.303e+24, grad_sumsq=4.980e+26, orig_rms_sq=4.624e-03
2024-10-08 21:14:35,374 INFO [train.py:1152] Epoch 1, batch 5350, loss[loss=0.7152, ctc_loss=0.9471, attn_decoder_loss=0.6572, over 4978.00 frames. ], tot_loss[loss=0.7777, ctc_loss=1.049, attn_decoder_loss=0.7098, over 967293.95 frames. ], batch size: 19, lr: 4.06e-02,
2024-10-08 21:14:39,019 WARNING [optim.py:503] Scaling gradients by 0.08767759054899216, model_norm_threshold=229487378432.0
2024-10-08 21:14:39,172 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.273e+24, grad_sumsq=1.070e+24, orig_rms_sq=2.124e+00
2024-10-08 21:14:43,027 WARNING [optim.py:503] Scaling gradients by 0.052643951028585434, model_norm_threshold=229487378432.0
2024-10-08 21:14:43,193 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.227e+24, grad_sumsq=5.776e+23, orig_rms_sq=9.049e+00
2024-10-08 21:14:47,143 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=1786.6666666666667, ans=0.41625
2024-10-08 21:14:51,401 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=28.68 vs. limit=8.84
2024-10-08 21:14:55,797 WARNING [optim.py:503] Scaling gradients by 0.047430723905563354, model_norm_threshold=229487378432.0
2024-10-08 21:14:55,958 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.596e+24, grad_sumsq=1.965e+26, orig_rms_sq=4.376e-02
2024-10-08 21:14:58,517 WARNING [optim.py:503] Scaling gradients by 0.04606779292225838, model_norm_threshold=229487378432.0
2024-10-08 21:14:58,674 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.959e+24, grad_sumsq=2.063e+26, orig_rms_sq=4.343e-02
2024-10-08 21:14:59,341 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=23.46 vs. limit=8.84
2024-10-08 21:14:59,853 WARNING [optim.py:503] Scaling gradients by 0.03521181270480156, model_norm_threshold=229487378432.0
2024-10-08 21:15:00,009 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.095e+25, grad_sumsq=5.737e+24, orig_rms_sq=1.909e+00
2024-10-08 21:15:03,926 WARNING [optim.py:503] Scaling gradients by 0.09525210410356522, model_norm_threshold=229487378432.0
2024-10-08 21:15:04,082 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.727e+24, grad_sumsq=4.001e+25, orig_rms_sq=4.317e-02
2024-10-08 21:15:09,115 WARNING [optim.py:503] Scaling gradients by 0.0561547726392746, model_norm_threshold=229487378432.0
2024-10-08 21:15:09,271 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.030e+24, grad_sumsq=1.859e+26, orig_rms_sq=4.319e-02
2024-10-08 21:15:09,488 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.prob, batch_count=1790.0, ans=0.41609375
2024-10-08 21:15:11,773 WARNING [optim.py:503] Scaling gradients by 0.08651246875524521, model_norm_threshold=229487378432.0
2024-10-08 21:15:11,931 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.875e+24, grad_sumsq=1.065e+24, orig_rms_sq=1.761e+00
2024-10-08 21:15:13,220 WARNING [optim.py:503] Scaling gradients by 0.028151245787739754, model_norm_threshold=229487378432.0
2024-10-08 21:15:13,377 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.751e+25, grad_sumsq=9.942e+24, orig_rms_sq=1.761e+00
2024-10-08 21:15:21,008 WARNING [optim.py:503] Scaling gradients by 0.07489340752363205, model_norm_threshold=229487378432.0
2024-10-08 21:15:21,165 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.353e+24, grad_sumsq=6.882e+26, orig_rms_sq=4.871e-03
2024-10-08 21:15:27,587 WARNING [optim.py:503] Scaling gradients by 0.08561163395643234, model_norm_threshold=229487378432.0
2024-10-08 21:15:27,744 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.890e+24, grad_sumsq=4.684e+25, orig_rms_sq=4.035e-02
2024-10-08 21:15:28,954 WARNING [optim.py:503] Scaling gradients by 0.06129635497927666, model_norm_threshold=229487378432.0
2024-10-08 21:15:29,113 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.164e+24, grad_sumsq=7.841e+25, orig_rms_sq=4.035e-02
2024-10-08 21:15:30,325 WARNING [optim.py:503] Scaling gradients by 0.002374151721596718, model_norm_threshold=229487378432.0
2024-10-08 21:15:30,481 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.999e+27, grad_sumsq=4.278e+29, orig_rms_sq=4.674e-03
2024-10-08 21:15:32,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.37 vs. limit=8.17375
2024-10-08 21:15:33,788 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=26.70 vs. limit=8.8475
2024-10-08 21:15:40,660 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.695e+08 1.572e+10 1.787e+11 1.701e+12 9.666e+13, threshold=3.574e+11, percent-clipped=47.0
2024-10-08 21:15:40,712 INFO [train.py:1152] Epoch 1, batch 5400, loss[loss=0.8252, ctc_loss=1.133, attn_decoder_loss=0.7482, over 4773.00 frames. ], tot_loss[loss=0.7798, ctc_loss=1.052, attn_decoder_loss=0.7118, over 966532.54 frames. ], batch size: 49, lr: 4.05e-02,
2024-10-08 21:15:41,536 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.67 vs. limit=8.85
2024-10-08 21:15:42,239 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1800.0, ans=0.282
2024-10-08 21:15:46,277 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=8.40 vs. limit=8.175
2024-10-08 21:15:46,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=1800.0, ans=0.227
2024-10-08 21:15:47,719 WARNING [optim.py:503] Scaling gradients by 0.010972660966217518, model_norm_threshold=357379342336.0
2024-10-08 21:15:47,874 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.293e+26, grad_sumsq=8.685e+27, orig_rms_sq=3.792e-02
2024-10-08 21:15:49,243 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=1800.0, ans=0.1325
2024-10-08 21:15:49,803 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.53 vs. limit=5.9
2024-10-08 21:15:50,204 WARNING [optim.py:503] Scaling gradients by 0.08802437782287598, model_norm_threshold=357379342336.0
2024-10-08 21:15:50,359 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.327e+24, grad_sumsq=8.773e+25, orig_rms_sq=3.792e-02
2024-10-08 21:15:55,328 WARNING [optim.py:503] Scaling gradients by 0.002397148869931698, model_norm_threshold=357379342336.0
2024-10-08 21:15:55,482 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.491e+27, grad_sumsq=1.202e+30, orig_rms_sq=4.569e-03
2024-10-08 21:16:00,580 WARNING [optim.py:503] Scaling gradients by 0.05023932084441185, model_norm_threshold=357379342336.0
2024-10-08 21:16:00,736 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.224e+25, grad_sumsq=3.353e+26, orig_rms_sq=3.651e-02
2024-10-08 21:16:09,723 WARNING [optim.py:503] Scaling gradients by 0.010014054365456104, model_norm_threshold=357379342336.0
2024-10-08 21:16:09,878 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.233e+26, grad_sumsq=1.819e+26, orig_rms_sq=2.327e+00
2024-10-08 21:16:11,333 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1806.6666666666667, ans=0.2819333333333333
2024-10-08 21:16:16,005 WARNING [optim.py:503] Scaling gradients by 0.045280925929546356, model_norm_threshold=357379342336.0
2024-10-08 21:16:16,162 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.607e+25, grad_sumsq=3.620e+27, orig_rms_sq=4.440e-03
2024-10-08 21:16:16,932 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.45 vs. limit=3.271
2024-10-08 21:16:20,127 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1810.0, ans=0.2819
2024-10-08 21:16:20,173 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=1810.0, ans=0.41515625
2024-10-08 21:16:27,869 WARNING [optim.py:503] Scaling gradients by 0.08699146658182144, model_norm_threshold=357379342336.0
2024-10-08 21:16:28,027 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.208e+24, grad_sumsq=6.950e+26, orig_rms_sq=4.616e-03
2024-10-08 21:16:30,484 WARNING [optim.py:503] Scaling gradients by 0.00466573191806674, model_norm_threshold=357379342336.0
2024-10-08 21:16:30,646 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.635e+27, grad_sumsq=4.488e+28, orig_rms_sq=3.643e-02
2024-10-08 21:16:34,993 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=22.09 vs. limit=8.86
2024-10-08 21:16:35,186 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.88 vs. limit=8.18
2024-10-08 21:16:42,514 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=49.19 vs. limit=8.18
2024-10-08 21:16:45,732 INFO [train.py:1152] Epoch 1, batch 5450, loss[loss=0.696, ctc_loss=0.9509, attn_decoder_loss=0.6323, over 4940.00 frames. ], tot_loss[loss=0.7772, ctc_loss=1.05, attn_decoder_loss=0.7091, over 967208.69 frames. ], batch size: 19, lr: 4.05e-02,
2024-10-08 21:16:53,524 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=1816.6666666666667, ans=0.41484375
2024-10-08 21:17:01,184 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=1820.0, ans=0.8363
2024-10-08 21:17:01,649 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.33 vs. limit=5.91
2024-10-08 21:17:09,133 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=35.09 vs. limit=8.1825
2024-10-08 21:17:09,284 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=19.45 vs. limit=8.1825
2024-10-08 21:17:19,410 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.43 vs. limit=5.4558333333333335
2024-10-08 21:17:28,020 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=1826.6666666666667, ans=0.1315
2024-10-08 21:17:31,094 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.64 vs. limit=5.913333333333333
2024-10-08 21:17:31,708 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=1826.6666666666667, ans=0.1315
2024-10-08 21:17:34,234 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=1826.6666666666667, ans=0.058899999999999994
2024-10-08 21:17:36,488 WARNING [optim.py:503] Scaling gradients by 0.037019770592451096, model_norm_threshold=357379342336.0
2024-10-08 21:17:36,644 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.911e+25, grad_sumsq=5.063e+26, orig_rms_sq=3.775e-02
2024-10-08 21:17:36,870 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=1830.0, ans=0.41421874999999997
2024-10-08 21:17:39,466 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=1830.0, ans=0.41421874999999997
2024-10-08 21:17:41,632 WARNING [optim.py:503] Scaling gradients by 0.05500258132815361, model_norm_threshold=357379342336.0
2024-10-08 21:17:41,789 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.347e+24, grad_sumsq=2.199e+26, orig_rms_sq=3.795e-02
2024-10-08 21:17:45,779 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=9.19 vs. limit=8.18625
2024-10-08 21:17:46,765 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=4.528e+04
2024-10-08 21:17:49,057 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.186e+08 2.083e+10 1.338e+11 6.270e+11 1.491e+14, threshold=2.677e+11, percent-clipped=37.0
2024-10-08 21:17:49,108 INFO [train.py:1152] Epoch 1, batch 5500, loss[loss=0.7569, ctc_loss=1.024, attn_decoder_loss=0.6902, over 4811.00 frames. ], tot_loss[loss=0.7773, ctc_loss=1.051, attn_decoder_loss=0.709, over 967472.79 frames. ], batch size: 49, lr: 4.04e-02,
2024-10-08 21:17:53,696 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.04 vs. limit=8.875
2024-10-08 21:17:54,124 WARNING [optim.py:503] Scaling gradients by 0.001394067076034844, model_norm_threshold=267663998976.0
2024-10-08 21:17:54,280 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.274e+27, grad_sumsq=2.150e+29, orig_rms_sq=3.849e-02
2024-10-08 21:18:00,101 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.43 vs. limit=8.1875
2024-10-08 21:18:05,717 WARNING [optim.py:503] Scaling gradients by 0.0012292731553316116, model_norm_threshold=267663998976.0
2024-10-08 21:18:05,870 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.100e+28, grad_sumsq=2.857e+29, orig_rms_sq=3.851e-02
2024-10-08 21:18:10,622 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=23.30 vs. limit=8.8775
2024-10-08 21:18:11,954 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=17.82 vs. limit=8.8775
2024-10-08 21:18:13,554 WARNING [optim.py:503] Scaling gradients by 0.02868386171758175, model_norm_threshold=267663998976.0
2024-10-08 21:18:13,710 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.471e+25, grad_sumsq=6.422e+26, orig_rms_sq=3.847e-02
2024-10-08 21:18:18,131 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=21.87 vs. limit=8.879999999999999
2024-10-08 21:18:18,802 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1840.0, ans=0.2816
2024-10-08 21:18:19,829 WARNING [optim.py:503] Scaling gradients by 0.0009596382733434439, model_norm_threshold=267663998976.0
2024-10-08 21:18:19,983 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.871e+28, grad_sumsq=4.934e+29, orig_rms_sq=3.791e-02
2024-10-08 21:18:22,686 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=1840.0, ans=0.0586
2024-10-08 21:18:26,166 WARNING [optim.py:503] Scaling gradients by 0.005150897428393364, model_norm_threshold=267663998976.0
2024-10-08 21:18:26,321 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.246e+26, grad_sumsq=1.653e+28, orig_rms_sq=3.779e-02
2024-10-08 21:18:28,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.min_positive, batch_count=1843.3333333333333, ans=0.04423958333333333
2024-10-08 21:18:35,176 WARNING [optim.py:503] Scaling gradients by 0.03084447793662548, model_norm_threshold=267663998976.0
2024-10-08 21:18:35,333 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.649e+25, grad_sumsq=6.007e+27, orig_rms_sq=4.411e-03
2024-10-08 21:18:35,853 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.41 vs. limit=3.2765
2024-10-08 21:18:41,852 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=1846.6666666666667, ans=0.4134375
2024-10-08 21:18:47,245 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.50 vs. limit=8.1925
2024-10-08 21:18:53,121 INFO [train.py:1152] Epoch 1, batch 5550, loss[loss=0.7349, ctc_loss=1.008, attn_decoder_loss=0.6665, over 4799.00 frames. ], tot_loss[loss=0.7762, ctc_loss=1.049, attn_decoder_loss=0.708, over 967087.24 frames. ], batch size: 19, lr: 4.03e-02,
2024-10-08 21:18:53,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.61 vs. limit=8.19375
2024-10-08 21:18:55,557 WARNING [optim.py:503] Scaling gradients by 0.004862322472035885, model_norm_threshold=267663998976.0
2024-10-08 21:18:55,715 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.734e+26, grad_sumsq=2.409e+28, orig_rms_sq=3.626e-02
2024-10-08 21:18:56,914 WARNING [optim.py:503] Scaling gradients by 0.0849265605211258, model_norm_threshold=267663998976.0
2024-10-08 21:18:57,071 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.741e+24, grad_sumsq=7.561e+25, orig_rms_sq=3.626e-02
2024-10-08 21:18:59,574 WARNING [optim.py:503] Scaling gradients by 0.0081401402130723, model_norm_threshold=267663998976.0
2024-10-08 21:18:59,729 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.492e+26, grad_sumsq=1.009e+26, orig_rms_sq=2.470e+00
2024-10-08 21:18:59,979 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=1850.0, ans=0.130625
2024-10-08 21:19:00,451 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.72 vs. limit=5.925
2024-10-08 21:19:03,170 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.83 vs. limit=8.8875
2024-10-08 21:19:05,479 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.10 vs. limit=5.926666666666667
2024-10-08 21:19:07,667 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=1853.3333333333333, ans=0.413125
2024-10-08 21:19:18,558 WARNING [optim.py:503] Scaling gradients by 0.09470834583044052, model_norm_threshold=267663998976.0
2024-10-08 21:19:18,713 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.568e+24, grad_sumsq=5.730e+26, orig_rms_sq=4.482e-03
2024-10-08 21:19:21,770 WARNING [optim.py:503] Scaling gradients by 0.03781518712639809, model_norm_threshold=267663998976.0
2024-10-08 21:19:21,927 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.204e+25, grad_sumsq=2.691e+27, orig_rms_sq=4.472e-03
2024-10-08 21:19:26,917 WARNING [optim.py:503] Scaling gradients by 0.014317474327981472, model_norm_threshold=267663998976.0
2024-10-08 21:19:27,073 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.556e+26, grad_sumsq=3.444e+28, orig_rms_sq=4.516e-03
2024-10-08 21:19:35,185 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=2.119e+01
2024-10-08 21:19:39,391 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=23.63 vs. limit=8.895
2024-10-08 21:19:47,483 WARNING [optim.py:503] Scaling gradients by 0.009060325101017952, model_norm_threshold=267663998976.0
2024-10-08 21:19:47,638 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.085e+26, grad_sumsq=8.100e+25, orig_rms_sq=2.574e+00
2024-10-08 21:19:56,469 WARNING [optim.py:503] Scaling gradients by 0.022024864330887794, model_norm_threshold=267663998976.0
2024-10-08 21:19:56,627 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.545e+25, grad_sumsq=1.255e+27, orig_rms_sq=3.621e-02
2024-10-08 21:19:58,266 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.967e+08 3.527e+10 1.388e+11 1.178e+12 2.789e+14, threshold=2.776e+11, percent-clipped=39.0
2024-10-08 21:19:58,320 INFO [train.py:1152] Epoch 1, batch 5600, loss[loss=0.7794, ctc_loss=1.097, attn_decoder_loss=0.7, over 4862.00 frames. ], tot_loss[loss=0.7761, ctc_loss=1.05, attn_decoder_loss=0.7076, over 967173.94 frames. ], batch size: 28, lr: 4.03e-02,
2024-10-08 21:20:05,847 WARNING [optim.py:503] Scaling gradients by 0.05657600611448288, model_norm_threshold=277550137344.0
2024-10-08 21:20:06,002 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.037e+24, grad_sumsq=2.259e+24, orig_rms_sq=2.672e+00
2024-10-08 21:20:08,509 WARNING [optim.py:503] Scaling gradients by 0.01968233287334442, model_norm_threshold=277550137344.0
2024-10-08 21:20:08,665 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.369e+25, grad_sumsq=1.205e+27, orig_rms_sq=3.627e-02
2024-10-08 21:20:13,885 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=1870.0, ans=0.22805
2024-10-08 21:20:15,415 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=34.52 vs. limit=8.9025
2024-10-08 21:20:17,266 WARNING [optim.py:503] Scaling gradients by 0.03562896326184273, model_norm_threshold=277550137344.0
2024-10-08 21:20:17,421 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.962e+25, grad_sumsq=5.454e+26, orig_rms_sq=3.598e-02
2024-10-08 21:20:18,635 WARNING [optim.py:503] Scaling gradients by 0.05952182412147522, model_norm_threshold=277550137344.0
2024-10-08 21:20:18,792 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.824e+24, grad_sumsq=1.615e+26, orig_rms_sq=3.606e-02
2024-10-08 21:20:18,943 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=1870.0, ans=0.83455
2024-10-08 21:20:23,148 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten.whitening_limit, batch_count=1873.3333333333333, ans=8.2025
2024-10-08 21:20:26,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=1873.3333333333333, ans=0.4121875
2024-10-08 21:20:27,686 WARNING [optim.py:503] Scaling gradients by 0.0024855651427060366, model_norm_threshold=277550137344.0
2024-10-08 21:20:27,843 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.961e+27, grad_sumsq=6.382e+29, orig_rms_sq=4.640e-03
2024-10-08 21:20:29,048 WARNING [optim.py:503] Scaling gradients by 0.01453807856887579, model_norm_threshold=277550137344.0
2024-10-08 21:20:29,204 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.900e+25, grad_sumsq=2.484e+27, orig_rms_sq=3.583e-02
2024-10-08 21:20:32,346 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.26 vs. limit=8.2025
2024-10-08 21:20:37,773 WARNING [optim.py:503] Scaling gradients by 0.004385782405734062, model_norm_threshold=277550137344.0
2024-10-08 21:20:37,930 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.473e+27, grad_sumsq=4.084e+28, orig_rms_sq=3.606e-02
2024-10-08 21:20:39,420 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=1876.6666666666667, ans=0.08827083333333334
2024-10-08 21:20:41,538 WARNING [optim.py:503] Scaling gradients by 0.0302648413926363, model_norm_threshold=277550137344.0
2024-10-08 21:20:41,696 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.231e+25, grad_sumsq=4.877e+27, orig_rms_sq=4.574e-03
2024-10-08 21:20:42,280 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=44.33 vs. limit=8.9075
2024-10-08 21:20:42,923 WARNING [optim.py:503] Scaling gradients by 0.008902084082365036, model_norm_threshold=277550137344.0
2024-10-08 21:20:43,079 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.956e+26, grad_sumsq=8.179e+27, orig_rms_sq=3.615e-02
2024-10-08 21:20:46,745 WARNING [optim.py:503] Scaling gradients by 0.004136326257139444, model_norm_threshold=277550137344.0
2024-10-08 21:20:46,901 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.180e+27, grad_sumsq=4.404e+26, orig_rms_sq=2.679e+00
2024-10-08 21:20:54,390 WARNING [optim.py:503] Scaling gradients by 0.08273662626743317, model_norm_threshold=277550137344.0
2024-10-08 21:20:54,547 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.177e+24, grad_sumsq=5.979e+25, orig_rms_sq=3.641e-02
2024-10-08 21:20:55,717 WARNING [optim.py:503] Scaling gradients by 0.005618789233267307, model_norm_threshold=277550137344.0
2024-10-08 21:20:55,874 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.215e+26, grad_sumsq=1.982e+28, orig_rms_sq=3.641e-02
2024-10-08 21:20:58,321 WARNING [optim.py:503] Scaling gradients by 0.01166814099997282, model_norm_threshold=277550137344.0
2024-10-08 21:20:58,476 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.577e+26, grad_sumsq=1.130e+26, orig_rms_sq=1.395e+00
2024-10-08 21:21:02,322 INFO [train.py:1152] Epoch 1, batch 5650, loss[loss=0.8357, ctc_loss=1.121, attn_decoder_loss=0.7644, over 4739.00 frames. ], tot_loss[loss=0.7731, ctc_loss=1.046, attn_decoder_loss=0.7048, over 967022.70 frames. ], batch size: 45, lr: 4.02e-02,
2024-10-08 21:21:08,655 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1883.3333333333333, ans=0.2811666666666667
2024-10-08 21:21:10,387 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=73.97 vs. limit=8.20625
2024-10-08 21:21:12,289 WARNING [optim.py:503] Scaling gradients by 0.07580604404211044, model_norm_threshold=277550137344.0
2024-10-08 21:21:12,444 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.729e+24, grad_sumsq=4.135e+23, orig_rms_sq=9.018e+00
2024-10-08 21:21:13,655 WARNING [optim.py:503] Scaling gradients by 0.027425670996308327, model_norm_threshold=277550137344.0
2024-10-08 21:21:13,825 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.623e+25, grad_sumsq=9.628e+26, orig_rms_sq=3.763e-02
2024-10-08 21:21:16,194 WARNING [optim.py:503] Scaling gradients by 0.0011181328445672989, model_norm_threshold=277550137344.0
2024-10-08 21:21:16,357 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.649e+28, grad_sumsq=1.828e+27, orig_rms_sq=9.025e+00
2024-10-08 21:21:19,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=1886.6666666666667, ans=0.12925
2024-10-08 21:21:19,728 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.93 vs. limit=4.754666666666667
2024-10-08 21:21:21,306 WARNING [optim.py:503] Scaling gradients by 0.0932457447052002, model_norm_threshold=277550137344.0
2024-10-08 21:21:21,466 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.876e+24, grad_sumsq=7.559e+25, orig_rms_sq=3.804e-02
2024-10-08 21:21:27,704 WARNING [optim.py:503] Scaling gradients by 0.0938105583190918, model_norm_threshold=277550137344.0
2024-10-08 21:21:27,859 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.799e+24, grad_sumsq=1.982e+23, orig_rms_sq=9.078e+00
2024-10-08 21:21:32,746 WARNING [optim.py:503] Scaling gradients by 0.06361524760723114, model_norm_threshold=277550137344.0
2024-10-08 21:21:32,903 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.286e+24, grad_sumsq=1.141e+26, orig_rms_sq=3.758e-02
2024-10-08 21:21:34,409 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=1890.0, ans=0.41140625
2024-10-08 21:21:40,523 WARNING [optim.py:503] Scaling gradients by 0.0748547911643982, model_norm_threshold=277550137344.0
2024-10-08 21:21:40,679 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.921e+24, grad_sumsq=1.034e+27, orig_rms_sq=4.758e-03
2024-10-08 21:21:42,571 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=23.77 vs. limit=8.92
2024-10-08 21:21:43,029 WARNING [optim.py:503] Scaling gradients by 0.0386066734790802, model_norm_threshold=277550137344.0
2024-10-08 21:21:43,187 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.200e+25, grad_sumsq=3.209e+26, orig_rms_sq=3.738e-02
2024-10-08 21:21:43,821 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=19.16 vs. limit=8.21
2024-10-08 21:21:45,743 WARNING [optim.py:503] Scaling gradients by 0.022378332912921906, model_norm_threshold=277550137344.0
2024-10-08 21:21:45,900 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.030e+25, grad_sumsq=1.607e+27, orig_rms_sq=3.752e-02
2024-10-08 21:21:46,399 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=14.06 vs. limit=8.21
2024-10-08 21:21:50,908 WARNING [optim.py:503] Scaling gradients by 0.0769498273730278, model_norm_threshold=277550137344.0
2024-10-08 21:21:51,063 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.641e+24, grad_sumsq=7.419e+26, orig_rms_sq=4.907e-03
2024-10-08 21:21:56,139 WARNING [optim.py:503] Scaling gradients by 0.030994873493909836, model_norm_threshold=277550137344.0
2024-10-08 21:21:56,295 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.61, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.870e+25, grad_sumsq=9.940e+27, orig_rms_sq=4.900e-03
2024-10-08 21:22:05,040 WARNING [optim.py:503] Scaling gradients by 0.04676250368356705, model_norm_threshold=277550137344.0
2024-10-08 21:22:05,200 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.548e+24, grad_sumsq=2.519e+26, orig_rms_sq=3.790e-02
2024-10-08 21:22:06,426 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.538e+09 7.735e+10 4.062e+11 2.977e+12 2.482e+14, threshold=8.124e+11, percent-clipped=62.0
2024-10-08 21:22:06,426 WARNING [optim.py:503] Scaling gradients by 0.07499340921640396, model_norm_threshold=812358500352.0
2024-10-08 21:22:06,583 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.245e+25, grad_sumsq=4.731e+27, orig_rms_sq=4.745e-03
2024-10-08 21:22:06,641 INFO [train.py:1152] Epoch 1, batch 5700, loss[loss=0.7932, ctc_loss=1.106, attn_decoder_loss=0.715, over 4888.00 frames. ], tot_loss[loss=0.7712, ctc_loss=1.045, attn_decoder_loss=0.7028, over 966420.61 frames. ], batch size: 22, lr: 4.02e-02,
2024-10-08 21:22:12,994 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=1900.0, ans=0.143125
2024-10-08 21:22:16,586 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.min_abs, batch_count=1900.0, ans=0.2285
2024-10-08 21:22:16,607 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer_na.min_abs, batch_count=1900.0, ans=0.011600000000000001
2024-10-08 21:22:17,626 WARNING [optim.py:503] Scaling gradients by 0.08234067261219025, model_norm_threshold=812358500352.0
2024-10-08 21:22:17,781 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.255e+25, grad_sumsq=5.996e+26, orig_rms_sq=3.761e-02
2024-10-08 21:22:21,209 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=19.61 vs. limit=8.21375
2024-10-08 21:22:25,285 WARNING [optim.py:503] Scaling gradients by 0.07554849237203598, model_norm_threshold=812358500352.0
2024-10-08 21:22:25,447 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.818e+25, grad_sumsq=1.000e+27, orig_rms_sq=3.817e-02
2024-10-08 21:22:27,488 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=100.26 vs. limit=8.21375
2024-10-08 21:22:27,997 WARNING [optim.py:503] Scaling gradients by 0.03395545110106468, model_norm_threshold=812358500352.0
2024-10-08 21:22:28,152 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.695e+26, grad_sumsq=4.402e+27, orig_rms_sq=3.850e-02
2024-10-08 21:22:30,171 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.09 vs. limit=5.475833333333333
2024-10-08 21:22:34,492 WARNING [optim.py:503] Scaling gradients by 0.07299651205539703, model_norm_threshold=812358500352.0
2024-10-08 21:22:34,650 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.139e+25, grad_sumsq=1.160e+25, orig_rms_sq=2.706e+00
2024-10-08 21:22:35,828 WARNING [optim.py:503] Scaling gradients by 0.02315974421799183, model_norm_threshold=812358500352.0
2024-10-08 21:22:35,986 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.455e+26, grad_sumsq=8.917e+27, orig_rms_sq=3.875e-02
2024-10-08 21:22:38,498 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=23.78 vs. limit=8.215
2024-10-08 21:22:46,519 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=107.02 vs. limit=8.21625
2024-10-08 21:22:46,803 WARNING [optim.py:503] Scaling gradients by 0.01281388383358717, model_norm_threshold=812358500352.0
2024-10-08 21:22:46,959 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.156e+27, grad_sumsq=6.439e+26, orig_rms_sq=1.795e+00
2024-10-08 21:22:48,711 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=14.11 vs. limit=5.955
2024-10-08 21:22:52,594 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=29.74 vs. limit=8.932500000000001
2024-10-08 21:22:53,145 WARNING [optim.py:503] Scaling gradients by 0.010760958306491375, model_norm_threshold=812358500352.0
2024-10-08 21:22:53,301 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.308e+27, grad_sumsq=5.854e+29, orig_rms_sq=3.942e-03
2024-10-08 21:22:57,252 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=1913.3333333333333, ans=6.195833333333333
2024-10-08 21:22:57,669 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.64 vs. limit=8.935
2024-10-08 21:23:02,576 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=37.95 vs. limit=8.935
2024-10-08 21:23:03,073 WARNING [optim.py:503] Scaling gradients by 0.0006494986009784043, model_norm_threshold=812358500352.0
2024-10-08 21:23:03,229 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.404e+29, grad_sumsq=2.261e+32, orig_rms_sq=3.716e-03
2024-10-08 21:23:08,934 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=19.55 vs. limit=8.2175
2024-10-08 21:23:09,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=1916.6666666666667, ans=0.41015625
2024-10-08 21:23:10,659 INFO [train.py:1152] Epoch 1, batch 5750, loss[loss=0.8609, ctc_loss=1.16, attn_decoder_loss=0.7861, over 4851.00 frames. ], tot_loss[loss=0.7727, ctc_loss=1.047, attn_decoder_loss=0.7041, over 966852.46 frames. ], batch size: 43, lr: 4.01e-02,
2024-10-08 21:23:12,141 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1916.6666666666667, ans=0.2808333333333333
2024-10-08 21:23:13,085 WARNING [optim.py:503] Scaling gradients by 0.0709732323884964, model_norm_threshold=812358500352.0
2024-10-08 21:23:13,241 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.094e+25, grad_sumsq=1.636e+25, orig_rms_sq=2.502e+00
2024-10-08 21:23:13,868 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=24.69 vs. limit=5.958333333333333
2024-10-08 21:23:14,566 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.min_abs, batch_count=1916.6666666666667, ans=0.22875
2024-10-08 21:23:15,852 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=1916.6666666666667, ans=0.41015625
2024-10-08 21:23:15,855 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=1916.6666666666667, ans=0.26041666666666663
2024-10-08 21:23:16,911 WARNING [optim.py:503] Scaling gradients by 0.0036718270275741816, model_norm_threshold=812358500352.0
2024-10-08 21:23:17,068 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.135e+28, grad_sumsq=8.534e+27, orig_rms_sq=2.502e+00
2024-10-08 21:23:27,865 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.17 vs. limit=8.22
2024-10-08 21:23:28,753 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=1920.0, ans=0.41000000000000003
2024-10-08 21:23:29,435 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=20.48 vs. limit=8.22
2024-10-08 21:23:31,920 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.74 vs. limit=5.48
2024-10-08 21:23:32,327 WARNING [optim.py:503] Scaling gradients by 0.0472571961581707, model_norm_threshold=812358500352.0
2024-10-08 21:23:32,484 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.918e+25, grad_sumsq=1.592e+28, orig_rms_sq=3.717e-03
2024-10-08 21:23:38,327 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=19.32 vs. limit=8.22125
2024-10-08 21:23:39,380 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.30 vs. limit=3.2885
2024-10-08 21:23:45,857 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=11.04 vs. limit=5.480833333333333
2024-10-08 21:23:46,929 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.35 vs. limit=5.480833333333333
2024-10-08 21:23:47,721 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=1926.6666666666667, ans=0.141625
2024-10-08 21:23:53,541 WARNING [optim.py:503] Scaling gradients by 0.046608053147792816, model_norm_threshold=812358500352.0
2024-10-08 21:23:53,696 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.588e+25, grad_sumsq=2.291e+25, orig_rms_sq=2.440e+00
2024-10-08 21:24:01,530 WARNING [optim.py:503] Scaling gradients by 0.03803364187479019, model_norm_threshold=812358500352.0
2024-10-08 21:24:01,686 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.543e+26, grad_sumsq=4.092e+28, orig_rms_sq=3.771e-03
2024-10-08 21:24:03,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=30.50 vs. limit=8.22375
2024-10-08 21:24:04,112 WARNING [optim.py:503] Scaling gradients by 0.04450216144323349, model_norm_threshold=812358500352.0
2024-10-08 21:24:04,269 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.928e+25, grad_sumsq=2.770e+27, orig_rms_sq=3.585e-02
2024-10-08 21:24:05,468 WARNING [optim.py:503] Scaling gradients by 0.018128523603081703, model_norm_threshold=812358500352.0
2024-10-08 21:24:05,628 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.53, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.071e+27, grad_sumsq=2.989e+28, orig_rms_sq=3.585e-02
2024-10-08 21:24:05,760 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=1930.0, ans=0.127625
2024-10-08 21:24:08,161 WARNING [optim.py:503] Scaling gradients by 0.04907585307955742, model_norm_threshold=812358500352.0
2024-10-08 21:24:08,317 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.803e+25, grad_sumsq=2.304e+25, orig_rms_sq=2.519e+00
2024-10-08 21:24:08,490 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=1930.0, ans=0.22895000000000001
2024-10-08 21:24:13,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=1933.3333333333333, ans=0.409375
2024-10-08 21:24:14,971 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.901e+09 5.124e+10 2.351e+11 2.590e+12 1.251e+15, threshold=4.703e+11, percent-clipped=39.0
2024-10-08 21:24:15,023 INFO [train.py:1152] Epoch 1, batch 5800, loss[loss=0.7898, ctc_loss=1.076, attn_decoder_loss=0.7183, over 4846.00 frames. ], tot_loss[loss=0.7711, ctc_loss=1.045, attn_decoder_loss=0.7026, over 966144.94 frames. ], batch size: 43, lr: 4.00e-02,
2024-10-08 21:24:15,113 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=1933.3333333333333, ans=0.409375
2024-10-08 21:24:18,953 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.min_positive, batch_count=1933.3333333333333, ans=0.23066666666666666
2024-10-08 21:24:21,870 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=39.15 vs. limit=8.95
2024-10-08 21:24:23,667 WARNING [optim.py:503] Scaling gradients by 0.008890332654118538, model_norm_threshold=470274506752.0
2024-10-08 21:24:23,825 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.717e+26, grad_sumsq=2.200e+26, orig_rms_sq=2.599e+00
2024-10-08 21:24:28,822 WARNING [optim.py:503] Scaling gradients by 0.07061077654361725, model_norm_threshold=470274506752.0
2024-10-08 21:24:28,979 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.318e+24, grad_sumsq=3.130e+24, orig_rms_sq=2.658e+00
2024-10-08 21:24:29,157 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=1936.6666666666667, ans=0.23063333333333333
2024-10-08 21:24:34,104 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=1936.6666666666667, ans=0.40921875
2024-10-08 21:24:35,012 WARNING [optim.py:503] Scaling gradients by 0.09416631609201431, model_norm_threshold=470274506752.0
2024-10-08 21:24:35,168 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.473e+24, grad_sumsq=2.581e+26, orig_rms_sq=3.283e-02
2024-10-08 21:24:41,441 WARNING [optim.py:503] Scaling gradients by 0.001357381697744131, model_norm_threshold=470274506752.0
2024-10-08 21:24:41,597 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.802e+28, grad_sumsq=9.771e+30, orig_rms_sq=3.891e-03
2024-10-08 21:24:42,855 WARNING [optim.py:503] Scaling gradients by 0.007001340854912996, model_norm_threshold=470274506752.0
2024-10-08 21:24:43,010 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.265e+27, grad_sumsq=3.252e+29, orig_rms_sq=3.891e-03
2024-10-08 21:24:45,217 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.59 vs. limit=8.955
2024-10-08 21:24:49,367 WARNING [optim.py:503] Scaling gradients by 0.09663320332765579, model_norm_threshold=470274506752.0
2024-10-08 21:24:49,524 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.249e+24, grad_sumsq=2.207e+26, orig_rms_sq=3.285e-02
2024-10-08 21:24:49,789 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=1940.0, ans=0.4090625
2024-10-08 21:24:51,982 WARNING [optim.py:503] Scaling gradients by 0.013746446929872036, model_norm_threshold=470274506752.0
2024-10-08 21:24:52,138 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.195e+26, grad_sumsq=5.674e+28, orig_rms_sq=3.868e-03
2024-10-08 21:24:53,981 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=14.50 vs. limit=8.22875
2024-10-08 21:24:55,419 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten.whitening_limit, batch_count=1943.3333333333333, ans=8.22875
2024-10-08 21:24:55,795 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.42 vs. limit=8.9575
2024-10-08 21:24:56,207 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1943.3333333333333, ans=0.28056666666666663
2024-10-08 21:24:56,787 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.89 vs. limit=5.485833333333333
2024-10-08 21:25:01,323 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=1943.3333333333333, ans=0.40890625
2024-10-08 21:25:02,355 WARNING [optim.py:503] Scaling gradients by 0.07117579132318497, model_norm_threshold=470274506752.0
2024-10-08 21:25:02,514 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.410e+25, grad_sumsq=4.398e+26, orig_rms_sq=3.205e-02
2024-10-08 21:25:03,331 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=18.20 vs. limit=8.22875
2024-10-08 21:25:03,873 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=1943.3333333333333, ans=0.22915000000000002
2024-10-08 21:25:04,912 WARNING [optim.py:503] Scaling gradients by 0.002497333800420165, model_norm_threshold=470274506752.0
2024-10-08 21:25:05,067 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.874e+27, grad_sumsq=3.587e+27, orig_rms_sq=1.916e+00
2024-10-08 21:25:10,933 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.84 vs. limit=8.96
2024-10-08 21:25:11,310 WARNING [optim.py:503] Scaling gradients by 0.01373879425227642, model_norm_threshold=470274506752.0
2024-10-08 21:25:11,466 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.230e+26, grad_sumsq=1.719e+26, orig_rms_sq=1.879e+00
2024-10-08 21:25:12,267 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.03 vs. limit=8.96
2024-10-08 21:25:13,820 WARNING [optim.py:503] Scaling gradients by 0.016318820416927338, model_norm_threshold=470274506752.0
2024-10-08 21:25:13,977 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.640e+26, grad_sumsq=5.084e+27, orig_rms_sq=3.226e-02
2024-10-08 21:25:16,026 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=28.29 vs. limit=8.96
2024-10-08 21:25:16,950 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.57 vs. limit=8.23
2024-10-08 21:25:19,001 INFO [train.py:1152] Epoch 1, batch 5850, loss[loss=0.813, ctc_loss=1.055, attn_decoder_loss=0.7525, over 4733.00 frames. ], tot_loss[loss=0.7722, ctc_loss=1.047, attn_decoder_loss=0.7036, over 966531.89 frames. ], batch size: 45, lr: 4.00e-02,
2024-10-08 21:25:19,178 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=1950.0, ans=6.21875
2024-10-08 21:25:21,613 WARNING [optim.py:503] Scaling gradients by 0.01616031304001808, model_norm_threshold=470274506752.0
2024-10-08 21:25:21,770 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.757e+26, grad_sumsq=4.447e+28, orig_rms_sq=3.950e-03
2024-10-08 21:25:22,002 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=1950.0, ans=0.1403125
2024-10-08 21:25:24,208 WARNING [optim.py:503] Scaling gradients by 0.03991131857037544, model_norm_threshold=470274506752.0
2024-10-08 21:25:24,363 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.821e+25, grad_sumsq=8.947e+26, orig_rms_sq=3.153e-02
2024-10-08 21:25:25,170 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.13 vs. limit=8.23125
2024-10-08 21:25:26,801 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=5.26 vs. limit=5.0
2024-10-08 21:25:33,263 WARNING [optim.py:503] Scaling gradients by 0.004691306501626968, model_norm_threshold=470274506752.0
2024-10-08 21:25:33,421 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.068e+27, grad_sumsq=6.592e+28, orig_rms_sq=3.138e-02
2024-10-08 21:25:35,577 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.13 vs. limit=8.965
2024-10-08 21:25:41,057 WARNING [optim.py:503] Scaling gradients by 0.005513951648026705, model_norm_threshold=470274506752.0
2024-10-08 21:25:41,214 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.710e+27, grad_sumsq=1.886e+26, orig_rms_sq=9.067e+00
2024-10-08 21:25:46,858 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=44.24 vs. limit=8.23375
2024-10-08 21:25:46,868 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=29.12 vs. limit=8.23375
2024-10-08 21:25:47,344 WARNING [optim.py:503] Scaling gradients by 0.07871808111667633, model_norm_threshold=470274506752.0
2024-10-08 21:25:47,502 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.079e+25, grad_sumsq=2.899e+27, orig_rms_sq=3.722e-03
2024-10-08 21:25:47,630 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1956.6666666666667, ans=0.2804333333333333
2024-10-08 21:25:48,697 WARNING [optim.py:503] Scaling gradients by 0.04469374939799309, model_norm_threshold=470274506752.0
2024-10-08 21:25:48,855 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.004e+25, grad_sumsq=2.208e+24, orig_rms_sq=9.075e+00
2024-10-08 21:25:48,990 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.min_positive, batch_count=1956.6666666666667, ans=0.08777083333333334
2024-10-08 21:25:49,922 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module2.whiten, num_groups=1, num_channels=192, metric=2.12 vs. limit=8.23375
2024-10-08 21:25:50,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=1956.6666666666667, ans=0.40828125
2024-10-08 21:25:51,178 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.76 vs. limit=5.9783333333333335
2024-10-08 21:25:51,367 WARNING [optim.py:503] Scaling gradients by 0.0034134644083678722, model_norm_threshold=470274506752.0
2024-10-08 21:25:51,523 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.977e+27, grad_sumsq=1.606e+30, orig_rms_sq=3.722e-03
2024-10-08 21:25:53,689 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.11 vs. limit=4.782666666666667
2024-10-08 21:25:56,254 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=46.53 vs. limit=8.23375
2024-10-08 21:25:56,745 WARNING [optim.py:503] Scaling gradients by 0.061993252485990524, model_norm_threshold=470274506752.0
2024-10-08 21:25:57,409 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.303e+25, grad_sumsq=4.080e+26, orig_rms_sq=3.193e-02
2024-10-08 21:25:58,637 WARNING [optim.py:503] Scaling gradients by 0.006238467525690794, model_norm_threshold=470274506752.0
2024-10-08 21:25:58,793 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.047e+27, grad_sumsq=3.245e+28, orig_rms_sq=3.227e-02
2024-10-08 21:26:00,751 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=23.90 vs. limit=8.97
2024-10-08 21:26:00,767 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=18.28 vs. limit=8.235
2024-10-08 21:26:06,999 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.22 vs. limit=8.235
2024-10-08 21:26:08,745 WARNING [optim.py:503] Scaling gradients by 0.09468371421098709, model_norm_threshold=470274506752.0
2024-10-08 21:26:08,900 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.436e+24, grad_sumsq=1.952e+24, orig_rms_sq=2.785e+00
2024-10-08 21:26:10,146 WARNING [optim.py:503] Scaling gradients by 0.09590302407741547, model_norm_threshold=470274506752.0
2024-10-08 21:26:10,301 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.005e+24, grad_sumsq=1.240e+26, orig_rms_sq=3.230e-02
2024-10-08 21:26:11,171 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.10 vs. limit=8.23625
2024-10-08 21:26:12,666 WARNING [optim.py:503] Scaling gradients by 0.006876222789287567, model_norm_threshold=470274506752.0
2024-10-08 21:26:12,822 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.663e+26, grad_sumsq=2.682e+28, orig_rms_sq=3.230e-02
2024-10-08 21:26:19,034 WARNING [optim.py:503] Scaling gradients by 0.02617643214762211, model_norm_threshold=470274506752.0
2024-10-08 21:26:19,190 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.510e+25, grad_sumsq=1.986e+25, orig_rms_sq=2.774e+00
2024-10-08 21:26:20,408 WARNING [optim.py:503] Scaling gradients by 0.055150024592876434, model_norm_threshold=470274506752.0
2024-10-08 21:26:20,569 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.501e+25, grad_sumsq=5.410e+24, orig_rms_sq=2.774e+00
2024-10-08 21:26:24,385 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.094e+09 1.262e+11 6.038e+11 4.867e+12 3.465e+14, threshold=1.208e+12, percent-clipped=52.0
2024-10-08 21:26:24,436 INFO [train.py:1152] Epoch 1, batch 5900, loss[loss=0.7757, ctc_loss=1.063, attn_decoder_loss=0.7039, over 4808.00 frames. ], tot_loss[loss=0.771, ctc_loss=1.044, attn_decoder_loss=0.7027, over 966715.50 frames. ], batch size: 34, lr: 3.99e-02,
2024-10-08 21:26:27,796 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.51 vs. limit=8.975
2024-10-08 21:26:30,409 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=5.36 vs. limit=4.786666666666667
2024-10-08 21:26:31,049 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=1966.6666666666667, ans=0.4078125
2024-10-08 21:26:34,819 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=1966.6666666666667, ans=0.4078125
2024-10-08 21:26:35,435 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=10.74 vs. limit=5.491666666666667
2024-10-08 21:26:35,773 WARNING [optim.py:503] Scaling gradients by 0.07043133676052094, model_norm_threshold=1207605526528.0
2024-10-08 21:26:35,930 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.737e+25, grad_sumsq=2.200e+28, orig_rms_sq=3.972e-03
2024-10-08 21:26:39,421 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.44 vs. limit=8.23875
2024-10-08 21:26:46,026 WARNING [optim.py:503] Scaling gradients by 0.04013452306389809, model_norm_threshold=1207605526528.0
2024-10-08 21:26:46,184 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.635e+26, grad_sumsq=8.136e+27, orig_rms_sq=3.238e-02
2024-10-08 21:26:46,382 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=1970.0, ans=0.055675
2024-10-08 21:26:48,606 WARNING [optim.py:503] Scaling gradients by 0.09032987803220749, model_norm_threshold=1207605526528.0
2024-10-08 21:26:48,763 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.351e+25, grad_sumsq=1.893e+28, orig_rms_sq=3.884e-03
2024-10-08 21:26:50,256 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=1973.3333333333333, ans=0.4075
2024-10-08 21:26:53,181 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=19.79 vs. limit=8.24
2024-10-08 21:26:58,548 WARNING [optim.py:503] Scaling gradients by 0.04876163601875305, model_norm_threshold=1207605526528.0
2024-10-08 21:26:58,708 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.994e+26, grad_sumsq=6.114e+27, orig_rms_sq=3.262e-02
2024-10-08 21:27:00,525 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=14.92 vs. limit=8.24
2024-10-08 21:27:01,196 WARNING [optim.py:503] Scaling gradients by 0.08228477835655212, model_norm_threshold=1207605526528.0
2024-10-08 21:27:01,354 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.438e+25, grad_sumsq=2.251e+27, orig_rms_sq=3.305e-02
2024-10-08 21:27:02,545 WARNING [optim.py:503] Scaling gradients by 0.024778220802545547, model_norm_threshold=1207605526528.0
2024-10-08 21:27:02,701 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.246e+26, grad_sumsq=1.890e+28, orig_rms_sq=3.305e-02
2024-10-08 21:27:05,148 WARNING [optim.py:503] Scaling gradients by 0.06933672726154327, model_norm_threshold=1207605526528.0
2024-10-08 21:27:05,305 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.033e+26, grad_sumsq=3.073e+27, orig_rms_sq=3.361e-02
2024-10-08 21:27:06,461 WARNING [optim.py:503] Scaling gradients by 0.03680157661437988, model_norm_threshold=1207605526528.0
2024-10-08 21:27:06,618 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.000e+26, grad_sumsq=1.016e+29, orig_rms_sq=3.938e-03
2024-10-08 21:27:10,577 WARNING [optim.py:503] Scaling gradients by 0.09756362438201904, model_norm_threshold=1207605526528.0
2024-10-08 21:27:10,738 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.059e+25, grad_sumsq=1.181e+27, orig_rms_sq=3.437e-02
2024-10-08 21:27:13,673 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.34 vs. limit=3.2965
2024-10-08 21:27:28,135 INFO [train.py:1152] Epoch 1, batch 5950, loss[loss=0.7577, ctc_loss=1.003, attn_decoder_loss=0.6963, over 4823.00 frames. ], tot_loss[loss=0.7708, ctc_loss=1.043, attn_decoder_loss=0.7027, over 966052.83 frames. ], batch size: 34, lr: 3.98e-02,
2024-10-08 21:27:30,326 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.29 vs. limit=8.24375
2024-10-08 21:27:37,638 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=14.25 vs. limit=5.991666666666666
2024-10-08 21:27:41,826 WARNING [optim.py:503] Scaling gradients by 0.0033372859470546246, model_norm_threshold=1207605526528.0
2024-10-08 21:27:41,980 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.214e+28, grad_sumsq=1.160e+30, orig_rms_sq=3.631e-02
2024-10-08 21:27:47,143 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=1986.6666666666667, ans=0.8304666666666667
2024-10-08 21:27:54,175 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=25.47 vs. limit=8.9925
2024-10-08 21:27:57,578 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=31.80 vs. limit=8.24625
2024-10-08 21:28:03,313 WARNING [optim.py:503] Scaling gradients by 0.08439341932535172, model_norm_threshold=1207605526528.0
2024-10-08 21:28:03,470 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.292e+25, grad_sumsq=1.129e+27, orig_rms_sq=3.801e-02
2024-10-08 21:28:06,079 WARNING [optim.py:503] Scaling gradients by 0.04011773318052292, model_norm_threshold=1207605526528.0
2024-10-08 21:28:06,236 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.829e+26, grad_sumsq=1.002e+28, orig_rms_sq=3.822e-02
2024-10-08 21:28:09,942 WARNING [optim.py:503] Scaling gradients by 0.047058816999197006, model_norm_threshold=1207605526528.0
2024-10-08 21:28:10,098 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.592e+26, grad_sumsq=6.310e+25, orig_rms_sq=2.523e+00
2024-10-08 21:28:11,600 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=1993.3333333333333, ans=0.8302333333333334
2024-10-08 21:28:11,615 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=1993.3333333333333, ans=0.055150000000000005
2024-10-08 21:28:16,380 WARNING [optim.py:503] Scaling gradients by 0.02141188457608223, model_norm_threshold=1207605526528.0
2024-10-08 21:28:16,537 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.167e+27, grad_sumsq=4.592e+26, orig_rms_sq=2.542e+00
2024-10-08 21:28:18,404 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=29.84 vs. limit=8.24875
2024-10-08 21:28:18,991 WARNING [optim.py:503] Scaling gradients by 0.09583476185798645, model_norm_threshold=1207605526528.0
2024-10-08 21:28:19,149 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.427e+25, grad_sumsq=8.772e+27, orig_rms_sq=3.907e-03
2024-10-08 21:28:20,758 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=1996.6666666666667, ans=6.247916666666667
2024-10-08 21:28:21,764 WARNING [optim.py:503] Scaling gradients by 0.001325894263572991, model_norm_threshold=1207605526528.0
2024-10-08 21:28:21,919 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.968e+29, grad_sumsq=7.340e+31, orig_rms_sq=4.044e-03
2024-10-08 21:28:23,009 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.94 vs. limit=8.9975
2024-10-08 21:28:24,013 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=47.55 vs. limit=8.24875
2024-10-08 21:28:24,409 WARNING [optim.py:503] Scaling gradients by 0.04888880252838135, model_norm_threshold=1207605526528.0
2024-10-08 21:28:24,564 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.850e+26, grad_sumsq=5.080e+27, orig_rms_sq=3.641e-02
2024-10-08 21:28:32,321 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.487e+09 9.515e+10 5.610e+11 6.515e+12 9.108e+14, threshold=1.122e+12, percent-clipped=38.0
2024-10-08 21:28:32,322 WARNING [optim.py:503] Scaling gradients by 0.003378157503902912, model_norm_threshold=1121961639936.0
2024-10-08 21:28:32,479 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.163e+28, grad_sumsq=8.850e+29, orig_rms_sq=3.573e-02
2024-10-08 21:28:32,537 INFO [train.py:1152] Epoch 1, batch 6000, loss[loss=0.8215, ctc_loss=1.15, attn_decoder_loss=0.7394, over 4795.00 frames. ], tot_loss[loss=0.7722, ctc_loss=1.046, attn_decoder_loss=0.7038, over 966652.71 frames. ], batch size: 49, lr: 3.98e-02,
2024-10-08 21:28:32,538 INFO [train.py:1175] Computing validation loss
2024-10-08 21:28:36,468 INFO [zipformer.py:1858] name=encoder.encoders.3.encoder.layers.2.self_attn_weights, attn_weights_entropy = tensor([0.0575, 0.0776, 0.0973, 0.1085, 0.1113, 0.0998, 0.0801, 0.1148],
       device='cuda:0')
2024-10-08 21:28:38,726 INFO [train.py:1184] Epoch 1, validation: loss=0.8066, ctc_loss=1.082, attn_decoder_loss=0.7379, over 90464.00 frames.
2024-10-08 21:28:38,727 INFO [train.py:1185] Maximum memory allocated so far is 6925MB
2024-10-08 21:28:39,880 WARNING [optim.py:503] Scaling gradients by 0.0059779128059744835, model_norm_threshold=1121961639936.0
2024-10-08 21:28:40,036 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.440e+27, grad_sumsq=3.211e+27, orig_rms_sq=2.940e+00
2024-10-08 21:28:40,174 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=2000.0, ans=0.40625
2024-10-08 21:28:42,030 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=28.71 vs. limit=9.0
2024-10-08 21:28:44,189 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.04 vs. limit=8.25
2024-10-08 21:28:48,639 WARNING [optim.py:503] Scaling gradients by 0.00571634154766798, model_norm_threshold=1121961639936.0
2024-10-08 21:28:48,795 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.870e+28, grad_sumsq=7.018e+27, orig_rms_sq=2.665e+00
2024-10-08 21:28:53,964 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2003.3333333333333, ans=0.27996666666666664
2024-10-08 21:28:55,018 WARNING [optim.py:503] Scaling gradients by 0.04471973329782486, model_norm_threshold=1121961639936.0
2024-10-08 21:28:55,175 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.866e+26, grad_sumsq=5.310e+27, orig_rms_sq=3.515e-02
2024-10-08 21:28:56,432 WARNING [optim.py:503] Scaling gradients by 0.052789200097322464, model_norm_threshold=1121961639936.0
2024-10-08 21:28:56,587 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.251e+26, grad_sumsq=2.829e+28, orig_rms_sq=4.420e-03
2024-10-08 21:29:03,646 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.01 vs. limit=5.501666666666667
2024-10-08 21:29:04,155 WARNING [optim.py:503] Scaling gradients by 0.014555390924215317, model_norm_threshold=1121961639936.0
2024-10-08 21:29:04,311 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.032e+27, grad_sumsq=7.452e+26, orig_rms_sq=2.727e+00
2024-10-08 21:29:04,888 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.99 vs. limit=8.2525
2024-10-08 21:29:06,775 WARNING [optim.py:503] Scaling gradients by 0.013374989852309227, model_norm_threshold=1121961639936.0
2024-10-08 21:29:07,437 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.761e+27, grad_sumsq=6.307e+26, orig_rms_sq=2.793e+00
2024-10-08 21:29:08,901 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=2006.6666666666667, ans=0.8297666666666667
2024-10-08 21:29:09,559 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.53 vs. limit=6.003333333333334
2024-10-08 21:29:12,521 WARNING [optim.py:503] Scaling gradients by 0.003151163924485445, model_norm_threshold=1121961639936.0
2024-10-08 21:29:12,679 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.536e+28, grad_sumsq=9.547e+27, orig_rms_sq=2.657e+00
2024-10-08 21:29:13,184 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.73 vs. limit=4.802666666666667
2024-10-08 21:29:14,024 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=2006.6666666666667, ans=0.12475
2024-10-08 21:29:14,864 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module1.whiten, num_groups=1, num_channels=192, metric=4.26 vs. limit=8.2525
2024-10-08 21:29:16,259 WARNING [optim.py:503] Scaling gradients by 0.0001748087233863771, model_norm_threshold=1121961639936.0
2024-10-08 21:29:16,415 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.550e+31, grad_sumsq=3.365e+33, orig_rms_sq=4.608e-03
2024-10-08 21:29:20,436 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2010.0, ans=0.2799
2024-10-08 21:29:22,659 WARNING [optim.py:503] Scaling gradients by 0.018846947699785233, model_norm_threshold=1121961639936.0
2024-10-08 21:29:22,814 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.560e+26, grad_sumsq=2.740e+28, orig_rms_sq=3.489e-02
2024-10-08 21:29:30,405 WARNING [optim.py:503] Scaling gradients by 0.018908139318227768, model_norm_threshold=1121961639936.0
2024-10-08 21:29:30,562 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.006e+27, grad_sumsq=2.850e+28, orig_rms_sq=3.529e-02
2024-10-08 21:29:33,127 WARNING [optim.py:503] Scaling gradients by 0.084182508289814, model_norm_threshold=1121961639936.0
2024-10-08 21:29:33,284 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.184e+25, grad_sumsq=1.341e+28, orig_rms_sq=4.610e-03
2024-10-08 21:29:38,302 WARNING [optim.py:503] Scaling gradients by 0.012430872768163681, model_norm_threshold=1121961639936.0
2024-10-08 21:29:38,459 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.340e+27, grad_sumsq=6.605e+28, orig_rms_sq=3.543e-02
2024-10-08 21:29:41,705 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=27.63 vs. limit=9.01
2024-10-08 21:29:43,525 INFO [train.py:1152] Epoch 1, batch 6050, loss[loss=0.8059, ctc_loss=1.084, attn_decoder_loss=0.7363, over 4813.00 frames. ], tot_loss[loss=0.7722, ctc_loss=1.045, attn_decoder_loss=0.7039, over 966584.47 frames. ], batch size: 19, lr: 3.97e-02,
2024-10-08 21:29:46,563 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=12.88 vs. limit=6.008333333333333
2024-10-08 21:29:47,118 WARNING [optim.py:503] Scaling gradients by 0.04151812940835953, model_norm_threshold=1121961639936.0
2024-10-08 21:29:47,276 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.520e+26, grad_sumsq=4.256e+27, orig_rms_sq=3.571e-02
2024-10-08 21:29:49,040 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=44.29 vs. limit=8.25625
2024-10-08 21:29:53,415 WARNING [optim.py:503] Scaling gradients by 0.0035933591425418854, model_norm_threshold=1121961639936.0
2024-10-08 21:29:53,570 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.795e+28, grad_sumsq=7.786e+29, orig_rms_sq=3.590e-02
2024-10-08 21:29:56,051 WARNING [optim.py:503] Scaling gradients by 0.026302238926291466, model_norm_threshold=1121961639936.0
2024-10-08 21:29:56,209 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.989e+26, grad_sumsq=1.372e+28, orig_rms_sq=3.636e-02
2024-10-08 21:29:59,867 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.70 vs. limit=9.015
2024-10-08 21:30:03,593 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.27 vs. limit=8.2575
2024-10-08 21:30:03,849 WARNING [optim.py:503] Scaling gradients by 0.015224488452076912, model_norm_threshold=1121961639936.0
2024-10-08 21:30:04,005 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.517e+27, grad_sumsq=4.099e+28, orig_rms_sq=3.702e-02
2024-10-08 21:30:05,257 WARNING [optim.py:503] Scaling gradients by 0.014037140645086765, model_norm_threshold=1121961639936.0
2024-10-08 21:30:05,415 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.998e+27, grad_sumsq=6.546e+26, orig_rms_sq=3.052e+00
2024-10-08 21:30:07,886 WARNING [optim.py:503] Scaling gradients by 0.004547284450381994, model_norm_threshold=1121961639936.0
2024-10-08 21:30:08,041 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.594e+28, grad_sumsq=3.299e+30, orig_rms_sq=4.832e-03
2024-10-08 21:30:08,721 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=10.76 vs. limit=5.505833333333333
2024-10-08 21:30:09,290 WARNING [optim.py:503] Scaling gradients by 0.034634336829185486, model_norm_threshold=1121961639936.0
2024-10-08 21:30:09,447 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.120e+26, grad_sumsq=8.526e+28, orig_rms_sq=4.832e-03
2024-10-08 21:30:10,974 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=2023.3333333333333, ans=0.124125
2024-10-08 21:30:12,671 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.80 vs. limit=6.011666666666667
2024-10-08 21:30:13,237 WARNING [optim.py:503] Scaling gradients by 7.87658427725546e-05, model_norm_threshold=1121961639936.0
2024-10-08 21:30:13,394 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.733e+31, grad_sumsq=2.318e+33, orig_rms_sq=3.768e-02
2024-10-08 21:30:18,572 WARNING [optim.py:503] Scaling gradients by 0.0024065766483545303, model_norm_threshold=1121961639936.0
2024-10-08 21:30:18,728 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.795e+28, grad_sumsq=1.280e+30, orig_rms_sq=3.746e-02
2024-10-08 21:30:31,130 WARNING [optim.py:503] Scaling gradients by 0.020195920020341873, model_norm_threshold=1121961639936.0
2024-10-08 21:30:31,285 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.554e+26, grad_sumsq=2.374e+28, orig_rms_sq=3.603e-02
2024-10-08 21:30:32,501 WARNING [optim.py:503] Scaling gradients by 0.01672319695353508, model_norm_threshold=1121961639936.0
2024-10-08 21:30:32,658 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.089e+27, grad_sumsq=3.782e+26, orig_rms_sq=2.878e+00
2024-10-08 21:30:32,804 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=2026.6666666666667, ans=0.405
2024-10-08 21:30:35,604 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.98 vs. limit=9.0225
2024-10-08 21:30:40,224 WARNING [optim.py:503] Scaling gradients by 0.011141372844576836, model_norm_threshold=1121961639936.0
2024-10-08 21:30:40,378 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.335e+27, grad_sumsq=5.274e+29, orig_rms_sq=4.428e-03
2024-10-08 21:30:42,683 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.70 vs. limit=6.015
2024-10-08 21:30:47,969 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.237e+09 1.431e+11 7.945e+11 7.746e+12 1.424e+16, threshold=1.589e+12, percent-clipped=45.0
2024-10-08 21:30:48,020 INFO [train.py:1152] Epoch 1, batch 6100, loss[loss=0.8054, ctc_loss=1.099, attn_decoder_loss=0.732, over 4787.00 frames. ], tot_loss[loss=0.7723, ctc_loss=1.047, attn_decoder_loss=0.7037, over 966083.82 frames. ], batch size: 34, lr: 3.96e-02,
2024-10-08 21:30:55,566 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=2033.3333333333333, ans=0.4046875
2024-10-08 21:30:57,769 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.00 vs. limit=3.305
2024-10-08 21:31:00,577 WARNING [optim.py:503] Scaling gradients by 0.0016071107238531113, model_norm_threshold=1589089533952.0
2024-10-08 21:31:00,733 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.828e+29, grad_sumsq=9.001e+31, orig_rms_sq=4.252e-03
2024-10-08 21:31:02,042 WARNING [optim.py:503] Scaling gradients by 0.06810344755649567, model_norm_threshold=1589089533952.0
2024-10-08 21:31:02,198 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.682e+26, grad_sumsq=6.167e+25, orig_rms_sq=2.728e+00
2024-10-08 21:31:05,814 WARNING [optim.py:503] Scaling gradients by 0.0008773775189183652, model_norm_threshold=1589089533952.0
2024-10-08 21:31:05,969 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.457e+29, grad_sumsq=2.520e+29, orig_rms_sq=2.959e+00
2024-10-08 21:31:08,398 WARNING [optim.py:503] Scaling gradients by 0.001670520519837737, model_norm_threshold=1589089533952.0
2024-10-08 21:31:08,554 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.611e+29, grad_sumsq=5.690e+31, orig_rms_sq=4.588e-03
2024-10-08 21:31:09,187 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=25.50 vs. limit=9.0275
2024-10-08 21:31:10,034 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=2036.6666666666667, ans=0.8287166666666667
2024-10-08 21:31:12,219 WARNING [optim.py:503] Scaling gradients by 0.020079869776964188, model_norm_threshold=1589089533952.0
2024-10-08 21:31:12,375 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.739e+27, grad_sumsq=5.037e+28, orig_rms_sq=3.452e-02
2024-10-08 21:31:13,600 WARNING [optim.py:503] Scaling gradients by 0.0044794934801757336, model_norm_threshold=1589089533952.0
2024-10-08 21:31:13,760 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.417e+28, grad_sumsq=9.895e+29, orig_rms_sq=3.453e-02
2024-10-08 21:31:16,156 WARNING [optim.py:503] Scaling gradients by 0.0021987033542245626, model_norm_threshold=1589089533952.0
2024-10-08 21:31:16,312 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.553e+29, grad_sumsq=4.497e+30, orig_rms_sq=3.453e-02
2024-10-08 21:31:17,516 WARNING [optim.py:503] Scaling gradients by 0.015861960127949715, model_norm_threshold=1589089533952.0
2024-10-08 21:31:17,676 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.877e+27, grad_sumsq=5.436e+28, orig_rms_sq=3.453e-02
2024-10-08 21:31:21,525 WARNING [optim.py:503] Scaling gradients by 0.03359716013073921, model_norm_threshold=1589089533952.0
2024-10-08 21:31:21,701 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.066e+26, grad_sumsq=1.461e+28, orig_rms_sq=3.467e-02
2024-10-08 21:31:23,723 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=4.99 vs. limit=4.816
2024-10-08 21:31:23,734 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=2040.0, ans=9.03
2024-10-08 21:31:24,232 WARNING [optim.py:503] Scaling gradients by 0.020084090530872345, model_norm_threshold=1589089533952.0
2024-10-08 21:31:24,392 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.441e+27, grad_sumsq=4.197e+28, orig_rms_sq=3.434e-02
2024-10-08 21:31:25,005 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.21 vs. limit=6.02
2024-10-08 21:31:28,197 WARNING [optim.py:503] Scaling gradients by 0.0012174990260973573, model_norm_threshold=1589089533952.0
2024-10-08 21:31:28,353 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.087e+29, grad_sumsq=1.748e+29, orig_rms_sq=2.910e+00
2024-10-08 21:31:30,795 WARNING [optim.py:503] Scaling gradients by 0.01954483427107334, model_norm_threshold=1589089533952.0
2024-10-08 21:31:30,950 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.214e+27, grad_sumsq=6.576e+28, orig_rms_sq=3.367e-02
2024-10-08 21:31:32,156 WARNING [optim.py:503] Scaling gradients by 0.010164231061935425, model_norm_threshold=1589089533952.0
2024-10-08 21:31:32,311 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.168e+27, grad_sumsq=3.232e+27, orig_rms_sq=2.218e+00
2024-10-08 21:31:36,011 WARNING [optim.py:503] Scaling gradients by 0.0022604221012443304, model_norm_threshold=1589089533952.0
2024-10-08 21:31:36,168 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.036e+29, grad_sumsq=3.507e+28, orig_rms_sq=2.954e+00
2024-10-08 21:31:37,383 WARNING [optim.py:503] Scaling gradients by 0.0009773990605026484, model_norm_threshold=1589089533952.0
2024-10-08 21:31:37,539 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.667e+29, grad_sumsq=2.934e+29, orig_rms_sq=2.954e+00
2024-10-08 21:31:40,068 WARNING [optim.py:503] Scaling gradients by 0.00022538538905791938, model_norm_threshold=1589089533952.0
2024-10-08 21:31:40,224 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.245e+31, grad_sumsq=3.716e+32, orig_rms_sq=3.351e-02
2024-10-08 21:31:42,246 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.45 vs. limit=9.035
2024-10-08 21:31:50,229 WARNING [optim.py:503] Scaling gradients by 0.08825404196977615, model_norm_threshold=1589089533952.0
2024-10-08 21:31:50,384 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.859e+25, grad_sumsq=6.560e+24, orig_rms_sq=8.932e+00
2024-10-08 21:31:53,042 INFO [train.py:1152] Epoch 1, batch 6150, loss[loss=0.82, ctc_loss=1.118, attn_decoder_loss=0.7456, over 4829.00 frames. ], tot_loss[loss=0.7725, ctc_loss=1.047, attn_decoder_loss=0.7039, over 966182.97 frames. ], batch size: 43, lr: 3.96e-02,
2024-10-08 21:31:53,768 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.87 vs. limit=8.26875
2024-10-08 21:31:56,997 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=2050.0, ans=0.13468750000000002
2024-10-08 21:31:57,693 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.03 vs. limit=9.0375
2024-10-08 21:31:59,418 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=4.266e+05
2024-10-08 21:31:59,843 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=44.46 vs. limit=8.26875
2024-10-08 21:32:03,010 WARNING [optim.py:503] Scaling gradients by 0.013511939905583858, model_norm_threshold=1589089533952.0
2024-10-08 21:32:03,165 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.358e+27, grad_sumsq=9.717e+29, orig_rms_sq=4.485e-03
2024-10-08 21:32:06,812 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=19.98 vs. limit=8.27
2024-10-08 21:32:08,669 WARNING [optim.py:503] Scaling gradients by 0.0027214880101382732, model_norm_threshold=1589089533952.0
2024-10-08 21:32:08,825 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.427e+29, grad_sumsq=4.171e+30, orig_rms_sq=3.422e-02
2024-10-08 21:32:13,945 WARNING [optim.py:503] Scaling gradients by 0.048416949808597565, model_norm_threshold=1589089533952.0
2024-10-08 21:32:14,103 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.275e+26, grad_sumsq=4.957e+28, orig_rms_sq=4.590e-03
2024-10-08 21:32:21,300 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.26 vs. limit=9.0425
2024-10-08 21:32:23,402 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.51 vs. limit=6.028333333333333
2024-10-08 21:32:23,560 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=5.56 vs. limit=4.822666666666667
2024-10-08 21:32:26,214 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.76 vs. limit=9.0425
2024-10-08 21:32:28,443 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.01 vs. limit=5.514166666666666
2024-10-08 21:32:28,508 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=26.76 vs. limit=8.27125
2024-10-08 21:32:35,226 WARNING [optim.py:503] Scaling gradients by 0.07776989787817001, model_norm_threshold=1589089533952.0
2024-10-08 21:32:35,381 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.126e+25, grad_sumsq=1.600e+28, orig_rms_sq=4.453e-03
2024-10-08 21:32:41,254 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.09 vs. limit=6.03
2024-10-08 21:32:42,710 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=4.80 vs. limit=4.412
2024-10-08 21:32:45,392 WARNING [optim.py:503] Scaling gradients by 0.007491671480238438, model_norm_threshold=1589089533952.0
2024-10-08 21:32:45,547 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.703e+27, grad_sumsq=2.636e+29, orig_rms_sq=3.301e-02
2024-10-08 21:32:54,427 WARNING [optim.py:503] Scaling gradients by 0.047427576035261154, model_norm_threshold=1589089533952.0
2024-10-08 21:32:54,584 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.909e+26, grad_sumsq=8.637e+27, orig_rms_sq=3.368e-02
2024-10-08 21:32:57,439 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.587e+10 3.165e+11 2.275e+12 1.341e+13 7.051e+15, threshold=4.550e+12, percent-clipped=54.0
2024-10-08 21:32:57,491 INFO [train.py:1152] Epoch 1, batch 6200, loss[loss=0.8804, ctc_loss=1.17, attn_decoder_loss=0.8081, over 4786.00 frames. ], tot_loss[loss=0.773, ctc_loss=1.048, attn_decoder_loss=0.7043, over 966399.01 frames. ], batch size: 29, lr: 3.95e-02,
2024-10-08 21:33:00,297 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 21:33:01,208 WARNING [optim.py:503] Scaling gradients by 0.06016898527741432, model_norm_threshold=4550168674304.0
2024-10-08 21:33:01,365 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.697e+27, grad_sumsq=4.966e+28, orig_rms_sq=3.417e-02
2024-10-08 21:33:02,136 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=21.21 vs. limit=8.275
2024-10-08 21:33:03,254 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.09 vs. limit=9.05
2024-10-08 21:33:05,987 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.06 vs. limit=6.033333333333333
2024-10-08 21:33:09,817 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.05 vs. limit=9.0525
2024-10-08 21:33:10,225 WARNING [optim.py:503] Scaling gradients by 0.08035053312778473, model_norm_threshold=4550168674304.0
2024-10-08 21:33:10,381 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.180e+27, grad_sumsq=2.419e+29, orig_rms_sq=4.877e-03
2024-10-08 21:33:14,052 WARNING [optim.py:503] Scaling gradients by 0.06167302280664444, model_norm_threshold=4550168674304.0
2024-10-08 21:33:14,209 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.105e+27, grad_sumsq=2.289e+29, orig_rms_sq=4.826e-03
2024-10-08 21:33:15,055 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.65 vs. limit=8.27625
2024-10-08 21:33:23,051 WARNING [optim.py:503] Scaling gradients by 0.039202988147735596, model_norm_threshold=4550168674304.0
2024-10-08 21:33:23,209 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.131e+27, grad_sumsq=1.173e+29, orig_rms_sq=3.521e-02
2024-10-08 21:33:29,532 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=2073.3333333333335, ans=0.05335
2024-10-08 21:33:31,874 WARNING [optim.py:503] Scaling gradients by 0.020220911130309105, model_norm_threshold=4550168674304.0
2024-10-08 21:33:32,031 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.551e+28, grad_sumsq=4.379e+29, orig_rms_sq=3.541e-02
2024-10-08 21:33:33,260 WARNING [optim.py:503] Scaling gradients by 0.012436717748641968, model_norm_threshold=4550168674304.0
2024-10-08 21:33:33,421 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.577e+28, grad_sumsq=1.014e+30, orig_rms_sq=3.527e-02
2024-10-08 21:33:47,838 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=23.25 vs. limit=8.28
2024-10-08 21:33:50,834 WARNING [optim.py:503] Scaling gradients by 0.062484923750162125, model_norm_threshold=4550168674304.0
2024-10-08 21:33:50,990 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.582e+27, grad_sumsq=4.507e+28, orig_rms_sq=3.510e-02
2024-10-08 21:33:53,410 WARNING [optim.py:503] Scaling gradients by 0.0898391604423523, model_norm_threshold=4550168674304.0
2024-10-08 21:33:53,566 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.941e+26, grad_sumsq=1.638e+29, orig_rms_sq=4.238e-03
2024-10-08 21:33:54,782 WARNING [optim.py:503] Scaling gradients by 0.02609284035861492, model_norm_threshold=4550168674304.0
2024-10-08 21:33:54,947 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.773e+27, grad_sumsq=1.942e+29, orig_rms_sq=3.487e-02
2024-10-08 21:33:55,460 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=5.21 vs. limit=4.832
2024-10-08 21:33:56,149 WARNING [optim.py:503] Scaling gradients by 0.023732610046863556, model_norm_threshold=4550168674304.0
2024-10-08 21:33:56,304 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.340e+27, grad_sumsq=2.855e+27, orig_rms_sq=2.921e+00
2024-10-08 21:34:00,290 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=2083.3333333333335, ans=0.8270833333333334
2024-10-08 21:34:00,624 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=15.12 vs. limit=8.28125
2024-10-08 21:34:01,474 INFO [train.py:1152] Epoch 1, batch 6250, loss[loss=0.7757, ctc_loss=1.059, attn_decoder_loss=0.7048, over 4731.00 frames. ], tot_loss[loss=0.7724, ctc_loss=1.045, attn_decoder_loss=0.7042, over 966780.96 frames. ], batch size: 26, lr: 3.94e-02,
2024-10-08 21:34:03,359 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=7.03 vs. limit=4.833333333333333
2024-10-08 21:34:08,321 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=81.71 vs. limit=8.28125
2024-10-08 21:34:13,712 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.47 vs. limit=6.043333333333333
2024-10-08 21:34:14,712 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=33.32 vs. limit=9.065
2024-10-08 21:34:15,314 WARNING [optim.py:503] Scaling gradients by 0.04500285163521767, model_norm_threshold=4550168674304.0
2024-10-08 21:34:15,474 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.476e+27, grad_sumsq=8.752e+26, orig_rms_sq=2.829e+00
2024-10-08 21:34:20,469 WARNING [optim.py:503] Scaling gradients by 0.09854157269001007, model_norm_threshold=4550168674304.0
2024-10-08 21:34:20,623 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.639e+26, grad_sumsq=2.254e+28, orig_rms_sq=3.389e-02
2024-10-08 21:34:24,900 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=38.46 vs. limit=6.043333333333333
2024-10-08 21:34:29,290 WARNING [optim.py:503] Scaling gradients by 0.010946903377771378, model_norm_threshold=4550168674304.0
2024-10-08 21:34:29,445 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.267e+28, grad_sumsq=1.194e+28, orig_rms_sq=2.736e+00
2024-10-08 21:34:33,012 WARNING [optim.py:503] Scaling gradients by 0.024567287415266037, model_norm_threshold=4550168674304.0
2024-10-08 21:34:33,167 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.406e+27, grad_sumsq=2.300e+29, orig_rms_sq=3.220e-02
2024-10-08 21:34:43,061 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.44 vs. limit=9.07
2024-10-08 21:34:53,028 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=46.93 vs. limit=6.048333333333333
2024-10-08 21:34:53,735 WARNING [optim.py:503] Scaling gradients by 0.008258139714598656, model_norm_threshold=4550168674304.0
2024-10-08 21:34:53,892 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.576e+28, grad_sumsq=3.093e+30, orig_rms_sq=3.097e-02
2024-10-08 21:34:57,661 WARNING [optim.py:503] Scaling gradients by 0.003264946397393942, model_norm_threshold=4550168674304.0
2024-10-08 21:34:57,820 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.146e+29, grad_sumsq=1.335e+31, orig_rms_sq=3.104e-02
2024-10-08 21:34:59,345 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2096.6666666666665, ans=0.40171875
2024-10-08 21:34:59,894 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=17.31 vs. limit=8.286249999999999
2024-10-08 21:35:04,287 WARNING [optim.py:503] Scaling gradients by 0.020724989473819733, model_norm_threshold=4550168674304.0
2024-10-08 21:35:04,444 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.525e+28, grad_sumsq=4.667e+27, orig_rms_sq=3.268e+00
2024-10-08 21:35:05,703 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.505e+10 1.759e+11 8.300e+11 2.130e+13 1.394e+15, threshold=1.660e+12, percent-clipped=37.0
2024-10-08 21:35:05,755 INFO [train.py:1152] Epoch 1, batch 6300, loss[loss=0.8006, ctc_loss=1.075, attn_decoder_loss=0.7321, over 4978.00 frames. ], tot_loss[loss=0.7706, ctc_loss=1.045, attn_decoder_loss=0.702, over 966447.34 frames. ], batch size: 19, lr: 3.94e-02,
2024-10-08 21:35:10,630 WARNING [optim.py:503] Scaling gradients by 0.048946037888526917, model_norm_threshold=1659914682368.0
2024-10-08 21:35:10,791 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.892e+26, grad_sumsq=6.471e+28, orig_rms_sq=4.470e-03
2024-10-08 21:35:11,348 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=24.29 vs. limit=9.075
2024-10-08 21:35:12,189 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=2100.0, ans=0.13187500000000002
2024-10-08 21:35:13,477 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=2100.0, ans=0.2315
2024-10-08 21:35:14,063 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=4.84 vs. limit=4.84
2024-10-08 21:35:17,232 WARNING [optim.py:503] Scaling gradients by 0.01910087838768959, model_norm_threshold=1659914682368.0
2024-10-08 21:35:17,389 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.195e+27, grad_sumsq=1.020e+29, orig_rms_sq=3.132e-02
2024-10-08 21:35:18,748 WARNING [optim.py:503] Scaling gradients by 0.016955681145191193, model_norm_threshold=1659914682368.0
2024-10-08 21:35:18,906 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.135e+27, grad_sumsq=1.001e+29, orig_rms_sq=3.132e-02
2024-10-08 21:35:22,205 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=15.34 vs. limit=8.28875
2024-10-08 21:35:22,702 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=2103.3333333333335, ans=0.40140625
2024-10-08 21:35:27,655 WARNING [optim.py:503] Scaling gradients by 0.004246002994477749, model_norm_threshold=1659914682368.0
2024-10-08 21:35:27,811 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.851e+28, grad_sumsq=1.545e+30, orig_rms_sq=3.139e-02
2024-10-08 21:35:28,995 WARNING [optim.py:503] Scaling gradients by 0.0004041413776576519, model_norm_threshold=1659914682368.0
2024-10-08 21:35:29,150 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.228e+30, grad_sumsq=1.347e+32, orig_rms_sq=3.139e-02
2024-10-08 21:35:30,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=2106.6666666666665, ans=0.47420000000000073
2024-10-08 21:35:31,652 WARNING [optim.py:503] Scaling gradients by 0.05023008957505226, model_norm_threshold=1659914682368.0
2024-10-08 21:35:31,808 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.470e+26, grad_sumsq=1.103e+28, orig_rms_sq=3.146e-02
2024-10-08 21:35:32,502 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=4.65 vs. limit=4.842666666666666
2024-10-08 21:35:33,864 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=15.64 vs. limit=8.29
2024-10-08 21:35:37,169 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=2106.6666666666665, ans=0.8262666666666667
2024-10-08 21:35:43,884 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=34.12 vs. limit=8.29125
2024-10-08 21:35:46,902 WARNING [optim.py:503] Scaling gradients by 0.09636810421943665, model_norm_threshold=1659914682368.0
2024-10-08 21:35:47,058 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.112e+26, grad_sumsq=2.411e+28, orig_rms_sq=4.612e-03
2024-10-08 21:35:48,295 WARNING [optim.py:503] Scaling gradients by 0.09633654356002808, model_norm_threshold=1659914682368.0
2024-10-08 21:35:48,451 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.619e+25, grad_sumsq=2.695e+27, orig_rms_sq=3.198e-02
2024-10-08 21:35:50,362 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.93 vs. limit=8.29125
2024-10-08 21:35:53,533 WARNING [optim.py:503] Scaling gradients by 0.0913562923669815, model_norm_threshold=1659914682368.0
2024-10-08 21:35:53,689 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.622e+25, grad_sumsq=1.446e+28, orig_rms_sq=4.581e-03
2024-10-08 21:35:57,263 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.97 vs. limit=9.085
2024-10-08 21:35:58,632 WARNING [optim.py:503] Scaling gradients by 0.0242125503718853, model_norm_threshold=1659914682368.0
2024-10-08 21:35:58,789 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.322e+27, grad_sumsq=3.927e+26, orig_rms_sq=3.366e+00
2024-10-08 21:36:02,407 WARNING [optim.py:503] Scaling gradients by 0.03460774943232536, model_norm_threshold=1659914682368.0
2024-10-08 21:36:02,562 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.258e+26, grad_sumsq=1.989e+28, orig_rms_sq=3.146e-02
2024-10-08 21:36:02,773 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=2113.3333333333335, ans=0.12075
2024-10-08 21:36:03,469 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.39 vs. limit=5.528333333333333
2024-10-08 21:36:07,447 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.33 vs. limit=8.2925
2024-10-08 21:36:07,800 WARNING [optim.py:503] Scaling gradients by 0.0033238576725125313, model_norm_threshold=1659914682368.0
2024-10-08 21:36:07,957 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.60, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.488e+29, grad_sumsq=4.459e+28, orig_rms_sq=3.336e+00
2024-10-08 21:36:10,476 INFO [train.py:1152] Epoch 1, batch 6350, loss[loss=0.8452, ctc_loss=1.177, attn_decoder_loss=0.7622, over 4817.00 frames. ], tot_loss[loss=0.7698, ctc_loss=1.045, attn_decoder_loss=0.701, over 966135.03 frames. ], batch size: 36, lr: 3.93e-02,
2024-10-08 21:36:11,720 WARNING [optim.py:503] Scaling gradients by 0.00951236393302679, model_norm_threshold=1659914682368.0
2024-10-08 21:36:11,877 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.53, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.627e+28, grad_sumsq=4.878e+27, orig_rms_sq=3.336e+00
2024-10-08 21:36:14,219 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=13.49 vs. limit=9.0875
2024-10-08 21:36:15,158 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=30.94 vs. limit=9.0875
2024-10-08 21:36:22,923 WARNING [optim.py:503] Scaling gradients by 0.06923523545265198, model_norm_threshold=1659914682368.0
2024-10-08 21:36:23,077 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.044e+26, grad_sumsq=1.161e+25, orig_rms_sq=8.993e+00
2024-10-08 21:36:23,712 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=18.12 vs. limit=8.295
2024-10-08 21:36:23,864 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.33 vs. limit=3.318
2024-10-08 21:36:30,749 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=2120.0, ans=0.1205
2024-10-08 21:36:31,763 WARNING [optim.py:503] Scaling gradients by 0.010048451833426952, model_norm_threshold=1659914682368.0
2024-10-08 21:36:31,924 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.226e+27, grad_sumsq=2.335e+27, orig_rms_sq=1.810e+00
2024-10-08 21:36:33,695 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=11.77 vs. limit=6.0600000000000005
2024-10-08 21:36:33,896 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.17 vs. limit=3.318
2024-10-08 21:36:34,319 WARNING [optim.py:503] Scaling gradients by 0.004987790714949369, model_norm_threshold=1659914682368.0
2024-10-08 21:36:34,475 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.197e+28, grad_sumsq=9.355e+30, orig_rms_sq=4.487e-03
2024-10-08 21:36:37,757 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=22.11 vs. limit=8.29625
2024-10-08 21:36:38,206 WARNING [optim.py:503] Scaling gradients by 0.03107406198978424, model_norm_threshold=1659914682368.0
2024-10-08 21:36:38,363 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.118e+27, grad_sumsq=3.534e+28, orig_rms_sq=3.163e-02
2024-10-08 21:36:43,341 WARNING [optim.py:503] Scaling gradients by 0.00861283577978611, model_norm_threshold=1659914682368.0
2024-10-08 21:36:43,496 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.53, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.963e+28, grad_sumsq=4.427e+30, orig_rms_sq=4.435e-03
2024-10-08 21:36:44,991 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=2123.3333333333335, ans=0.2345833333333333
2024-10-08 21:36:46,815 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.18 vs. limit=9.092500000000001
2024-10-08 21:36:57,019 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=6.31 vs. limit=8.2975
2024-10-08 21:37:02,235 WARNING [optim.py:503] Scaling gradients by 0.0007051468710415065, model_norm_threshold=1659914682368.0
2024-10-08 21:37:02,391 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.998e+29, grad_sumsq=2.865e+31, orig_rms_sq=3.141e-02
2024-10-08 21:37:04,862 WARNING [optim.py:503] Scaling gradients by 0.06982407718896866, model_norm_threshold=1659914682368.0
2024-10-08 21:37:05,018 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.050e+26, grad_sumsq=1.172e+25, orig_rms_sq=8.964e+00
2024-10-08 21:37:07,755 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=2130.0, ans=0.82545
2024-10-08 21:37:11,100 WARNING [optim.py:503] Scaling gradients by 0.03023008443415165, model_norm_threshold=1659914682368.0
2024-10-08 21:37:11,257 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.177e+26, grad_sumsq=2.905e+28, orig_rms_sq=3.160e-02
2024-10-08 21:37:11,520 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=2130.0, ans=0.40015625
2024-10-08 21:37:12,420 WARNING [optim.py:503] Scaling gradients by 0.008405218832194805, model_norm_threshold=1659914682368.0
2024-10-08 21:37:12,575 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.020e+28, grad_sumsq=3.229e+29, orig_rms_sq=3.160e-02
2024-10-08 21:37:14,096 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.418e+09 2.691e+11 1.999e+12 1.576e+13 4.107e+15, threshold=3.998e+12, percent-clipped=50.0
2024-10-08 21:37:14,096 WARNING [optim.py:503] Scaling gradients by 0.09409330040216446, model_norm_threshold=3997697572864.0
2024-10-08 21:37:14,251 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.194e+26, grad_sumsq=6.789e+28, orig_rms_sq=4.705e-03
2024-10-08 21:37:14,309 INFO [train.py:1152] Epoch 1, batch 6400, loss[loss=0.7832, ctc_loss=1.077, attn_decoder_loss=0.7096, over 4882.00 frames. ], tot_loss[loss=0.768, ctc_loss=1.043, attn_decoder_loss=0.6992, over 965893.85 frames. ], batch size: 23, lr: 3.92e-02,
2024-10-08 21:37:16,773 WARNING [optim.py:503] Scaling gradients by 0.06078524887561798, model_norm_threshold=3997697572864.0
2024-10-08 21:37:16,929 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.806e+26, grad_sumsq=2.549e+26, orig_rms_sq=3.455e+00
2024-10-08 21:37:18,147 WARNING [optim.py:503] Scaling gradients by 0.011976495385169983, model_norm_threshold=3997697572864.0
2024-10-08 21:37:18,302 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.384e+28, grad_sumsq=7.192e+30, orig_rms_sq=4.705e-03
2024-10-08 21:37:24,571 WARNING [optim.py:503] Scaling gradients by 0.08588515967130661, model_norm_threshold=3997697572864.0
2024-10-08 21:37:24,728 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.936e+26, grad_sumsq=1.428e+29, orig_rms_sq=4.857e-03
2024-10-08 21:37:27,126 WARNING [optim.py:503] Scaling gradients by 0.008626547642052174, model_norm_threshold=3997697572864.0
2024-10-08 21:37:27,280 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.735e+28, grad_sumsq=1.593e+31, orig_rms_sq=4.857e-03
2024-10-08 21:37:28,110 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.84 vs. limit=9.1025
2024-10-08 21:37:29,798 WARNING [optim.py:503] Scaling gradients by 0.034512441605329514, model_norm_threshold=3997697572864.0
2024-10-08 21:37:29,955 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.178e+27, grad_sumsq=4.605e+26, orig_rms_sq=9.071e+00
2024-10-08 21:37:32,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2136.6666666666665, ans=0.39984375
2024-10-08 21:37:37,314 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.91 vs. limit=3.3205
2024-10-08 21:37:38,789 WARNING [optim.py:503] Scaling gradients by 0.06377612054347992, model_norm_threshold=3997697572864.0
2024-10-08 21:37:38,945 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.380e+27, grad_sumsq=3.046e+29, orig_rms_sq=4.531e-03
2024-10-08 21:37:41,128 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.86 vs. limit=5.535
2024-10-08 21:37:42,674 WARNING [optim.py:503] Scaling gradients by 0.0735597163438797, model_norm_threshold=3997697572864.0
2024-10-08 21:37:42,830 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.100e+27, grad_sumsq=3.560e+26, orig_rms_sq=3.089e+00
2024-10-08 21:37:44,738 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.02 vs. limit=3.321
2024-10-08 21:37:45,400 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=2140.0, ans=0.3996875
2024-10-08 21:37:49,231 WARNING [optim.py:503] Scaling gradients by 0.011811603792011738, model_norm_threshold=3997697572864.0
2024-10-08 21:37:49,388 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.694e+28, grad_sumsq=1.288e+31, orig_rms_sq=4.422e-03
2024-10-08 21:37:53,035 WARNING [optim.py:503] Scaling gradients by 0.08119191974401474, model_norm_threshold=3997697572864.0
2024-10-08 21:37:53,192 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.747e+26, grad_sumsq=1.163e+28, orig_rms_sq=3.221e-02
2024-10-08 21:37:54,379 WARNING [optim.py:503] Scaling gradients by 0.08048932254314423, model_norm_threshold=3997697572864.0
2024-10-08 21:37:54,537 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.083e+26, grad_sumsq=1.268e+28, orig_rms_sq=3.221e-02
2024-10-08 21:37:57,051 WARNING [optim.py:503] Scaling gradients by 0.011200353503227234, model_norm_threshold=3997697572864.0
2024-10-08 21:37:57,207 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.004e+28, grad_sumsq=9.117e+27, orig_rms_sq=2.198e+00
2024-10-08 21:37:59,206 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=23.53 vs. limit=9.1075
2024-10-08 21:38:00,916 WARNING [optim.py:503] Scaling gradients by 0.004781256429851055, model_norm_threshold=3997697572864.0
2024-10-08 21:38:01,073 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.989e+29, grad_sumsq=4.851e+31, orig_rms_sq=4.099e-03
2024-10-08 21:38:01,938 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.36 vs. limit=9.1075
2024-10-08 21:38:07,250 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=4.02 vs. limit=8.305
2024-10-08 21:38:11,451 WARNING [optim.py:503] Scaling gradients by 0.004818376619368792, model_norm_threshold=3997697572864.0
2024-10-08 21:38:11,612 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.936e+29, grad_sumsq=4.844e+31, orig_rms_sq=3.997e-03
2024-10-08 21:38:12,344 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.55 vs. limit=5.536666666666667
2024-10-08 21:38:16,041 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.63 vs. limit=3.322
2024-10-08 21:38:18,557 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.12 vs. limit=9.1125
2024-10-08 21:38:19,080 INFO [train.py:1152] Epoch 1, batch 6450, loss[loss=0.7536, ctc_loss=1.056, attn_decoder_loss=0.6781, over 4740.00 frames. ], tot_loss[loss=0.7684, ctc_loss=1.045, attn_decoder_loss=0.6991, over 965238.39 frames. ], batch size: 26, lr: 3.92e-02,
2024-10-08 21:38:19,652 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.45 vs. limit=6.075
2024-10-08 21:38:20,235 WARNING [optim.py:503] Scaling gradients by 1.618998976482544e-05, model_norm_threshold=3997697572864.0
2024-10-08 21:38:20,392 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.534e+34, grad_sumsq=4.946e+35, orig_rms_sq=3.102e-02
2024-10-08 21:38:21,642 WARNING [optim.py:503] Scaling gradients by 0.0012782798148691654, model_norm_threshold=3997697572864.0
2024-10-08 21:38:21,799 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.631e+30, grad_sumsq=9.430e+29, orig_rms_sq=2.790e+00
2024-10-08 21:38:24,301 WARNING [optim.py:503] Scaling gradients by 0.0035059223882853985, model_norm_threshold=3997697572864.0
2024-10-08 21:38:24,454 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.512e+29, grad_sumsq=1.306e+29, orig_rms_sq=2.689e+00
2024-10-08 21:38:27,735 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.59 vs. limit=9.1125
2024-10-08 21:38:28,197 WARNING [optim.py:503] Scaling gradients by 0.009776699356734753, model_norm_threshold=3997697572864.0
2024-10-08 21:38:28,353 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.077e+28, grad_sumsq=1.456e+28, orig_rms_sq=2.801e+00
2024-10-08 21:38:32,195 WARNING [optim.py:503] Scaling gradients by 0.012046162970364094, model_norm_threshold=3997697572864.0
2024-10-08 21:38:32,352 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.315e+28, grad_sumsq=7.958e+27, orig_rms_sq=2.909e+00
2024-10-08 21:38:34,726 WARNING [optim.py:503] Scaling gradients by 0.0832841545343399, model_norm_threshold=3997697572864.0
2024-10-08 21:38:34,881 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.869e+26, grad_sumsq=2.361e+26, orig_rms_sq=2.909e+00
2024-10-08 21:38:39,493 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=22.21 vs. limit=8.307500000000001
2024-10-08 21:38:43,015 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=2153.3333333333335, ans=0.05155
2024-10-08 21:38:44,049 WARNING [optim.py:503] Scaling gradients by 0.04723057895898819, model_norm_threshold=3997697572864.0
2024-10-08 21:38:44,205 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.980e+27, grad_sumsq=6.709e+28, orig_rms_sq=2.951e-02
2024-10-08 21:38:44,403 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=2156.6666666666665, ans=0.11912500000000001
2024-10-08 21:38:54,199 WARNING [optim.py:503] Scaling gradients by 0.054478660225868225, model_norm_threshold=3997697572864.0
2024-10-08 21:38:54,355 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.360e+27, grad_sumsq=3.098e+29, orig_rms_sq=4.390e-03
2024-10-08 21:38:57,571 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.25 vs. limit=9.120000000000001
2024-10-08 21:38:58,220 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=2160.0, ans=0.119
2024-10-08 21:39:00,343 WARNING [optim.py:503] Scaling gradients by 0.0069716027937829494, model_norm_threshold=3997697572864.0
2024-10-08 21:39:00,500 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.645e+29, grad_sumsq=3.649e+31, orig_rms_sq=4.509e-03
2024-10-08 21:39:01,882 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=2160.0, ans=0.39875
2024-10-08 21:39:03,116 WARNING [optim.py:503] Scaling gradients by 0.03133672475814819, model_norm_threshold=3997697572864.0
2024-10-08 21:39:03,273 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.208e+27, grad_sumsq=1.960e+27, orig_rms_sq=3.168e+00
2024-10-08 21:39:04,445 WARNING [optim.py:503] Scaling gradients by 0.0006028887000866234, model_norm_threshold=3997697572864.0
2024-10-08 21:39:04,601 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.477e+31, grad_sumsq=3.208e+33, orig_rms_sq=4.605e-03
2024-10-08 21:39:05,423 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.45 vs. limit=8.31
2024-10-08 21:39:07,001 WARNING [optim.py:503] Scaling gradients by 0.0015895324759185314, model_norm_threshold=3997697572864.0
2024-10-08 21:39:07,157 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.787e+30, grad_sumsq=6.267e+31, orig_rms_sq=2.851e-02
2024-10-08 21:39:07,967 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.58 vs. limit=3.324
2024-10-08 21:39:08,373 WARNING [optim.py:503] Scaling gradients by 0.016696374863386154, model_norm_threshold=3997697572864.0
2024-10-08 21:39:08,533 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.496e+28, grad_sumsq=5.260e+29, orig_rms_sq=2.845e-02
2024-10-08 21:39:11,115 WARNING [optim.py:503] Scaling gradients by 0.011304530315101147, model_norm_threshold=3997697572864.0
2024-10-08 21:39:11,276 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.930e+28, grad_sumsq=8.472e+30, orig_rms_sq=4.639e-03
2024-10-08 21:39:14,143 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer_ff2.min_abs, batch_count=2163.3333333333335, ans=0.054083333333333344
2024-10-08 21:39:14,938 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=12.69 vs. limit=8.31125
2024-10-08 21:39:16,583 WARNING [optim.py:503] Scaling gradients by 0.07917168736457825, model_norm_threshold=3997697572864.0
2024-10-08 21:39:16,739 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.167e+26, grad_sumsq=1.933e+29, orig_rms_sq=4.742e-03
2024-10-08 21:39:16,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=2163.3333333333335, ans=0.39859374999999997
2024-10-08 21:39:17,940 WARNING [optim.py:503] Scaling gradients by 0.021506493911147118, model_norm_threshold=3997697572864.0
2024-10-08 21:39:18,097 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.136e+27, grad_sumsq=2.146e+29, orig_rms_sq=2.859e-02
2024-10-08 21:39:22,652 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=8.09 vs. limit=8.31125
2024-10-08 21:39:23,932 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=28.49 vs. limit=9.125
2024-10-08 21:39:24,413 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.468e+10 4.904e+11 3.389e+12 5.049e+13 2.469e+17, threshold=6.779e+12, percent-clipped=50.0
2024-10-08 21:39:24,464 INFO [train.py:1152] Epoch 1, batch 6500, loss[loss=0.7507, ctc_loss=1.003, attn_decoder_loss=0.6876, over 4735.00 frames. ], tot_loss[loss=0.7669, ctc_loss=1.044, attn_decoder_loss=0.6976, over 964973.97 frames. ], batch size: 26, lr: 3.91e-02,
2024-10-08 21:39:25,587 WARNING [optim.py:503] Scaling gradients by 0.044924020767211914, model_norm_threshold=6778833600512.0
2024-10-08 21:39:25,743 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.647e+27, grad_sumsq=1.590e+29, orig_rms_sq=2.923e-02
2024-10-08 21:39:29,598 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2166.6666666666665, ans=0.3984375
2024-10-08 21:39:29,933 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=11.22 vs. limit=6.083333333333333
2024-10-08 21:39:31,978 WARNING [optim.py:503] Scaling gradients by 0.02880876511335373, model_norm_threshold=6778833600512.0
2024-10-08 21:39:32,134 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.720e+28, grad_sumsq=3.474e+30, orig_rms_sq=4.952e-03
2024-10-08 21:39:34,575 WARNING [optim.py:503] Scaling gradients by 0.016548536717891693, model_norm_threshold=6778833600512.0
2024-10-08 21:39:34,734 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.705e+28, grad_sumsq=5.241e+27, orig_rms_sq=8.977e+00
2024-10-08 21:39:35,581 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=44.05 vs. limit=8.3125
2024-10-08 21:39:35,940 WARNING [optim.py:503] Scaling gradients by 0.0027417538221925497, model_norm_threshold=6778833600512.0
2024-10-08 21:39:36,097 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.784e+30, grad_sumsq=6.127e+31, orig_rms_sq=2.912e-02
2024-10-08 21:39:36,363 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2170.0, ans=0.2783
2024-10-08 21:39:37,428 WARNING [optim.py:503] Scaling gradients by 0.010374157689511776, model_norm_threshold=6778833600512.0
2024-10-08 21:39:37,594 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.520e+29, grad_sumsq=3.083e+31, orig_rms_sq=4.930e-03
2024-10-08 21:39:38,871 WARNING [optim.py:503] Scaling gradients by 0.06459914892911911, model_norm_threshold=6778833600512.0
2024-10-08 21:39:39,027 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.490e+27, grad_sumsq=8.549e+28, orig_rms_sq=2.912e-02
2024-10-08 21:39:40,068 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.57 vs. limit=8.31375
2024-10-08 21:39:42,818 WARNING [optim.py:503] Scaling gradients by 0.057115763425827026, model_norm_threshold=6778833600512.0
2024-10-08 21:39:42,973 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.385e+27, grad_sumsq=9.957e+26, orig_rms_sq=3.399e+00
2024-10-08 21:39:50,711 WARNING [optim.py:503] Scaling gradients by 0.015650110319256783, model_norm_threshold=6778833600512.0
2024-10-08 21:39:50,868 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.213e+28, grad_sumsq=1.927e+30, orig_rms_sq=2.706e-02
2024-10-08 21:39:55,502 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=31.12 vs. limit=8.315
2024-10-08 21:39:59,926 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=2173.3333333333335, ans=0.398125
2024-10-08 21:40:00,874 WARNING [optim.py:503] Scaling gradients by 0.034277819097042084, model_norm_threshold=6778833600512.0
2024-10-08 21:40:01,030 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.898e+27, grad_sumsq=9.826e+26, orig_rms_sq=9.055e+00
2024-10-08 21:40:01,984 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.18 vs. limit=9.13
2024-10-08 21:40:02,240 WARNING [optim.py:503] Scaling gradients by 0.05605107918381691, model_norm_threshold=6778833600512.0
2024-10-08 21:40:02,395 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.951e+27, grad_sumsq=7.554e+29, orig_rms_sq=5.231e-03
2024-10-08 21:40:03,645 WARNING [optim.py:503] Scaling gradients by 0.052605729550123215, model_norm_threshold=6778833600512.0
2024-10-08 21:40:04,149 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.815e+27, grad_sumsq=1.436e+29, orig_rms_sq=2.657e-02
2024-10-08 21:40:06,100 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=6.12 vs. limit=4.870666666666667
2024-10-08 21:40:07,869 WARNING [optim.py:503] Scaling gradients by 0.035651929676532745, model_norm_threshold=6778833600512.0
2024-10-08 21:40:08,024 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.428e+27, grad_sumsq=3.102e+29, orig_rms_sq=2.717e-02
2024-10-08 21:40:13,056 WARNING [optim.py:503] Scaling gradients by 0.004122629761695862, model_norm_threshold=6778833600512.0
2024-10-08 21:40:13,212 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.843e+29, grad_sumsq=2.101e+29, orig_rms_sq=3.732e+00
2024-10-08 21:40:15,660 WARNING [optim.py:503] Scaling gradients by 0.09518758952617645, model_norm_threshold=6778833600512.0
2024-10-08 21:40:15,816 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.356e+27, grad_sumsq=4.876e+28, orig_rms_sq=2.781e-02
2024-10-08 21:40:19,589 WARNING [optim.py:503] Scaling gradients by 0.07933218032121658, model_norm_threshold=6778833600512.0
2024-10-08 21:40:19,746 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.852e+27, grad_sumsq=3.377e+29, orig_rms_sq=5.484e-03
2024-10-08 21:40:22,710 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=79.97 vs. limit=8.3175
2024-10-08 21:40:24,145 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=22.98 vs. limit=8.3175
2024-10-08 21:40:27,092 WARNING [optim.py:503] Scaling gradients by 0.051839616149663925, model_norm_threshold=6778833600512.0
2024-10-08 21:40:27,248 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.940e+27, grad_sumsq=9.325e+29, orig_rms_sq=5.298e-03
2024-10-08 21:40:29,677 WARNING [optim.py:503] Scaling gradients by 0.01119344774633646, model_norm_threshold=6778833600512.0
2024-10-08 21:40:29,834 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.412e+29, grad_sumsq=5.188e+30, orig_rms_sq=2.722e-02
2024-10-08 21:40:29,893 INFO [train.py:1152] Epoch 1, batch 6550, loss[loss=0.7371, ctc_loss=0.9807, attn_decoder_loss=0.6762, over 4978.00 frames. ], tot_loss[loss=0.7654, ctc_loss=1.041, attn_decoder_loss=0.6965, over 964827.84 frames. ], batch size: 19, lr: 3.91e-02,
2024-10-08 21:40:30,868 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.45 vs. limit=9.1375
2024-10-08 21:40:32,436 WARNING [optim.py:503] Scaling gradients by 0.005049525760114193, model_norm_threshold=6778833600512.0
2024-10-08 21:40:32,592 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.364e+29, grad_sumsq=4.821e+28, orig_rms_sq=9.054e+00
2024-10-08 21:40:33,775 WARNING [optim.py:503] Scaling gradients by 0.0978863388299942, model_norm_threshold=6778833600512.0
2024-10-08 21:40:33,930 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.707e+26, grad_sumsq=2.583e+26, orig_rms_sq=3.758e+00
2024-10-08 21:40:37,580 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.80 vs. limit=9.1375
2024-10-08 21:40:37,757 WARNING [optim.py:503] Scaling gradients by 0.05810539051890373, model_norm_threshold=6778833600512.0
2024-10-08 21:40:37,914 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.913e+27, grad_sumsq=1.062e+29, orig_rms_sq=2.743e-02
2024-10-08 21:40:40,629 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.min_positive, batch_count=2183.3333333333335, ans=0.08635416666666668
2024-10-08 21:40:42,941 WARNING [optim.py:503] Scaling gradients by 0.015229087322950363, model_norm_threshold=6778833600512.0
2024-10-08 21:40:43,098 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.400e+28, grad_sumsq=2.326e+30, orig_rms_sq=2.752e-02
2024-10-08 21:40:48,006 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=39.27 vs. limit=8.32
2024-10-08 21:40:52,137 WARNING [optim.py:503] Scaling gradients by 0.02615734562277794, model_norm_threshold=6778833600512.0
2024-10-08 21:40:52,293 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.608e+28, grad_sumsq=5.848e+29, orig_rms_sq=2.749e-02
2024-10-08 21:40:58,589 WARNING [optim.py:503] Scaling gradients by 0.08809895813465118, model_norm_threshold=6778833600512.0
2024-10-08 21:40:58,744 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.946e+27, grad_sumsq=4.033e+29, orig_rms_sq=4.824e-03
2024-10-08 21:41:01,248 WARNING [optim.py:503] Scaling gradients by 0.006432522553950548, model_norm_threshold=6778833600512.0
2024-10-08 21:41:01,406 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.486e+29, grad_sumsq=1.298e+31, orig_rms_sq=2.686e-02
2024-10-08 21:41:01,543 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=2190.0, ans=0.117875
2024-10-08 21:41:02,389 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.15 vs. limit=8.32125
2024-10-08 21:41:08,328 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=21.34 vs. limit=9.145
2024-10-08 21:41:10,268 WARNING [optim.py:503] Scaling gradients by 0.01825948804616928, model_norm_threshold=6778833600512.0
2024-10-08 21:41:10,426 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.354e+28, grad_sumsq=1.250e+30, orig_rms_sq=2.683e-02
2024-10-08 21:41:11,676 WARNING [optim.py:503] Scaling gradients by 0.02605966106057167, model_norm_threshold=6778833600512.0
2024-10-08 21:41:11,831 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.548e+28, grad_sumsq=5.768e+29, orig_rms_sq=2.683e-02
2024-10-08 21:41:12,009 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=2193.3333333333335, ans=0.3971875
2024-10-08 21:41:14,041 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=14.68 vs. limit=9.145
2024-10-08 21:41:14,332 WARNING [optim.py:503] Scaling gradients by 0.023450734093785286, model_norm_threshold=6778833600512.0
2024-10-08 21:41:14,500 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.520e+28, grad_sumsq=5.352e+30, orig_rms_sq=4.708e-03
2024-10-08 21:41:17,117 WARNING [optim.py:503] Scaling gradients by 0.0012002838775515556, model_norm_threshold=6778833600512.0
2024-10-08 21:41:17,280 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.191e+30, grad_sumsq=7.951e+29, orig_rms_sq=9.045e+00
2024-10-08 21:41:19,902 WARNING [optim.py:503] Scaling gradients by 0.06643485277891159, model_norm_threshold=6778833600512.0
2024-10-08 21:41:20,069 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.246e+27, grad_sumsq=9.475e+26, orig_rms_sq=2.370e+00
2024-10-08 21:41:22,147 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=5.65 vs. limit=4.878666666666667
2024-10-08 21:41:22,641 WARNING [optim.py:503] Scaling gradients by 0.0424715094268322, model_norm_threshold=6778833600512.0
2024-10-08 21:41:22,801 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.823e+27, grad_sumsq=2.531e+29, orig_rms_sq=2.696e-02
2024-10-08 21:41:29,494 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2196.6666666666665, ans=0.27803333333333335
2024-10-08 21:41:31,662 WARNING [optim.py:503] Scaling gradients by 0.0022790676448494196, model_norm_threshold=6778833600512.0
2024-10-08 21:41:31,820 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.659e+30, grad_sumsq=9.835e+31, orig_rms_sq=2.704e-02
2024-10-08 21:41:32,704 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=18.48 vs. limit=8.32375
2024-10-08 21:41:35,878 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.754e+10 1.324e+12 1.249e+13 1.187e+14 5.099e+17, threshold=2.499e+13, percent-clipped=56.0
2024-10-08 21:41:35,879 WARNING [optim.py:503] Scaling gradients by 4.900190106127411e-05, model_norm_threshold=24986832076800.0
2024-10-08 21:41:36,034 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.752e+34, grad_sumsq=1.892e+37, orig_rms_sq=5.154e-03
2024-10-08 21:41:36,093 INFO [train.py:1152] Epoch 1, batch 6600, loss[loss=0.8068, ctc_loss=1.142, attn_decoder_loss=0.7228, over 4835.00 frames. ], tot_loss[loss=0.7651, ctc_loss=1.042, attn_decoder_loss=0.696, over 965321.62 frames. ], batch size: 23, lr: 3.90e-02,
2024-10-08 21:41:36,248 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=2200.0, ans=0.5
2024-10-08 21:41:37,968 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=176.19 vs. limit=8.325
2024-10-08 21:41:53,663 WARNING [optim.py:503] Scaling gradients by 0.07887770235538483, model_norm_threshold=24986832076800.0
2024-10-08 21:41:53,820 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.568e+28, grad_sumsq=5.135e+30, orig_rms_sq=5.001e-03
2024-10-08 21:41:57,822 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=2203.3333333333335, ans=0.050425
2024-10-08 21:41:58,589 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=30.99 vs. limit=9.1525
2024-10-08 21:41:58,685 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.07 vs. limit=9.1525
2024-10-08 21:42:01,511 WARNING [optim.py:503] Scaling gradients by 0.00027303508250042796, model_norm_threshold=24986832076800.0
2024-10-08 21:42:01,665 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.391e+33, grad_sumsq=1.216e+35, orig_rms_sq=2.790e-02
2024-10-08 21:42:10,375 WARNING [optim.py:503] Scaling gradients by 0.03593594208359718, model_norm_threshold=24986832076800.0
2024-10-08 21:42:10,533 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.446e+29, grad_sumsq=2.886e+31, orig_rms_sq=5.011e-03
2024-10-08 21:42:14,977 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=22.32 vs. limit=9.1575
2024-10-08 21:42:18,929 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=27.35 vs. limit=9.1575
2024-10-08 21:42:19,146 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.81 vs. limit=6.105
2024-10-08 21:42:20,445 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.63 vs. limit=3.3315
2024-10-08 21:42:20,649 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.56 vs. limit=8.32875
2024-10-08 21:42:21,590 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.94 vs. limit=8.32875
2024-10-08 21:42:22,122 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=54.01 vs. limit=8.32875
2024-10-08 21:42:26,481 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2213.3333333333335, ans=0.27786666666666665
2024-10-08 21:42:32,837 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=2213.3333333333335, ans=0.11699999999999999
2024-10-08 21:42:40,254 WARNING [optim.py:503] Scaling gradients by 0.04610023275017738, model_norm_threshold=24986832076800.0
2024-10-08 21:42:40,420 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.100e+28, grad_sumsq=3.555e+30, orig_rms_sq=2.560e-02
2024-10-08 21:42:40,479 INFO [train.py:1152] Epoch 1, batch 6650, loss[loss=0.7957, ctc_loss=1.087, attn_decoder_loss=0.7228, over 4748.00 frames. ], tot_loss[loss=0.7637, ctc_loss=1.04, attn_decoder_loss=0.6946, over 967033.23 frames. ], batch size: 20, lr: 3.89e-02,
2024-10-08 21:42:43,140 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=5.123e+01
2024-10-08 21:42:49,325 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.min_positive, batch_count=2216.6666666666665, ans=0.04307291666666667
2024-10-08 21:42:50,379 WARNING [optim.py:503] Scaling gradients by 0.01155655924230814, model_norm_threshold=24986832076800.0
2024-10-08 21:42:50,535 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.431e+30, grad_sumsq=5.640e+31, orig_rms_sq=2.537e-02
2024-10-08 21:42:54,313 WARNING [optim.py:503] Scaling gradients by 0.01939668133854866, model_norm_threshold=24986832076800.0
2024-10-08 21:42:54,791 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.676e+29, grad_sumsq=2.207e+31, orig_rms_sq=2.572e-02
2024-10-08 21:42:56,850 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=37.49 vs. limit=8.3325
2024-10-08 21:43:01,287 WARNING [optim.py:503] Scaling gradients by 0.04437917098402977, model_norm_threshold=24986832076800.0
2024-10-08 21:43:01,443 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.611e+28, grad_sumsq=9.977e+30, orig_rms_sq=5.624e-03
2024-10-08 21:43:05,958 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.46 vs. limit=5.555833333333333
2024-10-08 21:43:07,194 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=66.04 vs. limit=8.33375
2024-10-08 21:43:07,459 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=12.03 vs. limit=8.33375
2024-10-08 21:43:07,738 WARNING [optim.py:503] Scaling gradients by 0.0006547054508700967, model_norm_threshold=24986832076800.0
2024-10-08 21:43:07,894 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.561e+32, grad_sumsq=6.317e+34, orig_rms_sq=5.637e-03
2024-10-08 21:43:10,681 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=2223.3333333333335, ans=0.7722333333333333
2024-10-08 21:43:11,735 WARNING [optim.py:503] Scaling gradients by 0.00997898355126381, model_norm_threshold=24986832076800.0
2024-10-08 21:43:11,889 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.102e+30, grad_sumsq=1.979e+32, orig_rms_sq=5.568e-03
2024-10-08 21:43:19,541 WARNING [optim.py:503] Scaling gradients by 0.012776443734765053, model_norm_threshold=24986832076800.0
2024-10-08 21:43:19,697 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.521e+29, grad_sumsq=2.643e+29, orig_rms_sq=2.845e+00
2024-10-08 21:43:21,048 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2226.6666666666665, ans=0.27773333333333333
2024-10-08 21:43:23,718 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.max_abs, batch_count=2226.6666666666665, ans=6.391666666666667
2024-10-08 21:43:29,637 WARNING [optim.py:503] Scaling gradients by 0.041813358664512634, model_norm_threshold=24986832076800.0
2024-10-08 21:43:29,793 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.273e+29, grad_sumsq=2.304e+31, orig_rms_sq=5.524e-03
2024-10-08 21:43:34,525 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=23.06 vs. limit=9.1725
2024-10-08 21:43:36,011 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=14.18 vs. limit=8.33625
2024-10-08 21:43:45,206 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.789e+10 2.639e+12 1.483e+13 8.445e+13 9.152e+16, threshold=2.966e+13, percent-clipped=41.0
2024-10-08 21:43:45,207 WARNING [optim.py:503] Scaling gradients by 0.08522669970989227, model_norm_threshold=29656380080128.0
2024-10-08 21:43:45,363 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.370e+28, grad_sumsq=8.584e+29, orig_rms_sq=2.761e-02
2024-10-08 21:43:45,422 INFO [train.py:1152] Epoch 1, batch 6700, loss[loss=0.7995, ctc_loss=1.093, attn_decoder_loss=0.7261, over 4938.00 frames. ], tot_loss[loss=0.7614, ctc_loss=1.037, attn_decoder_loss=0.6925, over 969213.20 frames. ], batch size: 20, lr: 3.89e-02,
2024-10-08 21:43:46,733 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=2233.3333333333335, ans=0.11624999999999999
2024-10-08 21:43:51,605 WARNING [optim.py:503] Scaling gradients by 0.00028022221522405744, model_norm_threshold=29656380080128.0
2024-10-08 21:43:51,760 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.773e+33, grad_sumsq=4.791e+35, orig_rms_sq=5.787e-03
2024-10-08 21:43:52,695 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.58 vs. limit=5.558333333333334
2024-10-08 21:43:58,009 WARNING [optim.py:503] Scaling gradients by 0.0025350630749017, model_norm_threshold=29656380080128.0
2024-10-08 21:43:58,164 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.588e+31, grad_sumsq=6.214e+33, orig_rms_sq=5.773e-03
2024-10-08 21:43:58,356 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2236.6666666666665, ans=0.27763333333333334
2024-10-08 21:44:03,306 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=2236.6666666666665, ans=0.049675000000000004
2024-10-08 21:44:13,452 WARNING [optim.py:503] Scaling gradients by 0.05843000113964081, model_norm_threshold=29656380080128.0
2024-10-08 21:44:13,608 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.634e+28, grad_sumsq=2.926e+28, orig_rms_sq=2.609e+00
2024-10-08 21:44:18,704 WARNING [optim.py:503] Scaling gradients by 0.014971486292779446, model_norm_threshold=29656380080128.0
2024-10-08 21:44:18,860 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.310e+30, grad_sumsq=4.943e+31, orig_rms_sq=2.651e-02
2024-10-08 21:44:23,016 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=2243.3333333333335, ans=0.39484375
2024-10-08 21:44:28,797 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.48 vs. limit=5.560833333333333
2024-10-08 21:44:29,347 WARNING [optim.py:503] Scaling gradients by 0.012157555669546127, model_norm_threshold=29656380080128.0
2024-10-08 21:44:29,504 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.102e+30, grad_sumsq=1.788e+32, orig_rms_sq=6.161e-03
2024-10-08 21:44:31,655 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.88 vs. limit=3.3365
2024-10-08 21:44:40,910 WARNING [optim.py:503] Scaling gradients by 0.029223209246993065, model_norm_threshold=29656380080128.0
2024-10-08 21:44:41,067 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.152e+29, grad_sumsq=1.012e+29, orig_rms_sq=4.103e+00
2024-10-08 21:44:44,867 WARNING [optim.py:503] Scaling gradients by 0.015301663428544998, model_norm_threshold=29656380080128.0
2024-10-08 21:44:45,024 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.55, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.083e+30, grad_sumsq=8.228e+31, orig_rms_sq=2.532e-02
2024-10-08 21:44:46,204 WARNING [optim.py:503] Scaling gradients by 0.008023577742278576, model_norm_threshold=29656380080128.0
2024-10-08 21:44:46,360 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.222e+30, grad_sumsq=1.273e+30, orig_rms_sq=4.102e+00
2024-10-08 21:44:50,173 INFO [train.py:1152] Epoch 1, batch 6750, loss[loss=0.6817, ctc_loss=0.9348, attn_decoder_loss=0.6185, over 4911.00 frames. ], tot_loss[loss=0.7533, ctc_loss=1.025, attn_decoder_loss=0.6853, over 972265.95 frames. ], batch size: 19, lr: 3.88e-02,
2024-10-08 21:44:50,717 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=5.32 vs. limit=4.9
2024-10-08 21:44:51,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=2250.0, ans=0.21875
2024-10-08 21:44:54,092 WARNING [optim.py:503] Scaling gradients by 0.09536673128604889, model_norm_threshold=29656380080128.0
2024-10-08 21:44:54,249 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.179e+28, grad_sumsq=4.546e+30, orig_rms_sq=6.993e-03
2024-10-08 21:44:54,452 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=2250.0, ans=0.39453125
2024-10-08 21:44:57,074 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=2250.0, ans=0.39453125
2024-10-08 21:44:57,529 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.38 vs. limit=3.3375
2024-10-08 21:44:59,840 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.74 vs. limit=9.1875
2024-10-08 21:45:00,578 WARNING [optim.py:503] Scaling gradients by 0.01064671203494072, model_norm_threshold=29656380080128.0
2024-10-08 21:45:00,733 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.454e+30, grad_sumsq=1.029e+32, orig_rms_sq=2.384e-02
2024-10-08 21:45:01,675 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=24.67 vs. limit=9.1875
2024-10-08 21:45:03,433 WARNING [optim.py:503] Scaling gradients by 0.041065409779548645, model_norm_threshold=29656380080128.0
2024-10-08 21:45:03,590 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.346e+29, grad_sumsq=1.997e+31, orig_rms_sq=6.743e-03
2024-10-08 21:45:09,929 WARNING [optim.py:503] Scaling gradients by 0.02713923715054989, model_norm_threshold=29656380080128.0
2024-10-08 21:45:10,084 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.415e+29, grad_sumsq=5.225e+31, orig_rms_sq=6.536e-03
2024-10-08 21:45:13,155 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.68 vs. limit=8.345
2024-10-08 21:45:14,588 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=109.90 vs. limit=8.345
2024-10-08 21:45:15,253 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2256.6666666666665, ans=0.39421875
2024-10-08 21:45:17,067 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=25.96 vs. limit=8.34625
2024-10-08 21:45:17,520 WARNING [optim.py:503] Scaling gradients by 0.030252642929553986, model_norm_threshold=29656380080128.0
2024-10-08 21:45:17,677 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.834e+29, grad_sumsq=2.829e+31, orig_rms_sq=6.484e-03
2024-10-08 21:45:21,029 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=37.09 vs. limit=8.34625
2024-10-08 21:45:21,771 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=2256.6666666666665, ans=0.8210166666666667
2024-10-08 21:45:24,108 WARNING [optim.py:503] Scaling gradients by 0.027662312611937523, model_norm_threshold=29656380080128.0
2024-10-08 21:45:24,264 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.435e+29, grad_sumsq=3.814e+31, orig_rms_sq=6.385e-03
2024-10-08 21:45:25,505 WARNING [optim.py:503] Scaling gradients by 0.06658147275447845, model_norm_threshold=29656380080128.0
2024-10-08 21:45:25,659 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.024e+28, grad_sumsq=1.287e+30, orig_rms_sq=2.350e-02
2024-10-08 21:45:26,843 WARNING [optim.py:503] Scaling gradients by 0.0071317460387945175, model_norm_threshold=29656380080128.0
2024-10-08 21:45:26,997 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.639e+30, grad_sumsq=5.700e+32, orig_rms_sq=6.385e-03
2024-10-08 21:45:27,208 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=2256.6666666666665, ans=0.049225
2024-10-08 21:45:28,949 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=15.56 vs. limit=5.5649999999999995
2024-10-08 21:45:29,081 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=35.92 vs. limit=8.3475
2024-10-08 21:45:34,314 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.04 vs. limit=6.13
2024-10-08 21:45:38,939 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=2260.0, ans=0.04915
2024-10-08 21:45:47,454 WARNING [optim.py:503] Scaling gradients by 0.06207861378788948, model_norm_threshold=29656380080128.0
2024-10-08 21:45:47,614 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.681e+28, grad_sumsq=1.380e+31, orig_rms_sq=6.290e-03
2024-10-08 21:45:48,237 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=16.01 vs. limit=8.34875
2024-10-08 21:45:48,843 WARNING [optim.py:503] Scaling gradients by 0.03975696489214897, model_norm_threshold=29656380080128.0
2024-10-08 21:45:49,000 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.413e+29, grad_sumsq=2.247e+31, orig_rms_sq=6.290e-03
2024-10-08 21:45:52,171 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.66 vs. limit=5.565833333333334
2024-10-08 21:45:55,154 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=16.21 vs. limit=8.35
2024-10-08 21:45:55,741 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.551e+10 2.432e+12 1.583e+13 2.005e+14 1.058e+17, threshold=3.165e+13, percent-clipped=44.0
2024-10-08 21:45:55,796 INFO [train.py:1152] Epoch 1, batch 6800, loss[loss=0.7287, ctc_loss=0.9944, attn_decoder_loss=0.6623, over 4912.00 frames. ], tot_loss[loss=0.7525, ctc_loss=1.022, attn_decoder_loss=0.6852, over 974591.32 frames. ], batch size: 19, lr: 3.87e-02,
2024-10-08 21:45:57,076 WARNING [optim.py:503] Scaling gradients by 0.06188087537884712, model_norm_threshold=31650285092864.0
2024-10-08 21:45:57,232 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.633e+28, grad_sumsq=2.755e+30, orig_rms_sq=2.408e-02
2024-10-08 21:46:05,114 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=2266.6666666666665, ans=0.234
2024-10-08 21:46:05,488 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=58.02 vs. limit=8.35
2024-10-08 21:46:07,634 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=2270.0, ans=0.11487499999999999
2024-10-08 21:46:11,486 WARNING [optim.py:503] Scaling gradients by 0.0170343779027462, model_norm_threshold=31650285092864.0
2024-10-08 21:46:11,644 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.375e+29, grad_sumsq=3.402e+31, orig_rms_sq=2.462e-02
2024-10-08 21:46:17,682 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=12.80 vs. limit=5.5675
2024-10-08 21:46:28,287 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten.whitening_limit, batch_count=2273.3333333333335, ans=9.205
2024-10-08 21:46:35,136 WARNING [optim.py:503] Scaling gradients by 0.03586549311876297, model_norm_threshold=31650285092864.0
2024-10-08 21:46:35,290 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.818e+29, grad_sumsq=7.183e+30, orig_rms_sq=2.531e-02
2024-10-08 21:46:37,397 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.49 vs. limit=9.2075
2024-10-08 21:46:39,078 WARNING [optim.py:503] Scaling gradients by 0.00605390639975667, model_norm_threshold=31650285092864.0
2024-10-08 21:46:39,236 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.157e+31, grad_sumsq=1.939e+33, orig_rms_sq=5.965e-03
2024-10-08 21:46:40,636 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2276.6666666666665, ans=0.27723333333333333
2024-10-08 21:46:42,925 WARNING [optim.py:503] Scaling gradients by 0.030003009364008904, model_norm_threshold=31650285092864.0
2024-10-08 21:46:43,082 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.418e+29, grad_sumsq=1.337e+31, orig_rms_sq=2.557e-02
2024-10-08 21:46:45,714 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2276.6666666666665, ans=0.27723333333333333
2024-10-08 21:46:54,572 WARNING [optim.py:503] Scaling gradients by 0.03256654366850853, model_norm_threshold=31650285092864.0
2024-10-08 21:46:54,726 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.715e+29, grad_sumsq=1.084e+31, orig_rms_sq=2.505e-02
2024-10-08 21:46:55,405 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.whiten.whitening_limit, batch_count=2280.0, ans=4.912
2024-10-08 21:46:56,016 WARNING [optim.py:503] Scaling gradients by 0.06217445805668831, model_norm_threshold=31650285092864.0
2024-10-08 21:46:56,172 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.779e+28, grad_sumsq=3.903e+30, orig_rms_sq=2.505e-02
2024-10-08 21:47:01,347 INFO [train.py:1152] Epoch 1, batch 6850, loss[loss=0.7441, ctc_loss=1.031, attn_decoder_loss=0.6724, over 4978.00 frames. ], tot_loss[loss=0.7476, ctc_loss=1.014, attn_decoder_loss=0.681, over 978927.87 frames. ], batch size: 19, lr: 3.87e-02,
2024-10-08 21:47:02,535 WARNING [optim.py:503] Scaling gradients by 0.027477441355586052, model_norm_threshold=31650285092864.0
2024-10-08 21:47:02,692 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.405e+29, grad_sumsq=1.076e+32, orig_rms_sq=5.952e-03
2024-10-08 21:47:02,850 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_True/epoch-1.pt
2024-10-08 21:47:19,094 INFO [train.py:1152] Epoch 2, batch 0, loss[loss=0.709, ctc_loss=0.9624, attn_decoder_loss=0.6457, over 4855.00 frames. ], tot_loss[loss=0.709, ctc_loss=0.9624, attn_decoder_loss=0.6457, over 4855.00 frames. ], batch size: 19, lr: 3.79e-02,
2024-10-08 21:47:19,094 INFO [train.py:1175] Computing validation loss
2024-10-08 21:47:24,816 INFO [train.py:1184] Epoch 2, validation: loss=0.8144, ctc_loss=1.092, attn_decoder_loss=0.7449, over 90464.00 frames.
2024-10-08 21:47:24,816 INFO [train.py:1185] Maximum memory allocated so far is 6947MB
2024-10-08 21:47:25,980 WARNING [optim.py:503] Scaling gradients by 0.01125256810337305, model_norm_threshold=31650285092864.0
2024-10-08 21:47:26,135 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.63, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.974e+30, grad_sumsq=8.172e+32, orig_rms_sq=6.087e-03
2024-10-08 21:47:32,243 WARNING [optim.py:503] Scaling gradients by 0.07861753553152084, model_norm_threshold=31650285092864.0
2024-10-08 21:47:32,397 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.081e+28, grad_sumsq=1.140e+31, orig_rms_sq=6.209e-03
2024-10-08 21:47:32,705 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=1.488e+01
2024-10-08 21:47:33,590 WARNING [optim.py:503] Scaling gradients by 0.06389744579792023, model_norm_threshold=31650285092864.0
2024-10-08 21:47:33,746 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.279e+28, grad_sumsq=2.583e+30, orig_rms_sq=2.431e-02
2024-10-08 21:47:39,480 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=30.97 vs. limit=8.35775
2024-10-08 21:47:39,697 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=43.75 vs. limit=8.35775
2024-10-08 21:47:41,429 WARNING [optim.py:503] Scaling gradients by 0.03567072004079819, model_norm_threshold=31650285092864.0
2024-10-08 21:47:41,583 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.801e+29, grad_sumsq=2.890e+31, orig_rms_sq=6.232e-03
2024-10-08 21:47:42,749 WARNING [optim.py:503] Scaling gradients by 0.014617851935327053, model_norm_threshold=31650285092864.0
2024-10-08 21:47:42,903 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.122e+30, grad_sumsq=3.406e+32, orig_rms_sq=6.232e-03
2024-10-08 21:47:47,275 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.27 vs. limit=9.2155
2024-10-08 21:47:52,684 WARNING [optim.py:503] Scaling gradients by 0.004239212721586227, model_norm_threshold=31650285092864.0
2024-10-08 21:47:52,840 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.52, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.910e+31, grad_sumsq=4.650e+33, orig_rms_sq=6.260e-03
2024-10-08 21:47:53,043 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=2290.6666666666665, ans=0.392625
2024-10-08 21:47:53,637 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten.whitening_limit, batch_count=2290.6666666666665, ans=8.359
2024-10-08 21:47:54,056 WARNING [optim.py:503] Scaling gradients by 0.03387327864766121, model_norm_threshold=31650285092864.0
2024-10-08 21:47:54,214 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.244e+29, grad_sumsq=9.344e+30, orig_rms_sq=2.401e-02
2024-10-08 21:47:56,746 WARNING [optim.py:503] Scaling gradients by 0.0355694442987442, model_norm_threshold=31650285092864.0
2024-10-08 21:47:56,902 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.704e+29, grad_sumsq=6.995e+30, orig_rms_sq=2.436e-02
2024-10-08 21:48:00,485 WARNING [optim.py:503] Scaling gradients by 0.027375441044569016, model_norm_threshold=31650285092864.0
2024-10-08 21:48:00,641 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.573e+29, grad_sumsq=7.649e+28, orig_rms_sq=3.364e+00
2024-10-08 21:48:01,785 WARNING [optim.py:503] Scaling gradients by 0.05031709372997284, model_norm_threshold=31650285092864.0
2024-10-08 21:48:01,939 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.154e+29, grad_sumsq=1.846e+31, orig_rms_sq=6.250e-03
2024-10-08 21:48:06,639 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.56 vs. limit=4.9176
2024-10-08 21:48:09,698 WARNING [optim.py:503] Scaling gradients by 0.08401665836572647, model_norm_threshold=31650285092864.0
2024-10-08 21:48:09,852 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.496e+28, grad_sumsq=1.413e+30, orig_rms_sq=2.474e-02
2024-10-08 21:48:14,717 WARNING [optim.py:503] Scaling gradients by 0.055437058210372925, model_norm_threshold=31650285092864.0
2024-10-08 21:48:14,873 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.212e+28, grad_sumsq=3.690e+30, orig_rms_sq=2.496e-02
2024-10-08 21:48:15,841 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.37 vs. limit=8.3615
2024-10-08 21:48:16,378 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=2297.3333333333335, ans=0.3923125
2024-10-08 21:48:18,669 WARNING [optim.py:503] Scaling gradients by 0.022908085957169533, model_norm_threshold=31650285092864.0
2024-10-08 21:48:18,825 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.265e+29, grad_sumsq=1.153e+32, orig_rms_sq=6.299e-03
2024-10-08 21:48:25,829 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=26.54 vs. limit=8.3615
2024-10-08 21:48:26,213 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.107e+10 3.316e+12 2.028e+13 1.747e+14 7.466e+15, threshold=4.055e+13, percent-clipped=47.0
2024-10-08 21:48:28,857 INFO [train.py:1152] Epoch 2, batch 50, loss[loss=0.6997, ctc_loss=0.9501, attn_decoder_loss=0.6372, over 4909.00 frames. ], tot_loss[loss=0.7732, ctc_loss=1.052, attn_decoder_loss=0.7034, over 217758.01 frames. ], batch size: 19, lr: 3.79e-02,
2024-10-08 21:48:30,590 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.07 vs. limit=8.36275
2024-10-08 21:48:36,274 WARNING [optim.py:503] Scaling gradients by 0.03736433386802673, model_norm_threshold=40550623346688.0
2024-10-08 21:48:36,429 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.092e+29, grad_sumsq=8.425e+28, orig_rms_sq=3.670e+00
2024-10-08 21:48:40,233 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=2304.0, ans=0.392
2024-10-08 21:48:46,374 WARNING [optim.py:503] Scaling gradients by 0.033082783222198486, model_norm_threshold=40550623346688.0
2024-10-08 21:48:46,532 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.006e+29, grad_sumsq=1.086e+29, orig_rms_sq=3.690e+00
2024-10-08 21:48:53,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=2307.3333333333335, ans=0.11347499999999999
2024-10-08 21:48:54,386 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2307.3333333333335, ans=0.27692666666666665
2024-10-08 21:48:56,009 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.30 vs. limit=9.2305
2024-10-08 21:49:01,239 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=28.52 vs. limit=8.36525
2024-10-08 21:49:08,113 WARNING [optim.py:503] Scaling gradients by 0.09476280212402344, model_norm_threshold=40550623346688.0
2024-10-08 21:49:08,272 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.752e+28, grad_sumsq=1.824e+30, orig_rms_sq=2.606e-02
2024-10-08 21:49:14,225 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=25.42 vs. limit=9.233
2024-10-08 21:49:18,010 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=18.28 vs. limit=8.3665
2024-10-08 21:49:20,893 WARNING [optim.py:503] Scaling gradients by 0.015876809135079384, model_norm_threshold=40550623346688.0
2024-10-08 21:49:21,050 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.265e+30, grad_sumsq=1.406e+29, orig_rms_sq=8.996e+00
2024-10-08 21:49:26,253 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=2314.0, ans=0.047935
2024-10-08 21:49:26,265 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=2314.0, ans=0.04949747468305833
2024-10-08 21:49:27,552 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2314.0, ans=0.27686
2024-10-08 21:49:29,326 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=19.18 vs. limit=8.367750000000001
2024-10-08 21:49:32,267 INFO [train.py:1152] Epoch 2, batch 100, loss[loss=0.767, ctc_loss=1.028, attn_decoder_loss=0.7018, over 4755.00 frames. ], tot_loss[loss=0.7794, ctc_loss=1.062, attn_decoder_loss=0.7087, over 383327.18 frames. ], batch size: 19, lr: 3.78e-02,
2024-10-08 21:49:37,189 WARNING [optim.py:503] Scaling gradients by 0.027944132685661316, model_norm_threshold=40550623346688.0
2024-10-08 21:49:37,345 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.72, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.524e+30, grad_sumsq=2.301e+32, orig_rms_sq=6.625e-03
2024-10-08 21:49:39,240 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.10 vs. limit=5.5793333333333335
2024-10-08 21:49:40,903 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=8.57 vs. limit=8.369
2024-10-08 21:49:47,574 WARNING [optim.py:503] Scaling gradients by 0.04914214462041855, model_norm_threshold=40550623346688.0
2024-10-08 21:49:47,732 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.363e+29, grad_sumsq=5.978e+30, orig_rms_sq=2.280e-02
2024-10-08 21:49:55,184 WARNING [optim.py:503] Scaling gradients by 0.0020535988733172417, model_norm_threshold=40550623346688.0
2024-10-08 21:49:55,340 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.084e+32, grad_sumsq=1.688e+34, orig_rms_sq=6.422e-03
2024-10-08 21:49:56,557 WARNING [optim.py:503] Scaling gradients by 0.032847411930561066, model_norm_threshold=40550623346688.0
2024-10-08 21:49:56,714 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.767e+29, grad_sumsq=1.655e+31, orig_rms_sq=2.275e-02
2024-10-08 21:49:59,180 WARNING [optim.py:503] Scaling gradients by 0.028033768758177757, model_norm_threshold=40550623346688.0
2024-10-08 21:49:59,336 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.573e+29, grad_sumsq=1.700e+29, orig_rms_sq=3.866e+00
2024-10-08 21:50:01,788 WARNING [optim.py:503] Scaling gradients by 0.05011780560016632, model_norm_threshold=40550623346688.0
2024-10-08 21:50:01,944 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.744e+29, grad_sumsq=4.512e+28, orig_rms_sq=3.866e+00
2024-10-08 21:50:03,196 WARNING [optim.py:503] Scaling gradients by 0.01883375272154808, model_norm_threshold=40550623346688.0
2024-10-08 21:50:03,352 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.309e+29, grad_sumsq=2.375e+29, orig_rms_sq=3.920e+00
2024-10-08 21:50:05,789 WARNING [optim.py:503] Scaling gradients by 0.009180312044918537, model_norm_threshold=40550623346688.0
2024-10-08 21:50:05,943 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.242e+30, grad_sumsq=2.102e+30, orig_rms_sq=3.920e+00
2024-10-08 21:50:08,621 WARNING [optim.py:503] Scaling gradients by 0.09126780182123184, model_norm_threshold=40550623346688.0
2024-10-08 21:50:08,780 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.656e+28, grad_sumsq=2.411e+28, orig_rms_sq=4.005e+00
2024-10-08 21:50:10,176 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=2327.3333333333335, ans=0.8185433333333334
2024-10-08 21:50:10,684 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=28.85 vs. limit=8.37275
2024-10-08 21:50:12,763 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=2327.3333333333335, ans=0.11272499999999999
2024-10-08 21:50:15,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.80 vs. limit=9.2455
2024-10-08 21:50:16,214 WARNING [optim.py:503] Scaling gradients by 0.012320769019424915, model_norm_threshold=40550623346688.0
2024-10-08 21:50:16,371 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.543e+30, grad_sumsq=5.286e+32, orig_rms_sq=6.703e-03
2024-10-08 21:50:17,560 WARNING [optim.py:503] Scaling gradients by 0.09894765913486481, model_norm_threshold=40550623346688.0
2024-10-08 21:50:17,717 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.323e+28, grad_sumsq=1.066e+28, orig_rms_sq=4.056e+00
2024-10-08 21:50:21,360 WARNING [optim.py:503] Scaling gradients by 0.05890291929244995, model_norm_threshold=40550623346688.0
2024-10-08 21:50:21,517 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.063e+29, grad_sumsq=2.598e+28, orig_rms_sq=4.093e+00
2024-10-08 21:50:25,204 WARNING [optim.py:503] Scaling gradients by 0.002264726674184203, model_norm_threshold=40550623346688.0
2024-10-08 21:50:25,358 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.765e+31, grad_sumsq=1.634e+31, orig_rms_sq=4.141e+00
2024-10-08 21:50:34,484 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.720e+10 3.702e+12 1.433e+13 1.953e+14 1.975e+16, threshold=2.866e+13, percent-clipped=37.0
2024-10-08 21:50:35,954 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=2334.0, ans=0.20825
2024-10-08 21:50:36,965 WARNING [optim.py:503] Scaling gradients by 3.6510537029244006e-05, model_norm_threshold=28655923232768.0
2024-10-08 21:50:37,121 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.890e+35, grad_sumsq=2.857e+37, orig_rms_sq=6.615e-03
2024-10-08 21:50:37,180 INFO [train.py:1152] Epoch 2, batch 150, loss[loss=0.7666, ctc_loss=1.04, attn_decoder_loss=0.6982, over 4908.00 frames. ], tot_loss[loss=0.773, ctc_loss=1.054, attn_decoder_loss=0.7028, over 513287.79 frames. ], batch size: 19, lr: 3.77e-02,
2024-10-08 21:50:39,563 WARNING [optim.py:503] Scaling gradients by 0.00032972428016364574, model_norm_threshold=28655923232768.0
2024-10-08 21:50:39,717 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.52, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.904e+33, grad_sumsq=6.000e+35, orig_rms_sq=6.506e-03
2024-10-08 21:50:42,262 WARNING [optim.py:503] Scaling gradients by 0.010212952271103859, model_norm_threshold=28655923232768.0
2024-10-08 21:50:42,419 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.202e+30, grad_sumsq=3.385e+32, orig_rms_sq=6.506e-03
2024-10-08 21:50:42,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=2334.0, ans=0.047485
2024-10-08 21:50:43,083 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=21.66 vs. limit=8.37525
2024-10-08 21:50:43,901 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=2334.0, ans=0.39059375
2024-10-08 21:50:46,975 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.91 vs. limit=4.9336
2024-10-08 21:50:47,386 WARNING [optim.py:503] Scaling gradients by 0.029509764164686203, model_norm_threshold=28655923232768.0
2024-10-08 21:50:47,542 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.731e+29, grad_sumsq=1.172e+31, orig_rms_sq=2.330e-02
2024-10-08 21:50:53,164 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=23.84 vs. limit=9.253
2024-10-08 21:50:55,063 WARNING [optim.py:503] Scaling gradients by 0.0719783753156662, model_norm_threshold=28655923232768.0
2024-10-08 21:50:55,219 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.266e+28, grad_sumsq=1.385e+30, orig_rms_sq=2.359e-02
2024-10-08 21:50:55,407 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=2337.3333333333335, ans=0.8181933333333333
2024-10-08 21:51:05,005 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.42 vs. limit=5.585166666666667
2024-10-08 21:51:05,064 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.13 vs. limit=9.2555
2024-10-08 21:51:06,559 WARNING [optim.py:503] Scaling gradients by 0.011324627324938774, model_norm_threshold=28655923232768.0
2024-10-08 21:51:06,714 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.62, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.981e+30, grad_sumsq=6.538e+32, orig_rms_sq=6.089e-03
2024-10-08 21:51:07,865 WARNING [optim.py:503] Scaling gradients by 0.014530275948345661, model_norm_threshold=28655923232768.0
2024-10-08 21:51:08,022 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.125e+30, grad_sumsq=4.719e+31, orig_rms_sq=2.385e-02
2024-10-08 21:51:14,302 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=2344.0, ans=0.20700000000000002
2024-10-08 21:51:16,767 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=2344.0, ans=0.390125
2024-10-08 21:51:17,269 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.29 vs. limit=3.3516
2024-10-08 21:51:20,225 WARNING [optim.py:503] Scaling gradients by 0.009552071802318096, model_norm_threshold=28655923232768.0
2024-10-08 21:51:20,383 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.783e+30, grad_sumsq=7.345e+31, orig_rms_sq=2.427e-02
2024-10-08 21:51:22,147 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.05 vs. limit=6.172
2024-10-08 21:51:23,544 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=125.83 vs. limit=8.379
2024-10-08 21:51:24,473 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=2344.0, ans=0.390125
2024-10-08 21:51:26,755 WARNING [optim.py:503] Scaling gradients by 0.05324827507138252, model_norm_threshold=28655923232768.0
2024-10-08 21:51:26,912 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.079e+29, grad_sumsq=2.904e+28, orig_rms_sq=3.717e+00
2024-10-08 21:51:28,109 WARNING [optim.py:503] Scaling gradients by 0.0770539864897728, model_norm_threshold=28655923232768.0
2024-10-08 21:51:28,281 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.552e+28, grad_sumsq=1.024e+28, orig_rms_sq=3.467e+00
2024-10-08 21:51:31,653 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=29.70 vs. limit=9.2605
2024-10-08 21:51:33,182 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.36 vs. limit=6.173666666666667
2024-10-08 21:51:33,446 WARNING [optim.py:503] Scaling gradients by 0.061142127960920334, model_norm_threshold=28655923232768.0
2024-10-08 21:51:33,602 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.896e+28, grad_sumsq=1.843e+28, orig_rms_sq=3.742e+00
2024-10-08 21:51:35,525 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten.whitening_limit, batch_count=2347.3333333333335, ans=8.38025
2024-10-08 21:51:36,142 WARNING [optim.py:503] Scaling gradients by 0.07608398795127869, model_norm_threshold=28655923232768.0
2024-10-08 21:51:36,300 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.407e+28, grad_sumsq=6.457e+27, orig_rms_sq=3.727e+00
2024-10-08 21:51:36,504 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=2347.3333333333335, ans=0.3270331548265845
2024-10-08 21:51:37,456 WARNING [optim.py:503] Scaling gradients by 0.0614033043384552, model_norm_threshold=28655923232768.0
2024-10-08 21:51:37,625 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.397e+28, grad_sumsq=1.448e+28, orig_rms_sq=3.727e+00
2024-10-08 21:51:39,001 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=2347.3333333333335, ans=0.08532916666666668
2024-10-08 21:51:41,321 INFO [train.py:1152] Epoch 2, batch 200, loss[loss=0.8625, ctc_loss=1.171, attn_decoder_loss=0.7853, over 4761.00 frames. ], tot_loss[loss=0.7687, ctc_loss=1.049, attn_decoder_loss=0.6987, over 613727.61 frames. ], batch size: 45, lr: 3.77e-02,
2024-10-08 21:51:42,788 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=2350.6666666666665, ans=0.8177266666666667
2024-10-08 21:51:46,872 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=14.16 vs. limit=8.381499999999999
2024-10-08 21:51:48,471 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.12 vs. limit=9.263
2024-10-08 21:51:52,602 WARNING [optim.py:503] Scaling gradients by 0.023352397605776787, model_norm_threshold=28655923232768.0
2024-10-08 21:51:52,760 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.557e+29, grad_sumsq=1.073e+31, orig_rms_sq=2.382e-02
2024-10-08 21:52:00,237 WARNING [optim.py:503] Scaling gradients by 0.08419086784124374, model_norm_threshold=28655923232768.0
2024-10-08 21:52:00,395 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.189e+28, grad_sumsq=3.755e+30, orig_rms_sq=5.829e-03
2024-10-08 21:52:09,172 WARNING [optim.py:503] Scaling gradients by 0.01143553201109171, model_norm_threshold=28655923232768.0
2024-10-08 21:52:09,329 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.367e+30, grad_sumsq=5.761e+31, orig_rms_sq=2.372e-02
2024-10-08 21:52:10,020 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=22.64 vs. limit=8.384
2024-10-08 21:52:14,271 WARNING [optim.py:503] Scaling gradients by 0.027923233807086945, model_norm_threshold=28655923232768.0
2024-10-08 21:52:14,428 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.467e+29, grad_sumsq=3.954e+31, orig_rms_sq=6.240e-03
2024-10-08 21:52:17,680 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=10.92 vs. limit=5.589333333333333
2024-10-08 21:52:17,687 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.61 vs. limit=3.3536
2024-10-08 21:52:19,338 WARNING [optim.py:503] Scaling gradients by 0.008090971037745476, model_norm_threshold=28655923232768.0
2024-10-08 21:52:19,502 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.137e+30, grad_sumsq=1.015e+30, orig_rms_sq=4.077e+00
2024-10-08 21:52:19,758 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=2360.6666666666665, ans=0.38934375
2024-10-08 21:52:32,052 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=2364.0, ans=0.20450000000000002
2024-10-08 21:52:34,221 WARNING [optim.py:503] Scaling gradients by 0.014026235789060593, model_norm_threshold=28655923232768.0
2024-10-08 21:52:34,378 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.330e+30, grad_sumsq=5.618e+31, orig_rms_sq=2.367e-02
2024-10-08 21:52:38,822 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.63 vs. limit=3.3546
2024-10-08 21:52:40,158 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=25.34 vs. limit=8.3865
2024-10-08 21:52:41,829 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.762e+10 2.249e+12 1.455e+13 1.879e+14 7.849e+17, threshold=2.910e+13, percent-clipped=44.0
2024-10-08 21:52:41,829 WARNING [optim.py:503] Scaling gradients by 0.008190743625164032, model_norm_threshold=29104722149376.0
2024-10-08 21:52:41,988 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.760e+30, grad_sumsq=1.566e+32, orig_rms_sq=2.400e-02
2024-10-08 21:52:42,654 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.38 vs. limit=6.182
2024-10-08 21:52:44,494 INFO [train.py:1152] Epoch 2, batch 250, loss[loss=0.7597, ctc_loss=1.028, attn_decoder_loss=0.6926, over 4832.00 frames. ], tot_loss[loss=0.7672, ctc_loss=1.047, attn_decoder_loss=0.6972, over 692427.42 frames. ], batch size: 38, lr: 3.76e-02,
2024-10-08 21:52:44,927 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=18.81 vs. limit=9.275500000000001
2024-10-08 21:52:46,438 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=16.81 vs. limit=8.38775
2024-10-08 21:52:46,856 WARNING [optim.py:503] Scaling gradients by 0.0030966107733547688, model_norm_threshold=29104722149376.0
2024-10-08 21:52:47,014 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.586e+31, grad_sumsq=5.424e+30, orig_rms_sq=2.924e+00
2024-10-08 21:52:48,803 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.02 vs. limit=8.38775
2024-10-08 21:52:50,745 WARNING [optim.py:503] Scaling gradients by 0.0003807242610491812, model_norm_threshold=29104722149376.0
2024-10-08 21:52:50,904 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.722e+33, grad_sumsq=7.169e+34, orig_rms_sq=2.402e-02
2024-10-08 21:53:03,464 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=22.27 vs. limit=8.389
2024-10-08 21:53:10,406 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=2374.0, ans=0.11097499999999999
2024-10-08 21:53:13,795 WARNING [optim.py:503] Scaling gradients by 0.0032028730493038893, model_norm_threshold=29104722149376.0
2024-10-08 21:53:13,951 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.443e+31, grad_sumsq=3.538e+30, orig_rms_sq=4.079e+00
2024-10-08 21:53:14,726 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=15.57 vs. limit=8.39025
2024-10-08 21:53:16,352 WARNING [optim.py:503] Scaling gradients by 0.003295362461358309, model_norm_threshold=29104722149376.0
2024-10-08 21:53:16,508 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.580e+31, grad_sumsq=2.364e+33, orig_rms_sq=6.682e-03
2024-10-08 21:53:17,118 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=3.79 vs. limit=4.9496
2024-10-08 21:53:17,684 WARNING [optim.py:503] Scaling gradients by 0.032496362924575806, model_norm_threshold=29104722149376.0
2024-10-08 21:53:17,841 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.887e+29, grad_sumsq=2.115e+28, orig_rms_sq=8.925e+00
2024-10-08 21:53:18,425 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.18 vs. limit=6.187
2024-10-08 21:53:19,094 WARNING [optim.py:503] Scaling gradients by 0.029423175379633904, model_norm_threshold=29104722149376.0
2024-10-08 21:53:19,252 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.463e+29, grad_sumsq=6.019e+28, orig_rms_sq=4.092e+00
2024-10-08 21:53:22,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.15 vs. limit=6.187
2024-10-08 21:53:25,570 WARNING [optim.py:503] Scaling gradients by 0.007523524574935436, model_norm_threshold=29104722149376.0
2024-10-08 21:53:25,729 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.094e+30, grad_sumsq=1.297e+32, orig_rms_sq=2.385e-02
2024-10-08 21:53:26,197 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.73 vs. limit=8.3915
2024-10-08 21:53:27,057 WARNING [optim.py:503] Scaling gradients by 0.024145331233739853, model_norm_threshold=29104722149376.0
2024-10-08 21:53:27,213 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.784e+29, grad_sumsq=4.074e+31, orig_rms_sq=6.834e-03
2024-10-08 21:53:27,386 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2377.3333333333335, ans=0.2762266666666667
2024-10-08 21:53:34,731 WARNING [optim.py:503] Scaling gradients by 0.022651979699730873, model_norm_threshold=29104722149376.0
2024-10-08 21:53:34,889 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.147e+29, grad_sumsq=1.777e+31, orig_rms_sq=2.333e-02
2024-10-08 21:53:37,314 WARNING [optim.py:503] Scaling gradients by 0.003305293619632721, model_norm_threshold=29104722149376.0
2024-10-08 21:53:37,470 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.799e+31, grad_sumsq=4.093e+33, orig_rms_sq=6.837e-03
2024-10-08 21:53:38,234 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=24.75 vs. limit=9.2855
2024-10-08 21:53:38,436 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.63 vs. limit=9.2855
2024-10-08 21:53:39,851 WARNING [optim.py:503] Scaling gradients by 0.01950867660343647, model_norm_threshold=29104722149376.0
2024-10-08 21:53:40,008 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.315e+29, grad_sumsq=1.360e+32, orig_rms_sq=6.848e-03
2024-10-08 21:53:44,994 WARNING [optim.py:503] Scaling gradients by 0.03790006786584854, model_norm_threshold=29104722149376.0
2024-10-08 21:53:45,151 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.088e+29, grad_sumsq=4.669e+30, orig_rms_sq=2.331e-02
2024-10-08 21:53:46,354 WARNING [optim.py:503] Scaling gradients by 0.04818003252148628, model_norm_threshold=29104722149376.0
2024-10-08 21:53:46,509 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.040e+28, grad_sumsq=3.450e+30, orig_rms_sq=2.331e-02
2024-10-08 21:53:46,711 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2380.6666666666665, ans=0.38840625
2024-10-08 21:53:47,147 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=24.31 vs. limit=8.39275
2024-10-08 21:53:47,685 WARNING [optim.py:503] Scaling gradients by 0.030937839299440384, model_norm_threshold=29104722149376.0
2024-10-08 21:53:47,841 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.932e+29, grad_sumsq=8.288e+30, orig_rms_sq=2.331e-02
2024-10-08 21:53:50,393 INFO [train.py:1152] Epoch 2, batch 300, loss[loss=0.7804, ctc_loss=1.11, attn_decoder_loss=0.698, over 4786.00 frames. ], tot_loss[loss=0.7671, ctc_loss=1.048, attn_decoder_loss=0.6968, over 752845.68 frames. ], batch size: 32, lr: 3.75e-02,
2024-10-08 21:53:51,820 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=2384.0, ans=0.11059999999999999
2024-10-08 21:54:02,815 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=2387.3333333333335, ans=0.11047499999999999
2024-10-08 21:54:06,197 WARNING [optim.py:503] Scaling gradients by 0.02612162195146084, model_norm_threshold=29104722149376.0
2024-10-08 21:54:06,355 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.028e+29, grad_sumsq=1.272e+31, orig_rms_sq=2.381e-02
2024-10-08 21:54:07,229 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=17.06 vs. limit=9.2905
2024-10-08 21:54:08,828 WARNING [optim.py:503] Scaling gradients by 0.05613960698246956, model_norm_threshold=29104722149376.0
2024-10-08 21:54:08,985 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.302e+29, grad_sumsq=5.470e+30, orig_rms_sq=2.381e-02
2024-10-08 21:54:10,684 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.86 vs. limit=5.5968333333333335
2024-10-08 21:54:13,394 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.60 vs. limit=9.2905
2024-10-08 21:54:15,741 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.30 vs. limit=9.293
2024-10-08 21:54:17,562 WARNING [optim.py:503] Scaling gradients by 0.03114865906536579, model_norm_threshold=29104722149376.0
2024-10-08 21:54:17,716 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.014e+29, grad_sumsq=3.231e+31, orig_rms_sq=6.233e-03
2024-10-08 21:54:18,508 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.16 vs. limit=9.293
2024-10-08 21:54:19,056 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=2390.6666666666665, ans=0.23586000000000001
2024-10-08 21:54:19,524 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=11.46 vs. limit=8.3965
2024-10-08 21:54:22,577 WARNING [optim.py:503] Scaling gradients by 0.012010026723146439, model_norm_threshold=29104722149376.0
2024-10-08 21:54:22,737 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.399e+30, grad_sumsq=2.254e+32, orig_rms_sq=6.208e-03
2024-10-08 21:54:27,470 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.32 vs. limit=3.3590999999999998
2024-10-08 21:54:39,678 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=20.92 vs. limit=8.399000000000001
2024-10-08 21:54:41,606 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=2397.3333333333335, ans=0.387625
2024-10-08 21:54:42,547 WARNING [optim.py:503] Scaling gradients by 0.019105250015854836, model_norm_threshold=29104722149376.0
2024-10-08 21:54:42,705 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.452e+29, grad_sumsq=1.608e+29, orig_rms_sq=4.013e+00
2024-10-08 21:54:42,839 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.min_positive, batch_count=2397.3333333333335, ans=0.22602666666666665
2024-10-08 21:54:44,713 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=28.22 vs. limit=9.298
2024-10-08 21:54:48,779 WARNING [optim.py:503] Scaling gradients by 0.015569707378745079, model_norm_threshold=29104722149376.0
2024-10-08 21:54:48,937 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.867e+29, grad_sumsq=1.469e+29, orig_rms_sq=3.995e+00
2024-10-08 21:54:50,558 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.081e+10 1.043e+12 8.076e+12 1.925e+14 7.645e+16, threshold=1.615e+13, percent-clipped=40.0
2024-10-08 21:54:50,785 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=2397.3333333333335, ans=0.387625
2024-10-08 21:54:53,068 INFO [train.py:1152] Epoch 2, batch 350, loss[loss=0.7767, ctc_loss=1.053, attn_decoder_loss=0.7077, over 4883.00 frames. ], tot_loss[loss=0.7653, ctc_loss=1.047, attn_decoder_loss=0.6948, over 800170.97 frames. ], batch size: 19, lr: 3.75e-02,
2024-10-08 21:54:56,810 WARNING [optim.py:503] Scaling gradients by 0.014790000393986702, model_norm_threshold=16151905042432.0
2024-10-08 21:54:56,967 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.545e+29, grad_sumsq=9.025e+28, orig_rms_sq=3.928e+00
2024-10-08 21:54:57,197 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=2400.6666666666665, ans=0.045985000000000005
2024-10-08 21:54:58,127 WARNING [optim.py:503] Scaling gradients by 0.0009241247898899019, model_norm_threshold=16151905042432.0
2024-10-08 21:54:58,285 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.66, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.020e+32, grad_sumsq=3.399e+34, orig_rms_sq=5.942e-03
2024-10-08 21:55:03,724 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.33 vs. limit=5.6001666666666665
2024-10-08 21:55:04,209 WARNING [optim.py:503] Scaling gradients by 0.00020945598953403533, model_norm_threshold=16151905042432.0
2024-10-08 21:55:04,367 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.885e+33, grad_sumsq=8.182e+34, orig_rms_sq=2.304e-02
2024-10-08 21:55:05,789 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=2404.0, ans=0.04591
2024-10-08 21:55:06,658 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=9.33 vs. limit=8.4015
2024-10-08 21:55:09,402 WARNING [optim.py:503] Scaling gradients by 0.0006480177398771048, model_norm_threshold=16151905042432.0
2024-10-08 21:55:09,558 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.325e+32, grad_sumsq=5.670e+34, orig_rms_sq=5.865e-03
2024-10-08 21:55:17,779 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=57.29 vs. limit=8.40275
2024-10-08 21:55:17,857 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.36 vs. limit=6.203666666666667
2024-10-08 21:55:18,864 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=26.28 vs. limit=8.40275
2024-10-08 21:55:19,349 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1.whitening_limit, batch_count=2407.3333333333335, ans=5.601833333333333
2024-10-08 21:55:21,389 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.86 vs. limit=8.40275
2024-10-08 21:55:22,053 WARNING [optim.py:503] Scaling gradients by 0.028475232422351837, model_norm_threshold=16151905042432.0
2024-10-08 21:55:22,211 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.039e+28, grad_sumsq=1.213e+31, orig_rms_sq=5.802e-03
2024-10-08 21:55:28,443 WARNING [optim.py:503] Scaling gradients by 0.021754978224635124, model_norm_threshold=16151905042432.0
2024-10-08 21:55:28,602 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.544e+29, grad_sumsq=2.647e+31, orig_rms_sq=5.835e-03
2024-10-08 21:55:29,940 WARNING [optim.py:503] Scaling gradients by 0.005040628835558891, model_norm_threshold=16151905042432.0
2024-10-08 21:55:30,098 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.797e+30, grad_sumsq=4.795e+32, orig_rms_sq=5.835e-03
2024-10-08 21:55:36,613 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=2410.6666666666665, ans=0.387
2024-10-08 21:55:38,987 WARNING [optim.py:503] Scaling gradients by 0.0692199096083641, model_norm_threshold=16151905042432.0
2024-10-08 21:55:39,148 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.972e+28, grad_sumsq=3.417e+30, orig_rms_sq=5.772e-03
2024-10-08 21:55:40,717 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.min_abs, batch_count=2410.6666666666665, ans=0.23616
2024-10-08 21:55:42,917 WARNING [optim.py:503] Scaling gradients by 0.04076208546757698, model_norm_threshold=16151905042432.0
2024-10-08 21:55:43,072 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.887e+28, grad_sumsq=7.222e+27, orig_rms_sq=3.998e+00
2024-10-08 21:55:43,254 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=2414.0, ans=0.19824999999999998
2024-10-08 21:55:45,170 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=11.35 vs. limit=8.40525
2024-10-08 21:55:50,427 WARNING [optim.py:503] Scaling gradients by 0.05751660093665123, model_norm_threshold=16151905042432.0
2024-10-08 21:55:50,584 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.867e+28, grad_sumsq=2.079e+27, orig_rms_sq=8.977e+00
2024-10-08 21:55:56,006 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.83 vs. limit=8.4065
2024-10-08 21:55:56,872 INFO [train.py:1152] Epoch 2, batch 400, loss[loss=0.7838, ctc_loss=1.071, attn_decoder_loss=0.712, over 4879.00 frames. ], tot_loss[loss=0.7662, ctc_loss=1.046, attn_decoder_loss=0.6962, over 836930.10 frames. ], batch size: 22, lr: 3.74e-02,
2024-10-08 21:55:57,058 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=2417.3333333333335, ans=0.3866875
2024-10-08 21:55:57,982 WARNING [optim.py:503] Scaling gradients by 0.07286838442087173, model_norm_threshold=16151905042432.0
2024-10-08 21:55:58,142 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.013e+28, grad_sumsq=1.764e+30, orig_rms_sq=5.743e-03
2024-10-08 21:55:58,745 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.56 vs. limit=8.4065
2024-10-08 21:55:59,648 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2417.3333333333335, ans=0.27582666666666666
2024-10-08 21:56:01,826 WARNING [optim.py:503] Scaling gradients by 0.010490918532013893, model_norm_threshold=16151905042432.0
2024-10-08 21:56:01,982 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.319e+29, grad_sumsq=1.768e+29, orig_rms_sq=2.443e+00
2024-10-08 21:56:05,168 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.16 vs. limit=3.3626
2024-10-08 21:56:05,943 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2417.3333333333335, ans=0.27582666666666666
2024-10-08 21:56:11,215 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.65 vs. limit=6.210333333333333
2024-10-08 21:56:11,987 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=2420.6666666666665, ans=0.07
2024-10-08 21:56:12,374 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=97.80 vs. limit=8.40775
2024-10-08 21:56:12,635 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.42 vs. limit=3.3631
2024-10-08 21:56:14,183 WARNING [optim.py:503] Scaling gradients by 0.04926641657948494, model_norm_threshold=16151905042432.0
2024-10-08 21:56:14,341 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.321e+28, grad_sumsq=4.175e+30, orig_rms_sq=5.560e-03
2024-10-08 21:56:15,519 WARNING [optim.py:503] Scaling gradients by 0.07883121818304062, model_norm_threshold=16151905042432.0
2024-10-08 21:56:15,674 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.084e+28, grad_sumsq=1.950e+30, orig_rms_sq=5.560e-03
2024-10-08 21:56:15,867 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=2420.6666666666665, ans=0.109225
2024-10-08 21:56:17,657 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.01 vs. limit=9.3155
2024-10-08 21:56:19,820 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=41.92 vs. limit=9.3155
2024-10-08 21:56:38,311 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=2427.3333333333335, ans=0.38621875
2024-10-08 21:56:38,619 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=13.16 vs. limit=8.41025
2024-10-08 21:56:41,727 WARNING [optim.py:503] Scaling gradients by 0.03126191720366478, model_norm_threshold=16151905042432.0
2024-10-08 21:56:41,883 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.832e+28, grad_sumsq=1.522e+31, orig_rms_sq=5.803e-03
2024-10-08 21:56:42,819 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.11 vs. limit=9.3205
2024-10-08 21:56:45,180 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.70 vs. limit=8.41025
2024-10-08 21:56:46,593 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=13.63 vs. limit=8.4115
2024-10-08 21:56:53,243 WARNING [optim.py:503] Scaling gradients by 0.003789301263168454, model_norm_threshold=16151905042432.0
2024-10-08 21:56:53,399 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.640e+30, grad_sumsq=1.822e+32, orig_rms_sq=1.998e-02
2024-10-08 21:56:56,047 WARNING [optim.py:503] Scaling gradients by 0.031186245381832123, model_norm_threshold=16151905042432.0
2024-10-08 21:56:56,204 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.487e+28, grad_sumsq=9.155e+30, orig_rms_sq=5.994e-03
2024-10-08 21:56:57,497 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.388e+10 1.186e+12 9.638e+12 1.113e+14 7.711e+16, threshold=1.928e+13, percent-clipped=41.0
2024-10-08 21:56:58,314 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=26.80 vs. limit=9.323
2024-10-08 21:56:58,868 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=2434.0, ans=0.81481
2024-10-08 21:56:59,870 WARNING [optim.py:503] Scaling gradients by 0.007462508510798216, model_norm_threshold=19275112775680.0
2024-10-08 21:57:00,028 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.176e+30, grad_sumsq=1.952e+32, orig_rms_sq=6.026e-03
2024-10-08 21:57:00,086 INFO [train.py:1152] Epoch 2, batch 450, loss[loss=0.7854, ctc_loss=1.089, attn_decoder_loss=0.7095, over 4871.00 frames. ], tot_loss[loss=0.7675, ctc_loss=1.047, attn_decoder_loss=0.6978, over 865484.37 frames. ], batch size: 23, lr: 3.73e-02,
2024-10-08 21:57:01,208 WARNING [optim.py:503] Scaling gradients by 0.008663663640618324, model_norm_threshold=19275112775680.0
2024-10-08 21:57:01,364 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.656e+29, grad_sumsq=8.763e+28, orig_rms_sq=8.736e+00
2024-10-08 21:57:02,101 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.20 vs. limit=6.2170000000000005
2024-10-08 21:57:02,593 WARNING [optim.py:503] Scaling gradients by 0.09529239684343338, model_norm_threshold=19275112775680.0
2024-10-08 21:57:02,749 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.721e+27, grad_sumsq=3.664e+29, orig_rms_sq=2.107e-02
2024-10-08 21:57:09,185 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=2434.0, ans=0.81481
2024-10-08 21:57:12,213 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=6.56 vs. limit=4.974933333333333
2024-10-08 21:57:15,775 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=24.23 vs. limit=9.328
2024-10-08 21:57:18,380 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=10.77 vs. limit=5.609333333333334
2024-10-08 21:57:18,847 WARNING [optim.py:503] Scaling gradients by 0.0008429887238889933, model_norm_threshold=19275112775680.0
2024-10-08 21:57:19,003 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.873e+32, grad_sumsq=2.834e+34, orig_rms_sq=6.609e-03
2024-10-08 21:57:25,963 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=14.96 vs. limit=6.2203333333333335
2024-10-08 21:57:25,973 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=6.44 vs. limit=4.976266666666667
2024-10-08 21:57:27,652 WARNING [optim.py:503] Scaling gradients by 0.03559846803545952, model_norm_threshold=19275112775680.0
2024-10-08 21:57:27,807 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.274e+28, grad_sumsq=2.877e+30, orig_rms_sq=2.181e-02
2024-10-08 21:57:28,542 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=27.80 vs. limit=9.3305
2024-10-08 21:57:30,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.25 vs. limit=6.2203333333333335
2024-10-08 21:57:32,837 WARNING [optim.py:503] Scaling gradients by 0.004408761393278837, model_norm_threshold=19275112775680.0
2024-10-08 21:57:32,995 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.010e+30, grad_sumsq=9.730e+29, orig_rms_sq=4.121e+00
2024-10-08 21:57:34,239 WARNING [optim.py:503] Scaling gradients by 0.022724471986293793, model_norm_threshold=19275112775680.0
2024-10-08 21:57:34,396 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.902e+29, grad_sumsq=1.320e+31, orig_rms_sq=2.198e-02
2024-10-08 21:57:35,583 WARNING [optim.py:503] Scaling gradients by 0.02013452723622322, model_norm_threshold=19275112775680.0
2024-10-08 21:57:35,737 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.468e+29, grad_sumsq=5.989e+28, orig_rms_sq=4.121e+00
2024-10-08 21:57:36,952 WARNING [optim.py:503] Scaling gradients by 0.025546500459313393, model_norm_threshold=19275112775680.0
2024-10-08 21:57:37,108 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.435e+29, grad_sumsq=3.483e+28, orig_rms_sq=4.121e+00
2024-10-08 21:57:37,879 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=2444.0, ans=9.333
2024-10-08 21:57:40,978 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=2444.0, ans=0.08472500000000001
2024-10-08 21:57:41,986 WARNING [optim.py:503] Scaling gradients by 0.0005681316251866519, model_norm_threshold=19275112775680.0
2024-10-08 21:57:42,143 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.417e+32, grad_sumsq=1.096e+34, orig_rms_sq=2.206e-02
2024-10-08 21:57:45,847 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=12.07 vs. limit=8.4165
2024-10-08 21:58:03,867 INFO [train.py:1152] Epoch 2, batch 500, loss[loss=0.7921, ctc_loss=1.058, attn_decoder_loss=0.7257, over 4837.00 frames. ], tot_loss[loss=0.7641, ctc_loss=1.042, attn_decoder_loss=0.6946, over 888098.43 frames. ], batch size: 34, lr: 3.73e-02,
2024-10-08 21:58:04,321 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=22.51 vs. limit=9.338000000000001
2024-10-08 21:58:09,814 WARNING [optim.py:503] Scaling gradients by 0.012992074713110924, model_norm_threshold=19275112775680.0
2024-10-08 21:58:09,971 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.725e+29, grad_sumsq=6.883e+31, orig_rms_sq=6.864e-03
2024-10-08 21:58:15,573 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=6.94 vs. limit=4.9816
2024-10-08 21:58:18,338 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=25.50 vs. limit=9.3405
2024-10-08 21:58:20,111 WARNING [optim.py:503] Scaling gradients by 0.00048636080464348197, model_norm_threshold=19275112775680.0
2024-10-08 21:58:20,264 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.958e+32, grad_sumsq=9.271e+31, orig_rms_sq=4.270e+00
2024-10-08 21:58:23,589 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=16.24 vs. limit=8.42025
2024-10-08 21:58:26,984 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2454.0, ans=0.38496874999999997
2024-10-08 21:58:27,385 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=47.04 vs. limit=9.3405
2024-10-08 21:58:30,678 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2457.3333333333335, ans=0.27542666666666665
2024-10-08 21:58:31,779 WARNING [optim.py:503] Scaling gradients by 0.0003229815629310906, model_norm_threshold=19275112775680.0
2024-10-08 21:58:31,934 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.155e+33, grad_sumsq=2.701e+32, orig_rms_sq=4.276e+00
2024-10-08 21:58:32,172 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=2457.3333333333335, ans=0.3848125
2024-10-08 21:58:33,798 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=30.61 vs. limit=8.4215
2024-10-08 21:58:35,574 WARNING [optim.py:503] Scaling gradients by 0.008237279951572418, model_norm_threshold=19275112775680.0
2024-10-08 21:58:35,729 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.250e+30, grad_sumsq=2.903e+29, orig_rms_sq=4.306e+00
2024-10-08 21:58:38,543 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=2457.3333333333335, ans=0.8139933333333333
2024-10-08 21:58:44,575 WARNING [optim.py:503] Scaling gradients by 0.038228075951337814, model_norm_threshold=19275112775680.0
2024-10-08 21:58:44,732 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.56, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.434e+29, grad_sumsq=6.100e+30, orig_rms_sq=2.352e-02
2024-10-08 21:58:45,915 WARNING [optim.py:503] Scaling gradients by 0.06389620155096054, model_norm_threshold=19275112775680.0
2024-10-08 21:58:46,071 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.172e+28, grad_sumsq=9.236e+29, orig_rms_sq=2.352e-02
2024-10-08 21:58:48,988 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=6.36 vs. limit=4.984266666666667
2024-10-08 21:58:49,003 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=26.89 vs. limit=8.42275
2024-10-08 21:58:51,125 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=2460.6666666666665, ans=0.38465625000000003
2024-10-08 21:58:53,472 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=2464.0, ans=0.3845
2024-10-08 21:58:53,964 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=3.43 vs. limit=4.9856
2024-10-08 21:58:58,860 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.42 vs. limit=8.424
2024-10-08 21:58:59,883 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=2464.0, ans=0.3845
2024-10-08 21:59:01,089 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=2464.0, ans=0.3845
2024-10-08 21:59:03,969 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=2464.0, ans=0.08460000000000001
2024-10-08 21:59:04,582 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.24 vs. limit=5.616
2024-10-08 21:59:04,953 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.338e+10 1.903e+12 9.926e+12 7.453e+13 5.968e+16, threshold=1.985e+13, percent-clipped=39.0
2024-10-08 21:59:07,622 INFO [train.py:1152] Epoch 2, batch 550, loss[loss=0.7972, ctc_loss=1.077, attn_decoder_loss=0.7272, over 4790.00 frames. ], tot_loss[loss=0.7655, ctc_loss=1.044, attn_decoder_loss=0.6958, over 905465.58 frames. ], batch size: 40, lr: 3.72e-02,
2024-10-08 21:59:13,618 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=22.35 vs. limit=9.3505
2024-10-08 21:59:15,682 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=2467.3333333333335, ans=0.044485
2024-10-08 21:59:16,707 WARNING [optim.py:503] Scaling gradients by 0.0734783187508583, model_norm_threshold=19852129468416.0
2024-10-08 21:59:16,864 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.189e+28, grad_sumsq=5.217e+29, orig_rms_sq=2.279e-02
2024-10-08 21:59:18,072 WARNING [optim.py:503] Scaling gradients by 0.00019501594942994416, model_norm_threshold=19852129468416.0
2024-10-08 21:59:18,230 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.867e+33, grad_sumsq=5.819e+33, orig_rms_sq=3.208e-01
2024-10-08 21:59:18,430 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=2467.3333333333335, ans=0.38434375
2024-10-08 21:59:18,519 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=1.319e+02
2024-10-08 21:59:22,042 WARNING [optim.py:503] Scaling gradients by 0.025386208668351173, model_norm_threshold=19852129468416.0
2024-10-08 21:59:22,200 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.085e+29, grad_sumsq=4.713e+30, orig_rms_sq=2.301e-02
2024-10-08 21:59:24,687 WARNING [optim.py:503] Scaling gradients by 0.005737158469855785, model_norm_threshold=19852129468416.0
2024-10-08 21:59:24,846 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.315e+30, grad_sumsq=5.648e+29, orig_rms_sq=4.099e+00
2024-10-08 21:59:26,085 WARNING [optim.py:503] Scaling gradients by 0.050897590816020966, model_norm_threshold=19852129468416.0
2024-10-08 21:59:26,243 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.323e+28, grad_sumsq=1.425e+30, orig_rms_sq=2.332e-02
2024-10-08 21:59:27,043 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=6.66 vs. limit=4.988266666666666
2024-10-08 21:59:32,009 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten.whitening_limit, batch_count=2470.6666666666665, ans=8.4265
2024-10-08 21:59:39,263 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=26.68 vs. limit=8.42775
2024-10-08 21:59:39,930 WARNING [optim.py:503] Scaling gradients by 0.0554862804710865, model_norm_threshold=19852129468416.0
2024-10-08 21:59:40,086 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.549e+28, grad_sumsq=3.639e+30, orig_rms_sq=7.006e-03
2024-10-08 21:59:43,220 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=19.73 vs. limit=8.42775
2024-10-08 21:59:44,014 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=2474.0, ans=0.10722499999999999
2024-10-08 21:59:45,986 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.44 vs. limit=8.429
2024-10-08 21:59:48,478 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.21 vs. limit=9.358
2024-10-08 21:59:48,802 WARNING [optim.py:503] Scaling gradients by 0.040914613753557205, model_norm_threshold=19852129468416.0
2024-10-08 21:59:48,958 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.910e+28, grad_sumsq=1.182e+28, orig_rms_sq=4.153e+00
2024-10-08 21:59:52,050 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.68 vs. limit=9.358
2024-10-08 21:59:57,593 WARNING [optim.py:503] Scaling gradients by 0.0019977062474936247, model_norm_threshold=19852129468416.0
2024-10-08 21:59:57,747 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.317e+31, grad_sumsq=9.552e+32, orig_rms_sq=2.426e-02
2024-10-08 22:00:00,154 WARNING [optim.py:503] Scaling gradients by 0.02062303014099598, model_norm_threshold=19852129468416.0
2024-10-08 22:00:00,307 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.639e+29, grad_sumsq=1.815e+28, orig_rms_sq=9.030e+00
2024-10-08 22:00:03,373 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.52 vs. limit=5.620166666666667
2024-10-08 22:00:11,733 INFO [train.py:1152] Epoch 2, batch 600, loss[loss=0.7763, ctc_loss=1.071, attn_decoder_loss=0.7027, over 4810.00 frames. ], tot_loss[loss=0.7652, ctc_loss=1.046, attn_decoder_loss=0.6951, over 919259.30 frames. ], batch size: 38, lr: 3.72e-02,
2024-10-08 22:00:12,192 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.49 vs. limit=5.621
2024-10-08 22:00:17,126 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=2484.0, ans=0.1895
2024-10-08 22:00:19,128 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.59 vs. limit=6.242
2024-10-08 22:00:20,049 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.57 vs. limit=6.242
2024-10-08 22:00:25,549 WARNING [optim.py:503] Scaling gradients by 0.003962860442698002, model_norm_threshold=19852129468416.0
2024-10-08 22:00:25,703 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.251e+30, grad_sumsq=6.904e+29, orig_rms_sq=9.054e+00
2024-10-08 22:00:26,923 WARNING [optim.py:503] Scaling gradients by 0.0014230908127501607, model_norm_threshold=19852129468416.0
2024-10-08 22:00:27,080 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.127e+31, grad_sumsq=1.816e+33, orig_rms_sq=2.273e-02
2024-10-08 22:00:28,769 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.40 vs. limit=5.621833333333333
2024-10-08 22:00:30,642 WARNING [optim.py:503] Scaling gradients by 0.08391701430082321, model_norm_threshold=19852129468416.0
2024-10-08 22:00:30,801 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.286e+28, grad_sumsq=3.180e+27, orig_rms_sq=4.044e+00
2024-10-08 22:00:35,825 WARNING [optim.py:503] Scaling gradients by 0.02229403518140316, model_norm_threshold=19852129468416.0
2024-10-08 22:00:35,982 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.036e+29, grad_sumsq=6.056e+31, orig_rms_sq=6.664e-03
2024-10-08 22:00:37,243 WARNING [optim.py:503] Scaling gradients by 0.0035892806481570005, model_norm_threshold=19852129468416.0
2024-10-08 22:00:37,400 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.005e+31, grad_sumsq=1.491e+33, orig_rms_sq=6.739e-03
2024-10-08 22:00:42,612 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=2490.6666666666665, ans=0.1066
2024-10-08 22:00:47,596 WARNING [optim.py:503] Scaling gradients by 0.01621904969215393, model_norm_threshold=19852129468416.0
2024-10-08 22:00:47,753 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.14, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.058e+29, grad_sumsq=2.278e+28, orig_rms_sq=9.033e+00
2024-10-08 22:00:48,993 WARNING [optim.py:503] Scaling gradients by 0.01861490309238434, model_norm_threshold=19852129468416.0
2024-10-08 22:00:49,150 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.853e+29, grad_sumsq=5.736e+31, orig_rms_sq=6.717e-03
2024-10-08 22:00:58,975 WARNING [optim.py:503] Scaling gradients by 0.007499772123992443, model_norm_threshold=19852129468416.0
2024-10-08 22:00:59,131 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.14, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.747e+29, grad_sumsq=1.088e+29, orig_rms_sq=8.960e+00
2024-10-08 22:01:00,054 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1.whitening_limit, batch_count=2494.0, ans=5.6235
2024-10-08 22:01:07,511 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=26.80 vs. limit=8.4365
2024-10-08 22:01:08,105 WARNING [optim.py:503] Scaling gradients by 0.04006465524435043, model_norm_threshold=19852129468416.0
2024-10-08 22:01:08,261 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.096e+28, grad_sumsq=1.047e+31, orig_rms_sq=6.779e-03
2024-10-08 22:01:13,240 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.435e+10 1.655e+12 1.839e+13 1.183e+14 1.018e+17, threshold=3.679e+13, percent-clipped=48.0
2024-10-08 22:01:13,240 WARNING [optim.py:503] Scaling gradients by 0.0023471952881664038, model_norm_threshold=36787292471296.0
2024-10-08 22:01:13,396 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.357e+31, grad_sumsq=1.055e+31, orig_rms_sq=4.129e+00
2024-10-08 22:01:16,007 INFO [train.py:1152] Epoch 2, batch 650, loss[loss=0.72, ctc_loss=1.025, attn_decoder_loss=0.6438, over 4843.00 frames. ], tot_loss[loss=0.7627, ctc_loss=1.044, attn_decoder_loss=0.6923, over 930188.13 frames. ], batch size: 21, lr: 3.71e-02,
2024-10-08 22:01:22,753 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=25.39 vs. limit=9.3755
2024-10-08 22:01:26,964 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=21.44 vs. limit=9.3755
2024-10-08 22:01:29,411 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.98 vs. limit=6.252
2024-10-08 22:01:31,144 WARNING [optim.py:503] Scaling gradients by 0.03764544799923897, model_norm_threshold=36787292471296.0
2024-10-08 22:01:31,316 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.991e+29, grad_sumsq=4.469e+31, orig_rms_sq=6.693e-03
2024-10-08 22:01:38,801 WARNING [optim.py:503] Scaling gradients by 0.00732222106307745, model_norm_threshold=36787292471296.0
2024-10-08 22:01:38,962 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.135e+31, grad_sumsq=1.709e+33, orig_rms_sq=6.639e-03
2024-10-08 22:01:39,756 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.12 vs. limit=9.378
2024-10-08 22:01:42,878 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=2507.3333333333335, ans=0.10597499999999999
2024-10-08 22:01:44,056 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=2507.3333333333335, ans=6.567083333333334
2024-10-08 22:01:49,898 WARNING [optim.py:503] Scaling gradients by 0.0706627145409584, model_norm_threshold=36787292471296.0
2024-10-08 22:01:50,054 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.851e+28, grad_sumsq=5.993e+27, orig_rms_sq=8.094e+00
2024-10-08 22:01:52,034 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.76 vs. limit=3.3761
2024-10-08 22:01:54,764 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.37 vs. limit=8.4415
2024-10-08 22:01:58,771 WARNING [optim.py:503] Scaling gradients by 0.0008260697359219193, model_norm_threshold=36787292471296.0
2024-10-08 22:01:58,926 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.504e+32, grad_sumsq=1.993e+34, orig_rms_sq=2.260e-02
2024-10-08 22:02:01,124 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=5.54 vs. limit=5.004266666666666
2024-10-08 22:02:02,640 WARNING [optim.py:503] Scaling gradients by 0.08703196793794632, model_norm_threshold=36787292471296.0
2024-10-08 22:02:02,797 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.958e+28, grad_sumsq=1.251e+28, orig_rms_sq=3.964e+00
2024-10-08 22:02:05,341 WARNING [optim.py:503] Scaling gradients by 0.05189504846930504, model_norm_threshold=36787292471296.0
2024-10-08 22:02:05,498 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.005e+29, grad_sumsq=4.373e+30, orig_rms_sq=2.298e-02
2024-10-08 22:02:06,704 WARNING [optim.py:503] Scaling gradients by 0.03132301941514015, model_norm_threshold=36787292471296.0
2024-10-08 22:02:06,858 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.258e+29, grad_sumsq=6.843e+31, orig_rms_sq=6.222e-03
2024-10-08 22:02:08,750 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.94 vs. limit=6.257
2024-10-08 22:02:08,837 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.52 vs. limit=6.257
2024-10-08 22:02:09,220 WARNING [optim.py:503] Scaling gradients by 0.057321175932884216, model_norm_threshold=36787292471296.0
2024-10-08 22:02:09,379 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.894e+28, grad_sumsq=1.429e+31, orig_rms_sq=6.222e-03
2024-10-08 22:02:13,079 WARNING [optim.py:503] Scaling gradients by 0.03332467004656792, model_norm_threshold=36787292471296.0
2024-10-08 22:02:13,234 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.874e+29, grad_sumsq=8.068e+30, orig_rms_sq=2.322e-02
2024-10-08 22:02:17,736 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=20.69 vs. limit=8.44275
2024-10-08 22:02:20,863 INFO [train.py:1152] Epoch 2, batch 700, loss[loss=0.799, ctc_loss=1.089, attn_decoder_loss=0.7264, over 4740.00 frames. ], tot_loss[loss=0.7632, ctc_loss=1.046, attn_decoder_loss=0.6925, over 937971.52 frames. ], batch size: 19, lr: 3.70e-02,
2024-10-08 22:02:22,957 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.51 vs. limit=8.443999999999999
2024-10-08 22:02:28,398 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=2517.3333333333335, ans=0.043359999999999996
2024-10-08 22:02:30,770 WARNING [optim.py:503] Scaling gradients by 0.04304167628288269, model_norm_threshold=36787292471296.0
2024-10-08 22:02:30,925 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.276e+29, grad_sumsq=1.509e+28, orig_rms_sq=8.452e+00
2024-10-08 22:02:32,781 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=21.51 vs. limit=8.44525
2024-10-08 22:02:34,036 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.08 vs. limit=6.2603333333333335
2024-10-08 22:02:46,442 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.74 vs. limit=8.4465
2024-10-08 22:02:47,407 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=2524.0, ans=0.3816875
2024-10-08 22:02:47,701 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=35.00 vs. limit=8.4465
2024-10-08 22:02:55,952 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.min_positive, batch_count=2524.0, ans=0.042112500000000004
2024-10-08 22:03:03,990 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=27.82 vs. limit=8.44775
2024-10-08 22:03:08,292 WARNING [optim.py:503] Scaling gradients by 0.0023798493202775717, model_norm_threshold=36787292471296.0
2024-10-08 22:03:08,448 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.749e+31, grad_sumsq=4.171e+33, orig_rms_sq=2.337e-02
2024-10-08 22:03:08,663 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2527.3333333333335, ans=0.2747266666666667
2024-10-08 22:03:14,639 WARNING [optim.py:503] Scaling gradients by 0.03745381906628609, model_norm_threshold=36787292471296.0
2024-10-08 22:03:14,796 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.463e+29, grad_sumsq=6.010e+28, orig_rms_sq=4.098e+00
2024-10-08 22:03:18,167 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=25.58 vs. limit=8.449
2024-10-08 22:03:21,385 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.068e+10 3.153e+12 1.289e+13 1.398e+14 4.453e+16, threshold=2.578e+13, percent-clipped=36.0
2024-10-08 22:03:21,385 WARNING [optim.py:503] Scaling gradients by 0.0023163137957453728, model_norm_threshold=25784869715968.0
2024-10-08 22:03:21,538 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.248e+31, grad_sumsq=1.822e+33, orig_rms_sq=2.332e-02
2024-10-08 22:03:22,140 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=19.38 vs. limit=8.449
2024-10-08 22:03:22,468 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=18.66 vs. limit=8.449
2024-10-08 22:03:23,348 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.52 vs. limit=3.3801
2024-10-08 22:03:24,064 INFO [train.py:1152] Epoch 2, batch 750, loss[loss=0.7375, ctc_loss=0.9914, attn_decoder_loss=0.6741, over 4894.00 frames. ], tot_loss[loss=0.7617, ctc_loss=1.044, attn_decoder_loss=0.691, over 944733.25 frames. ], batch size: 22, lr: 3.70e-02,
2024-10-08 22:03:30,068 WARNING [optim.py:503] Scaling gradients by 0.008535612374544144, model_norm_threshold=25784869715968.0
2024-10-08 22:03:30,223 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.623e+30, grad_sumsq=6.902e+31, orig_rms_sq=2.351e-02
2024-10-08 22:03:30,484 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2534.0, ans=0.27466
2024-10-08 22:03:32,624 WARNING [optim.py:503] Scaling gradients by 0.004175717011094093, model_norm_threshold=25784869715968.0
2024-10-08 22:03:32,779 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.095e+31, grad_sumsq=1.790e+33, orig_rms_sq=6.115e-03
2024-10-08 22:03:34,070 WARNING [optim.py:503] Scaling gradients by 0.018131593242287636, model_norm_threshold=25784869715968.0
2024-10-08 22:03:34,225 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.328e+29, grad_sumsq=2.264e+31, orig_rms_sq=2.353e-02
2024-10-08 22:03:44,752 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=2537.3333333333335, ans=9.403
2024-10-08 22:03:46,970 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=2537.3333333333335, ans=0.38106249999999997
2024-10-08 22:03:50,643 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=2540.6666666666665, ans=0.38090625
2024-10-08 22:03:51,141 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.09 vs. limit=6.270333333333333
2024-10-08 22:03:51,653 WARNING [optim.py:503] Scaling gradients by 0.02227151393890381, model_norm_threshold=25784869715968.0
2024-10-08 22:03:51,809 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.414e+29, grad_sumsq=1.425e+31, orig_rms_sq=2.396e-02
2024-10-08 22:03:53,878 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.49 vs. limit=9.4055
2024-10-08 22:03:55,821 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=2540.6666666666665, ans=0.38090625
2024-10-08 22:04:04,669 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=2544.0, ans=0.04276
2024-10-08 22:04:05,798 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=2544.0, ans=0.38075000000000003
2024-10-08 22:04:08,559 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.48 vs. limit=8.454
2024-10-08 22:04:09,136 WARNING [optim.py:503] Scaling gradients by 0.057758431881666183, model_norm_threshold=25784869715968.0
2024-10-08 22:04:09,292 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.415e+28, grad_sumsq=8.114e+27, orig_rms_sq=4.209e+00
2024-10-08 22:04:10,475 WARNING [optim.py:503] Scaling gradients by 0.012033571489155293, model_norm_threshold=25784869715968.0
2024-10-08 22:04:10,629 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.383e+30, grad_sumsq=2.299e+32, orig_rms_sq=6.014e-03
2024-10-08 22:04:11,352 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=27.96 vs. limit=8.454
2024-10-08 22:04:12,995 WARNING [optim.py:503] Scaling gradients by 0.0030992033425718546, model_norm_threshold=25784869715968.0
2024-10-08 22:04:13,152 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.559e+31, grad_sumsq=1.041e+33, orig_rms_sq=2.458e-02
2024-10-08 22:04:16,321 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=30.88 vs. limit=9.4105
2024-10-08 22:04:22,648 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2.whitening_limit, batch_count=2547.3333333333335, ans=6.273666666666667
2024-10-08 22:04:26,926 INFO [train.py:1152] Epoch 2, batch 800, loss[loss=0.6767, ctc_loss=0.9563, attn_decoder_loss=0.6068, over 4852.00 frames. ], tot_loss[loss=0.7607, ctc_loss=1.043, attn_decoder_loss=0.6902, over 949630.50 frames. ], batch size: 19, lr: 3.69e-02,
2024-10-08 22:04:29,227 WARNING [optim.py:503] Scaling gradients by 0.02455797977745533, model_norm_threshold=25784869715968.0
2024-10-08 22:04:29,384 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.510e+29, grad_sumsq=1.036e+31, orig_rms_sq=2.422e-02
2024-10-08 22:04:30,625 WARNING [optim.py:503] Scaling gradients by 0.0497288703918457, model_norm_threshold=25784869715968.0
2024-10-08 22:04:30,779 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.654e+28, grad_sumsq=3.573e+30, orig_rms_sq=2.422e-02
2024-10-08 22:04:31,049 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=2550.6666666666665, ans=0.3804375
2024-10-08 22:04:33,292 WARNING [optim.py:503] Scaling gradients by 0.0009892662055790424, model_norm_threshold=25784869715968.0
2024-10-08 22:04:33,448 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.829e+32, grad_sumsq=3.079e+34, orig_rms_sq=5.939e-03
2024-10-08 22:04:37,140 WARNING [optim.py:503] Scaling gradients by 0.0014195158146321774, model_norm_threshold=25784869715968.0
2024-10-08 22:04:37,295 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.520e+32, grad_sumsq=2.551e+34, orig_rms_sq=5.958e-03
2024-10-08 22:04:38,485 WARNING [optim.py:503] Scaling gradients by 0.048379767686128616, model_norm_threshold=25784869715968.0
2024-10-08 22:04:38,642 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.830e+28, grad_sumsq=2.406e+30, orig_rms_sq=2.423e-02
2024-10-08 22:04:40,210 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=2554.0, ans=0.81061
2024-10-08 22:04:49,176 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=2554.0, ans=0.81061
2024-10-08 22:04:49,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=5.67 vs. limit=5.0216
2024-10-08 22:04:50,817 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.19 vs. limit=9.4155
2024-10-08 22:04:54,749 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=6.21 vs. limit=5.0229333333333335
2024-10-08 22:04:55,403 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=2557.3333333333335, ans=0.380125
2024-10-08 22:04:57,582 WARNING [optim.py:503] Scaling gradients by 0.006700466386973858, model_norm_threshold=25784869715968.0
2024-10-08 22:04:57,738 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.412e+30, grad_sumsq=3.962e+32, orig_rms_sq=6.089e-03
2024-10-08 22:04:58,913 WARNING [optim.py:503] Scaling gradients by 0.05673707649111748, model_norm_threshold=25784869715968.0
2024-10-08 22:04:59,071 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.467e+28, grad_sumsq=1.442e+29, orig_rms_sq=3.097e-01
2024-10-08 22:05:01,888 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=2557.3333333333335, ans=0.8104933333333333
2024-10-08 22:05:01,896 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=2557.3333333333335, ans=0.04245999999999999
2024-10-08 22:05:04,317 WARNING [optim.py:503] Scaling gradients by 0.017105091363191605, model_norm_threshold=25784869715968.0
2024-10-08 22:05:04,472 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.386e+29, grad_sumsq=1.368e+32, orig_rms_sq=6.130e-03
2024-10-08 22:05:09,385 WARNING [optim.py:503] Scaling gradients by 0.07069084793329239, model_norm_threshold=25784869715968.0
2024-10-08 22:05:09,544 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.715e+28, grad_sumsq=1.549e+30, orig_rms_sq=2.399e-02
2024-10-08 22:05:14,019 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=5.93 vs. limit=5.024266666666667
2024-10-08 22:05:15,253 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=5.70 vs. limit=5.024266666666667
2024-10-08 22:05:15,547 WARNING [optim.py:503] Scaling gradients by 0.04659423604607582, model_norm_threshold=25784869715968.0
2024-10-08 22:05:15,704 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.706e+28, grad_sumsq=2.796e+30, orig_rms_sq=2.399e-02
2024-10-08 22:05:19,371 WARNING [optim.py:503] Scaling gradients by 0.08994210511445999, model_norm_threshold=25784869715968.0
2024-10-08 22:05:19,528 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.513e+28, grad_sumsq=4.797e+28, orig_rms_sq=3.154e-01
2024-10-08 22:05:21,942 WARNING [optim.py:503] Scaling gradients by 0.007856583222746849, model_norm_threshold=25784869715968.0
2024-10-08 22:05:22,098 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.364e+30, grad_sumsq=3.762e+32, orig_rms_sq=6.283e-03
2024-10-08 22:05:25,846 WARNING [optim.py:503] Scaling gradients by 0.04093264043331146, model_norm_threshold=25784869715968.0
2024-10-08 22:05:26,005 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.865e+28, grad_sumsq=1.575e+31, orig_rms_sq=6.263e-03
2024-10-08 22:05:26,580 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=25.50 vs. limit=8.461500000000001
2024-10-08 22:05:27,216 WARNING [optim.py:503] Scaling gradients by 0.006239853333681822, model_norm_threshold=25784869715968.0
2024-10-08 22:05:27,374 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.612e+30, grad_sumsq=5.768e+32, orig_rms_sq=6.263e-03
2024-10-08 22:05:28,646 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.279e+10 2.895e+12 1.848e+13 2.182e+14 2.606e+16, threshold=3.697e+13, percent-clipped=46.0
2024-10-08 22:05:31,265 INFO [train.py:1152] Epoch 2, batch 850, loss[loss=0.7828, ctc_loss=1.091, attn_decoder_loss=0.7057, over 4804.00 frames. ], tot_loss[loss=0.7588, ctc_loss=1.041, attn_decoder_loss=0.6884, over 953989.62 frames. ], batch size: 29, lr: 3.69e-02,
2024-10-08 22:05:35,213 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=2567.3333333333335, ans=0.37965625000000003
2024-10-08 22:05:37,382 WARNING [optim.py:503] Scaling gradients by 0.035643357783555984, model_norm_threshold=36965290344448.0
2024-10-08 22:05:37,540 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.617e+29, grad_sumsq=4.206e+31, orig_rms_sq=6.222e-03
2024-10-08 22:05:42,613 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.min_abs, batch_count=2570.6666666666665, ans=0.23856
2024-10-08 22:05:44,001 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=2570.6666666666665, ans=0.08393333333333333
2024-10-08 22:05:47,487 WARNING [optim.py:503] Scaling gradients by 0.0857202336192131, model_norm_threshold=36965290344448.0
2024-10-08 22:05:47,643 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.676e+28, grad_sumsq=1.901e+30, orig_rms_sq=2.460e-02
2024-10-08 22:05:52,165 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.15 vs. limit=6.285333333333333
2024-10-08 22:05:57,502 WARNING [optim.py:503] Scaling gradients by 0.08932067453861237, model_norm_threshold=36965290344448.0
2024-10-08 22:05:57,658 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.207e+28, grad_sumsq=2.495e+30, orig_rms_sq=2.488e-02
2024-10-08 22:06:01,569 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=2574.0, ans=0.5
2024-10-08 22:06:11,180 WARNING [optim.py:503] Scaling gradients by 0.00042787654092535377, model_norm_threshold=36965290344448.0
2024-10-08 22:06:11,337 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.669e+33, grad_sumsq=3.836e+32, orig_rms_sq=4.350e+00
2024-10-08 22:06:13,559 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.37 vs. limit=9.433
2024-10-08 22:06:17,736 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2577.3333333333335, ans=0.3791875
2024-10-08 22:06:18,315 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=8.28 vs. limit=8.4665
2024-10-08 22:06:22,537 WARNING [optim.py:503] Scaling gradients by 0.005089343525469303, model_norm_threshold=36965290344448.0
2024-10-08 22:06:22,694 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.183e+31, grad_sumsq=1.812e+33, orig_rms_sq=6.527e-03
2024-10-08 22:06:33,846 INFO [train.py:1152] Epoch 2, batch 900, loss[loss=0.8018, ctc_loss=1.062, attn_decoder_loss=0.7369, over 4855.00 frames. ], tot_loss[loss=0.7595, ctc_loss=1.043, attn_decoder_loss=0.6887, over 956955.00 frames. ], batch size: 19, lr: 3.68e-02,
2024-10-08 22:06:34,041 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=2584.0, ans=0.5
2024-10-08 22:06:41,592 WARNING [optim.py:503] Scaling gradients by 0.05249941349029541, model_norm_threshold=36965290344448.0
2024-10-08 22:06:41,750 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.949e+29, grad_sumsq=7.842e+30, orig_rms_sq=2.485e-02
2024-10-08 22:06:48,829 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=10.98 vs. limit=8.47025
2024-10-08 22:06:50,591 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.max_abs, batch_count=2587.3333333333335, ans=6.617083333333333
2024-10-08 22:06:52,785 WARNING [optim.py:503] Scaling gradients by 0.021480174735188484, model_norm_threshold=36965290344448.0
2024-10-08 22:06:52,941 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.338e+29, grad_sumsq=1.679e+29, orig_rms_sq=4.369e+00
2024-10-08 22:06:55,320 WARNING [optim.py:503] Scaling gradients by 0.010808467864990234, model_norm_threshold=36965290344448.0
2024-10-08 22:06:55,477 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.984e+30, grad_sumsq=4.632e+29, orig_rms_sq=4.283e+00
2024-10-08 22:06:55,713 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=2587.3333333333335, ans=0.23881000000000002
2024-10-08 22:07:02,570 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.28 vs. limit=9.443
2024-10-08 22:07:03,170 WARNING [optim.py:503] Scaling gradients by 0.014290275052189827, model_norm_threshold=36965290344448.0
2024-10-08 22:07:03,326 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.987e+30, grad_sumsq=8.038e+31, orig_rms_sq=2.473e-02
2024-10-08 22:07:03,573 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=2590.6666666666665, ans=0.2740933333333333
2024-10-08 22:07:07,065 WARNING [optim.py:503] Scaling gradients by 0.04294666275382042, model_norm_threshold=36965290344448.0
2024-10-08 22:07:07,221 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.423e+29, grad_sumsq=5.632e+28, orig_rms_sq=2.527e+00
2024-10-08 22:07:08,380 WARNING [optim.py:503] Scaling gradients by 0.00024394072534050792, model_norm_threshold=36965290344448.0
2024-10-08 22:07:08,536 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.075e+33, grad_sumsq=7.986e+35, orig_rms_sq=6.355e-03
2024-10-08 22:07:09,312 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.62 vs. limit=8.4715
2024-10-08 22:07:11,062 WARNING [optim.py:503] Scaling gradients by 0.023618875071406364, model_norm_threshold=36965290344448.0
2024-10-08 22:07:11,219 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.354e+29, grad_sumsq=2.954e+31, orig_rms_sq=2.490e-02
2024-10-08 22:07:12,411 WARNING [optim.py:503] Scaling gradients by 0.03562403842806816, model_norm_threshold=36965290344448.0
2024-10-08 22:07:12,569 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.845e+29, grad_sumsq=1.143e+31, orig_rms_sq=2.490e-02
2024-10-08 22:07:13,771 WARNING [optim.py:503] Scaling gradients by 0.0344650037586689, model_norm_threshold=36965290344448.0
2024-10-08 22:07:13,927 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.423e+29, grad_sumsq=1.411e+29, orig_rms_sq=2.425e+00
2024-10-08 22:07:15,961 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=5.99 vs. limit=5.0376
2024-10-08 22:07:18,079 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.51 vs. limit=3.3891
2024-10-08 22:07:18,306 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=18.52 vs. limit=8.47275
2024-10-08 22:07:18,727 WARNING [optim.py:503] Scaling gradients by 0.02130878157913685, model_norm_threshold=36965290344448.0
2024-10-08 22:07:18,882 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.284e+29, grad_sumsq=3.274e+31, orig_rms_sq=2.530e-02
2024-10-08 22:07:20,838 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=30.54 vs. limit=8.47275
2024-10-08 22:07:25,130 WARNING [optim.py:503] Scaling gradients by 0.008226539008319378, model_norm_threshold=36965290344448.0
2024-10-08 22:07:25,296 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.59, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.199e+31, grad_sumsq=1.869e+33, orig_rms_sq=6.416e-03
2024-10-08 22:07:28,156 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.30 vs. limit=3.3896
2024-10-08 22:07:33,943 WARNING [optim.py:503] Scaling gradients by 0.01368358638137579, model_norm_threshold=36965290344448.0
2024-10-08 22:07:34,098 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.577e+30, grad_sumsq=4.113e+32, orig_rms_sq=6.265e-03
2024-10-08 22:07:35,583 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.727e+10 2.869e+12 1.542e+13 1.303e+14 1.515e+17, threshold=3.084e+13, percent-clipped=39.0
2024-10-08 22:07:36,440 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=12.37 vs. limit=5.649333333333334
2024-10-08 22:07:36,875 WARNING [optim.py:503] Scaling gradients by 0.005198577418923378, model_norm_threshold=30843135328256.0
2024-10-08 22:07:37,032 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.083e+31, grad_sumsq=1.757e+33, orig_rms_sq=6.164e-03
2024-10-08 22:07:38,305 INFO [train.py:1152] Epoch 2, batch 950, loss[loss=0.7177, ctc_loss=0.9449, attn_decoder_loss=0.6609, over 4817.00 frames. ], tot_loss[loss=0.7602, ctc_loss=1.044, attn_decoder_loss=0.6893, over 958786.93 frames. ], batch size: 19, lr: 3.67e-02,
2024-10-08 22:07:39,439 WARNING [optim.py:503] Scaling gradients by 0.04819956421852112, model_norm_threshold=30843135328256.0
2024-10-08 22:07:39,594 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.169e+29, grad_sumsq=2.816e+28, orig_rms_sq=4.153e+00
2024-10-08 22:07:44,050 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=7.82 vs. limit=5.040266666666667
2024-10-08 22:07:44,584 WARNING [optim.py:503] Scaling gradients by 0.015291859395802021, model_norm_threshold=30843135328256.0
2024-10-08 22:07:44,739 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.320e+30, grad_sumsq=3.258e+29, orig_rms_sq=4.053e+00
2024-10-08 22:07:49,781 WARNING [optim.py:503] Scaling gradients by 0.015459169633686543, model_norm_threshold=30843135328256.0
2024-10-08 22:07:49,938 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.868e+30, grad_sumsq=3.171e+32, orig_rms_sq=5.891e-03
2024-10-08 22:07:56,864 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=13.58 vs. limit=5.651
2024-10-08 22:08:01,057 WARNING [optim.py:503] Scaling gradients by 0.01913999579846859, model_norm_threshold=30843135328256.0
2024-10-08 22:08:01,211 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.703e+29, grad_sumsq=2.009e+29, orig_rms_sq=3.835e+00
2024-10-08 22:08:06,346 WARNING [optim.py:503] Scaling gradients by 0.04029788076877594, model_norm_threshold=30843135328256.0
2024-10-08 22:08:06,504 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.529e+29, grad_sumsq=2.736e+31, orig_rms_sq=5.589e-03
2024-10-08 22:08:10,184 WARNING [optim.py:503] Scaling gradients by 0.06892432272434235, model_norm_threshold=30843135328256.0
2024-10-08 22:08:10,341 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.777e+28, grad_sumsq=6.758e+30, orig_rms_sq=5.589e-03
2024-10-08 22:08:11,286 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=7.67 vs. limit=5.042933333333334
2024-10-08 22:08:18,954 WARNING [optim.py:503] Scaling gradients by 0.07525312155485153, model_norm_threshold=30843135328256.0
2024-10-08 22:08:19,113 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.325e+28, grad_sumsq=1.338e+30, orig_rms_sq=2.485e-02
2024-10-08 22:08:21,228 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.27 vs. limit=9.458
2024-10-08 22:08:22,106 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=15.86 vs. limit=9.458
2024-10-08 22:08:23,699 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.14 vs. limit=9.458
2024-10-08 22:08:24,261 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=2610.6666666666665, ans=0.23916
2024-10-08 22:08:25,230 WARNING [optim.py:503] Scaling gradients by 0.0010881915222853422, model_norm_threshold=30843135328256.0
2024-10-08 22:08:25,388 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.138e+32, grad_sumsq=5.774e+31, orig_rms_sq=3.703e+00
2024-10-08 22:08:26,128 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=12.10 vs. limit=5.652666666666667
2024-10-08 22:08:27,387 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.45 vs. limit=9.458
2024-10-08 22:08:27,801 WARNING [optim.py:503] Scaling gradients by 0.00011568935588002205, model_norm_threshold=30843135328256.0
2024-10-08 22:08:27,959 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.006e+34, grad_sumsq=8.011e+35, orig_rms_sq=2.503e-02
2024-10-08 22:08:33,059 WARNING [optim.py:503] Scaling gradients by 0.0011240021558478475, model_norm_threshold=30843135328256.0
2024-10-08 22:08:33,215 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.262e+32, grad_sumsq=5.002e+33, orig_rms_sq=2.523e-02
2024-10-08 22:08:34,813 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=2614.0, ans=0.04183125
2024-10-08 22:08:37,052 WARNING [optim.py:503] Scaling gradients by 0.002226852811872959, model_norm_threshold=30843135328256.0
2024-10-08 22:08:37,211 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.376e+31, grad_sumsq=2.934e+33, orig_rms_sq=2.514e-02
2024-10-08 22:08:42,279 INFO [train.py:1152] Epoch 2, batch 1000, loss[loss=0.7262, ctc_loss=0.9938, attn_decoder_loss=0.6592, over 4946.00 frames. ], tot_loss[loss=0.7621, ctc_loss=1.045, attn_decoder_loss=0.6913, over 960713.58 frames. ], batch size: 20, lr: 3.67e-02,
2024-10-08 22:08:49,218 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.01 vs. limit=9.463000000000001
2024-10-08 22:08:51,323 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2617.3333333333335, ans=0.27382666666666666
2024-10-08 22:08:51,713 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=44.84 vs. limit=8.4815
2024-10-08 22:08:54,427 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=14.38 vs. limit=8.48275
2024-10-08 22:08:54,827 WARNING [optim.py:503] Scaling gradients by 0.05828654766082764, model_norm_threshold=30843135328256.0
2024-10-08 22:08:54,984 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.301e+28, grad_sumsq=3.703e+30, orig_rms_sq=2.512e-02
2024-10-08 22:08:59,616 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=27.73 vs. limit=8.48275
2024-10-08 22:09:00,095 WARNING [optim.py:503] Scaling gradients by 0.012550138868391514, model_norm_threshold=30843135328256.0
2024-10-08 22:09:00,249 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.113e+30, grad_sumsq=2.604e+29, orig_rms_sq=4.275e+00
2024-10-08 22:09:05,144 WARNING [optim.py:503] Scaling gradients by 0.09428545832633972, model_norm_threshold=30843135328256.0
2024-10-08 22:09:05,300 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.693e+28, grad_sumsq=1.067e+30, orig_rms_sq=2.523e-02
2024-10-08 22:09:18,625 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=4.45 vs. limit=8.484
2024-10-08 22:09:24,421 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.10 vs. limit=6.313666666666666
2024-10-08 22:09:24,913 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=1.338e+04
2024-10-08 22:09:25,873 WARNING [optim.py:503] Scaling gradients by 0.0007142478134483099, model_norm_threshold=30843135328256.0
2024-10-08 22:09:26,029 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.463e+32, grad_sumsq=6.051e+31, orig_rms_sq=9.028e+00
2024-10-08 22:09:26,723 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=34.63 vs. limit=8.48525
2024-10-08 22:09:27,214 WARNING [optim.py:503] Scaling gradients by 0.002204691059887409, model_norm_threshold=30843135328256.0
2024-10-08 22:09:27,371 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.298e+31, grad_sumsq=2.148e+33, orig_rms_sq=2.467e-02
2024-10-08 22:09:33,592 WARNING [optim.py:503] Scaling gradients by 0.006449572741985321, model_norm_threshold=30843135328256.0
2024-10-08 22:09:33,748 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.663e+30, grad_sumsq=2.330e+32, orig_rms_sq=2.430e-02
2024-10-08 22:09:42,602 WARNING [optim.py:503] Scaling gradients by 0.09909068793058395, model_norm_threshold=30843135328256.0
2024-10-08 22:09:42,757 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.477e+28, grad_sumsq=5.515e+30, orig_rms_sq=6.305e-03
2024-10-08 22:09:45,208 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.995e+11 2.823e+12 1.356e+13 1.292e+14 2.666e+17, threshold=2.711e+13, percent-clipped=41.0
2024-10-08 22:09:47,867 WARNING [optim.py:503] Scaling gradients by 0.09492908418178558, model_norm_threshold=27110525632512.0
2024-10-08 22:09:48,024 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.348e+28, grad_sumsq=2.140e+30, orig_rms_sq=6.300e-03
2024-10-08 22:09:48,082 INFO [train.py:1152] Epoch 2, batch 1050, loss[loss=0.7742, ctc_loss=1.039, attn_decoder_loss=0.7079, over 4823.00 frames. ], tot_loss[loss=0.7606, ctc_loss=1.043, attn_decoder_loss=0.6899, over 962809.92 frames. ], batch size: 25, lr: 3.66e-02,
2024-10-08 22:09:53,140 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2634.0, ans=0.27366
2024-10-08 22:09:56,660 WARNING [optim.py:503] Scaling gradients by 0.029485763981938362, model_norm_threshold=27110525632512.0
2024-10-08 22:09:56,815 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.135e+29, grad_sumsq=6.725e+31, orig_rms_sq=6.149e-03
2024-10-08 22:09:59,460 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=2637.3333333333335, ans=0.376375
2024-10-08 22:10:00,455 WARNING [optim.py:503] Scaling gradients by 0.020236719399690628, model_norm_threshold=27110525632512.0
2024-10-08 22:10:00,616 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.411e+29, grad_sumsq=1.812e+31, orig_rms_sq=2.435e-02
2024-10-08 22:10:01,797 WARNING [optim.py:503] Scaling gradients by 0.06254880130290985, model_norm_threshold=27110525632512.0
2024-10-08 22:10:01,976 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.923e+28, grad_sumsq=1.299e+31, orig_rms_sq=6.101e-03
2024-10-08 22:10:07,117 WARNING [optim.py:503] Scaling gradients by 0.02843628078699112, model_norm_threshold=27110525632512.0
2024-10-08 22:10:07,276 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.339e+29, grad_sumsq=9.593e+30, orig_rms_sq=2.439e-02
2024-10-08 22:10:16,984 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.64 vs. limit=6.320333333333333
2024-10-08 22:10:19,319 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=36.21 vs. limit=9.4805
2024-10-08 22:10:25,259 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer_na.min_abs, batch_count=2644.0, ans=0.014576
2024-10-08 22:10:25,273 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=2644.0, ans=0.37606249999999997
2024-10-08 22:10:30,015 WARNING [optim.py:503] Scaling gradients by 0.07125920802354813, model_norm_threshold=27110525632512.0
2024-10-08 22:10:30,172 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.211e+28, grad_sumsq=5.186e+30, orig_rms_sq=6.192e-03
2024-10-08 22:10:30,797 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.27 vs. limit=8.4915
2024-10-08 22:10:30,848 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=33.49 vs. limit=9.483
2024-10-08 22:10:30,929 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=29.69 vs. limit=8.4915
2024-10-08 22:10:31,226 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=4.90 vs. limit=5.0576
2024-10-08 22:10:36,408 WARNING [optim.py:503] Scaling gradients by 0.0021929924841970205, model_norm_threshold=27110525632512.0
2024-10-08 22:10:36,561 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.53, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.169e+31, grad_sumsq=1.334e+34, orig_rms_sq=6.126e-03
2024-10-08 22:10:40,246 WARNING [optim.py:503] Scaling gradients by 0.02446562610566616, model_norm_threshold=27110525632512.0
2024-10-08 22:10:40,402 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.994e+29, grad_sumsq=3.254e+31, orig_rms_sq=6.126e-03
2024-10-08 22:10:41,707 WARNING [optim.py:503] Scaling gradients by 0.014670424163341522, model_norm_threshold=27110525632512.0
2024-10-08 22:10:41,863 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.889e+29, grad_sumsq=3.236e+31, orig_rms_sq=2.438e-02
2024-10-08 22:10:44,906 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.63 vs. limit=6.323666666666667
2024-10-08 22:10:46,644 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.39 vs. limit=9.4855
2024-10-08 22:10:47,488 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=24.49 vs. limit=8.492750000000001
2024-10-08 22:10:51,920 INFO [train.py:1152] Epoch 2, batch 1100, loss[loss=0.6973, ctc_loss=0.9552, attn_decoder_loss=0.6328, over 4862.00 frames. ], tot_loss[loss=0.759, ctc_loss=1.042, attn_decoder_loss=0.6883, over 964135.11 frames. ], batch size: 20, lr: 3.65e-02,
2024-10-08 22:10:54,258 WARNING [optim.py:503] Scaling gradients by 0.03322633355855942, model_norm_threshold=27110525632512.0
2024-10-08 22:10:54,417 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.396e+29, grad_sumsq=5.632e+30, orig_rms_sq=2.479e-02
2024-10-08 22:10:55,614 WARNING [optim.py:503] Scaling gradients by 0.03298749774694443, model_norm_threshold=27110525632512.0
2024-10-08 22:10:55,773 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.903e+29, grad_sumsq=7.673e+30, orig_rms_sq=2.479e-02
2024-10-08 22:10:59,529 WARNING [optim.py:503] Scaling gradients by 0.057006485760211945, model_norm_threshold=27110525632512.0
2024-10-08 22:10:59,684 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.306e+28, grad_sumsq=8.445e+30, orig_rms_sq=6.283e-03
2024-10-08 22:11:00,888 WARNING [optim.py:503] Scaling gradients by 0.0023096902295947075, model_norm_threshold=27110525632512.0
2024-10-08 22:11:01,044 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.150e+31, grad_sumsq=8.197e+33, orig_rms_sq=6.283e-03
2024-10-08 22:11:03,740 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=2654.0, ans=0.100475
2024-10-08 22:11:09,700 WARNING [optim.py:503] Scaling gradients by 0.013661523349583149, model_norm_threshold=27110525632512.0
2024-10-08 22:11:09,854 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.176e+30, grad_sumsq=4.581e+31, orig_rms_sq=2.568e-02
2024-10-08 22:11:10,033 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=2654.0, ans=0.37559375
2024-10-08 22:11:10,095 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2654.0, ans=0.27346
2024-10-08 22:11:11,007 WARNING [optim.py:503] Scaling gradients by 0.0070433602668344975, model_norm_threshold=27110525632512.0
2024-10-08 22:11:11,164 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.436e+30, grad_sumsq=7.098e+32, orig_rms_sq=6.249e-03
2024-10-08 22:11:11,737 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=42.31 vs. limit=8.49525
2024-10-08 22:11:11,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.43 vs. limit=9.4905
2024-10-08 22:11:12,393 WARNING [optim.py:503] Scaling gradients by 0.0019194124033674598, model_norm_threshold=27110525632512.0
2024-10-08 22:11:12,553 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.057e+31, grad_sumsq=2.341e+33, orig_rms_sq=2.588e-02
2024-10-08 22:11:28,775 WARNING [optim.py:503] Scaling gradients by 0.05041977018117905, model_norm_threshold=27110525632512.0
2024-10-08 22:11:28,929 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.749e+28, grad_sumsq=1.627e+29, orig_rms_sq=2.918e-01
2024-10-08 22:11:29,555 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.59 vs. limit=9.4955
2024-10-08 22:11:30,280 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=2660.6666666666665, ans=0.37528125
2024-10-08 22:11:38,815 WARNING [optim.py:503] Scaling gradients by 0.007270544767379761, model_norm_threshold=27110525632512.0
2024-10-08 22:11:38,971 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.382e+30, grad_sumsq=1.354e+32, orig_rms_sq=2.497e-02
2024-10-08 22:11:40,141 WARNING [optim.py:503] Scaling gradients by 0.058776404708623886, model_norm_threshold=27110525632512.0
2024-10-08 22:11:40,297 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.521e+28, grad_sumsq=1.810e+30, orig_rms_sq=2.497e-02
2024-10-08 22:11:44,339 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.min_positive, batch_count=2664.0, ans=0.22336
2024-10-08 22:11:45,321 WARNING [optim.py:503] Scaling gradients by 0.02950321137905121, model_norm_threshold=27110525632512.0
2024-10-08 22:11:45,474 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.005e+29, grad_sumsq=1.626e+31, orig_rms_sq=2.463e-02
2024-10-08 22:11:46,093 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.26 vs. limit=9.498
2024-10-08 22:11:51,566 WARNING [optim.py:503] Scaling gradients by 0.005731389857828617, model_norm_threshold=27110525632512.0
2024-10-08 22:11:51,723 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.672e+30, grad_sumsq=1.042e+33, orig_rms_sq=5.444e-03
2024-10-08 22:11:52,060 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_True/checkpoint-8000.pt
2024-10-08 22:11:54,664 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.098e+10 2.162e+12 1.035e+13 1.574e+14 1.412e+16, threshold=2.070e+13, percent-clipped=40.0
2024-10-08 22:11:54,665 WARNING [optim.py:503] Scaling gradients by 0.03441252186894417, model_norm_threshold=20701639606272.0
2024-10-08 22:11:54,824 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.457e+28, grad_sumsq=1.204e+31, orig_rms_sq=5.362e-03
2024-10-08 22:11:55,011 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=2664.0, ans=0.80676
2024-10-08 22:11:55,980 WARNING [optim.py:503] Scaling gradients by 0.018024789169430733, model_norm_threshold=20701639606272.0
2024-10-08 22:11:56,149 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.256e+29, grad_sumsq=7.939e+31, orig_rms_sq=5.362e-03
2024-10-08 22:11:57,513 INFO [train.py:1152] Epoch 2, batch 1150, loss[loss=0.7073, ctc_loss=0.9742, attn_decoder_loss=0.6406, over 4865.00 frames. ], tot_loss[loss=0.7601, ctc_loss=1.042, attn_decoder_loss=0.6896, over 964398.55 frames. ], batch size: 20, lr: 3.65e-02,
2024-10-08 22:11:59,993 WARNING [optim.py:503] Scaling gradients by 0.03984781354665756, model_norm_threshold=20701639606272.0
2024-10-08 22:12:00,150 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.547e+28, grad_sumsq=2.342e+30, orig_rms_sq=2.369e-02
2024-10-08 22:12:03,886 WARNING [optim.py:503] Scaling gradients by 0.036998506635427475, model_norm_threshold=20701639606272.0
2024-10-08 22:12:04,042 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.326e+28, grad_sumsq=1.016e+31, orig_rms_sq=5.243e-03
2024-10-08 22:12:05,250 WARNING [optim.py:503] Scaling gradients by 0.02530834823846817, model_norm_threshold=20701639606272.0
2024-10-08 22:12:05,409 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.525e+29, grad_sumsq=2.940e+31, orig_rms_sq=5.188e-03
2024-10-08 22:12:06,127 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.07 vs. limit=8.50025
2024-10-08 22:12:11,793 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=2670.6666666666665, ans=0.3748125
2024-10-08 22:12:12,737 WARNING [optim.py:503] Scaling gradients by 0.09381076693534851, model_norm_threshold=20701639606272.0
2024-10-08 22:12:12,895 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.421e+28, grad_sumsq=2.756e+30, orig_rms_sq=5.158e-03
2024-10-08 22:12:16,601 WARNING [optim.py:503] Scaling gradients by 0.0017383741214871407, model_norm_threshold=20701639606272.0
2024-10-08 22:12:16,759 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.554e+31, grad_sumsq=6.882e+33, orig_rms_sq=5.164e-03
2024-10-08 22:12:20,711 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=2670.6666666666665, ans=0.3748125
2024-10-08 22:12:22,839 WARNING [optim.py:503] Scaling gradients by 0.060812320560216904, model_norm_threshold=20701639606272.0
2024-10-08 22:12:22,998 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.869e+28, grad_sumsq=9.394e+30, orig_rms_sq=5.183e-03
2024-10-08 22:12:26,638 WARNING [optim.py:503] Scaling gradients by 0.024095861241221428, model_norm_threshold=20701639606272.0
2024-10-08 22:12:26,793 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.325e+29, grad_sumsq=2.555e+31, orig_rms_sq=5.186e-03
2024-10-08 22:12:38,952 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=5.57 vs. limit=5.0709333333333335
2024-10-08 22:12:42,988 WARNING [optim.py:503] Scaling gradients by 0.07964817434549332, model_norm_threshold=20701639606272.0
2024-10-08 22:12:43,146 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.519e+28, grad_sumsq=6.518e+29, orig_rms_sq=2.330e-02
2024-10-08 22:12:43,315 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2677.3333333333335, ans=0.3745
2024-10-08 22:12:44,367 WARNING [optim.py:503] Scaling gradients by 0.011340022087097168, model_norm_threshold=20701639606272.0
2024-10-08 22:12:44,523 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.345e+29, grad_sumsq=2.723e+31, orig_rms_sq=2.330e-02
2024-10-08 22:12:52,177 WARNING [optim.py:503] Scaling gradients by 0.0901964008808136, model_norm_threshold=20701639606272.0
2024-10-08 22:12:52,337 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.634e+28, grad_sumsq=2.971e+30, orig_rms_sq=5.498e-03
2024-10-08 22:12:55,836 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.82 vs. limit=5.670166666666667
2024-10-08 22:12:56,817 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=19.32 vs. limit=8.50525
2024-10-08 22:12:57,827 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=26.89 vs. limit=8.50525
2024-10-08 22:12:58,496 WARNING [optim.py:503] Scaling gradients by 0.0002716828021220863, model_norm_threshold=20701639606272.0
2024-10-08 22:12:58,652 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.496e+33, grad_sumsq=6.277e+34, orig_rms_sq=2.383e-02
2024-10-08 22:13:00,617 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.31 vs. limit=6.3420000000000005
2024-10-08 22:13:01,328 INFO [train.py:1152] Epoch 2, batch 1200, loss[loss=0.7652, ctc_loss=1.078, attn_decoder_loss=0.6869, over 4808.00 frames. ], tot_loss[loss=0.7602, ctc_loss=1.044, attn_decoder_loss=0.6894, over 964262.33 frames. ], batch size: 25, lr: 3.64e-02,
2024-10-08 22:13:02,213 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.80 vs. limit=5.671
2024-10-08 22:13:05,269 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=2684.0, ans=0.27316
2024-10-08 22:13:06,245 WARNING [optim.py:503] Scaling gradients by 0.016384748741984367, model_norm_threshold=20701639606272.0
2024-10-08 22:13:06,401 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.854e+29, grad_sumsq=1.199e+32, orig_rms_sq=5.716e-03
2024-10-08 22:13:09,634 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten.whitening_limit, batch_count=2684.0, ans=8.506499999999999
2024-10-08 22:13:09,730 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten.whitening_limit, batch_count=2684.0, ans=8.506499999999999
2024-10-08 22:13:09,874 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.13 vs. limit=8.506499999999999
2024-10-08 22:13:11,507 WARNING [optim.py:503] Scaling gradients by 0.05686097592115402, model_norm_threshold=20701639606272.0
2024-10-08 22:13:11,663 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.273e+28, grad_sumsq=3.957e+30, orig_rms_sq=5.744e-03
2024-10-08 22:13:15,321 WARNING [optim.py:503] Scaling gradients by 0.01592794805765152, model_norm_threshold=20701639606272.0
2024-10-08 22:13:15,479 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.797e+29, grad_sumsq=1.954e+31, orig_rms_sq=2.456e-02
2024-10-08 22:13:17,961 WARNING [optim.py:503] Scaling gradients by 0.05875029414892197, model_norm_threshold=20701639606272.0
2024-10-08 22:13:18,116 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.326e+28, grad_sumsq=1.347e+30, orig_rms_sq=2.470e-02
2024-10-08 22:13:24,661 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=2687.3333333333335, ans=0.1640833333333333
2024-10-08 22:13:25,755 WARNING [optim.py:503] Scaling gradients by 7.92147548054345e-05, model_norm_threshold=20701639606272.0
2024-10-08 22:13:25,907 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.778e+34, grad_sumsq=1.968e+33, orig_rms_sq=9.033e+00
2024-10-08 22:13:27,141 WARNING [optim.py:503] Scaling gradients by 0.09090310335159302, model_norm_threshold=20701639606272.0
2024-10-08 22:13:27,296 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.208e+28, grad_sumsq=2.069e+30, orig_rms_sq=5.839e-03
2024-10-08 22:13:30,393 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=34.97 vs. limit=8.509
2024-10-08 22:13:32,233 WARNING [optim.py:503] Scaling gradients by 0.014509189873933792, model_norm_threshold=20701639606272.0
2024-10-08 22:13:32,391 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.692e+29, grad_sumsq=1.934e+31, orig_rms_sq=2.426e-02
2024-10-08 22:13:34,246 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.03 vs. limit=8.509
2024-10-08 22:13:35,505 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=32.29 vs. limit=9.518
2024-10-08 22:13:36,148 WARNING [optim.py:503] Scaling gradients by 0.006320727989077568, model_norm_threshold=20701639606272.0
2024-10-08 22:13:36,315 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.14, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.537e+30, grad_sumsq=6.336e+31, orig_rms_sq=2.426e-02
2024-10-08 22:13:39,391 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.82 vs. limit=6.3469999999999995
2024-10-08 22:13:40,027 WARNING [optim.py:503] Scaling gradients by 0.05531809851527214, model_norm_threshold=20701639606272.0
2024-10-08 22:13:40,182 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.799e+28, grad_sumsq=9.785e+30, orig_rms_sq=5.926e-03
2024-10-08 22:13:41,369 WARNING [optim.py:503] Scaling gradients by 0.04671859368681908, model_norm_threshold=20701639606272.0
2024-10-08 22:13:41,526 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.518e+28, grad_sumsq=1.606e+31, orig_rms_sq=5.926e-03
2024-10-08 22:13:44,031 WARNING [optim.py:503] Scaling gradients by 0.02885843627154827, model_norm_threshold=20701639606272.0
2024-10-08 22:13:44,189 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.472e+29, grad_sumsq=3.511e+28, orig_rms_sq=4.193e+00
2024-10-08 22:13:49,342 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=2694.0, ans=0.098975
2024-10-08 22:13:52,821 WARNING [optim.py:503] Scaling gradients by 0.017955530434846878, model_norm_threshold=20701639606272.0
2024-10-08 22:13:52,979 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.317e+29, grad_sumsq=1.067e+30, orig_rms_sq=3.110e-01
2024-10-08 22:13:57,450 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=10.89 vs. limit=5.674333333333333
2024-10-08 22:14:02,514 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=28.17 vs. limit=9.523
2024-10-08 22:14:03,027 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.413e+10 7.734e+11 5.934e+12 2.207e+14 2.613e+17, threshold=1.187e+13, percent-clipped=32.0
2024-10-08 22:14:03,028 WARNING [optim.py:503] Scaling gradients by 0.04121822863817215, model_norm_threshold=11868700672000.0
2024-10-08 22:14:03,187 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.252e+28, grad_sumsq=8.969e+29, orig_rms_sq=2.511e-02
2024-10-08 22:14:04,579 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=2700.6666666666665, ans=0.07
2024-10-08 22:14:05,177 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.14 vs. limit=6.350333333333333
2024-10-08 22:14:05,573 WARNING [optim.py:503] Scaling gradients by 0.0398838147521019, model_norm_threshold=11868700672000.0
2024-10-08 22:14:05,732 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.636e+28, grad_sumsq=4.138e+30, orig_rms_sq=6.370e-03
2024-10-08 22:14:05,790 INFO [train.py:1152] Epoch 2, batch 1250, loss[loss=0.7689, ctc_loss=1.057, attn_decoder_loss=0.6969, over 4745.00 frames. ], tot_loss[loss=0.7603, ctc_loss=1.045, attn_decoder_loss=0.6892, over 964185.52 frames. ], batch size: 32, lr: 3.64e-02,
2024-10-08 22:14:10,838 WARNING [optim.py:503] Scaling gradients by 0.07759008556604385, model_norm_threshold=11868700672000.0
2024-10-08 22:14:10,996 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.764e+27, grad_sumsq=1.890e+29, orig_rms_sq=2.521e-02
2024-10-08 22:14:13,244 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=11.11 vs. limit=5.675166666666667
2024-10-08 22:14:13,412 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=10.69 vs. limit=8.51275
2024-10-08 22:14:14,371 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=22.09 vs. limit=9.525500000000001
2024-10-08 22:14:15,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=2700.6666666666665, ans=0.8054766666666667
2024-10-08 22:14:16,035 WARNING [optim.py:503] Scaling gradients by 0.002336207078769803, model_norm_threshold=11868700672000.0
2024-10-08 22:14:16,192 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.172e+30, grad_sumsq=2.052e+32, orig_rms_sq=2.520e-02
2024-10-08 22:14:18,781 WARNING [optim.py:503] Scaling gradients by 0.013913150876760483, model_norm_threshold=11868700672000.0
2024-10-08 22:14:18,937 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.843e+29, grad_sumsq=2.111e+28, orig_rms_sq=8.733e+00
2024-10-08 22:14:22,509 WARNING [optim.py:503] Scaling gradients by 0.004489739891141653, model_norm_threshold=11868700672000.0
2024-10-08 22:14:22,664 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.799e+30, grad_sumsq=7.154e+31, orig_rms_sq=2.514e-02
2024-10-08 22:14:31,435 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=2707.3333333333335, ans=0.09771249999999998
2024-10-08 22:14:38,719 WARNING [optim.py:503] Scaling gradients by 0.009684399701654911, model_norm_threshold=11868700672000.0
2024-10-08 22:14:38,876 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.843e+29, grad_sumsq=1.120e+31, orig_rms_sq=2.538e-02
2024-10-08 22:14:40,684 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=14.06 vs. limit=6.353666666666667
2024-10-08 22:14:47,724 WARNING [optim.py:503] Scaling gradients by 0.010256800800561905, model_norm_threshold=11868700672000.0
2024-10-08 22:14:47,879 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.468e+29, grad_sumsq=3.884e+31, orig_rms_sq=6.355e-03
2024-10-08 22:14:49,096 WARNING [optim.py:503] Scaling gradients by 0.040820904076099396, model_norm_threshold=11868700672000.0
2024-10-08 22:14:49,254 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.586e+28, grad_sumsq=1.402e+30, orig_rms_sq=2.558e-02
2024-10-08 22:14:51,617 WARNING [optim.py:503] Scaling gradients by 0.001736526028253138, model_norm_threshold=11868700672000.0
2024-10-08 22:14:51,773 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.159e+31, grad_sumsq=4.532e+32, orig_rms_sq=2.558e-02
2024-10-08 22:14:53,097 WARNING [optim.py:503] Scaling gradients by 0.0033354260958731174, model_norm_threshold=11868700672000.0
2024-10-08 22:14:53,252 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.847e+30, grad_sumsq=1.504e+32, orig_rms_sq=2.558e-02
2024-10-08 22:14:54,446 WARNING [optim.py:503] Scaling gradients by 0.0013432229170575738, model_norm_threshold=11868700672000.0
2024-10-08 22:14:54,604 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.842e+31, grad_sumsq=7.184e+32, orig_rms_sq=2.564e-02
2024-10-08 22:14:55,761 WARNING [optim.py:503] Scaling gradients by 0.04259722679853439, model_norm_threshold=11868700672000.0
2024-10-08 22:14:55,919 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.606e+28, grad_sumsq=6.265e+29, orig_rms_sq=2.564e-02
2024-10-08 22:14:57,147 WARNING [optim.py:503] Scaling gradients by 0.00013983636745251715, model_norm_threshold=11868700672000.0
2024-10-08 22:14:57,301 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.758e+33, grad_sumsq=4.371e+35, orig_rms_sq=6.309e-03
2024-10-08 22:14:59,308 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.38 vs. limit=8.51775
2024-10-08 22:15:00,081 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.min_positive, batch_count=2714.0, ans=0.0830375
2024-10-08 22:15:04,677 WARNING [optim.py:503] Scaling gradients by 0.023588940501213074, model_norm_threshold=11868700672000.0
2024-10-08 22:15:04,835 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.021e+28, grad_sumsq=8.086e+30, orig_rms_sq=6.209e-03
2024-10-08 22:15:07,184 WARNING [optim.py:503] Scaling gradients by 0.0003725782153196633, model_norm_threshold=11868700672000.0
2024-10-08 22:15:07,339 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.736e+32, grad_sumsq=1.463e+34, orig_rms_sq=2.554e-02
2024-10-08 22:15:09,781 WARNING [optim.py:503] Scaling gradients by 0.05418173596262932, model_norm_threshold=11868700672000.0
2024-10-08 22:15:09,939 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.026e+28, grad_sumsq=1.680e+30, orig_rms_sq=6.104e-03
2024-10-08 22:15:09,997 INFO [train.py:1152] Epoch 2, batch 1300, loss[loss=0.7902, ctc_loss=1.061, attn_decoder_loss=0.7225, over 4826.00 frames. ], tot_loss[loss=0.7609, ctc_loss=1.044, attn_decoder_loss=0.69, over 965636.55 frames. ], batch size: 43, lr: 3.63e-02,
2024-10-08 22:15:10,621 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=27.76 vs. limit=9.538
2024-10-08 22:15:14,767 WARNING [optim.py:503] Scaling gradients by 0.03172669559717178, model_norm_threshold=11868700672000.0
2024-10-08 22:15:14,923 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.14, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.925e+28, grad_sumsq=3.202e+30, orig_rms_sq=6.012e-03
2024-10-08 22:15:14,989 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=2717.3333333333335, ans=0.372625
2024-10-08 22:15:15,174 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=2717.3333333333335, ans=0.041508333333333335
2024-10-08 22:15:17,601 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=2717.3333333333335, ans=0.03886
2024-10-08 22:15:18,648 WARNING [optim.py:503] Scaling gradients by 0.005396030377596617, model_norm_threshold=11868700672000.0
2024-10-08 22:15:18,803 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.348e+30, grad_sumsq=5.291e+31, orig_rms_sq=2.548e-02
2024-10-08 22:15:20,015 WARNING [optim.py:503] Scaling gradients by 0.07224846631288528, model_norm_threshold=11868700672000.0
2024-10-08 22:15:20,173 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.588e+27, grad_sumsq=1.797e+29, orig_rms_sq=2.554e-02
2024-10-08 22:15:20,901 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.90 vs. limit=8.519
2024-10-08 22:15:22,045 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=35.93 vs. limit=8.52025
2024-10-08 22:15:22,336 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=18.95 vs. limit=8.52025
2024-10-08 22:15:22,686 WARNING [optim.py:503] Scaling gradients by 0.004667989443987608, model_norm_threshold=11868700672000.0
2024-10-08 22:15:22,842 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.404e+30, grad_sumsq=5.499e+31, orig_rms_sq=2.554e-02
2024-10-08 22:15:23,648 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.81 vs. limit=6.360333333333333
2024-10-08 22:15:28,395 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=43.65 vs. limit=8.52025
2024-10-08 22:15:33,936 WARNING [optim.py:503] Scaling gradients by 0.07826389372348785, model_norm_threshold=11868700672000.0
2024-10-08 22:15:34,091 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.536e+27, grad_sumsq=7.723e+29, orig_rms_sq=5.873e-03
2024-10-08 22:15:35,654 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=2.245e+01
2024-10-08 22:15:36,652 WARNING [optim.py:503] Scaling gradients by 0.006843201816082001, model_norm_threshold=11868700672000.0
2024-10-08 22:15:36,808 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.783e+29, grad_sumsq=2.220e+31, orig_rms_sq=2.605e-02
2024-10-08 22:15:36,991 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=2724.0, ans=0.15949999999999998
2024-10-08 22:15:39,647 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=2724.0, ans=0.08297500000000001
2024-10-08 22:15:50,765 WARNING [optim.py:503] Scaling gradients by 8.223023905884475e-05, model_norm_threshold=11868700672000.0
2024-10-08 22:15:50,922 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.121e+34, grad_sumsq=4.213e+35, orig_rms_sq=2.661e-02
2024-10-08 22:15:53,527 WARNING [optim.py:503] Scaling gradients by 0.050483278930187225, model_norm_threshold=11868700672000.0
2024-10-08 22:15:53,684 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.366e+27, grad_sumsq=1.374e+30, orig_rms_sq=6.090e-03
2024-10-08 22:16:04,420 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=23.12 vs. limit=8.524000000000001
2024-10-08 22:16:06,937 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=18.57 vs. limit=8.524000000000001
2024-10-08 22:16:08,229 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.98 vs. limit=8.524000000000001
2024-10-08 22:16:09,869 WARNING [optim.py:503] Scaling gradients by 0.053229767829179764, model_norm_threshold=11868700672000.0
2024-10-08 22:16:10,027 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.321e+28, grad_sumsq=5.173e+29, orig_rms_sq=2.554e-02
2024-10-08 22:16:10,962 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=41.21 vs. limit=9.548
2024-10-08 22:16:11,494 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.980e+10 2.321e+12 2.006e+13 1.530e+14 1.443e+17, threshold=4.012e+13, percent-clipped=56.0
2024-10-08 22:16:11,494 WARNING [optim.py:503] Scaling gradients by 0.06656654924154282, model_norm_threshold=40124041658368.0
2024-10-08 22:16:11,650 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.465e+28, grad_sumsq=1.326e+31, orig_rms_sq=5.629e-03
2024-10-08 22:16:15,804 INFO [train.py:1152] Epoch 2, batch 1350, loss[loss=0.787, ctc_loss=1.063, attn_decoder_loss=0.7178, over 4833.00 frames. ], tot_loss[loss=0.7608, ctc_loss=1.043, attn_decoder_loss=0.6902, over 966306.21 frames. ], batch size: 21, lr: 3.62e-02,
2024-10-08 22:16:20,998 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=2734.0, ans=0.09747499999999999
2024-10-08 22:16:22,253 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=2734.0, ans=0.035
2024-10-08 22:16:26,984 WARNING [optim.py:503] Scaling gradients by 0.0013104735407978296, model_norm_threshold=40124041658368.0
2024-10-08 22:16:27,142 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.501e+32, grad_sumsq=1.011e+34, orig_rms_sq=2.474e-02
2024-10-08 22:16:27,354 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2737.3333333333335, ans=0.3716875
2024-10-08 22:16:29,571 WARNING [optim.py:503] Scaling gradients by 0.0004033320874441415, model_norm_threshold=40124041658368.0
2024-10-08 22:16:29,727 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.679e+33, grad_sumsq=6.788e+34, orig_rms_sq=2.474e-02
2024-10-08 22:16:30,993 WARNING [optim.py:503] Scaling gradients by 0.01450092438608408, model_norm_threshold=40124041658368.0
2024-10-08 22:16:31,151 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.156e+30, grad_sumsq=3.655e+29, orig_rms_sq=3.163e+00
2024-10-08 22:16:33,748 WARNING [optim.py:503] Scaling gradients by 0.004804609809070826, model_norm_threshold=40124041658368.0
2024-10-08 22:16:33,904 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.789e+31, grad_sumsq=7.228e+32, orig_rms_sq=2.474e-02
2024-10-08 22:16:34,658 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=24.73 vs. limit=8.5265
2024-10-08 22:16:37,347 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.08 vs. limit=8.5265
2024-10-08 22:16:37,702 WARNING [optim.py:503] Scaling gradients by 0.0010110841831192374, model_norm_threshold=40124041658368.0
2024-10-08 22:16:37,857 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.813e+32, grad_sumsq=8.227e+31, orig_rms_sq=3.419e+00
2024-10-08 22:16:41,608 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=2740.6666666666665, ans=0.038335
2024-10-08 22:16:46,475 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=2740.6666666666665, ans=0.035
2024-10-08 22:16:50,008 WARNING [optim.py:503] Scaling gradients by 0.0030413195490837097, model_norm_threshold=40124041658368.0
2024-10-08 22:16:50,164 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.810e+31, grad_sumsq=1.503e+33, orig_rms_sq=2.534e-02
2024-10-08 22:17:02,720 WARNING [optim.py:503] Scaling gradients by 0.0022460094187408686, model_norm_threshold=40124041658368.0
2024-10-08 22:17:02,875 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.429e+32, grad_sumsq=5.507e+33, orig_rms_sq=2.594e-02
2024-10-08 22:17:06,554 WARNING [optim.py:503] Scaling gradients by 0.0739406943321228, model_norm_threshold=40124041658368.0
2024-10-08 22:17:06,713 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.355e+28, grad_sumsq=2.823e+30, orig_rms_sq=2.605e-02
2024-10-08 22:17:07,902 WARNING [optim.py:503] Scaling gradients by 0.0036279980558902025, model_norm_threshold=40124041658368.0
2024-10-08 22:17:08,055 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.291e+31, grad_sumsq=1.647e+33, orig_rms_sq=2.605e-02
2024-10-08 22:17:08,571 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.77 vs. limit=8.53025
2024-10-08 22:17:10,590 WARNING [optim.py:503] Scaling gradients by 0.002270667115226388, model_norm_threshold=40124041658368.0
2024-10-08 22:17:10,748 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.517e+31, grad_sumsq=1.208e+34, orig_rms_sq=6.224e-03
2024-10-08 22:17:10,999 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=2747.3333333333335, ans=0.37121875
2024-10-08 22:17:11,916 WARNING [optim.py:503] Scaling gradients by 0.09734790027141571, model_norm_threshold=40124041658368.0
2024-10-08 22:17:12,073 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.354e+28, grad_sumsq=3.766e+27, orig_rms_sq=8.907e+00
2024-10-08 22:17:13,984 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.94 vs. limit=5.686833333333333
2024-10-08 22:17:14,513 WARNING [optim.py:503] Scaling gradients by 0.00451112724840641, model_norm_threshold=40124041658368.0
2024-10-08 22:17:14,669 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.290e+31, grad_sumsq=3.778e+33, orig_rms_sq=6.063e-03
2024-10-08 22:17:15,880 WARNING [optim.py:503] Scaling gradients by 0.03195061534643173, model_norm_threshold=40124041658368.0
2024-10-08 22:17:16,036 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.084e+29, grad_sumsq=1.333e+32, orig_rms_sq=6.063e-03
2024-10-08 22:17:19,870 INFO [train.py:1152] Epoch 2, batch 1400, loss[loss=0.7234, ctc_loss=1.005, attn_decoder_loss=0.6531, over 4940.00 frames. ], tot_loss[loss=0.76, ctc_loss=1.042, attn_decoder_loss=0.6896, over 966654.78 frames. ], batch size: 19, lr: 3.62e-02,
2024-10-08 22:17:22,026 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=22.64 vs. limit=8.5315
2024-10-08 22:17:22,703 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=2750.6666666666665, ans=0.37106249999999996
2024-10-08 22:17:23,623 WARNING [optim.py:503] Scaling gradients by 0.029662059620022774, model_norm_threshold=40124041658368.0
2024-10-08 22:17:23,780 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.303e+29, grad_sumsq=7.353e+31, orig_rms_sq=5.852e-03
2024-10-08 22:17:26,228 WARNING [optim.py:503] Scaling gradients by 0.016319245100021362, model_norm_threshold=40124041658368.0
2024-10-08 22:17:26,384 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.136e+30, grad_sumsq=2.814e+29, orig_rms_sq=4.038e+00
2024-10-08 22:17:29,402 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.15 vs. limit=6.375333333333334
2024-10-08 22:17:34,332 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.34 vs. limit=8.53275
2024-10-08 22:17:34,608 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.96 vs. limit=8.53275
2024-10-08 22:17:36,117 WARNING [optim.py:503] Scaling gradients by 0.013497925363481045, model_norm_threshold=40124041658368.0
2024-10-08 22:17:36,274 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.579e+30, grad_sumsq=6.398e+31, orig_rms_sq=2.468e-02
2024-10-08 22:17:37,333 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=7.44 vs. limit=5.6885
2024-10-08 22:17:45,202 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=2757.3333333333335, ans=0.09659999999999999
2024-10-08 22:17:49,524 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=12.81 vs. limit=5.689333333333334
2024-10-08 22:17:49,690 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.53 vs. limit=5.689333333333334
2024-10-08 22:17:50,209 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=2757.3333333333335, ans=0.08276666666666667
2024-10-08 22:17:53,180 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.31 vs. limit=5.689333333333334
2024-10-08 22:17:55,786 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.13 vs. limit=5.689333333333334
2024-10-08 22:17:58,383 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=18.33 vs. limit=8.53525
2024-10-08 22:17:59,014 WARNING [optim.py:503] Scaling gradients by 0.05745045468211174, model_norm_threshold=40124041658368.0
2024-10-08 22:17:59,169 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.294e+29, grad_sumsq=5.325e+30, orig_rms_sq=2.430e-02
2024-10-08 22:18:00,120 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.65 vs. limit=6.380333333333333
2024-10-08 22:18:00,958 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=10.43 vs. limit=9.5705
2024-10-08 22:18:05,214 WARNING [optim.py:503] Scaling gradients by 0.012177571654319763, model_norm_threshold=40124041658368.0
2024-10-08 22:18:05,371 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.168e+30, grad_sumsq=1.316e+32, orig_rms_sq=2.407e-02
2024-10-08 22:18:10,296 WARNING [optim.py:503] Scaling gradients by 0.029161762446165085, model_norm_threshold=40124041658368.0
2024-10-08 22:18:10,452 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.985e+29, grad_sumsq=3.339e+31, orig_rms_sq=2.391e-02
2024-10-08 22:18:13,845 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=7.47 vs. limit=5.1056
2024-10-08 22:18:20,663 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.284e+10 1.455e+12 9.415e+12 2.331e+14 9.948e+16, threshold=1.883e+13, percent-clipped=40.0
2024-10-08 22:18:23,239 INFO [train.py:1152] Epoch 2, batch 1450, loss[loss=0.7672, ctc_loss=1.035, attn_decoder_loss=0.7003, over 4795.00 frames. ], tot_loss[loss=0.7598, ctc_loss=1.041, attn_decoder_loss=0.6894, over 966538.71 frames. ], batch size: 34, lr: 3.61e-02,
2024-10-08 22:18:25,012 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.46 vs. limit=6.383666666666667
2024-10-08 22:18:30,628 WARNING [optim.py:503] Scaling gradients by 0.01014027837663889, model_norm_threshold=18830537523200.0
2024-10-08 22:18:30,784 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.170e+30, grad_sumsq=4.997e+31, orig_rms_sq=2.341e-02
2024-10-08 22:18:31,975 WARNING [optim.py:503] Scaling gradients by 0.007504533044993877, model_norm_threshold=18830537523200.0
2024-10-08 22:18:32,131 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.987e+30, grad_sumsq=8.488e+31, orig_rms_sq=2.341e-02
2024-10-08 22:18:34,098 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=26.46 vs. limit=9.5755
2024-10-08 22:18:37,146 WARNING [optim.py:503] Scaling gradients by 0.023288292810320854, model_norm_threshold=18830537523200.0
2024-10-08 22:18:37,302 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.975e+29, grad_sumsq=8.461e+30, orig_rms_sq=2.335e-02
2024-10-08 22:18:37,558 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=2770.6666666666665, ans=0.37012500000000004
2024-10-08 22:18:40,253 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.44 vs. limit=9.578
2024-10-08 22:18:42,286 WARNING [optim.py:503] Scaling gradients by 0.0006327775772660971, model_norm_threshold=18830537523200.0
2024-10-08 22:18:42,443 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.909e+32, grad_sumsq=8.179e+33, orig_rms_sq=2.334e-02
2024-10-08 22:18:45,677 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.74 vs. limit=8.539
2024-10-08 22:18:51,179 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=2774.0, ans=0.36996875
2024-10-08 22:18:52,276 WARNING [optim.py:503] Scaling gradients by 0.035397786647081375, model_norm_threshold=18830537523200.0
2024-10-08 22:18:52,430 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.559e+28, grad_sumsq=8.405e+27, orig_rms_sq=8.994e+00
2024-10-08 22:18:54,855 WARNING [optim.py:503] Scaling gradients by 0.03508831560611725, model_norm_threshold=18830537523200.0
2024-10-08 22:18:55,011 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.715e+28, grad_sumsq=1.647e+31, orig_rms_sq=5.899e-03
2024-10-08 22:18:55,783 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=32.41 vs. limit=9.5805
2024-10-08 22:18:56,265 WARNING [optim.py:503] Scaling gradients by 0.022194374352693558, model_norm_threshold=18830537523200.0
2024-10-08 22:18:56,421 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.854e+29, grad_sumsq=7.938e+30, orig_rms_sq=2.336e-02
2024-10-08 22:19:00,046 WARNING [optim.py:503] Scaling gradients by 0.03609935939311981, model_norm_threshold=18830537523200.0
2024-10-08 22:19:00,202 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.920e+28, grad_sumsq=5.438e+27, orig_rms_sq=9.047e+00
2024-10-08 22:19:05,170 WARNING [optim.py:503] Scaling gradients by 0.08091383427381516, model_norm_threshold=18830537523200.0
2024-10-08 22:19:05,329 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.108e+28, grad_sumsq=1.895e+30, orig_rms_sq=5.847e-03
2024-10-08 22:19:05,570 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=2777.3333333333335, ans=0.8027933333333334
2024-10-08 22:19:15,181 WARNING [optim.py:503] Scaling gradients by 0.003581235185265541, model_norm_threshold=18830537523200.0
2024-10-08 22:19:15,337 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.676e+30, grad_sumsq=2.831e+32, orig_rms_sq=2.358e-02
2024-10-08 22:19:21,709 WARNING [optim.py:503] Scaling gradients by 0.00962656456977129, model_norm_threshold=18830537523200.0
2024-10-08 22:19:21,872 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.091e+29, grad_sumsq=3.817e+31, orig_rms_sq=2.382e-02
2024-10-08 22:19:22,026 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2780.6666666666665, ans=0.27219333333333334
2024-10-08 22:19:23,066 WARNING [optim.py:503] Scaling gradients by 0.0003000653232447803, model_norm_threshold=18830537523200.0
2024-10-08 22:19:23,223 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.083e+33, grad_sumsq=4.547e+34, orig_rms_sq=2.382e-02
2024-10-08 22:19:27,084 INFO [train.py:1152] Epoch 2, batch 1500, loss[loss=0.709, ctc_loss=0.9752, attn_decoder_loss=0.6424, over 4751.00 frames. ], tot_loss[loss=0.761, ctc_loss=1.044, attn_decoder_loss=0.6903, over 966209.40 frames. ], batch size: 26, lr: 3.61e-02,
2024-10-08 22:19:28,167 WARNING [optim.py:503] Scaling gradients by 0.03499331325292587, model_norm_threshold=18830537523200.0
2024-10-08 22:19:28,322 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.343e+28, grad_sumsq=2.218e+30, orig_rms_sq=2.409e-02
2024-10-08 22:19:29,537 WARNING [optim.py:503] Scaling gradients by 0.01059715822339058, model_norm_threshold=18830537523200.0
2024-10-08 22:19:29,692 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.56, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.755e+30, grad_sumsq=7.287e+31, orig_rms_sq=2.409e-02
2024-10-08 22:19:32,531 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=2784.0, ans=0.3695
2024-10-08 22:19:32,803 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=7.91 vs. limit=5.1136
2024-10-08 22:19:33,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.66 vs. limit=6.3919999999999995
2024-10-08 22:19:33,620 WARNING [optim.py:503] Scaling gradients by 0.08575765788555145, model_norm_threshold=18830537523200.0
2024-10-08 22:19:33,777 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.240e+28, grad_sumsq=5.099e+29, orig_rms_sq=2.431e-02
2024-10-08 22:19:38,790 WARNING [optim.py:503] Scaling gradients by 0.07838412374258041, model_norm_threshold=18830537523200.0
2024-10-08 22:19:38,947 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.037e+28, grad_sumsq=4.370e+27, orig_rms_sq=2.374e+00
2024-10-08 22:19:43,438 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=18.21 vs. limit=8.54525
2024-10-08 22:19:45,147 WARNING [optim.py:503] Scaling gradients by 0.0421038456261158, model_norm_threshold=18830537523200.0
2024-10-08 22:19:45,305 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.681e+28, grad_sumsq=2.809e+30, orig_rms_sq=2.379e-02
2024-10-08 22:19:46,500 WARNING [optim.py:503] Scaling gradients by 0.013553174212574959, model_norm_threshold=18830537523200.0
2024-10-08 22:19:46,667 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.907e+29, grad_sumsq=2.063e+31, orig_rms_sq=2.379e-02
2024-10-08 22:19:46,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=2787.3333333333335, ans=0.1515833333333333
2024-10-08 22:19:47,489 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=14.69 vs. limit=8.54525
2024-10-08 22:19:47,876 WARNING [optim.py:503] Scaling gradients by 0.00047852194984443486, model_norm_threshold=18830537523200.0
2024-10-08 22:19:48,033 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.821e+32, grad_sumsq=8.877e+31, orig_rms_sq=4.304e+00
2024-10-08 22:19:50,384 WARNING [optim.py:503] Scaling gradients by 0.0599457323551178, model_norm_threshold=18830537523200.0
2024-10-08 22:19:50,539 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.161e+28, grad_sumsq=9.076e+29, orig_rms_sq=2.381e-02
2024-10-08 22:19:54,285 WARNING [optim.py:503] Scaling gradients by 0.00019113573944196105, model_norm_threshold=18830537523200.0
2024-10-08 22:19:54,441 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.416e+33, grad_sumsq=5.874e+35, orig_rms_sq=5.815e-03
2024-10-08 22:19:54,668 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=2790.6666666666665, ans=0.3691875
2024-10-08 22:19:57,036 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=2790.6666666666665, ans=0.3691875
2024-10-08 22:19:58,067 WARNING [optim.py:503] Scaling gradients by 0.001746489549987018, model_norm_threshold=18830537523200.0
2024-10-08 22:19:58,222 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.490e+31, grad_sumsq=6.153e+33, orig_rms_sq=5.672e-03
2024-10-08 22:20:01,976 WARNING [optim.py:503] Scaling gradients by 0.007866655476391315, model_norm_threshold=18830537523200.0
2024-10-08 22:20:02,131 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.615e+30, grad_sumsq=5.871e+29, orig_rms_sq=2.750e+00
2024-10-08 22:20:02,301 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=2790.6666666666665, ans=0.035
2024-10-08 22:20:06,891 WARNING [optim.py:503] Scaling gradients by 0.0061466386541724205, model_norm_threshold=18830537523200.0
2024-10-08 22:20:07,048 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.162e+30, grad_sumsq=7.436e+32, orig_rms_sq=5.597e-03
2024-10-08 22:20:10,802 WARNING [optim.py:503] Scaling gradients by 0.00021881725115235895, model_norm_threshold=18830537523200.0
2024-10-08 22:20:10,959 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.968e+33, grad_sumsq=7.145e+32, orig_rms_sq=2.755e+00
2024-10-08 22:20:13,307 WARNING [optim.py:503] Scaling gradients by 0.0017210860969498754, model_norm_threshold=18830537523200.0
2024-10-08 22:20:13,463 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.973e+31, grad_sumsq=1.061e+31, orig_rms_sq=2.802e+00
2024-10-08 22:20:14,464 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=7.56 vs. limit=8.54775
2024-10-08 22:20:15,296 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=5.37 vs. limit=5.1175999999999995
2024-10-08 22:20:19,233 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.63 vs. limit=5.118933333333334
2024-10-08 22:20:23,365 WARNING [optim.py:503] Scaling gradients by 0.03906640782952309, model_norm_threshold=18830537523200.0
2024-10-08 22:20:23,520 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.093e+28, grad_sumsq=9.238e+30, orig_rms_sq=5.513e-03
2024-10-08 22:20:25,968 WARNING [optim.py:503] Scaling gradients by 0.023029914125800133, model_norm_threshold=18830537523200.0
2024-10-08 22:20:26,123 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.451e+29, grad_sumsq=6.048e+30, orig_rms_sq=2.399e-02
2024-10-08 22:20:27,069 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=10.19 vs. limit=6.398666666666667
2024-10-08 22:20:28,804 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.283e+10 2.307e+12 1.895e+13 3.141e+14 9.852e+16, threshold=3.791e+13, percent-clipped=50.0
2024-10-08 22:20:29,943 WARNING [optim.py:503] Scaling gradients by 0.010916624218225479, model_norm_threshold=37905271619584.0
2024-10-08 22:20:30,099 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.476e+30, grad_sumsq=6.314e+32, orig_rms_sq=5.505e-03
2024-10-08 22:20:31,405 INFO [train.py:1152] Epoch 2, batch 1550, loss[loss=0.7946, ctc_loss=1.088, attn_decoder_loss=0.7212, over 4840.00 frames. ], tot_loss[loss=0.761, ctc_loss=1.044, attn_decoder_loss=0.6903, over 966004.73 frames. ], batch size: 31, lr: 3.60e-02,
2024-10-08 22:20:33,531 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.39 vs. limit=9.6005
2024-10-08 22:20:37,771 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=2800.6666666666665, ans=0.24201
2024-10-08 22:20:41,279 WARNING [optim.py:503] Scaling gradients by 0.020802780985832214, model_norm_threshold=37905271619584.0
2024-10-08 22:20:41,435 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.294e+30, grad_sumsq=2.288e+32, orig_rms_sq=5.657e-03
2024-10-08 22:20:42,322 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=14.86 vs. limit=8.55025
2024-10-08 22:20:43,922 WARNING [optim.py:503] Scaling gradients by 0.09100614488124847, model_norm_threshold=37905271619584.0
2024-10-08 22:20:44,075 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.363e+28, grad_sumsq=1.355e+30, orig_rms_sq=2.482e-02
2024-10-08 22:20:47,995 WARNING [optim.py:503] Scaling gradients by 0.05055491253733635, model_norm_threshold=37905271619584.0
2024-10-08 22:20:48,150 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.487e+29, grad_sumsq=5.990e+30, orig_rms_sq=2.482e-02
2024-10-08 22:20:52,683 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=8.65 vs. limit=5.1216
2024-10-08 22:20:54,552 WARNING [optim.py:503] Scaling gradients by 0.011309667490422726, model_norm_threshold=37905271619584.0
2024-10-08 22:20:54,709 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.301e+30, grad_sumsq=1.056e+31, orig_rms_sq=3.126e-01
2024-10-08 22:20:57,980 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=41.43 vs. limit=9.6055
2024-10-08 22:21:06,501 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=12.19 vs. limit=5.122933333333333
2024-10-08 22:21:06,717 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=51.65 vs. limit=8.55275
2024-10-08 22:21:07,268 WARNING [optim.py:503] Scaling gradients by 0.04546969756484032, model_norm_threshold=37905271619584.0
2024-10-08 22:21:07,425 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.558e+29, grad_sumsq=4.219e+28, orig_rms_sq=3.694e+00
2024-10-08 22:21:10,213 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=2810.6666666666665, ans=0.03676
2024-10-08 22:21:11,222 WARNING [optim.py:503] Scaling gradients by 0.06743795424699783, model_norm_threshold=37905271619584.0
2024-10-08 22:21:11,377 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.473e+28, grad_sumsq=2.700e+30, orig_rms_sq=2.398e-02
2024-10-08 22:21:12,070 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.19 vs. limit=9.608
2024-10-08 22:21:12,611 WARNING [optim.py:503] Scaling gradients by 0.01647447980940342, model_norm_threshold=37905271619584.0
2024-10-08 22:21:12,768 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.150e+30, grad_sumsq=4.795e+31, orig_rms_sq=2.398e-02
2024-10-08 22:21:13,526 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.87 vs. limit=6.405333333333333
2024-10-08 22:21:14,820 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.77 vs. limit=9.608
2024-10-08 22:21:14,823 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.94 vs. limit=6.405333333333333
2024-10-08 22:21:15,427 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=2810.6666666666665, ans=0.041216666666666665
2024-10-08 22:21:20,839 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=66.37 vs. limit=8.554
2024-10-08 22:21:22,843 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=2814.0, ans=0.09447499999999999
2024-10-08 22:21:24,465 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=27.68 vs. limit=8.555250000000001
2024-10-08 22:21:31,111 WARNING [optim.py:503] Scaling gradients by 0.0012614063452929258, model_norm_threshold=37905271619584.0
2024-10-08 22:21:31,277 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.741e+32, grad_sumsq=7.104e+33, orig_rms_sq=2.451e-02
2024-10-08 22:21:31,514 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=2814.0, ans=0.36809375
2024-10-08 22:21:31,515 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=2814.0, ans=0.09447499999999999
2024-10-08 22:21:32,464 WARNING [optim.py:503] Scaling gradients by 0.0021652954164892435, model_norm_threshold=37905271619584.0
2024-10-08 22:21:32,628 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.875e+31, grad_sumsq=2.805e+33, orig_rms_sq=2.451e-02
2024-10-08 22:21:35,079 INFO [train.py:1152] Epoch 2, batch 1600, loss[loss=0.8097, ctc_loss=1.102, attn_decoder_loss=0.7367, over 4799.00 frames. ], tot_loss[loss=0.7615, ctc_loss=1.044, attn_decoder_loss=0.6908, over 966251.12 frames. ], batch size: 25, lr: 3.59e-02,
2024-10-08 22:21:36,216 WARNING [optim.py:503] Scaling gradients by 0.011380044743418694, model_norm_threshold=37905271619584.0
2024-10-08 22:21:36,373 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.281e+30, grad_sumsq=1.337e+32, orig_rms_sq=2.453e-02
2024-10-08 22:21:38,381 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.93 vs. limit=5.7043333333333335
2024-10-08 22:21:40,651 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.28 vs. limit=9.613
2024-10-08 22:21:45,154 WARNING [optim.py:503] Scaling gradients by 0.005334438756108284, model_norm_threshold=37905271619584.0
2024-10-08 22:21:45,308 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.111e+31, grad_sumsq=1.877e+33, orig_rms_sq=5.919e-03
2024-10-08 22:21:49,006 WARNING [optim.py:503] Scaling gradients by 0.020802993327379227, model_norm_threshold=37905271619584.0
2024-10-08 22:21:49,162 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.656e+29, grad_sumsq=1.462e+32, orig_rms_sq=5.919e-03
2024-10-08 22:21:50,611 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=2820.6666666666665, ans=0.36778125
2024-10-08 22:21:51,272 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.10 vs. limit=8.55775
2024-10-08 22:21:52,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=18.01 vs. limit=5.705166666666667
2024-10-08 22:21:55,330 WARNING [optim.py:503] Scaling gradients by 0.00349543709307909, model_norm_threshold=37905271619584.0
2024-10-08 22:21:55,489 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.981e+31, grad_sumsq=5.095e+33, orig_rms_sq=5.851e-03
2024-10-08 22:21:56,136 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=65.06 vs. limit=8.55775
2024-10-08 22:21:58,537 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=25.62 vs. limit=9.6155
2024-10-08 22:22:02,516 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=25.36 vs. limit=8.559
2024-10-08 22:22:02,993 WARNING [optim.py:503] Scaling gradients by 0.04072784632444382, model_norm_threshold=37905271619584.0
2024-10-08 22:22:03,152 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.923e+29, grad_sumsq=3.302e+31, orig_rms_sq=5.822e-03
2024-10-08 22:22:05,856 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=2824.0, ans=0.09409999999999999
2024-10-08 22:22:07,057 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=2824.0, ans=0.03645999999999999
2024-10-08 22:22:10,753 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=2.761e+02
2024-10-08 22:22:11,768 WARNING [optim.py:503] Scaling gradients by 0.002630524104461074, model_norm_threshold=37905271619584.0
2024-10-08 22:22:11,925 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.433e+31, grad_sumsq=2.563e+33, orig_rms_sq=2.510e-02
2024-10-08 22:22:15,681 WARNING [optim.py:503] Scaling gradients by 0.000479754846310243, model_norm_threshold=37905271619584.0
2024-10-08 22:22:15,838 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.896e+33, grad_sumsq=3.485e+35, orig_rms_sq=5.441e-03
2024-10-08 22:22:17,024 WARNING [optim.py:503] Scaling gradients by 0.0006891401135362685, model_norm_threshold=37905271619584.0
2024-10-08 22:22:17,180 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.265e+32, grad_sumsq=8.028e+31, orig_rms_sq=9.049e+00
2024-10-08 22:22:20,834 WARNING [optim.py:503] Scaling gradients by 0.005837071221321821, model_norm_threshold=37905271619584.0
2024-10-08 22:22:20,992 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.243e+30, grad_sumsq=1.712e+33, orig_rms_sq=5.400e-03
2024-10-08 22:22:22,196 WARNING [optim.py:503] Scaling gradients by 0.08956888318061829, model_norm_threshold=37905271619584.0
2024-10-08 22:22:22,352 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.451e+28, grad_sumsq=1.782e+30, orig_rms_sq=2.498e-02
2024-10-08 22:22:23,581 WARNING [optim.py:503] Scaling gradients by 0.02102043107151985, model_norm_threshold=37905271619584.0
2024-10-08 22:22:23,739 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.006e+29, grad_sumsq=1.297e+32, orig_rms_sq=5.400e-03
2024-10-08 22:22:24,487 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=126.41 vs. limit=8.56025
2024-10-08 22:22:27,347 WARNING [optim.py:503] Scaling gradients by 0.02734193205833435, model_norm_threshold=37905271619584.0
2024-10-08 22:22:27,499 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.546e+29, grad_sumsq=6.688e+31, orig_rms_sq=5.302e-03
2024-10-08 22:22:30,463 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=9.89 vs. limit=5.132266666666666
2024-10-08 22:22:34,897 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.12 vs. limit=9.623000000000001
2024-10-08 22:22:36,509 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.181e+10 4.591e+12 3.664e+13 2.623e+14 7.901e+16, threshold=7.329e+13, percent-clipped=49.0
2024-10-08 22:22:37,729 WARNING [optim.py:503] Scaling gradients by 0.0072922841645777225, model_norm_threshold=73285245075456.0
2024-10-08 22:22:37,886 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.326e+31, grad_sumsq=1.354e+33, orig_rms_sq=2.456e-02
2024-10-08 22:22:39,086 WARNING [optim.py:503] Scaling gradients by 0.006956025492399931, model_norm_threshold=73285245075456.0
2024-10-08 22:22:39,241 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.327e+31, grad_sumsq=9.474e+32, orig_rms_sq=2.456e-02
2024-10-08 22:22:39,300 INFO [train.py:1152] Epoch 2, batch 1650, loss[loss=0.779, ctc_loss=1.104, attn_decoder_loss=0.6978, over 4780.00 frames. ], tot_loss[loss=0.7606, ctc_loss=1.043, attn_decoder_loss=0.69, over 966771.45 frames. ], batch size: 29, lr: 3.59e-02,
2024-10-08 22:22:45,291 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.69 vs. limit=6.417
2024-10-08 22:22:45,549 WARNING [optim.py:503] Scaling gradients by 0.00824518222361803, model_norm_threshold=73285245075456.0
2024-10-08 22:22:45,704 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.482e+31, grad_sumsq=6.128e+32, orig_rms_sq=2.419e-02
2024-10-08 22:22:48,231 WARNING [optim.py:503] Scaling gradients by 0.019119730219244957, model_norm_threshold=73285245075456.0
2024-10-08 22:22:48,387 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.790e+30, grad_sumsq=1.623e+32, orig_rms_sq=2.336e-02
2024-10-08 22:22:53,445 WARNING [optim.py:503] Scaling gradients by 0.00014470658788923174, model_norm_threshold=73285245075456.0
2024-10-08 22:22:53,600 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.551e+34, grad_sumsq=2.285e+34, orig_rms_sq=3.305e+00
2024-10-08 22:22:54,796 WARNING [optim.py:503] Scaling gradients by 0.02037920616567135, model_norm_threshold=73285245075456.0
2024-10-08 22:22:54,953 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.317e+30, grad_sumsq=1.009e+32, orig_rms_sq=2.298e-02
2024-10-08 22:22:57,330 WARNING [optim.py:503] Scaling gradients by 0.003407042473554611, model_norm_threshold=73285245075456.0
2024-10-08 22:22:57,485 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.179e+32, grad_sumsq=2.198e+34, orig_rms_sq=5.366e-03
2024-10-08 22:22:59,178 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.34 vs. limit=5.709333333333333
2024-10-08 22:23:01,152 WARNING [optim.py:503] Scaling gradients by 0.017418015748262405, model_norm_threshold=73285245075456.0
2024-10-08 22:23:02,917 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.079e+30, grad_sumsq=2.228e+32, orig_rms_sq=2.280e-02
2024-10-08 22:23:04,270 WARNING [optim.py:503] Scaling gradients by 0.07287204265594482, model_norm_threshold=73285245075456.0
2024-10-08 22:23:04,427 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.641e+29, grad_sumsq=7.217e+30, orig_rms_sq=2.274e-02
2024-10-08 22:23:05,936 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2840.6666666666665, ans=0.27159333333333335
2024-10-08 22:23:09,106 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=22.19 vs. limit=8.56525
2024-10-08 22:23:10,383 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=34.95 vs. limit=9.6305
2024-10-08 22:23:10,421 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.40 vs. limit=6.420333333333334
2024-10-08 22:23:14,617 WARNING [optim.py:503] Scaling gradients by 0.050904564559459686, model_norm_threshold=73285245075456.0
2024-10-08 22:23:14,771 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.835e+29, grad_sumsq=1.648e+31, orig_rms_sq=2.328e-02
2024-10-08 22:23:15,984 WARNING [optim.py:503] Scaling gradients by 0.01879075914621353, model_norm_threshold=73285245075456.0
2024-10-08 22:23:16,140 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.014e+30, grad_sumsq=2.583e+32, orig_rms_sq=2.328e-02
2024-10-08 22:23:17,411 WARNING [optim.py:503] Scaling gradients by 0.001905936049297452, model_norm_threshold=73285245075456.0
2024-10-08 22:23:17,568 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.082e+32, grad_sumsq=3.042e+34, orig_rms_sq=2.328e-02
2024-10-08 22:23:17,776 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=4.153e+05
2024-10-08 22:23:18,738 WARNING [optim.py:503] Scaling gradients by 0.03464532271027565, model_norm_threshold=73285245075456.0
2024-10-08 22:23:18,895 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.186e+30, grad_sumsq=2.055e+32, orig_rms_sq=5.774e-03
2024-10-08 22:23:20,158 WARNING [optim.py:503] Scaling gradients by 0.00016757477715145797, model_norm_threshold=73285245075456.0
2024-10-08 22:23:20,315 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.523e+34, grad_sumsq=3.638e+36, orig_rms_sq=2.343e-02
2024-10-08 22:23:21,846 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=2844.0, ans=0.14450000000000002
2024-10-08 22:23:30,258 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=4.47 vs. limit=5.0
2024-10-08 22:23:30,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=2844.0, ans=0.3666875
2024-10-08 22:23:31,775 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=2847.3333333333335, ans=0.36653125
2024-10-08 22:23:37,702 WARNING [optim.py:503] Scaling gradients by 0.06342367827892303, model_norm_threshold=73285245075456.0
2024-10-08 22:23:37,859 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.805e+29, grad_sumsq=1.693e+31, orig_rms_sq=2.247e-02
2024-10-08 22:23:43,349 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.45 vs. limit=5.711833333333334
2024-10-08 22:23:43,522 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.05 vs. limit=9.6355
2024-10-08 22:23:44,800 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=12.17 vs. limit=6.425333333333333
2024-10-08 22:23:45,327 WARNING [optim.py:503] Scaling gradients by 0.036495909094810486, model_norm_threshold=73285245075456.0
2024-10-08 22:23:45,484 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.451e+29, grad_sumsq=1.887e+32, orig_rms_sq=5.009e-03
2024-10-08 22:23:45,542 INFO [train.py:1152] Epoch 2, batch 1700, loss[loss=0.7524, ctc_loss=1.049, attn_decoder_loss=0.6784, over 4940.00 frames. ], tot_loss[loss=0.7603, ctc_loss=1.044, attn_decoder_loss=0.6894, over 966817.97 frames. ], batch size: 19, lr: 3.58e-02,
2024-10-08 22:23:46,966 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=2850.6666666666665, ans=0.366375
2024-10-08 22:23:50,599 WARNING [optim.py:503] Scaling gradients by 0.006429865024983883, model_norm_threshold=73285245075456.0
2024-10-08 22:23:50,752 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.549e+31, grad_sumsq=1.129e+33, orig_rms_sq=2.259e-02
2024-10-08 22:23:51,511 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=33.59 vs. limit=9.638
2024-10-08 22:23:52,143 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=2850.6666666666665, ans=0.8002266666666666
2024-10-08 22:23:59,413 WARNING [optim.py:503] Scaling gradients by 0.0324709378182888, model_norm_threshold=73285245075456.0
2024-10-08 22:23:59,569 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.177e+30, grad_sumsq=3.818e+29, orig_rms_sq=3.084e+00
2024-10-08 22:24:01,981 WARNING [optim.py:503] Scaling gradients by 0.004335397854447365, model_norm_threshold=73285245075456.0
2024-10-08 22:24:02,137 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.14, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.901e+31, grad_sumsq=9.146e+30, orig_rms_sq=4.265e+00
2024-10-08 22:24:04,667 WARNING [optim.py:503] Scaling gradients by 0.00544955488294363, model_norm_threshold=73285245075456.0
2024-10-08 22:24:04,821 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.479e+31, grad_sumsq=1.091e+31, orig_rms_sq=3.187e+00
2024-10-08 22:24:12,623 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=2857.3333333333335, ans=0.3660625
2024-10-08 22:24:15,184 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=2857.3333333333335, ans=0.7999933333333333
2024-10-08 22:24:27,542 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=2860.6666666666665, ans=0.36590625
2024-10-08 22:24:39,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=2864.0, ans=0.36575
2024-10-08 22:24:41,845 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.34 vs. limit=6.432
2024-10-08 22:24:42,365 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=2864.0, ans=0.79976
2024-10-08 22:24:44,182 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=15.83 vs. limit=8.574
2024-10-08 22:24:44,618 WARNING [optim.py:503] Scaling gradients by 0.033636946231126785, model_norm_threshold=73285245075456.0
2024-10-08 22:24:44,775 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.208e+30, grad_sumsq=5.218e+31, orig_rms_sq=2.315e-02
2024-10-08 22:24:46,289 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.593e+10 7.985e+12 4.273e+13 3.998e+14 5.064e+17, threshold=8.546e+13, percent-clipped=43.0
2024-10-08 22:24:48,876 INFO [train.py:1152] Epoch 2, batch 1750, loss[loss=0.7162, ctc_loss=0.9676, attn_decoder_loss=0.6533, over 4959.00 frames. ], tot_loss[loss=0.7597, ctc_loss=1.043, attn_decoder_loss=0.689, over 967075.83 frames. ], batch size: 19, lr: 3.58e-02,
2024-10-08 22:24:53,936 WARNING [optim.py:503] Scaling gradients by 0.00202643359079957, model_norm_threshold=85460823048192.0
2024-10-08 22:24:54,091 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.742e+32, grad_sumsq=2.018e+34, orig_rms_sq=2.350e-02
2024-10-08 22:24:55,311 WARNING [optim.py:503] Scaling gradients by 0.013745666481554508, model_norm_threshold=85460823048192.0
2024-10-08 22:24:55,469 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.756e+30, grad_sumsq=1.605e+30, orig_rms_sq=4.832e+00
2024-10-08 22:24:56,697 WARNING [optim.py:503] Scaling gradients by 0.006969651207327843, model_norm_threshold=85460823048192.0
2024-10-08 22:24:56,852 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.770e+31, grad_sumsq=5.143e+33, orig_rms_sq=5.385e-03
2024-10-08 22:24:58,017 WARNING [optim.py:503] Scaling gradients by 0.04621293395757675, model_norm_threshold=85460823048192.0
2024-10-08 22:24:58,173 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.754e+29, grad_sumsq=1.254e+32, orig_rms_sq=5.385e-03
2024-10-08 22:24:59,395 WARNING [optim.py:503] Scaling gradients by 0.0072478787042200565, model_norm_threshold=85460823048192.0
2024-10-08 22:24:59,554 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.230e+31, grad_sumsq=1.785e+33, orig_rms_sq=2.369e-02
2024-10-08 22:25:14,713 WARNING [optim.py:503] Scaling gradients by 0.05752614885568619, model_norm_threshold=85460823048192.0
2024-10-08 22:25:14,868 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.543e+29, grad_sumsq=3.160e+31, orig_rms_sq=2.387e-02
2024-10-08 22:25:16,819 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.73 vs. limit=6.437
2024-10-08 22:25:18,504 WARNING [optim.py:503] Scaling gradients by 8.6171567090787e-05, model_norm_threshold=85460823048192.0
2024-10-08 22:25:18,662 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.937e+35, grad_sumsq=9.112e+37, orig_rms_sq=5.418e-03
2024-10-08 22:25:22,093 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.93 vs. limit=9.6555
2024-10-08 22:25:22,754 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=8.79 vs. limit=5.1495999999999995
2024-10-08 22:25:22,867 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.39 vs. limit=5.7185
2024-10-08 22:25:23,481 WARNING [optim.py:503] Scaling gradients by 0.0005724659422412515, model_norm_threshold=85460823048192.0
2024-10-08 22:25:23,638 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.859e+33, grad_sumsq=1.249e+36, orig_rms_sq=5.491e-03
2024-10-08 22:25:25,531 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.01 vs. limit=6.437
2024-10-08 22:25:31,232 WARNING [optim.py:503] Scaling gradients by 0.07601708918809891, model_norm_threshold=85460823048192.0
2024-10-08 22:25:31,390 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.257e+29, grad_sumsq=7.590e+29, orig_rms_sq=2.973e-01
2024-10-08 22:25:31,564 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=2877.3333333333335, ans=0.09209999999999999
2024-10-08 22:25:33,988 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=2877.3333333333335, ans=0.7992933333333334
2024-10-08 22:25:36,277 WARNING [optim.py:503] Scaling gradients by 0.01119163166731596, model_norm_threshold=85460823048192.0
2024-10-08 22:25:36,434 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.569e+31, grad_sumsq=2.840e+33, orig_rms_sq=5.527e-03
2024-10-08 22:25:37,948 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=2877.3333333333335, ans=0.36512500000000003
2024-10-08 22:25:38,901 WARNING [optim.py:503] Scaling gradients by 0.022644558921456337, model_norm_threshold=85460823048192.0
2024-10-08 22:25:39,055 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.905e+30, grad_sumsq=1.199e+32, orig_rms_sq=2.422e-02
2024-10-08 22:25:42,761 WARNING [optim.py:503] Scaling gradients by 0.07097133994102478, model_norm_threshold=85460823048192.0
2024-10-08 22:25:42,921 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.694e+29, grad_sumsq=1.530e+31, orig_rms_sq=2.415e-02
2024-10-08 22:25:43,720 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=18.21 vs. limit=8.58025
2024-10-08 22:25:45,972 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=31.26 vs. limit=9.660499999999999
2024-10-08 22:25:46,521 WARNING [optim.py:503] Scaling gradients by 0.023583652451634407, model_norm_threshold=85460823048192.0
2024-10-08 22:25:46,675 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.792e+30, grad_sumsq=8.881e+32, orig_rms_sq=5.396e-03
2024-10-08 22:25:48,566 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.71 vs. limit=9.660499999999999
2024-10-08 22:25:52,877 INFO [train.py:1152] Epoch 2, batch 1800, loss[loss=0.7611, ctc_loss=1.062, attn_decoder_loss=0.6857, over 4842.00 frames. ], tot_loss[loss=0.7623, ctc_loss=1.045, attn_decoder_loss=0.6916, over 967791.06 frames. ], batch size: 23, lr: 3.57e-02,
2024-10-08 22:25:56,459 WARNING [optim.py:503] Scaling gradients by 0.08968046307563782, model_norm_threshold=85460823048192.0
2024-10-08 22:25:56,622 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.172e+29, grad_sumsq=8.983e+30, orig_rms_sq=2.418e-02
2024-10-08 22:25:58,138 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=2884.0, ans=0.09184999999999999
2024-10-08 22:25:59,539 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=2884.0, ans=0.08777499999999999
2024-10-08 22:26:07,517 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.42 vs. limit=6.443666666666667
2024-10-08 22:26:11,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=2887.3333333333335, ans=6.804583333333333
2024-10-08 22:26:12,952 WARNING [optim.py:503] Scaling gradients by 0.011019828729331493, model_norm_threshold=85460823048192.0
2024-10-08 22:26:13,111 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.457e+31, grad_sumsq=2.888e+33, orig_rms_sq=5.043e-03
2024-10-08 22:26:15,544 WARNING [optim.py:503] Scaling gradients by 0.06007948890328407, model_norm_threshold=85460823048192.0
2024-10-08 22:26:15,697 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.526e+29, grad_sumsq=2.222e+31, orig_rms_sq=2.487e-02
2024-10-08 22:26:22,049 WARNING [optim.py:503] Scaling gradients by 0.010782498866319656, model_norm_threshold=85460823048192.0
2024-10-08 22:26:22,203 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.918e+31, grad_sumsq=3.774e+33, orig_rms_sq=5.082e-03
2024-10-08 22:26:27,964 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.34 vs. limit=9.668
2024-10-08 22:26:28,264 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=24.11 vs. limit=9.668
2024-10-08 22:26:28,658 WARNING [optim.py:503] Scaling gradients by 0.0950557067990303, model_norm_threshold=85460823048192.0
2024-10-08 22:26:28,815 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.645e+29, grad_sumsq=7.423e+28, orig_rms_sq=3.563e+00
2024-10-08 22:26:30,011 WARNING [optim.py:503] Scaling gradients by 0.04878942668437958, model_norm_threshold=85460823048192.0
2024-10-08 22:26:30,167 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.188e+29, grad_sumsq=2.017e+29, orig_rms_sq=3.563e+00
2024-10-08 22:26:30,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=2894.0, ans=0.79871
2024-10-08 22:26:31,326 WARNING [optim.py:503] Scaling gradients by 0.0003031259111594409, model_norm_threshold=85460823048192.0
2024-10-08 22:26:31,483 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.850e+34, grad_sumsq=7.470e+35, orig_rms_sq=2.477e-02
2024-10-08 22:26:33,949 WARNING [optim.py:503] Scaling gradients by 0.012893629260361195, model_norm_threshold=85460823048192.0
2024-10-08 22:26:34,104 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.114e+31, grad_sumsq=3.093e+30, orig_rms_sq=3.602e+00
2024-10-08 22:26:34,771 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.42 vs. limit=6.447
2024-10-08 22:26:35,030 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.68 vs. limit=5.7235
2024-10-08 22:26:36,498 WARNING [optim.py:503] Scaling gradients by 0.03018808364868164, model_norm_threshold=85460823048192.0
2024-10-08 22:26:36,653 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.054e+30, grad_sumsq=4.875e+29, orig_rms_sq=4.213e+00
2024-10-08 22:26:36,840 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=2894.0, ans=0.36434374999999997
2024-10-08 22:26:36,893 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=2894.0, ans=0.36434374999999997
2024-10-08 22:26:40,314 WARNING [optim.py:503] Scaling gradients by 0.04210183396935463, model_norm_threshold=85460823048192.0
2024-10-08 22:26:40,470 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.662e+29, grad_sumsq=1.723e+32, orig_rms_sq=5.027e-03
2024-10-08 22:26:47,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=24.75 vs. limit=9.673
2024-10-08 22:26:53,006 WARNING [optim.py:503] Scaling gradients by 0.04610157385468483, model_norm_threshold=85460823048192.0
2024-10-08 22:26:53,162 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.028e+30, grad_sumsq=2.422e+29, orig_rms_sq=4.244e+00
2024-10-08 22:26:54,506 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.149e+10 9.515e+12 3.955e+13 8.991e+14 9.918e+17, threshold=7.910e+13, percent-clipped=41.0
2024-10-08 22:26:54,506 WARNING [optim.py:503] Scaling gradients by 0.033943936228752136, model_norm_threshold=79101259939840.0
2024-10-08 22:26:54,662 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.613e+30, grad_sumsq=1.063e+32, orig_rms_sq=2.458e-02
2024-10-08 22:26:57,117 WARNING [optim.py:503] Scaling gradients by 4.909010021947324e-05, model_norm_threshold=79101259939840.0
2024-10-08 22:26:57,273 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.162e+36, grad_sumsq=4.728e+37, orig_rms_sq=2.458e-02
2024-10-08 22:26:57,332 INFO [train.py:1152] Epoch 2, batch 1850, loss[loss=0.8109, ctc_loss=1.155, attn_decoder_loss=0.7248, over 4736.00 frames. ], tot_loss[loss=0.763, ctc_loss=1.047, attn_decoder_loss=0.6921, over 967990.35 frames. ], batch size: 26, lr: 3.57e-02,
2024-10-08 22:26:59,691 WARNING [optim.py:503] Scaling gradients by 0.00392932491376996, model_norm_threshold=79101259939840.0
2024-10-08 22:26:59,846 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.657e+32, grad_sumsq=6.748e+33, orig_rms_sq=2.455e-02
2024-10-08 22:27:02,340 WARNING [optim.py:503] Scaling gradients by 0.08247607946395874, model_norm_threshold=79101259939840.0
2024-10-08 22:27:02,492 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.902e+29, grad_sumsq=7.001e+28, orig_rms_sq=4.145e+00
2024-10-08 22:27:09,438 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=30.50 vs. limit=9.678
2024-10-08 22:27:11,705 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.40 vs. limit=3.4356
2024-10-08 22:27:14,764 WARNING [optim.py:503] Scaling gradients by 0.07480871677398682, model_norm_threshold=79101259939840.0
2024-10-08 22:27:14,922 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.201e+29, grad_sumsq=4.446e+31, orig_rms_sq=4.951e-03
2024-10-08 22:27:17,229 WARNING [optim.py:503] Scaling gradients by 0.06490331143140793, model_norm_threshold=79101259939840.0
2024-10-08 22:27:17,384 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.331e+29, grad_sumsq=6.397e+28, orig_rms_sq=3.644e+00
2024-10-08 22:27:19,374 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=6.45 vs. limit=5.1616
2024-10-08 22:27:23,475 WARNING [optim.py:503] Scaling gradients by 0.005209849216043949, model_norm_threshold=79101259939840.0
2024-10-08 22:27:23,631 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.027e+31, grad_sumsq=1.437e+31, orig_rms_sq=3.497e+00
2024-10-08 22:27:23,812 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=2907.3333333333335, ans=0.36371875
2024-10-08 22:27:24,881 WARNING [optim.py:503] Scaling gradients by 0.012468032538890839, model_norm_threshold=79101259939840.0
2024-10-08 22:27:25,043 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.434e+30, grad_sumsq=3.700e+32, orig_rms_sq=2.280e-02
2024-10-08 22:27:30,017 WARNING [optim.py:503] Scaling gradients by 0.0015749669400975108, model_norm_threshold=79101259939840.0
2024-10-08 22:27:30,172 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.660e+32, grad_sumsq=5.363e+31, orig_rms_sq=8.689e+00
2024-10-08 22:27:32,584 WARNING [optim.py:503] Scaling gradients by 0.00013535878679249436, model_norm_threshold=79101259939840.0
2024-10-08 22:27:32,741 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.292e+34, grad_sumsq=3.640e+36, orig_rms_sq=2.278e-02
2024-10-08 22:27:32,912 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=2907.3333333333335, ans=0.36371875
2024-10-08 22:27:37,587 WARNING [optim.py:503] Scaling gradients by 0.06887158006429672, model_norm_threshold=79101259939840.0
2024-10-08 22:27:37,742 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.059e+29, grad_sumsq=1.339e+31, orig_rms_sq=2.285e-02
2024-10-08 22:27:40,303 WARNING [optim.py:503] Scaling gradients by 0.016876332461833954, model_norm_threshold=79101259939840.0
2024-10-08 22:27:40,458 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.463e+30, grad_sumsq=1.453e+33, orig_rms_sq=5.137e-03
2024-10-08 22:27:42,347 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.74 vs. limit=6.455333333333333
2024-10-08 22:27:42,945 WARNING [optim.py:503] Scaling gradients by 0.019062768667936325, model_norm_threshold=79101259939840.0
2024-10-08 22:27:43,099 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.003e+30, grad_sumsq=7.794e+32, orig_rms_sq=5.137e-03
2024-10-08 22:27:45,581 WARNING [optim.py:503] Scaling gradients by 0.0003445624897722155, model_norm_threshold=79101259939840.0
2024-10-08 22:27:45,737 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.181e+34, grad_sumsq=2.302e+36, orig_rms_sq=5.129e-03
2024-10-08 22:27:46,937 WARNING [optim.py:503] Scaling gradients by 0.005122511647641659, model_norm_threshold=79101259939840.0
2024-10-08 22:27:47,091 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.368e+31, grad_sumsq=1.437e+34, orig_rms_sq=5.129e-03
2024-10-08 22:27:49,131 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=29.60 vs. limit=8.59275
2024-10-08 22:27:49,623 WARNING [optim.py:503] Scaling gradients by 0.004343228880316019, model_norm_threshold=79101259939840.0
2024-10-08 22:27:49,780 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.482e+31, grad_sumsq=3.666e+33, orig_rms_sq=2.314e-02
2024-10-08 22:27:50,529 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=7.53 vs. limit=5.1655999999999995
2024-10-08 22:27:57,217 WARNING [optim.py:503] Scaling gradients by 0.0048272074200212955, model_norm_threshold=79101259939840.0
2024-10-08 22:27:57,375 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.228e+31, grad_sumsq=2.287e+31, orig_rms_sq=3.160e+00
2024-10-08 22:28:01,051 WARNING [optim.py:503] Scaling gradients by 0.005924423690885305, model_norm_threshold=79101259939840.0
2024-10-08 22:28:01,208 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.079e+31, grad_sumsq=4.483e+30, orig_rms_sq=9.099e+00
2024-10-08 22:28:01,266 INFO [train.py:1152] Epoch 2, batch 1900, loss[loss=0.7341, ctc_loss=1.03, attn_decoder_loss=0.6601, over 4788.00 frames. ], tot_loss[loss=0.761, ctc_loss=1.045, attn_decoder_loss=0.69, over 967833.03 frames. ], batch size: 29, lr: 3.56e-02,
2024-10-08 22:28:02,401 WARNING [optim.py:503] Scaling gradients by 0.059679411351680756, model_norm_threshold=79101259939840.0
2024-10-08 22:28:02,557 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.58, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.016e+30, grad_sumsq=2.023e+32, orig_rms_sq=5.023e-03
2024-10-08 22:28:03,800 WARNING [optim.py:503] Scaling gradients by 0.021678581833839417, model_norm_threshold=79101259939840.0
2024-10-08 22:28:03,956 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.782e+30, grad_sumsq=9.157e+29, orig_rms_sq=4.130e+00
2024-10-08 22:28:05,392 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2917.3333333333335, ans=0.36324999999999996
2024-10-08 22:28:07,774 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2917.3333333333335, ans=0.27082666666666666
2024-10-08 22:28:13,937 WARNING [optim.py:503] Scaling gradients by 0.0022418152075260878, model_norm_threshold=79101259939840.0
2024-10-08 22:28:14,095 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.614e+32, grad_sumsq=1.550e+34, orig_rms_sq=2.332e-02
2024-10-08 22:28:14,313 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=2920.6666666666665, ans=0.090475
2024-10-08 22:28:18,016 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=2920.6666666666665, ans=0.7977766666666667
2024-10-08 22:28:20,205 WARNING [optim.py:503] Scaling gradients by 0.04750300943851471, model_norm_threshold=79101259939840.0
2024-10-08 22:28:20,361 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.115e+29, grad_sumsq=9.917e+31, orig_rms_sq=5.158e-03
2024-10-08 22:28:22,330 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=6.20 vs. limit=5.168266666666667
2024-10-08 22:28:23,918 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.71 vs. limit=5.730166666666666
2024-10-08 22:28:24,701 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=39.78 vs. limit=8.59525
2024-10-08 22:28:25,270 WARNING [optim.py:503] Scaling gradients by 0.0555403009057045, model_norm_threshold=79101259939840.0
2024-10-08 22:28:25,426 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.715e+29, grad_sumsq=1.626e+31, orig_rms_sq=2.284e-02
2024-10-08 22:28:27,444 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.05 vs. limit=6.462
2024-10-08 22:28:27,820 WARNING [optim.py:503] Scaling gradients by 9.261607192456722e-05, model_norm_threshold=79101259939840.0
2024-10-08 22:28:27,978 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.154e+35, grad_sumsq=5.101e+36, orig_rms_sq=2.261e-02
2024-10-08 22:28:30,502 WARNING [optim.py:503] Scaling gradients by 0.08767600357532501, model_norm_threshold=79101259939840.0
2024-10-08 22:28:30,659 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.461e+29, grad_sumsq=3.392e+28, orig_rms_sq=4.306e+00
2024-10-08 22:28:38,259 WARNING [optim.py:503] Scaling gradients by 0.012577231973409653, model_norm_threshold=79101259939840.0
2024-10-08 22:28:38,416 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.322e+31, grad_sumsq=5.878e+32, orig_rms_sq=2.248e-02
2024-10-08 22:28:39,592 WARNING [optim.py:503] Scaling gradients by 0.04908275231719017, model_norm_threshold=79101259939840.0
2024-10-08 22:28:39,748 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.074e+29, grad_sumsq=1.768e+32, orig_rms_sq=5.133e-03
2024-10-08 22:28:41,737 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=27.21 vs. limit=9.6955
2024-10-08 22:28:44,628 WARNING [optim.py:503] Scaling gradients by 0.0028534706216305494, model_norm_threshold=79101259939840.0
2024-10-08 22:28:44,785 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.182e+32, grad_sumsq=4.010e+31, orig_rms_sq=2.948e+00
2024-10-08 22:28:48,575 WARNING [optim.py:503] Scaling gradients by 0.003949002828449011, model_norm_threshold=79101259939840.0
2024-10-08 22:28:48,731 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.049e+32, grad_sumsq=4.698e+33, orig_rms_sq=2.233e-02
2024-10-08 22:28:49,700 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.31 vs. limit=6.463666666666667
2024-10-08 22:28:51,127 WARNING [optim.py:503] Scaling gradients by 0.006526418030261993, model_norm_threshold=79101259939840.0
2024-10-08 22:28:51,283 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.196e+31, grad_sumsq=3.559e+30, orig_rms_sq=8.980e+00
2024-10-08 22:28:52,245 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.59 vs. limit=6.465333333333334
2024-10-08 22:29:02,926 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.025e+11 1.200e+13 6.498e+13 1.219e+15 1.611e+18, threshold=1.300e+14, percent-clipped=49.0
2024-10-08 22:29:03,712 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=27.83 vs. limit=8.599
2024-10-08 22:29:05,446 INFO [train.py:1152] Epoch 2, batch 1950, loss[loss=0.8141, ctc_loss=1.116, attn_decoder_loss=0.7386, over 4866.00 frames. ], tot_loss[loss=0.7622, ctc_loss=1.047, attn_decoder_loss=0.6909, over 966712.50 frames. ], batch size: 20, lr: 3.55e-02,
2024-10-08 22:29:08,085 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2934.0, ans=0.27066
2024-10-08 22:29:10,396 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=2934.0, ans=0.7973100000000001
2024-10-08 22:29:15,208 WARNING [optim.py:503] Scaling gradients by 0.06130919232964516, model_norm_threshold=129955384000512.0
2024-10-08 22:29:15,366 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.751e+30, grad_sumsq=3.324e+32, orig_rms_sq=5.268e-03
2024-10-08 22:29:16,560 WARNING [optim.py:503] Scaling gradients by 0.01555000338703394, model_norm_threshold=129955384000512.0
2024-10-08 22:29:16,719 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.093e+31, grad_sumsq=4.907e+32, orig_rms_sq=2.228e-02
2024-10-08 22:29:18,775 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=52.85 vs. limit=8.6015
2024-10-08 22:29:19,213 WARNING [optim.py:503] Scaling gradients by 0.01823042519390583, model_norm_threshold=129955384000512.0
2024-10-08 22:29:19,368 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.211e+31, grad_sumsq=5.436e+32, orig_rms_sq=2.227e-02
2024-10-08 22:29:29,239 WARNING [optim.py:503] Scaling gradients by 0.00012920793960802257, model_norm_threshold=129955384000512.0
2024-10-08 22:29:29,394 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.164e+35, grad_sumsq=9.483e+36, orig_rms_sq=2.282e-02
2024-10-08 22:29:30,002 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.01 vs. limit=9.7055
2024-10-08 22:29:34,316 WARNING [optim.py:503] Scaling gradients by 0.021695060655474663, model_norm_threshold=129955384000512.0
2024-10-08 22:29:34,475 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.312e+30, grad_sumsq=4.062e+32, orig_rms_sq=2.292e-02
2024-10-08 22:29:35,907 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=2940.6666666666665, ans=0.089725
2024-10-08 22:29:38,001 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.50 vs. limit=9.7055
2024-10-08 22:29:40,684 WARNING [optim.py:503] Scaling gradients by 0.019395312294363976, model_norm_threshold=129955384000512.0
2024-10-08 22:29:40,840 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.62, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.791e+31, grad_sumsq=5.203e+33, orig_rms_sq=5.364e-03
2024-10-08 22:29:45,566 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.31 vs. limit=5.736
2024-10-08 22:29:47,412 WARNING [optim.py:503] Scaling gradients by 0.09391166269779205, model_norm_threshold=129955384000512.0
2024-10-08 22:29:47,568 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.14, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.712e+29, grad_sumsq=9.563e+28, orig_rms_sq=2.836e+00
2024-10-08 22:29:51,451 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=2944.0, ans=0.0816
2024-10-08 22:29:53,748 WARNING [optim.py:503] Scaling gradients by 0.044018033891916275, model_norm_threshold=129955384000512.0
2024-10-08 22:29:53,903 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.153e+30, grad_sumsq=1.353e+32, orig_rms_sq=2.330e-02
2024-10-08 22:29:55,108 WARNING [optim.py:503] Scaling gradients by 0.07024463266134262, model_norm_threshold=129955384000512.0
2024-10-08 22:29:55,265 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.868e+29, grad_sumsq=1.655e+32, orig_rms_sq=5.360e-03
2024-10-08 22:29:58,074 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=2947.3333333333335, ans=0.36184375
2024-10-08 22:29:59,937 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=36.91 vs. limit=8.60525
2024-10-08 22:30:00,076 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.28 vs. limit=6.4736666666666665
2024-10-08 22:30:00,306 WARNING [optim.py:503] Scaling gradients by 0.0307193323969841, model_norm_threshold=129955384000512.0
2024-10-08 22:30:00,459 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.921e+30, grad_sumsq=9.208e+32, orig_rms_sq=5.344e-03
2024-10-08 22:30:02,856 WARNING [optim.py:503] Scaling gradients by 0.032507557421922684, model_norm_threshold=129955384000512.0
2024-10-08 22:30:03,012 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.421e+30, grad_sumsq=6.401e+32, orig_rms_sq=5.344e-03
2024-10-08 22:30:06,215 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=6.35 vs. limit=8.60525
2024-10-08 22:30:08,211 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=2947.3333333333335, ans=0.36184375
2024-10-08 22:30:10,774 INFO [train.py:1152] Epoch 2, batch 2000, loss[loss=0.7538, ctc_loss=1.058, attn_decoder_loss=0.6779, over 4959.00 frames. ], tot_loss[loss=0.7626, ctc_loss=1.048, attn_decoder_loss=0.6912, over 966453.06 frames. ], batch size: 19, lr: 3.55e-02,
2024-10-08 22:30:14,510 WARNING [optim.py:503] Scaling gradients by 0.019445225596427917, model_norm_threshold=129955384000512.0
2024-10-08 22:30:14,667 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.626e+30, grad_sumsq=9.455e+29, orig_rms_sq=9.123e+00
2024-10-08 22:30:15,632 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=20.68 vs. limit=8.6065
2024-10-08 22:30:17,235 WARNING [optim.py:503] Scaling gradients by 0.0029579552356153727, model_norm_threshold=129955384000512.0
2024-10-08 22:30:17,391 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.658e+32, grad_sumsq=2.520e+34, orig_rms_sq=2.245e-02
2024-10-08 22:30:22,963 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=25.37 vs. limit=9.7155
2024-10-08 22:30:23,727 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=2954.0, ans=0.0815375
2024-10-08 22:30:28,425 WARNING [optim.py:503] Scaling gradients by 0.0005229580565355718, model_norm_threshold=129955384000512.0
2024-10-08 22:30:28,581 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.374e+34, grad_sumsq=3.008e+33, orig_rms_sq=4.566e+00
2024-10-08 22:30:30,626 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=13.16 vs. limit=5.7385
2024-10-08 22:30:31,059 WARNING [optim.py:503] Scaling gradients by 0.048794712871313095, model_norm_threshold=129955384000512.0
2024-10-08 22:30:31,213 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.042e+30, grad_sumsq=1.157e+29, orig_rms_sq=9.007e+00
2024-10-08 22:30:31,389 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2954.0, ans=0.36153124999999997
2024-10-08 22:30:35,646 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=34.95 vs. limit=8.609
2024-10-08 22:30:39,133 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2957.3333333333335, ans=0.361375
2024-10-08 22:30:48,386 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.08 vs. limit=6.480333333333333
2024-10-08 22:30:50,155 WARNING [optim.py:503] Scaling gradients by 0.04228442534804344, model_norm_threshold=129955384000512.0
2024-10-08 22:30:50,309 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.linear_pos.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.657e+30, grad_sumsq=1.998e+29, orig_rms_sq=8.295e+00
2024-10-08 22:30:52,945 WARNING [optim.py:503] Scaling gradients by 0.08354409039020538, model_norm_threshold=129955384000512.0
2024-10-08 22:30:53,102 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.019e+29, grad_sumsq=3.611e+31, orig_rms_sq=2.221e-02
2024-10-08 22:31:00,383 WARNING [optim.py:503] Scaling gradients by 0.0016247192397713661, model_norm_threshold=129955384000512.0
2024-10-08 22:31:00,538 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.838e+33, grad_sumsq=8.191e+34, orig_rms_sq=2.243e-02
2024-10-08 22:31:11,706 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.077e+11 9.064e+12 7.050e+13 4.275e+14 1.006e+18, threshold=1.410e+14, percent-clipped=42.0
2024-10-08 22:31:14,407 INFO [train.py:1152] Epoch 2, batch 2050, loss[loss=0.7417, ctc_loss=1.005, attn_decoder_loss=0.6758, over 4914.00 frames. ], tot_loss[loss=0.7607, ctc_loss=1.045, attn_decoder_loss=0.6895, over 966856.73 frames. ], batch size: 19, lr: 3.54e-02,
2024-10-08 22:31:17,991 WARNING [optim.py:503] Scaling gradients by 0.0043673887848854065, model_norm_threshold=141006343241728.0
2024-10-08 22:31:18,147 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.290e+32, grad_sumsq=4.707e+31, orig_rms_sq=4.864e+00
2024-10-08 22:31:22,063 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=2967.3333333333335, ans=0.36090625
2024-10-08 22:31:23,303 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.max_positive, batch_count=2967.3333333333335, ans=0.7796733333333333
2024-10-08 22:31:25,604 WARNING [optim.py:503] Scaling gradients by 0.01921909861266613, model_norm_threshold=141006343241728.0
2024-10-08 22:31:25,755 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.120e+31, grad_sumsq=1.993e+33, orig_rms_sq=5.618e-03
2024-10-08 22:31:25,950 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=2970.6666666666665, ans=0.36075
2024-10-08 22:31:27,363 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2970.6666666666665, ans=0.27029333333333333
2024-10-08 22:31:27,845 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.05 vs. limit=3.4455999999999998
2024-10-08 22:31:31,570 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.43 vs. limit=5.742666666666667
2024-10-08 22:31:35,084 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2970.6666666666665, ans=0.27029333333333333
2024-10-08 22:31:35,565 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=23.89 vs. limit=8.614
2024-10-08 22:31:37,270 WARNING [optim.py:503] Scaling gradients by 0.0044686137698590755, model_norm_threshold=141006343241728.0
2024-10-08 22:31:37,426 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.072e+32, grad_sumsq=3.807e+34, orig_rms_sq=5.441e-03
2024-10-08 22:31:38,621 WARNING [optim.py:503] Scaling gradients by 0.000310059025650844, model_norm_threshold=141006343241728.0
2024-10-08 22:31:38,778 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.268e+34, grad_sumsq=7.843e+36, orig_rms_sq=5.441e-03
2024-10-08 22:31:43,995 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=2974.0, ans=0.36059375
2024-10-08 22:31:45,074 WARNING [optim.py:503] Scaling gradients by 0.06452461332082748, model_norm_threshold=141006343241728.0
2024-10-08 22:31:45,233 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.131e+29, grad_sumsq=1.020e+29, orig_rms_sq=8.955e+00
2024-10-08 22:31:46,082 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.88 vs. limit=9.7305
2024-10-08 22:31:47,923 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2974.0, ans=0.36059375
2024-10-08 22:31:49,772 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.60 vs. limit=6.487
2024-10-08 22:31:49,776 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.85 vs. limit=8.61525
2024-10-08 22:31:52,601 WARNING [optim.py:503] Scaling gradients by 0.005423057358711958, model_norm_threshold=141006343241728.0
2024-10-08 22:31:52,758 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.750e+32, grad_sumsq=7.518e+33, orig_rms_sq=2.327e-02
2024-10-08 22:31:55,093 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=13.59 vs. limit=4.595466666666667
2024-10-08 22:31:55,239 WARNING [optim.py:503] Scaling gradients by 0.005088918376713991, model_norm_threshold=141006343241728.0
2024-10-08 22:31:55,395 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.961e+32, grad_sumsq=3.628e+34, orig_rms_sq=5.406e-03
2024-10-08 22:31:56,758 WARNING [optim.py:503] Scaling gradients by 0.0085674487054348, model_norm_threshold=141006343241728.0
2024-10-08 22:31:56,915 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.627e+31, grad_sumsq=2.847e+33, orig_rms_sq=2.327e-02
2024-10-08 22:32:03,173 WARNING [optim.py:503] Scaling gradients by 0.0001439945917809382, model_norm_threshold=141006343241728.0
2024-10-08 22:32:03,328 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.869e+35, grad_sumsq=2.089e+37, orig_rms_sq=2.331e-02
2024-10-08 22:32:06,473 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten.whitening_limit, batch_count=2980.6666666666665, ans=9.7355
2024-10-08 22:32:14,402 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=10.33 vs. limit=8.617750000000001
2024-10-08 22:32:17,151 WARNING [optim.py:503] Scaling gradients by 0.00963361281901598, model_norm_threshold=141006343241728.0
2024-10-08 22:32:17,306 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.149e+31, grad_sumsq=1.434e+31, orig_rms_sq=2.893e+00
2024-10-08 22:32:18,058 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.47 vs. limit=5.746
2024-10-08 22:32:18,736 INFO [train.py:1152] Epoch 2, batch 2100, loss[loss=0.7895, ctc_loss=1.112, attn_decoder_loss=0.7089, over 4842.00 frames. ], tot_loss[loss=0.7594, ctc_loss=1.044, attn_decoder_loss=0.6882, over 967073.81 frames. ], batch size: 21, lr: 3.54e-02,
2024-10-08 22:32:21,134 WARNING [optim.py:503] Scaling gradients by 0.02045051008462906, model_norm_threshold=141006343241728.0
2024-10-08 22:32:21,289 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.197e+31, grad_sumsq=2.702e+30, orig_rms_sq=4.429e+00
2024-10-08 22:32:23,685 WARNING [optim.py:503] Scaling gradients by 0.002530895173549652, model_norm_threshold=141006343241728.0
2024-10-08 22:32:23,840 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.410e+32, grad_sumsq=3.221e+34, orig_rms_sq=2.301e-02
2024-10-08 22:32:27,369 WARNING [optim.py:503] Scaling gradients by 0.011924575082957745, model_norm_threshold=141006343241728.0
2024-10-08 22:32:27,523 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.842e+31, grad_sumsq=8.487e+30, orig_rms_sq=4.527e+00
2024-10-08 22:32:29,571 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.73 vs. limit=9.738
2024-10-08 22:32:32,780 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2987.3333333333335, ans=0.2701266666666666
2024-10-08 22:32:33,871 WARNING [optim.py:503] Scaling gradients by 0.07755620032548904, model_norm_threshold=141006343241728.0
2024-10-08 22:32:34,025 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.836e+29, grad_sumsq=2.276e+29, orig_rms_sq=2.564e+00
2024-10-08 22:32:38,874 WARNING [optim.py:503] Scaling gradients by 0.08541236072778702, model_norm_threshold=141006343241728.0
2024-10-08 22:32:39,026 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.723e+29, grad_sumsq=1.163e+29, orig_rms_sq=4.920e+00
2024-10-08 22:32:40,184 WARNING [optim.py:503] Scaling gradients by 0.02497822791337967, model_norm_threshold=141006343241728.0
2024-10-08 22:32:40,342 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.202e+30, grad_sumsq=9.081e+29, orig_rms_sq=9.032e+00
2024-10-08 22:32:41,533 WARNING [optim.py:503] Scaling gradients by 0.029385093599557877, model_norm_threshold=141006343241728.0
2024-10-08 22:32:41,689 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.293e+30, grad_sumsq=1.857e+32, orig_rms_sq=2.312e-02
2024-10-08 22:32:49,950 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=8.50 vs. limit=5.196266666666666
2024-10-08 22:32:50,224 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.14 vs. limit=8.6215
2024-10-08 22:32:52,570 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=72.22 vs. limit=9.743
2024-10-08 22:32:54,513 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=2990.6666666666665, ans=0.08785000000000001
2024-10-08 22:32:55,597 WARNING [optim.py:503] Scaling gradients by 0.023371577262878418, model_norm_threshold=141006343241728.0
2024-10-08 22:32:55,764 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.266e+30, grad_sumsq=1.836e+30, orig_rms_sq=5.046e+00
2024-10-08 22:32:58,298 WARNING [optim.py:503] Scaling gradients by 0.032244451344013214, model_norm_threshold=141006343241728.0
2024-10-08 22:32:58,453 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.627e+30, grad_sumsq=1.967e+32, orig_rms_sq=2.352e-02
2024-10-08 22:32:59,642 WARNING [optim.py:503] Scaling gradients by 2.8475655199144967e-05, model_norm_threshold=141006343241728.0
2024-10-08 22:32:59,798 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.255e+36, grad_sumsq=1.843e+38, orig_rms_sq=2.309e-02
2024-10-08 22:33:02,930 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.45 vs. limit=3.4491
2024-10-08 22:33:03,570 WARNING [optim.py:503] Scaling gradients by 0.02308443933725357, model_norm_threshold=141006343241728.0
2024-10-08 22:33:03,726 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.754e+30, grad_sumsq=1.724e+33, orig_rms_sq=5.076e-03
2024-10-08 22:33:04,454 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=61.02 vs. limit=8.62275
2024-10-08 22:33:06,190 WARNING [optim.py:503] Scaling gradients by 0.006559077184647322, model_norm_threshold=141006343241728.0
2024-10-08 22:33:06,345 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.869e+32, grad_sumsq=3.601e+34, orig_rms_sq=5.190e-03
2024-10-08 22:33:06,495 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=2994.0, ans=0.7952100000000001
2024-10-08 22:33:06,522 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=2994.0, ans=0.032635
2024-10-08 22:33:07,626 WARNING [optim.py:503] Scaling gradients by 0.019209938123822212, model_norm_threshold=141006343241728.0
2024-10-08 22:33:07,780 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.463e+31, grad_sumsq=6.392e+32, orig_rms_sq=2.289e-02
2024-10-08 22:33:08,303 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=11.30 vs. limit=8.62275
2024-10-08 22:33:11,915 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2997.3333333333335, ans=0.3595
2024-10-08 22:33:12,486 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.59 vs. limit=9.748000000000001
2024-10-08 22:33:17,845 WARNING [optim.py:503] Scaling gradients by 0.004702946171164513, model_norm_threshold=141006343241728.0
2024-10-08 22:33:18,002 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.782e+32, grad_sumsq=7.807e+33, orig_rms_sq=2.282e-02
2024-10-08 22:33:19,227 WARNING [optim.py:503] Scaling gradients by 0.0010250558843836188, model_norm_threshold=141006343241728.0
2024-10-08 22:33:19,384 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.369e+33, grad_sumsq=5.985e+32, orig_rms_sq=8.970e+00
2024-10-08 22:33:20,906 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.394e+11 1.212e+13 1.086e+14 1.651e+15 4.952e+18, threshold=2.171e+14, percent-clipped=45.0
2024-10-08 22:33:21,575 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.53 vs. limit=3.4496
2024-10-08 22:33:22,039 WARNING [optim.py:503] Scaling gradients by 0.00608263723552227, model_norm_threshold=217120763805696.0
2024-10-08 22:33:22,194 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.576e+32, grad_sumsq=1.050e+32, orig_rms_sq=2.453e+00
2024-10-08 22:33:23,479 INFO [train.py:1152] Epoch 2, batch 2150, loss[loss=0.8023, ctc_loss=1.039, attn_decoder_loss=0.743, over 4869.00 frames. ], tot_loss[loss=0.7581, ctc_loss=1.041, attn_decoder_loss=0.6875, over 967927.80 frames. ], batch size: 20, lr: 3.53e-02,
2024-10-08 22:33:24,594 WARNING [optim.py:503] Scaling gradients by 0.001126380288042128, model_norm_threshold=217120763805696.0
2024-10-08 22:33:24,755 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.110e+34, grad_sumsq=4.852e+35, orig_rms_sq=2.288e-02
2024-10-08 22:33:27,583 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=3000.6666666666665, ans=0.7949766666666667
2024-10-08 22:33:28,548 WARNING [optim.py:503] Scaling gradients by 0.004106767941266298, model_norm_threshold=217120763805696.0
2024-10-08 22:33:28,705 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.128e+33, grad_sumsq=2.247e+32, orig_rms_sq=5.019e+00
2024-10-08 22:33:32,528 WARNING [optim.py:503] Scaling gradients by 0.09202750772237778, model_norm_threshold=217120763805696.0
2024-10-08 22:33:32,686 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.536e+30, grad_sumsq=3.065e+29, orig_rms_sq=5.011e+00
2024-10-08 22:33:32,882 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=3000.6666666666665, ans=0.08747500000000001
2024-10-08 22:33:33,978 WARNING [optim.py:503] Scaling gradients by 0.031295374035835266, model_norm_threshold=217120763805696.0
2024-10-08 22:33:34,133 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.040e+31, grad_sumsq=4.477e+32, orig_rms_sq=2.323e-02
2024-10-08 22:33:34,933 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=19.68 vs. limit=8.62525
2024-10-08 22:33:36,671 WARNING [optim.py:503] Scaling gradients by 0.0004880337801296264, model_norm_threshold=217120763805696.0
2024-10-08 22:33:36,827 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.014e+35, grad_sumsq=4.327e+36, orig_rms_sq=2.344e-02
2024-10-08 22:33:44,651 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3004.0, ans=0.26996
2024-10-08 22:33:46,948 WARNING [optim.py:503] Scaling gradients by 0.0846603736281395, model_norm_threshold=217120763805696.0
2024-10-08 22:33:47,105 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.448e+30, grad_sumsq=6.210e+31, orig_rms_sq=2.332e-02
2024-10-08 22:33:54,704 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=3007.3333333333335, ans=0.08083749999999998
2024-10-08 22:34:06,498 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.16 vs. limit=8.629
2024-10-08 22:34:08,611 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=3010.6666666666665, ans=0.08118333333333334
2024-10-08 22:34:10,335 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=5.27 vs. limit=3.4516
2024-10-08 22:34:10,434 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.21 vs. limit=8.629
2024-10-08 22:34:10,837 WARNING [optim.py:503] Scaling gradients by 0.017866240814328194, model_norm_threshold=217120763805696.0
2024-10-08 22:34:10,994 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.645e+31, grad_sumsq=5.210e+33, orig_rms_sq=5.076e-03
2024-10-08 22:34:15,853 WARNING [optim.py:503] Scaling gradients by 0.049340490251779556, model_norm_threshold=217120763805696.0
2024-10-08 22:34:16,009 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.826e+30, grad_sumsq=5.359e+29, orig_rms_sq=9.006e+00
2024-10-08 22:34:19,743 WARNING [optim.py:503] Scaling gradients by 0.029605325311422348, model_norm_threshold=217120763805696.0
2024-10-08 22:34:19,897 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.009e+30, grad_sumsq=2.513e+31, orig_rms_sq=3.187e-01
2024-10-08 22:34:27,341 INFO [train.py:1152] Epoch 2, batch 2200, loss[loss=0.7033, ctc_loss=0.9596, attn_decoder_loss=0.6393, over 4746.00 frames. ], tot_loss[loss=0.7574, ctc_loss=1.04, attn_decoder_loss=0.6867, over 967667.89 frames. ], batch size: 26, lr: 3.52e-02,
2024-10-08 22:34:31,553 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2.whitening_limit, batch_count=3017.3333333333335, ans=6.508666666666667
2024-10-08 22:34:45,655 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.33 vs. limit=5.755166666666667
2024-10-08 22:34:45,951 WARNING [optim.py:503] Scaling gradients by 0.08860557526350021, model_norm_threshold=217120763805696.0
2024-10-08 22:34:46,120 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.321e+30, grad_sumsq=1.462e+29, orig_rms_sq=9.032e+00
2024-10-08 22:34:48,900 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=3020.6666666666665, ans=0.35840625000000004
2024-10-08 22:34:58,515 WARNING [optim.py:503] Scaling gradients by 0.0025142652448266745, model_norm_threshold=217120763805696.0
2024-10-08 22:34:58,670 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.395e+33, grad_sumsq=4.664e+32, orig_rms_sq=2.992e+00
2024-10-08 22:35:04,769 WARNING [optim.py:503] Scaling gradients by 0.011621115729212761, model_norm_threshold=217120763805696.0
2024-10-08 22:35:04,925 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.146e+31, grad_sumsq=3.427e+33, orig_rms_sq=2.085e-02
2024-10-08 22:35:07,020 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.87 vs. limit=5.756833333333334
2024-10-08 22:35:07,338 WARNING [optim.py:503] Scaling gradients by 0.0005030670436099172, model_norm_threshold=217120763805696.0
2024-10-08 22:35:07,495 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.775e+34, grad_sumsq=1.179e+35, orig_rms_sq=3.202e-01
2024-10-08 22:35:13,962 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.max_positive, batch_count=3027.3333333333335, ans=0.7802733333333334
2024-10-08 22:35:16,203 WARNING [optim.py:503] Scaling gradients by 0.01199380587786436, model_norm_threshold=217120763805696.0
2024-10-08 22:35:16,362 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.713e+31, grad_sumsq=1.731e+34, orig_rms_sq=5.034e-03
2024-10-08 22:35:22,568 WARNING [optim.py:503] Scaling gradients by 0.014537562616169453, model_norm_threshold=217120763805696.0
2024-10-08 22:35:22,724 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.735e+31, grad_sumsq=9.433e+30, orig_rms_sq=5.020e+00
2024-10-08 22:35:27,673 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.650e+11 6.950e+12 7.290e+13 9.106e+14 4.449e+17, threshold=1.458e+14, percent-clipped=35.0
2024-10-08 22:35:28,870 WARNING [optim.py:503] Scaling gradients by 0.0021368733141571283, model_norm_threshold=145790349606912.0
2024-10-08 22:35:29,027 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.660e+32, grad_sumsq=1.071e+32, orig_rms_sq=9.021e+00
2024-10-08 22:35:30,392 WARNING [optim.py:503] Scaling gradients by 0.007742094807326794, model_norm_threshold=145790349606912.0
2024-10-08 22:35:30,550 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.690e+32, grad_sumsq=7.738e+33, orig_rms_sq=2.184e-02
2024-10-08 22:35:30,608 INFO [train.py:1152] Epoch 2, batch 2250, loss[loss=0.7815, ctc_loss=1.099, attn_decoder_loss=0.702, over 4876.00 frames. ], tot_loss[loss=0.7576, ctc_loss=1.041, attn_decoder_loss=0.6869, over 967733.55 frames. ], batch size: 22, lr: 3.52e-02,
2024-10-08 22:35:31,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.26 vs. limit=8.63775
2024-10-08 22:35:35,465 WARNING [optim.py:503] Scaling gradients by 0.013523519970476627, model_norm_threshold=145790349606912.0
2024-10-08 22:35:35,622 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.991e+31, grad_sumsq=8.138e+33, orig_rms_sq=4.904e-03
2024-10-08 22:35:38,188 WARNING [optim.py:503] Scaling gradients by 0.006007109768688679, model_norm_threshold=145790349606912.0
2024-10-08 22:35:38,346 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.410e+32, grad_sumsq=6.519e+33, orig_rms_sq=2.164e-02
2024-10-08 22:35:40,989 WARNING [optim.py:503] Scaling gradients by 0.0015858925180509686, model_norm_threshold=145790349606912.0
2024-10-08 22:35:41,148 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.598e+33, grad_sumsq=8.256e+32, orig_rms_sq=3.147e+00
2024-10-08 22:35:47,516 WARNING [optim.py:503] Scaling gradients by 0.001023275894112885, model_norm_threshold=145790349606912.0
2024-10-08 22:35:47,674 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.505e+33, grad_sumsq=2.090e+35, orig_rms_sq=2.156e-02
2024-10-08 22:35:48,434 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=5.93 vs. limit=5.214933333333334
2024-10-08 22:35:48,865 WARNING [optim.py:503] Scaling gradients by 0.001526866341009736, model_norm_threshold=145790349606912.0
2024-10-08 22:35:49,023 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.303e+33, grad_sumsq=7.590e+32, orig_rms_sq=3.034e+00
2024-10-08 22:35:51,367 WARNING [optim.py:503] Scaling gradients by 0.004337497986853123, model_norm_threshold=145790349606912.0
2024-10-08 22:35:51,524 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.248e+32, grad_sumsq=1.048e+34, orig_rms_sq=2.145e-02
2024-10-08 22:35:53,784 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=9.46 vs. limit=8.639
2024-10-08 22:36:00,389 WARNING [optim.py:503] Scaling gradients by 0.019394880160689354, model_norm_threshold=145790349606912.0
2024-10-08 22:36:00,545 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.390e+31, grad_sumsq=2.711e+30, orig_rms_sq=5.126e+00
2024-10-08 22:36:03,629 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=25.12 vs. limit=8.64025
2024-10-08 22:36:03,815 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=26.70 vs. limit=8.64025
2024-10-08 22:36:03,923 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.99 vs. limit=8.64025
2024-10-08 22:36:04,247 WARNING [optim.py:503] Scaling gradients by 0.0050039575435221195, model_norm_threshold=145790349606912.0
2024-10-08 22:36:04,403 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.285e+32, grad_sumsq=4.397e+31, orig_rms_sq=5.197e+00
2024-10-08 22:36:05,844 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=1.372e+06
2024-10-08 22:36:06,211 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.14 vs. limit=6.520333333333333
2024-10-08 22:36:10,901 WARNING [optim.py:503] Scaling gradients by 0.020695563405752182, model_norm_threshold=145790349606912.0
2024-10-08 22:36:11,056 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.679e+30, grad_sumsq=4.447e+32, orig_rms_sq=2.177e-02
2024-10-08 22:36:12,286 WARNING [optim.py:503] Scaling gradients by 0.001179936109110713, model_norm_threshold=145790349606912.0
2024-10-08 22:36:12,443 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.13, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.001e+33, grad_sumsq=9.194e+34, orig_rms_sq=2.177e-02
2024-10-08 22:36:14,990 WARNING [optim.py:503] Scaling gradients by 0.0019037355668842793, model_norm_threshold=145790349606912.0
2024-10-08 22:36:15,144 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.719e+33, grad_sumsq=3.461e+35, orig_rms_sq=4.965e-03
2024-10-08 22:36:16,523 WARNING [optim.py:503] Scaling gradients by 0.022885696962475777, model_norm_threshold=145790349606912.0
2024-10-08 22:36:16,679 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.176e+30, grad_sumsq=1.451e+33, orig_rms_sq=4.945e-03
2024-10-08 22:36:18,134 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=3044.0, ans=0.24566000000000002
2024-10-08 22:36:18,578 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.91 vs. limit=8.6415
2024-10-08 22:36:20,354 WARNING [optim.py:503] Scaling gradients by 0.02468097023665905, model_norm_threshold=145790349606912.0
2024-10-08 22:36:20,511 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.091e+30, grad_sumsq=1.636e+33, orig_rms_sq=4.945e-03
2024-10-08 22:36:20,695 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=3044.0, ans=0.79346
2024-10-08 22:36:31,918 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.min_abs, batch_count=3047.3333333333335, ans=0.24571
2024-10-08 22:36:33,038 WARNING [optim.py:503] Scaling gradients by 0.082940474152565, model_norm_threshold=145790349606912.0
2024-10-08 22:36:33,193 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.313e+30, grad_sumsq=2.695e+32, orig_rms_sq=4.873e-03
2024-10-08 22:36:35,161 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=6.52 vs. limit=5.218933333333333
2024-10-08 22:36:36,830 INFO [train.py:1152] Epoch 2, batch 2300, loss[loss=0.7247, ctc_loss=0.9985, attn_decoder_loss=0.6563, over 4883.00 frames. ], tot_loss[loss=0.7572, ctc_loss=1.04, attn_decoder_loss=0.6865, over 968263.98 frames. ], batch size: 19, lr: 3.51e-02,
2024-10-08 22:36:41,796 WARNING [optim.py:503] Scaling gradients by 0.007191590033471584, model_norm_threshold=145790349606912.0
2024-10-08 22:36:41,951 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.134e+31, grad_sumsq=2.695e+33, orig_rms_sq=2.276e-02
2024-10-08 22:36:44,406 WARNING [optim.py:503] Scaling gradients by 4.307123163016513e-05, model_norm_threshold=145790349606912.0
2024-10-08 22:36:44,560 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.166e+36, grad_sumsq=3.575e+35, orig_rms_sq=8.857e+00
2024-10-08 22:36:45,379 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=24.97 vs. limit=8.644
2024-10-08 22:36:48,249 WARNING [optim.py:503] Scaling gradients by 0.0009261880768463016, model_norm_threshold=145790349606912.0
2024-10-08 22:36:48,403 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.233e+33, grad_sumsq=2.303e+35, orig_rms_sq=2.272e-02
2024-10-08 22:36:48,652 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer_ff2.min_abs, batch_count=3054.0, ans=0.07635000000000002
2024-10-08 22:36:49,617 WARNING [optim.py:503] Scaling gradients by 0.01855391450226307, model_norm_threshold=145790349606912.0
2024-10-08 22:36:49,771 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.488e+31, grad_sumsq=3.116e+33, orig_rms_sq=4.775e-03
2024-10-08 22:36:53,323 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.73 vs. limit=8.64525
2024-10-08 22:36:58,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=3054.0, ans=0.35684375
2024-10-08 22:37:01,072 WARNING [optim.py:503] Scaling gradients by 0.008708986453711987, model_norm_threshold=145790349606912.0
2024-10-08 22:37:01,230 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.245e+31, grad_sumsq=1.122e+34, orig_rms_sq=4.676e-03
2024-10-08 22:37:01,415 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=3057.3333333333335, ans=0.3566875
2024-10-08 22:37:04,956 WARNING [optim.py:503] Scaling gradients by 0.020667266100645065, model_norm_threshold=145790349606912.0
2024-10-08 22:37:05,110 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.696e+31, grad_sumsq=5.249e+30, orig_rms_sq=3.232e+00
2024-10-08 22:37:10,148 WARNING [optim.py:503] Scaling gradients by 0.02803046628832817, model_norm_threshold=145790349606912.0
2024-10-08 22:37:10,303 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.163e+30, grad_sumsq=3.962e+32, orig_rms_sq=2.313e-02
2024-10-08 22:37:12,832 WARNING [optim.py:503] Scaling gradients by 0.01977812871336937, model_norm_threshold=145790349606912.0
2024-10-08 22:37:12,987 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.340e+31, grad_sumsq=5.791e+32, orig_rms_sq=2.314e-02
2024-10-08 22:37:15,447 WARNING [optim.py:503] Scaling gradients by 0.0003270468150731176, model_norm_threshold=145790349606912.0
2024-10-08 22:37:15,603 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.196e+34, grad_sumsq=1.381e+36, orig_rms_sq=2.314e-02
2024-10-08 22:37:16,856 WARNING [optim.py:503] Scaling gradients by 0.057408787310123444, model_norm_threshold=145790349606912.0
2024-10-08 22:37:17,009 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.262e+30, grad_sumsq=7.335e+29, orig_rms_sq=3.083e+00
2024-10-08 22:37:18,163 WARNING [optim.py:503] Scaling gradients by 0.004208664875477552, model_norm_threshold=145790349606912.0
2024-10-08 22:37:18,317 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.843e+32, grad_sumsq=1.680e+34, orig_rms_sq=2.288e-02
2024-10-08 22:37:21,998 WARNING [optim.py:503] Scaling gradients by 0.011477846652269363, model_norm_threshold=145790349606912.0
2024-10-08 22:37:22,155 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.289e+31, grad_sumsq=1.874e+33, orig_rms_sq=2.288e-02
2024-10-08 22:37:27,059 WARNING [optim.py:503] Scaling gradients by 0.0006175026064738631, model_norm_threshold=145790349606912.0
2024-10-08 22:37:27,215 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.122e+34, grad_sumsq=3.739e+33, orig_rms_sq=3.001e+00
2024-10-08 22:37:31,002 WARNING [optim.py:503] Scaling gradients by 0.005531954579055309, model_norm_threshold=145790349606912.0
2024-10-08 22:37:31,157 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.872e+32, grad_sumsq=8.227e+33, orig_rms_sq=2.276e-02
2024-10-08 22:37:34,979 WARNING [optim.py:503] Scaling gradients by 0.006101903039962053, model_norm_threshold=145790349606912.0
2024-10-08 22:37:35,136 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.388e+32, grad_sumsq=6.163e+33, orig_rms_sq=2.252e-02
2024-10-08 22:37:35,388 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=3064.0, ans=0.07765
2024-10-08 22:37:39,432 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.492e+11 2.980e+13 2.368e+14 7.054e+15 3.385e+18, threshold=4.735e+14, percent-clipped=55.0
2024-10-08 22:37:42,103 INFO [train.py:1152] Epoch 2, batch 2350, loss[loss=0.7614, ctc_loss=1.101, attn_decoder_loss=0.6764, over 4844.00 frames. ], tot_loss[loss=0.7549, ctc_loss=1.038, attn_decoder_loss=0.6842, over 968291.76 frames. ], batch size: 23, lr: 3.51e-02,
2024-10-08 22:37:46,886 WARNING [optim.py:503] Scaling gradients by 0.022763332352042198, model_norm_threshold=473548900532224.0
2024-10-08 22:37:47,041 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.082e+31, grad_sumsq=1.518e+34, orig_rms_sq=4.666e-03
2024-10-08 22:37:52,241 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=3067.3333333333335, ans=0.07746249999999999
2024-10-08 22:37:53,264 WARNING [optim.py:503] Scaling gradients by 0.022547757253050804, model_norm_threshold=473548900532224.0
2024-10-08 22:37:53,419 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.458e+31, grad_sumsq=3.865e+33, orig_rms_sq=2.188e-02
2024-10-08 22:37:56,128 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=3070.6666666666665, ans=0.08485000000000001
2024-10-08 22:37:59,317 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.10 vs. limit=6.535333333333333
2024-10-08 22:38:01,112 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=3070.6666666666665, ans=0.3560625
2024-10-08 22:38:02,121 WARNING [optim.py:503] Scaling gradients by 0.019866419956088066, model_norm_threshold=473548900532224.0
2024-10-08 22:38:02,277 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.581e+32, grad_sumsq=7.060e+33, orig_rms_sq=2.239e-02
2024-10-08 22:38:09,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.46 vs. limit=9.8055
2024-10-08 22:38:11,645 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=38.43 vs. limit=9.8055
2024-10-08 22:38:12,178 WARNING [optim.py:503] Scaling gradients by 0.015099620446562767, model_norm_threshold=473548900532224.0
2024-10-08 22:38:12,335 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.657e+32, grad_sumsq=5.802e+34, orig_rms_sq=4.579e-03
2024-10-08 22:38:16,010 WARNING [optim.py:503] Scaling gradients by 0.04360206797719002, model_norm_threshold=473548900532224.0
2024-10-08 22:38:16,166 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.467e+31, grad_sumsq=5.513e+33, orig_rms_sq=4.475e-03
2024-10-08 22:38:20,097 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=3077.3333333333335, ans=0.35575
2024-10-08 22:38:25,339 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=8.05 vs. limit=5.230933333333334
2024-10-08 22:38:26,690 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.11 vs. limit=8.654
2024-10-08 22:38:28,577 WARNING [optim.py:503] Scaling gradients by 0.004668954759836197, model_norm_threshold=473548900532224.0
2024-10-08 22:38:28,734 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.373e+33, grad_sumsq=4.975e+32, orig_rms_sq=4.770e+00
2024-10-08 22:38:31,077 WARNING [optim.py:503] Scaling gradients by 0.005345251876860857, model_norm_threshold=473548900532224.0
2024-10-08 22:38:31,232 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.204e+33, grad_sumsq=6.998e+35, orig_rms_sq=4.578e-03
2024-10-08 22:38:33,615 WARNING [optim.py:503] Scaling gradients by 0.013119676150381565, model_norm_threshold=473548900532224.0
2024-10-08 22:38:33,771 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.293e+32, grad_sumsq=1.387e+34, orig_rms_sq=2.375e-02
2024-10-08 22:38:35,837 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=10.19 vs. limit=5.770166666666666
2024-10-08 22:38:36,792 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=36.99 vs. limit=8.65525
2024-10-08 22:38:40,791 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=16.22 vs. limit=9.810500000000001
2024-10-08 22:38:44,785 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=7.12 vs. limit=4.6168
2024-10-08 22:38:44,976 INFO [train.py:1152] Epoch 2, batch 2400, loss[loss=0.7471, ctc_loss=1.056, attn_decoder_loss=0.6698, over 4755.00 frames. ], tot_loss[loss=0.7555, ctc_loss=1.039, attn_decoder_loss=0.6847, over 967510.32 frames. ], batch size: 19, lr: 3.50e-02,
2024-10-08 22:38:48,331 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=14.18 vs. limit=8.6565
2024-10-08 22:38:53,453 WARNING [optim.py:503] Scaling gradients by 0.006899772211909294, model_norm_threshold=473548900532224.0
2024-10-08 22:38:53,611 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.210e+33, grad_sumsq=9.172e+34, orig_rms_sq=2.409e-02
2024-10-08 22:38:55,649 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.42 vs. limit=8.6565
2024-10-08 22:38:56,986 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=10.57 vs. limit=8.65775
2024-10-08 22:39:01,102 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=3087.3333333333335, ans=0.04035208333333334
2024-10-08 22:39:03,334 WARNING [optim.py:503] Scaling gradients by 0.012694229371845722, model_norm_threshold=473548900532224.0
2024-10-08 22:39:03,489 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.165e+32, grad_sumsq=8.988e+34, orig_rms_sq=4.634e-03
2024-10-08 22:39:12,761 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=10.74 vs. limit=8.659
2024-10-08 22:39:15,388 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=36.09 vs. limit=8.659
2024-10-08 22:39:17,399 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=3090.6666666666665, ans=0.03046
2024-10-08 22:39:22,669 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=25.43 vs. limit=9.8205
2024-10-08 22:39:25,763 WARNING [optim.py:503] Scaling gradients by 0.06963613629341125, model_norm_threshold=473548900532224.0
2024-10-08 22:39:25,919 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.903e+30, grad_sumsq=1.992e+30, orig_rms_sq=4.970e+00
2024-10-08 22:39:30,864 WARNING [optim.py:503] Scaling gradients by 0.043970901519060135, model_norm_threshold=473548900532224.0
2024-10-08 22:39:31,021 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.472e+31, grad_sumsq=1.090e+33, orig_rms_sq=2.268e-02
2024-10-08 22:39:34,421 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.79 vs. limit=9.823
2024-10-08 22:39:37,953 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=23.10 vs. limit=9.823
2024-10-08 22:39:39,680 WARNING [optim.py:503] Scaling gradients by 0.0006869516801089048, model_norm_threshold=473548900532224.0
2024-10-08 22:39:39,838 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.289e+35, grad_sumsq=5.713e+36, orig_rms_sq=2.257e-02
2024-10-08 22:39:44,227 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.82 vs. limit=6.548666666666667
2024-10-08 22:39:44,828 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.829e+11 1.432e+13 1.196e+14 1.384e+15 6.893e+17, threshold=2.391e+14, percent-clipped=33.0
2024-10-08 22:39:47,359 INFO [train.py:1152] Epoch 2, batch 2450, loss[loss=0.7485, ctc_loss=1.032, attn_decoder_loss=0.6776, over 4886.00 frames. ], tot_loss[loss=0.7578, ctc_loss=1.042, attn_decoder_loss=0.6866, over 966929.13 frames. ], batch size: 22, lr: 3.50e-02,
2024-10-08 22:39:53,332 WARNING [optim.py:503] Scaling gradients by 0.006343245040625334, model_norm_threshold=239109234753536.0
2024-10-08 22:39:53,490 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.744e+32, grad_sumsq=2.537e+34, orig_rms_sq=2.264e-02
2024-10-08 22:39:54,729 WARNING [optim.py:503] Scaling gradients by 0.004655646160244942, model_norm_threshold=239109234753536.0
2024-10-08 22:39:54,899 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.361e+32, grad_sumsq=1.720e+35, orig_rms_sq=4.862e-03
2024-10-08 22:39:55,969 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.83 vs. limit=6.550333333333333
2024-10-08 22:39:57,304 WARNING [optim.py:503] Scaling gradients by 0.05386921390891075, model_norm_threshold=239109234753536.0
2024-10-08 22:39:57,460 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.766e+30, grad_sumsq=2.096e+32, orig_rms_sq=2.274e-02
2024-10-08 22:40:06,912 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=22.28 vs. limit=9.828
2024-10-08 22:40:07,404 WARNING [optim.py:503] Scaling gradients by 0.003255615709349513, model_norm_threshold=239109234753536.0
2024-10-08 22:40:07,560 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.630e+33, grad_sumsq=7.193e+34, orig_rms_sq=2.266e-02
2024-10-08 22:40:17,782 WARNING [optim.py:503] Scaling gradients by 0.07691697031259537, model_norm_threshold=239109234753536.0
2024-10-08 22:40:17,939 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.933e+30, grad_sumsq=4.012e+32, orig_rms_sq=4.817e-03
2024-10-08 22:40:24,826 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=30.75 vs. limit=9.833
2024-10-08 22:40:28,904 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=4.65 vs. limit=5.244266666666666
2024-10-08 22:40:29,122 WARNING [optim.py:503] Scaling gradients by 0.014977214857935905, model_norm_threshold=239109234753536.0
2024-10-08 22:40:29,280 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.189e+31, grad_sumsq=1.268e+34, orig_rms_sq=4.879e-03
2024-10-08 22:40:32,929 WARNING [optim.py:503] Scaling gradients by 0.007857377640902996, model_norm_threshold=239109234753536.0
2024-10-08 22:40:33,085 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.487e+32, grad_sumsq=6.437e+33, orig_rms_sq=2.310e-02
2024-10-08 22:40:34,250 WARNING [optim.py:503] Scaling gradients by 9.309581218985841e-05, model_norm_threshold=239109234753536.0
2024-10-08 22:40:34,405 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.440e+36, grad_sumsq=2.949e+38, orig_rms_sq=4.883e-03
2024-10-08 22:40:39,026 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=16.65 vs. limit=8.66775
2024-10-08 22:40:39,741 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=3114.0, ans=0.11075000000000002
2024-10-08 22:40:50,987 INFO [train.py:1152] Epoch 2, batch 2500, loss[loss=0.7444, ctc_loss=1.021, attn_decoder_loss=0.6753, over 4741.00 frames. ], tot_loss[loss=0.757, ctc_loss=1.042, attn_decoder_loss=0.6859, over 966616.28 frames. ], batch size: 26, lr: 3.49e-02,
2024-10-08 22:40:52,120 WARNING [optim.py:503] Scaling gradients by 0.011938469484448433, model_norm_threshold=239109234753536.0
2024-10-08 22:40:52,276 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.090e+31, grad_sumsq=1.670e+34, orig_rms_sq=4.844e-03
2024-10-08 22:40:53,449 WARNING [optim.py:503] Scaling gradients by 0.07354377955198288, model_norm_threshold=239109234753536.0
2024-10-08 22:40:53,605 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.247e+30, grad_sumsq=9.873e+31, orig_rms_sq=2.276e-02
2024-10-08 22:40:57,007 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.09 vs. limit=9.838000000000001
2024-10-08 22:40:58,594 WARNING [optim.py:503] Scaling gradients by 0.0320671945810318, model_norm_threshold=239109234753536.0
2024-10-08 22:40:58,749 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.127e+31, grad_sumsq=4.983e+32, orig_rms_sq=2.261e-02
2024-10-08 22:41:03,763 WARNING [optim.py:503] Scaling gradients by 0.09123130142688751, model_norm_threshold=239109234753536.0
2024-10-08 22:41:03,917 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.557e+30, grad_sumsq=6.891e+31, orig_rms_sq=2.259e-02
2024-10-08 22:41:07,531 WARNING [optim.py:503] Scaling gradients by 0.0002194805711042136, model_norm_threshold=239109234753536.0
2024-10-08 22:41:07,687 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.470e+35, grad_sumsq=1.536e+37, orig_rms_sq=2.259e-02
2024-10-08 22:41:08,847 WARNING [optim.py:503] Scaling gradients by 0.0014037508517503738, model_norm_threshold=239109234753536.0
2024-10-08 22:41:09,003 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.120e+33, grad_sumsq=4.037e+35, orig_rms_sq=2.259e-02
2024-10-08 22:41:10,202 WARNING [optim.py:503] Scaling gradients by 0.0357239730656147, model_norm_threshold=239109234753536.0
2024-10-08 22:41:10,359 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.262e+31, grad_sumsq=2.640e+33, orig_rms_sq=4.780e-03
2024-10-08 22:41:10,612 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=3120.6666666666665, ans=0.35371874999999997
2024-10-08 22:41:12,854 WARNING [optim.py:503] Scaling gradients by 0.0014246944338083267, model_norm_threshold=239109234753536.0
2024-10-08 22:41:13,012 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.912e+33, grad_sumsq=3.048e+35, orig_rms_sq=2.268e-02
2024-10-08 22:41:15,291 WARNING [optim.py:503] Scaling gradients by 0.08031580597162247, model_norm_threshold=239109234753536.0
2024-10-08 22:41:15,453 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.889e+30, grad_sumsq=2.161e+29, orig_rms_sq=8.742e+00
2024-10-08 22:41:17,949 WARNING [optim.py:503] Scaling gradients by 0.015437916852533817, model_norm_threshold=239109234753536.0
2024-10-08 22:41:18,117 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.713e+31, grad_sumsq=2.947e+33, orig_rms_sq=2.278e-02
2024-10-08 22:41:21,987 WARNING [optim.py:503] Scaling gradients by 0.07801108807325363, model_norm_threshold=239109234753536.0
2024-10-08 22:41:22,147 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.435e+30, grad_sumsq=1.063e+32, orig_rms_sq=2.290e-02
2024-10-08 22:41:23,552 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=3124.0, ans=0.07427500000000001
2024-10-08 22:41:25,889 WARNING [optim.py:503] Scaling gradients by 0.001659075845964253, model_norm_threshold=239109234753536.0
2024-10-08 22:41:26,044 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.563e+33, grad_sumsq=2.429e+35, orig_rms_sq=2.290e-02
2024-10-08 22:41:27,216 WARNING [optim.py:503] Scaling gradients by 0.005788581911474466, model_norm_threshold=239109234753536.0
2024-10-08 22:41:27,372 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.500e+32, grad_sumsq=1.526e+34, orig_rms_sq=2.294e-02
2024-10-08 22:41:28,514 WARNING [optim.py:503] Scaling gradients by 0.006280095782130957, model_norm_threshold=239109234753536.0
2024-10-08 22:41:28,671 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.354e+32, grad_sumsq=1.462e+34, orig_rms_sq=2.294e-02
2024-10-08 22:41:34,520 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.99 vs. limit=6.563666666666666
2024-10-08 22:41:34,839 WARNING [optim.py:503] Scaling gradients by 0.08088941872119904, model_norm_threshold=239109234753536.0
2024-10-08 22:41:35,001 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.200e+30, grad_sumsq=1.390e+32, orig_rms_sq=2.302e-02
2024-10-08 22:41:37,469 WARNING [optim.py:503] Scaling gradients by 0.009426047094166279, model_norm_threshold=239109234753536.0
2024-10-08 22:41:37,624 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.088e+32, grad_sumsq=8.938e+33, orig_rms_sq=2.336e-02
2024-10-08 22:41:42,548 WARNING [optim.py:503] Scaling gradients by 0.06156713515520096, model_norm_threshold=239109234753536.0
2024-10-08 22:41:42,703 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.972e+30, grad_sumsq=2.124e+32, orig_rms_sq=2.340e-02
2024-10-08 22:41:47,596 WARNING [optim.py:503] Scaling gradients by 0.006143060978502035, model_norm_threshold=239109234753536.0
2024-10-08 22:41:47,754 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.105e+32, grad_sumsq=1.768e+34, orig_rms_sq=2.322e-02
2024-10-08 22:41:48,008 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=3130.6666666666665, ans=0.35325
2024-10-08 22:41:51,396 WARNING [optim.py:503] Scaling gradients by 0.0009269576985388994, model_norm_threshold=239109234753536.0
2024-10-08 22:41:51,552 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.735e+34, grad_sumsq=7.473e+35, orig_rms_sq=2.322e-02
2024-10-08 22:41:53,083 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.519e+11 1.190e+13 1.620e+14 2.977e+15 2.568e+18, threshold=3.240e+14, percent-clipped=49.0
2024-10-08 22:41:55,205 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=26.97 vs. limit=9.8505
2024-10-08 22:41:55,632 WARNING [optim.py:503] Scaling gradients by 0.0017054093768820167, model_norm_threshold=324033505656832.0
2024-10-08 22:41:55,790 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.662e+33, grad_sumsq=1.075e+33, orig_rms_sq=8.991e+00
2024-10-08 22:41:55,849 INFO [train.py:1152] Epoch 2, batch 2550, loss[loss=0.7375, ctc_loss=1.002, attn_decoder_loss=0.6714, over 4959.00 frames. ], tot_loss[loss=0.7575, ctc_loss=1.042, attn_decoder_loss=0.6863, over 967028.66 frames. ], batch size: 19, lr: 3.48e-02,
2024-10-08 22:42:02,643 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.30 vs. limit=6.567
2024-10-08 22:42:04,600 WARNING [optim.py:503] Scaling gradients by 0.0225625392049551, model_norm_threshold=324033505656832.0
2024-10-08 22:42:04,755 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.898e+31, grad_sumsq=2.955e+33, orig_rms_sq=2.334e-02
2024-10-08 22:42:08,426 WARNING [optim.py:503] Scaling gradients by 0.000806185242254287, model_norm_threshold=324033505656832.0
2024-10-08 22:42:08,586 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.702e+34, grad_sumsq=9.569e+36, orig_rms_sq=4.914e-03
2024-10-08 22:42:09,789 WARNING [optim.py:503] Scaling gradients by 0.000252286612521857, model_norm_threshold=324033505656832.0
2024-10-08 22:42:09,945 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.789e+35, grad_sumsq=2.886e+37, orig_rms_sq=2.353e-02
2024-10-08 22:42:12,355 WARNING [optim.py:503] Scaling gradients by 0.047402940690517426, model_norm_threshold=324033505656832.0
2024-10-08 22:42:12,509 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.799e+31, grad_sumsq=7.646e+32, orig_rms_sq=2.353e-02
2024-10-08 22:42:19,961 WARNING [optim.py:503] Scaling gradients by 0.012503211386501789, model_norm_threshold=324033505656832.0
2024-10-08 22:42:20,117 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.989e+32, grad_sumsq=6.169e+34, orig_rms_sq=4.846e-03
2024-10-08 22:42:21,359 WARNING [optim.py:503] Scaling gradients by 0.06591825187206268, model_norm_threshold=324033505656832.0
2024-10-08 22:42:21,514 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.627e+30, grad_sumsq=2.382e+32, orig_rms_sq=2.362e-02
2024-10-08 22:42:27,749 WARNING [optim.py:503] Scaling gradients by 0.004062999039888382, model_norm_threshold=324033505656832.0
2024-10-08 22:42:27,906 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.941e+33, grad_sumsq=8.245e+34, orig_rms_sq=2.354e-02
2024-10-08 22:42:29,051 WARNING [optim.py:503] Scaling gradients by 0.005465009715408087, model_norm_threshold=324033505656832.0
2024-10-08 22:42:29,208 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.411e+32, grad_sumsq=1.765e+35, orig_rms_sq=4.766e-03
2024-10-08 22:42:30,414 WARNING [optim.py:503] Scaling gradients by 0.00900357123464346, model_norm_threshold=324033505656832.0
2024-10-08 22:42:30,573 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.568e+32, grad_sumsq=7.511e+34, orig_rms_sq=4.750e-03
2024-10-08 22:42:31,507 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=6.39 vs. limit=5.256266666666667
2024-10-08 22:42:33,665 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=26.83 vs. limit=9.8555
2024-10-08 22:42:35,933 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=3144.0, ans=0.78996
2024-10-08 22:42:39,515 WARNING [optim.py:503] Scaling gradients by 0.000596327125094831, model_norm_threshold=324033505656832.0
2024-10-08 22:42:39,671 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.028e+35, grad_sumsq=4.371e+36, orig_rms_sq=2.351e-02
2024-10-08 22:42:42,368 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.min_positive, batch_count=3144.0, ans=0.040175
2024-10-08 22:42:46,075 WARNING [optim.py:503] Scaling gradients by 0.014164894819259644, model_norm_threshold=324033505656832.0
2024-10-08 22:42:46,230 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.134e+32, grad_sumsq=4.809e+33, orig_rms_sq=2.357e-02
2024-10-08 22:42:46,454 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3144.0, ans=0.26856
2024-10-08 22:42:48,004 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.44 vs. limit=5.258933333333333
2024-10-08 22:42:50,568 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=24.02 vs. limit=9.8605
2024-10-08 22:42:50,763 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=5.89 vs. limit=5.258933333333333
2024-10-08 22:42:51,659 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.50 vs. limit=3.4721
2024-10-08 22:42:53,571 WARNING [optim.py:503] Scaling gradients by 0.03723982349038124, model_norm_threshold=324033505656832.0
2024-10-08 22:42:53,727 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.675e+31, grad_sumsq=7.035e+32, orig_rms_sq=2.381e-02
2024-10-08 22:42:53,985 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=3147.3333333333335, ans=0.5
2024-10-08 22:42:58,643 WARNING [optim.py:503] Scaling gradients by 0.0033979120198637247, model_norm_threshold=324033505656832.0
2024-10-08 22:42:58,800 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.240e+33, grad_sumsq=4.952e+35, orig_rms_sq=4.523e-03
2024-10-08 22:43:00,307 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=3150.6666666666665, ans=0.08185
2024-10-08 22:43:01,521 INFO [train.py:1152] Epoch 2, batch 2600, loss[loss=0.7944, ctc_loss=1.071, attn_decoder_loss=0.7252, over 4852.00 frames. ], tot_loss[loss=0.7573, ctc_loss=1.042, attn_decoder_loss=0.6862, over 966506.11 frames. ], batch size: 20, lr: 3.48e-02,
2024-10-08 22:43:03,937 WARNING [optim.py:503] Scaling gradients by 0.030270997434854507, model_norm_threshold=324033505656832.0
2024-10-08 22:43:04,092 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.481e+31, grad_sumsq=5.414e+33, orig_rms_sq=4.584e-03
2024-10-08 22:43:07,717 WARNING [optim.py:503] Scaling gradients by 0.0075437636114656925, model_norm_threshold=324033505656832.0
2024-10-08 22:43:07,871 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.285e+32, grad_sumsq=1.149e+35, orig_rms_sq=4.601e-03
2024-10-08 22:43:10,982 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=79.67 vs. limit=8.6815
2024-10-08 22:43:11,116 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.52 vs. limit=9.863
2024-10-08 22:43:12,701 WARNING [optim.py:503] Scaling gradients by 0.0029100680258125067, model_norm_threshold=324033505656832.0
2024-10-08 22:43:12,857 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.557e+33, grad_sumsq=1.854e+35, orig_rms_sq=2.458e-02
2024-10-08 22:43:14,055 WARNING [optim.py:503] Scaling gradients by 0.0026329096872359514, model_norm_threshold=324033505656832.0
2024-10-08 22:43:14,210 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.387e+33, grad_sumsq=1.378e+35, orig_rms_sq=2.458e-02
2024-10-08 22:43:16,668 WARNING [optim.py:503] Scaling gradients by 0.015049776993691921, model_norm_threshold=324033505656832.0
2024-10-08 22:43:16,823 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.377e+32, grad_sumsq=5.568e+33, orig_rms_sq=2.473e-02
2024-10-08 22:43:18,039 WARNING [optim.py:503] Scaling gradients by 0.00033099332358688116, model_norm_threshold=324033505656832.0
2024-10-08 22:43:18,195 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.849e+35, grad_sumsq=7.479e+36, orig_rms_sq=2.473e-02
2024-10-08 22:43:19,400 WARNING [optim.py:503] Scaling gradients by 0.0003546230145730078, model_norm_threshold=324033505656832.0
2024-10-08 22:43:19,556 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.820e+35, grad_sumsq=1.141e+37, orig_rms_sq=2.473e-02
2024-10-08 22:43:20,452 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=24.59 vs. limit=9.8655
2024-10-08 22:43:22,765 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=12.72 vs. limit=5.7885
2024-10-08 22:43:24,481 WARNING [optim.py:503] Scaling gradients by 0.019951676949858665, model_norm_threshold=324033505656832.0
2024-10-08 22:43:24,637 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.909e+31, grad_sumsq=9.880e+30, orig_rms_sq=4.968e+00
2024-10-08 22:43:28,490 WARNING [optim.py:503] Scaling gradients by 0.0017399350181221962, model_norm_threshold=324033505656832.0
2024-10-08 22:43:28,646 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.128e+34, grad_sumsq=4.623e+35, orig_rms_sq=2.440e-02
2024-10-08 22:43:35,930 WARNING [optim.py:503] Scaling gradients by 0.0012208219850435853, model_norm_threshold=324033505656832.0
2024-10-08 22:43:36,085 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.009e+34, grad_sumsq=8.323e+35, orig_rms_sq=2.413e-02
2024-10-08 22:43:36,263 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=3157.3333333333335, ans=0.352
2024-10-08 22:43:36,998 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=24.77 vs. limit=8.684
2024-10-08 22:43:40,155 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=3160.6666666666665, ans=0.081475
2024-10-08 22:43:41,718 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=35.53 vs. limit=9.8705
2024-10-08 22:43:42,322 WARNING [optim.py:503] Scaling gradients by 0.08013470470905304, model_norm_threshold=324033505656832.0
2024-10-08 22:43:42,477 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.743e+30, grad_sumsq=3.033e+29, orig_rms_sq=9.043e+00
2024-10-08 22:43:44,572 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=18.40 vs. limit=8.68525
2024-10-08 22:43:51,296 WARNING [optim.py:503] Scaling gradients by 0.052528589963912964, model_norm_threshold=324033505656832.0
2024-10-08 22:43:51,452 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.059e+30, grad_sumsq=8.921e+29, orig_rms_sq=9.033e+00
2024-10-08 22:43:52,680 WARNING [optim.py:503] Scaling gradients by 0.06727227568626404, model_norm_threshold=324033505656832.0
2024-10-08 22:43:52,836 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.907e+30, grad_sumsq=3.807e+32, orig_rms_sq=2.340e-02
2024-10-08 22:43:55,359 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=3164.0, ans=0.3516875
2024-10-08 22:43:56,356 WARNING [optim.py:503] Scaling gradients by 1.8060442016576417e-05, model_norm_threshold=324033505656832.0
2024-10-08 22:43:56,512 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.007e+38, grad_sumsq=inf, orig_rms_sq=2.340e-02
2024-10-08 22:44:01,576 WARNING [optim.py:503] Scaling gradients by 0.09897802770137787, model_norm_threshold=324033505656832.0
2024-10-08 22:44:01,732 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.831e+30, grad_sumsq=1.052e+31, orig_rms_sq=2.690e-01
2024-10-08 22:44:03,002 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.226e+12 2.895e+13 3.270e+14 6.169e+15 1.794e+19, threshold=6.541e+14, percent-clipped=50.0
2024-10-08 22:44:03,527 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.88 vs. limit=8.6865
2024-10-08 22:44:05,487 WARNING [optim.py:503] Scaling gradients by 0.004169158637523651, model_norm_threshold=654073925730304.0
2024-10-08 22:44:05,643 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.032e+34, grad_sumsq=4.384e+35, orig_rms_sq=2.354e-02
2024-10-08 22:44:05,702 INFO [train.py:1152] Epoch 2, batch 2650, loss[loss=0.7692, ctc_loss=1.089, attn_decoder_loss=0.6893, over 4824.00 frames. ], tot_loss[loss=0.7559, ctc_loss=1.04, attn_decoder_loss=0.6848, over 966162.46 frames. ], batch size: 38, lr: 3.47e-02,
2024-10-08 22:44:09,678 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=3167.3333333333335, ans=0.028734999999999997
2024-10-08 22:44:11,273 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=18.20 vs. limit=8.68775
2024-10-08 22:44:20,945 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=3170.6666666666665, ans=0.351375
2024-10-08 22:44:21,444 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=12.23 vs. limit=5.792666666666666
2024-10-08 22:44:27,602 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=14.43 vs. limit=5.792666666666666
2024-10-08 22:44:29,109 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.39 vs. limit=9.878
2024-10-08 22:44:33,065 WARNING [optim.py:503] Scaling gradients by 0.08345984667539597, model_norm_threshold=654073925730304.0
2024-10-08 22:44:33,221 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.459e+31, grad_sumsq=6.622e+32, orig_rms_sq=2.203e-02
2024-10-08 22:44:37,155 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=3174.0, ans=0.35121875
2024-10-08 22:44:39,421 WARNING [optim.py:503] Scaling gradients by 0.057361312210559845, model_norm_threshold=654073925730304.0
2024-10-08 22:44:39,577 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.300e+31, grad_sumsq=9.498e+30, orig_rms_sq=4.527e+00
2024-10-08 22:44:40,724 WARNING [optim.py:503] Scaling gradients by 0.006723814643919468, model_norm_threshold=654073925730304.0
2024-10-08 22:44:40,881 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.147e+33, grad_sumsq=9.724e+34, orig_rms_sq=2.208e-02
2024-10-08 22:44:53,591 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.max_abs, batch_count=3177.3333333333335, ans=6.985833333333334
2024-10-08 22:44:53,910 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.70 vs. limit=9.883
2024-10-08 22:44:56,595 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.whiten.whitening_limit, batch_count=3180.6666666666665, ans=5.272266666666667
2024-10-08 22:44:56,778 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=3180.6666666666665, ans=9.8855
2024-10-08 22:44:57,787 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.24 vs. limit=9.8855
2024-10-08 22:44:59,060 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=36.77 vs. limit=8.69275
2024-10-08 22:45:00,695 WARNING [optim.py:503] Scaling gradients by 0.003333989530801773, model_norm_threshold=654073925730304.0
2024-10-08 22:45:00,850 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.510e+33, grad_sumsq=1.952e+36, orig_rms_sq=4.361e-03
2024-10-08 22:45:08,174 WARNING [optim.py:503] Scaling gradients by 0.003511143149808049, model_norm_threshold=654073925730304.0
2024-10-08 22:45:08,332 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.273e+33, grad_sumsq=3.791e+35, orig_rms_sq=2.182e-02
2024-10-08 22:45:08,390 INFO [train.py:1152] Epoch 2, batch 2700, loss[loss=0.7025, ctc_loss=1.012, attn_decoder_loss=0.6251, over 4865.00 frames. ], tot_loss[loss=0.7563, ctc_loss=1.041, attn_decoder_loss=0.6851, over 966280.62 frames. ], batch size: 28, lr: 3.47e-02,
2024-10-08 22:45:16,444 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=40.88 vs. limit=9.888
2024-10-08 22:45:17,397 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=3184.0, ans=0.78856
2024-10-08 22:45:22,360 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3187.3333333333335, ans=0.2681266666666666
2024-10-08 22:45:25,013 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=3187.3333333333335, ans=0.2681266666666666
2024-10-08 22:45:26,281 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=3187.3333333333335, ans=0.35059375000000004
2024-10-08 22:45:27,315 WARNING [optim.py:503] Scaling gradients by 0.008054181933403015, model_norm_threshold=654073925730304.0
2024-10-08 22:45:27,470 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.536e+33, grad_sumsq=1.169e+35, orig_rms_sq=2.169e-02
2024-10-08 22:45:28,688 WARNING [optim.py:503] Scaling gradients by 0.02395303174853325, model_norm_threshold=654073925730304.0
2024-10-08 22:45:28,845 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.335e+32, grad_sumsq=1.083e+34, orig_rms_sq=2.156e-02
2024-10-08 22:45:40,566 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.54 vs. limit=8.6965
2024-10-08 22:45:49,938 WARNING [optim.py:503] Scaling gradients by 0.003622522810474038, model_norm_threshold=654073925730304.0
2024-10-08 22:45:50,097 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.452e+33, grad_sumsq=2.971e+35, orig_rms_sq=2.171e-02
2024-10-08 22:45:51,959 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=11.37 vs. limit=5.7985
2024-10-08 22:45:53,193 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=29.84 vs. limit=8.69775
2024-10-08 22:45:56,228 WARNING [optim.py:503] Scaling gradients by 0.08215362578630447, model_norm_threshold=654073925730304.0
2024-10-08 22:45:56,384 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.544e+31, grad_sumsq=7.043e+32, orig_rms_sq=2.193e-02
2024-10-08 22:45:57,513 WARNING [optim.py:503] Scaling gradients by 0.07001585513353348, model_norm_threshold=654073925730304.0
2024-10-08 22:45:57,668 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.697e+31, grad_sumsq=7.741e+32, orig_rms_sq=2.193e-02
2024-10-08 22:46:04,114 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=3197.3333333333335, ans=0.350125
2024-10-08 22:46:06,228 WARNING [optim.py:503] Scaling gradients by 0.0418969951570034, model_norm_threshold=654073925730304.0
2024-10-08 22:46:06,385 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.532e+31, grad_sumsq=3.410e+33, orig_rms_sq=2.209e-02
2024-10-08 22:46:09,251 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.034e+12 2.167e+13 1.350e+14 1.808e+15 1.962e+17, threshold=2.699e+14, percent-clipped=34.0
2024-10-08 22:46:11,716 WARNING [optim.py:503] Scaling gradients by 0.08192600309848785, model_norm_threshold=269925306335232.0
2024-10-08 22:46:11,870 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.115e+30, grad_sumsq=2.328e+29, orig_rms_sq=9.085e+00
2024-10-08 22:46:11,928 INFO [train.py:1152] Epoch 2, batch 2750, loss[loss=0.7684, ctc_loss=1.043, attn_decoder_loss=0.6997, over 4800.00 frames. ], tot_loss[loss=0.7558, ctc_loss=1.04, attn_decoder_loss=0.6847, over 966928.59 frames. ], batch size: 19, lr: 3.46e-02,
2024-10-08 22:46:12,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=3200.6666666666665, ans=0.03999791666666667
2024-10-08 22:46:13,387 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=3200.6666666666665, ans=0.34996875
2024-10-08 22:46:14,420 WARNING [optim.py:503] Scaling gradients by 0.0014612526865676045, model_norm_threshold=269925306335232.0
2024-10-08 22:46:14,576 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.799e+33, grad_sumsq=4.463e+35, orig_rms_sq=2.196e-02
2024-10-08 22:46:18,011 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=6.23 vs. limit=5.280266666666667
2024-10-08 22:46:18,383 WARNING [optim.py:503] Scaling gradients by 0.0022103902883827686, model_norm_threshold=269925306335232.0
2024-10-08 22:46:18,538 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.549e+33, grad_sumsq=1.161e+35, orig_rms_sq=2.196e-02
2024-10-08 22:46:22,042 WARNING [optim.py:503] Scaling gradients by 0.023736367002129555, model_norm_threshold=269925306335232.0
2024-10-08 22:46:22,198 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.648e+31, grad_sumsq=5.921e+33, orig_rms_sq=4.472e-03
2024-10-08 22:46:24,632 WARNING [optim.py:503] Scaling gradients by 0.07617531716823578, model_norm_threshold=269925306335232.0
2024-10-08 22:46:24,788 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.342e+30, grad_sumsq=1.513e+32, orig_rms_sq=2.208e-02
2024-10-08 22:46:25,968 WARNING [optim.py:503] Scaling gradients by 0.0009792352793738246, model_norm_threshold=269925306335232.0
2024-10-08 22:46:26,123 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.200e+34, grad_sumsq=4.892e+36, orig_rms_sq=4.497e-03
2024-10-08 22:46:33,740 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=3204.0, ans=0.09949999999999998
2024-10-08 22:46:34,825 WARNING [optim.py:503] Scaling gradients by 0.011137207970023155, model_norm_threshold=269925306335232.0
2024-10-08 22:46:34,983 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.355e+32, grad_sumsq=5.057e+32, orig_rms_sq=2.679e-01
2024-10-08 22:46:39,041 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=3207.3333333333335, ans=0.07972499999999999
2024-10-08 22:46:42,591 WARNING [optim.py:503] Scaling gradients by 0.016375461593270302, model_norm_threshold=269925306335232.0
2024-10-08 22:46:42,748 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.158e+31, grad_sumsq=2.307e+33, orig_rms_sq=2.236e-02
2024-10-08 22:46:43,614 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=14.06 vs. limit=8.70275
2024-10-08 22:46:46,417 WARNING [optim.py:503] Scaling gradients by 0.009045476093888283, model_norm_threshold=269925306335232.0
2024-10-08 22:46:46,572 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.206e+32, grad_sumsq=5.027e+34, orig_rms_sq=4.388e-03
2024-10-08 22:46:46,745 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=3207.3333333333335, ans=0.34965625
2024-10-08 22:46:47,439 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.68 vs. limit=9.9055
2024-10-08 22:46:49,396 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=3210.6666666666665, ans=0.027759999999999993
2024-10-08 22:46:50,697 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=3210.6666666666665, ans=0.34950000000000003
2024-10-08 22:46:52,612 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.82 vs. limit=6.605333333333333
2024-10-08 22:46:53,831 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.60 vs. limit=6.605333333333333
2024-10-08 22:46:54,973 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=23.20 vs. limit=8.704
2024-10-08 22:46:56,785 WARNING [optim.py:503] Scaling gradients by 0.017538920044898987, model_norm_threshold=269925306335232.0
2024-10-08 22:46:56,941 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.244e+31, grad_sumsq=1.656e+31, orig_rms_sq=4.977e+00
2024-10-08 22:46:59,302 WARNING [optim.py:503] Scaling gradients by 0.05741247534751892, model_norm_threshold=269925306335232.0
2024-10-08 22:46:59,458 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.840e+30, grad_sumsq=3.034e+32, orig_rms_sq=2.255e-02
2024-10-08 22:47:04,454 WARNING [optim.py:503] Scaling gradients by 0.06265133619308472, model_norm_threshold=269925306335232.0
2024-10-08 22:47:04,610 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.113e+30, grad_sumsq=2.271e+32, orig_rms_sq=2.252e-02
2024-10-08 22:47:07,267 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=3214.0, ans=0.34934375
2024-10-08 22:47:08,477 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3214.0, ans=0.26786
2024-10-08 22:47:09,410 WARNING [optim.py:503] Scaling gradients by 0.017582617700099945, model_norm_threshold=269925306335232.0
2024-10-08 22:47:09,567 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.945e+31, grad_sumsq=3.115e+33, orig_rms_sq=2.229e-02
2024-10-08 22:47:10,303 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=27.20 vs. limit=9.910499999999999
2024-10-08 22:47:13,312 WARNING [optim.py:503] Scaling gradients by 0.009736628271639347, model_norm_threshold=269925306335232.0
2024-10-08 22:47:13,468 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.906e+32, grad_sumsq=4.190e+34, orig_rms_sq=4.549e-03
2024-10-08 22:47:15,975 INFO [train.py:1152] Epoch 2, batch 2800, loss[loss=0.8141, ctc_loss=1.136, attn_decoder_loss=0.7338, over 4755.00 frames. ], tot_loss[loss=0.7559, ctc_loss=1.041, attn_decoder_loss=0.6846, over 967102.57 frames. ], batch size: 53, lr: 3.46e-02,
2024-10-08 22:47:16,622 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=19.83 vs. limit=8.7065
2024-10-08 22:47:16,760 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten.whitening_limit, batch_count=3217.3333333333335, ans=8.7065
2024-10-08 22:47:23,651 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3217.3333333333335, ans=0.26782666666666666
2024-10-08 22:47:24,638 WARNING [optim.py:503] Scaling gradients by 0.00023757312737870961, model_norm_threshold=269925306335232.0
2024-10-08 22:47:24,794 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.836e+35, grad_sumsq=9.744e+34, orig_rms_sq=4.963e+00
2024-10-08 22:47:29,812 WARNING [optim.py:503] Scaling gradients by 0.049598999321460724, model_norm_threshold=269925306335232.0
2024-10-08 22:47:29,962 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.814e+30, grad_sumsq=1.906e+33, orig_rms_sq=4.624e-03
2024-10-08 22:47:37,665 WARNING [optim.py:503] Scaling gradients by 0.011855208314955235, model_norm_threshold=269925306335232.0
2024-10-08 22:47:37,820 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.503e+32, grad_sumsq=3.019e+31, orig_rms_sq=4.977e+00
2024-10-08 22:47:40,179 WARNING [optim.py:503] Scaling gradients by 0.015391725115478039, model_norm_threshold=269925306335232.0
2024-10-08 22:47:40,334 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.956e+31, grad_sumsq=1.599e+31, orig_rms_sq=4.977e+00
2024-10-08 22:47:41,489 WARNING [optim.py:503] Scaling gradients by 0.01943470723927021, model_norm_threshold=269925306335232.0
2024-10-08 22:47:41,644 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.755e+31, grad_sumsq=7.476e+30, orig_rms_sq=5.022e+00
2024-10-08 22:47:44,073 WARNING [optim.py:503] Scaling gradients by 0.004173976834863424, model_norm_threshold=269925306335232.0
2024-10-08 22:47:44,230 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.056e+33, grad_sumsq=4.770e+34, orig_rms_sq=2.215e-02
2024-10-08 22:47:58,647 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.95 vs. limit=6.613666666666667
2024-10-08 22:48:04,593 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=19.44 vs. limit=8.71025
2024-10-08 22:48:04,640 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1.whitening_limit, batch_count=3227.3333333333335, ans=5.8068333333333335
2024-10-08 22:48:07,669 WARNING [optim.py:503] Scaling gradients by 0.005628048907965422, model_norm_threshold=269925306335232.0
2024-10-08 22:48:07,825 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.893e+32, grad_sumsq=2.563e+34, orig_rms_sq=2.299e-02
2024-10-08 22:48:09,685 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=42.47 vs. limit=9.923
2024-10-08 22:48:10,616 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=3230.6666666666665, ans=0.7869266666666667
2024-10-08 22:48:16,397 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.430e+11 1.702e+13 7.494e+13 1.811e+15 1.136e+18, threshold=1.499e+14, percent-clipped=36.0
2024-10-08 22:48:16,397 WARNING [optim.py:503] Scaling gradients by 0.09979794174432755, model_norm_threshold=149887815516160.0
2024-10-08 22:48:16,554 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.079e+29, grad_sumsq=3.051e+31, orig_rms_sq=2.320e-02
2024-10-08 22:48:17,758 WARNING [optim.py:503] Scaling gradients by 2.2195543351699598e-05, model_norm_threshold=149887815516160.0
2024-10-08 22:48:17,913 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.327e+37, grad_sumsq=inf, orig_rms_sq=2.320e-02
2024-10-08 22:48:19,349 INFO [train.py:1152] Epoch 2, batch 2850, loss[loss=0.7885, ctc_loss=1.065, attn_decoder_loss=0.7194, over 4938.00 frames. ], tot_loss[loss=0.7563, ctc_loss=1.042, attn_decoder_loss=0.6849, over 966836.47 frames. ], batch size: 20, lr: 3.45e-02,
2024-10-08 22:48:25,003 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten.whitening_limit, batch_count=3234.0, ans=8.71275
2024-10-08 22:48:39,761 WARNING [optim.py:503] Scaling gradients by 0.015572863630950451, model_norm_threshold=149887815516160.0
2024-10-08 22:48:39,918 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.354e+31, grad_sumsq=1.006e+33, orig_rms_sq=2.340e-02
2024-10-08 22:48:45,588 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.04 vs. limit=5.8101666666666665
2024-10-08 22:48:46,382 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=32.56 vs. limit=9.9305
2024-10-08 22:48:47,017 WARNING [optim.py:503] Scaling gradients by 0.0002583450695965439, model_norm_threshold=149887815516160.0
2024-10-08 22:48:47,175 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.117e+35, grad_sumsq=4.809e+36, orig_rms_sq=2.322e-02
2024-10-08 22:48:48,422 WARNING [optim.py:503] Scaling gradients by 0.04131917282938957, model_norm_threshold=149887815516160.0
2024-10-08 22:48:48,577 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.207e+30, grad_sumsq=1.383e+32, orig_rms_sq=2.318e-02
2024-10-08 22:48:53,800 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer1.prob, batch_count=3240.6666666666665, ans=0.34809375
2024-10-08 22:48:54,314 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.28 vs. limit=5.8101666666666665
2024-10-08 22:48:56,035 WARNING [optim.py:503] Scaling gradients by 0.06515578925609589, model_norm_threshold=149887815516160.0
2024-10-08 22:48:56,191 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.276e+30, grad_sumsq=5.541e+31, orig_rms_sq=2.303e-02
2024-10-08 22:48:59,407 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=27.38 vs. limit=9.933
2024-10-08 22:49:01,293 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=3244.0, ans=0.26756
2024-10-08 22:49:03,513 WARNING [optim.py:503] Scaling gradients by 0.0002796223561745137, model_norm_threshold=149887815516160.0
2024-10-08 22:49:03,670 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.219e+34, grad_sumsq=3.576e+36, orig_rms_sq=2.298e-02
2024-10-08 22:49:06,001 WARNING [optim.py:503] Scaling gradients by 0.08009432256221771, model_norm_threshold=149887815516160.0
2024-10-08 22:49:06,158 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.120e+30, grad_sumsq=2.539e+32, orig_rms_sq=4.414e-03
2024-10-08 22:49:07,569 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=3244.0, ans=0.07834999999999999
2024-10-08 22:49:10,711 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=2.92 vs. limit=8.71775
2024-10-08 22:49:12,209 WARNING [optim.py:503] Scaling gradients by 0.08197255432605743, model_norm_threshold=149887815516160.0
2024-10-08 22:49:12,364 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.878e+29, grad_sumsq=2.979e+31, orig_rms_sq=2.309e-02
2024-10-08 22:49:13,522 WARNING [optim.py:503] Scaling gradients by 0.003240754595026374, model_norm_threshold=149887815516160.0
2024-10-08 22:49:13,679 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.223e+32, grad_sumsq=2.262e+34, orig_rms_sq=2.309e-02
2024-10-08 22:49:22,291 WARNING [optim.py:503] Scaling gradients by 0.012093912810087204, model_norm_threshold=149887815516160.0
2024-10-08 22:49:22,448 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.516e+31, grad_sumsq=1.294e+31, orig_rms_sq=1.944e+00
2024-10-08 22:49:23,635 WARNING [optim.py:503] Scaling gradients by 0.008469508029520512, model_norm_threshold=149887815516160.0
2024-10-08 22:49:23,791 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.830e+31, grad_sumsq=7.858e+30, orig_rms_sq=8.692e+00
2024-10-08 22:49:23,850 INFO [train.py:1152] Epoch 2, batch 2900, loss[loss=0.7595, ctc_loss=1.025, attn_decoder_loss=0.6932, over 4747.00 frames. ], tot_loss[loss=0.7566, ctc_loss=1.042, attn_decoder_loss=0.6853, over 965928.43 frames. ], batch size: 20, lr: 3.45e-02,
2024-10-08 22:49:25,268 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.max_abs, batch_count=3250.6666666666665, ans=7.031666666666666
2024-10-08 22:49:26,221 WARNING [optim.py:503] Scaling gradients by 0.0057254210114479065, model_norm_threshold=149887815516160.0
2024-10-08 22:49:26,377 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.256e+32, grad_sumsq=2.510e+31, orig_rms_sq=8.991e+00
2024-10-08 22:49:28,810 WARNING [optim.py:503] Scaling gradients by 0.09675513207912445, model_norm_threshold=149887815516160.0
2024-10-08 22:49:28,964 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.322e+29, grad_sumsq=1.175e+32, orig_rms_sq=4.530e-03
2024-10-08 22:49:29,176 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=3250.6666666666665, ans=0.34762499999999996
2024-10-08 22:49:30,142 WARNING [optim.py:503] Scaling gradients by 0.0008514414075762033, model_norm_threshold=149887815516160.0
2024-10-08 22:49:30,298 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.068e+33, grad_sumsq=3.469e+35, orig_rms_sq=2.326e-02
2024-10-08 22:49:31,528 WARNING [optim.py:503] Scaling gradients by 0.05802253633737564, model_norm_threshold=149887815516160.0
2024-10-08 22:49:31,684 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.760e+30, grad_sumsq=7.568e+31, orig_rms_sq=2.326e-02
2024-10-08 22:49:33,705 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.00 vs. limit=9.938
2024-10-08 22:49:39,211 WARNING [optim.py:503] Scaling gradients by 0.03655841946601868, model_norm_threshold=149887815516160.0
2024-10-08 22:49:39,366 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.142e+30, grad_sumsq=2.208e+32, orig_rms_sq=2.329e-02
2024-10-08 22:49:41,287 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=45.16 vs. limit=8.72025
2024-10-08 22:49:43,071 WARNING [optim.py:503] Scaling gradients by 0.030251219868659973, model_norm_threshold=149887815516160.0
2024-10-08 22:49:43,229 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.412e+30, grad_sumsq=1.297e+30, orig_rms_sq=4.942e+00
2024-10-08 22:49:43,436 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=3254.0, ans=0.34746875
2024-10-08 22:49:44,523 WARNING [optim.py:503] Scaling gradients by 0.026078922674059868, model_norm_threshold=149887815516160.0
2024-10-08 22:49:44,678 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.075e+31, grad_sumsq=4.633e+32, orig_rms_sq=2.320e-02
2024-10-08 22:49:46,666 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=5.07 vs. limit=5.3016000000000005
2024-10-08 22:49:49,509 WARNING [optim.py:503] Scaling gradients by 0.00018174000433646142, model_norm_threshold=149887815516160.0
2024-10-08 22:49:49,666 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.990e+35, grad_sumsq=8.592e+36, orig_rms_sq=2.316e-02
2024-10-08 22:49:52,191 WARNING [optim.py:503] Scaling gradients by 0.0013334541581571102, model_norm_threshold=149887815516160.0
2024-10-08 22:49:52,349 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.166e+33, grad_sumsq=1.367e+35, orig_rms_sq=2.316e-02
2024-10-08 22:49:53,068 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=21.69 vs. limit=9.943
2024-10-08 22:49:55,970 WARNING [optim.py:503] Scaling gradients by 0.008650179952383041, model_norm_threshold=149887815516160.0
2024-10-08 22:49:56,126 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.186e+32, grad_sumsq=5.105e+33, orig_rms_sq=2.323e-02
2024-10-08 22:50:01,404 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=4.551e+06
2024-10-08 22:50:01,441 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=3260.6666666666665, ans=0.07962083333333334
2024-10-08 22:50:09,940 WARNING [optim.py:503] Scaling gradients by 0.0021229872945696115, model_norm_threshold=149887815516160.0
2024-10-08 22:50:10,094 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.890e+32, grad_sumsq=3.437e+34, orig_rms_sq=2.295e-02
2024-10-08 22:50:13,192 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=7.72 vs. limit=5.304266666666667
2024-10-08 22:50:15,131 WARNING [optim.py:503] Scaling gradients by 0.028236033394932747, model_norm_threshold=149887815516160.0
2024-10-08 22:50:15,287 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.030e+31, grad_sumsq=4.515e+32, orig_rms_sq=2.281e-02
2024-10-08 22:50:21,769 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=3264.0, ans=0.0796
2024-10-08 22:50:23,925 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=4.31 vs. limit=4.6528
2024-10-08 22:50:25,577 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.569e+11 9.482e+12 6.365e+13 1.238e+15 6.753e+18, threshold=1.273e+14, percent-clipped=41.0
2024-10-08 22:50:26,727 WARNING [optim.py:503] Scaling gradients by 0.0009986832737922668, model_norm_threshold=127308904005632.0
2024-10-08 22:50:26,895 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.780e+33, grad_sumsq=2.118e+35, orig_rms_sq=2.257e-02
2024-10-08 22:50:28,197 INFO [train.py:1152] Epoch 2, batch 2950, loss[loss=0.7651, ctc_loss=1.051, attn_decoder_loss=0.6936, over 4799.00 frames. ], tot_loss[loss=0.7567, ctc_loss=1.042, attn_decoder_loss=0.6855, over 966518.31 frames. ], batch size: 19, lr: 3.44e-02,
2024-10-08 22:50:29,284 WARNING [optim.py:503] Scaling gradients by 0.004677233751863241, model_norm_threshold=127308904005632.0
2024-10-08 22:50:29,439 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.136e+32, grad_sumsq=9.464e+33, orig_rms_sq=2.257e-02
2024-10-08 22:50:34,402 WARNING [optim.py:503] Scaling gradients by 0.007072089239954948, model_norm_threshold=127308904005632.0
2024-10-08 22:50:34,557 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.200e+31, grad_sumsq=3.629e+33, orig_rms_sq=2.260e-02
2024-10-08 22:50:35,813 WARNING [optim.py:503] Scaling gradients by 0.0009460346773266792, model_norm_threshold=127308904005632.0
2024-10-08 22:50:35,969 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.965e+33, grad_sumsq=3.530e+35, orig_rms_sq=2.256e-02
2024-10-08 22:50:37,179 WARNING [optim.py:503] Scaling gradients by 0.0007168104057200253, model_norm_threshold=127308904005632.0
2024-10-08 22:50:37,343 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.051e+34, grad_sumsq=4.659e+35, orig_rms_sq=2.256e-02
2024-10-08 22:50:38,478 WARNING [optim.py:503] Scaling gradients by 0.006461259908974171, model_norm_threshold=127308904005632.0
2024-10-08 22:50:38,634 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.854e+31, grad_sumsq=4.368e+33, orig_rms_sq=2.256e-02
2024-10-08 22:50:40,016 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3270.6666666666665, ans=0.2672933333333333
2024-10-08 22:50:42,318 WARNING [optim.py:503] Scaling gradients by 0.08351346105337143, model_norm_threshold=127308904005632.0
2024-10-08 22:50:42,473 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.234e+29, grad_sumsq=8.212e+28, orig_rms_sq=5.156e+00
2024-10-08 22:50:45,646 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.82 vs. limit=9.953
2024-10-08 22:50:46,306 WARNING [optim.py:503] Scaling gradients by 0.055493470281362534, model_norm_threshold=127308904005632.0
2024-10-08 22:50:46,457 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.171e+30, grad_sumsq=2.591e+32, orig_rms_sq=4.519e-03
2024-10-08 22:50:46,703 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=3270.6666666666665, ans=0.34668750000000004
2024-10-08 22:50:47,237 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=4.34 vs. limit=8.7265
2024-10-08 22:50:47,954 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=3270.6666666666665, ans=0.2672933333333333
2024-10-08 22:50:49,470 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=24.66 vs. limit=9.953
2024-10-08 22:50:51,557 WARNING [optim.py:503] Scaling gradients by 0.005723521579056978, model_norm_threshold=127308904005632.0
2024-10-08 22:50:51,713 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.344e+32, grad_sumsq=6.104e+33, orig_rms_sq=2.202e-02
2024-10-08 22:50:54,132 WARNING [optim.py:503] Scaling gradients by 0.06880947947502136, model_norm_threshold=127308904005632.0
2024-10-08 22:50:54,287 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.043e+30, grad_sumsq=4.737e+31, orig_rms_sq=2.202e-02
2024-10-08 22:50:56,450 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.63 vs. limit=9.9555
2024-10-08 22:51:04,332 WARNING [optim.py:503] Scaling gradients by 0.006062750704586506, model_norm_threshold=127308904005632.0
2024-10-08 22:51:04,489 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.284e+32, grad_sumsq=5.834e+33, orig_rms_sq=2.201e-02
2024-10-08 22:51:08,299 WARNING [optim.py:503] Scaling gradients by 0.09210368245840073, model_norm_threshold=127308904005632.0
2024-10-08 22:51:08,462 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.804e+29, grad_sumsq=4.249e+28, orig_rms_sq=8.952e+00
2024-10-08 22:51:10,949 WARNING [optim.py:503] Scaling gradients by 0.00876371655613184, model_norm_threshold=127308904005632.0
2024-10-08 22:51:11,105 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.271e+31, grad_sumsq=1.942e+33, orig_rms_sq=2.199e-02
2024-10-08 22:51:13,563 WARNING [optim.py:503] Scaling gradients by 0.07600930333137512, model_norm_threshold=127308904005632.0
2024-10-08 22:51:13,723 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.020e+29, grad_sumsq=7.831e+28, orig_rms_sq=8.965e+00
2024-10-08 22:51:16,155 WARNING [optim.py:503] Scaling gradients by 0.0024589654058218002, model_norm_threshold=127308904005632.0
2024-10-08 22:51:16,312 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.985e+32, grad_sumsq=2.722e+34, orig_rms_sq=2.199e-02
2024-10-08 22:51:23,751 WARNING [optim.py:503] Scaling gradients by 0.00013750385551247746, model_norm_threshold=127308904005632.0
2024-10-08 22:51:23,909 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.276e+35, grad_sumsq=5.760e+36, orig_rms_sq=2.214e-02
2024-10-08 22:51:24,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=3280.6666666666665, ans=0.21719333333333335
2024-10-08 22:51:24,678 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=22.49 vs. limit=9.9605
2024-10-08 22:51:26,707 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=3280.6666666666665, ans=0.0654625
2024-10-08 22:51:28,422 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.36 vs. limit=6.640333333333333
2024-10-08 22:51:32,970 INFO [train.py:1152] Epoch 2, batch 3000, loss[loss=0.7033, ctc_loss=0.9769, attn_decoder_loss=0.6348, over 4834.00 frames. ], tot_loss[loss=0.7558, ctc_loss=1.04, attn_decoder_loss=0.6846, over 967210.30 frames. ], batch size: 21, lr: 3.43e-02,
2024-10-08 22:51:32,970 INFO [train.py:1175] Computing validation loss
2024-10-08 22:51:39,155 INFO [train.py:1184] Epoch 2, validation: loss=0.7923, ctc_loss=1.074, attn_decoder_loss=0.722, over 90464.00 frames.
2024-10-08 22:51:39,156 INFO [train.py:1185] Maximum memory allocated so far is 6947MB
2024-10-08 22:51:41,557 WARNING [optim.py:503] Scaling gradients by 0.0038943628314882517, model_norm_threshold=127308904005632.0
2024-10-08 22:51:41,715 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.164e+32, grad_sumsq=9.419e+34, orig_rms_sq=4.421e-03
2024-10-08 22:51:41,855 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.min_abs, batch_count=3284.0, ans=0.24926
2024-10-08 22:51:45,688 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=3284.0, ans=0.24926
2024-10-08 22:51:50,390 WARNING [optim.py:503] Scaling gradients by 0.012678107246756554, model_norm_threshold=127308904005632.0
2024-10-08 22:51:50,549 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.693e+31, grad_sumsq=1.197e+33, orig_rms_sq=2.250e-02
2024-10-08 22:51:51,710 WARNING [optim.py:503] Scaling gradients by 0.02288699708878994, model_norm_threshold=127308904005632.0
2024-10-08 22:51:51,865 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.531e+30, grad_sumsq=4.236e+32, orig_rms_sq=2.250e-02
2024-10-08 22:51:53,043 WARNING [optim.py:503] Scaling gradients by 0.0077704619616270065, model_norm_threshold=127308904005632.0
2024-10-08 22:51:53,200 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.405e+31, grad_sumsq=4.179e+33, orig_rms_sq=2.250e-02
2024-10-08 22:51:57,607 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=6.27 vs. limit=5.314933333333333
2024-10-08 22:52:01,781 WARNING [optim.py:503] Scaling gradients by 0.04121113941073418, model_norm_threshold=127308904005632.0
2024-10-08 22:52:01,939 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.903e+30, grad_sumsq=8.489e+31, orig_rms_sq=2.241e-02
2024-10-08 22:52:02,865 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.50 vs. limit=8.73275
2024-10-08 22:52:04,430 WARNING [optim.py:503] Scaling gradients by 0.015066814608871937, model_norm_threshold=127308904005632.0
2024-10-08 22:52:04,586 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.257e+31, grad_sumsq=5.628e+32, orig_rms_sq=2.234e-02
2024-10-08 22:52:08,362 WARNING [optim.py:503] Scaling gradients by 0.05790773779153824, model_norm_threshold=127308904005632.0
2024-10-08 22:52:08,517 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.228e+30, grad_sumsq=5.496e+31, orig_rms_sq=2.234e-02
2024-10-08 22:52:10,000 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=3290.6666666666665, ans=0.34575
2024-10-08 22:52:13,597 WARNING [optim.py:503] Scaling gradients by 0.0073823812417685986, model_norm_threshold=127308904005632.0
2024-10-08 22:52:13,752 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.173e+31, grad_sumsq=1.393e+31, orig_rms_sq=5.150e+00
2024-10-08 22:52:13,944 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=3290.6666666666665, ans=0.03971666666666667
2024-10-08 22:52:16,836 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=21.19 vs. limit=8.73525
2024-10-08 22:52:17,441 WARNING [optim.py:503] Scaling gradients by 0.0341469943523407, model_norm_threshold=127308904005632.0
2024-10-08 22:52:17,599 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.579e+30, grad_sumsq=1.072e+30, orig_rms_sq=5.205e+00
2024-10-08 22:52:20,125 WARNING [optim.py:503] Scaling gradients by 0.06855271011590958, model_norm_threshold=127308904005632.0
2024-10-08 22:52:20,278 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.771e+29, grad_sumsq=4.305e+31, orig_rms_sq=2.270e-02
2024-10-08 22:52:21,055 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2.whitening_limit, batch_count=3294.0, ans=6.647
2024-10-08 22:52:21,133 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.76 vs. limit=9.9705
2024-10-08 22:52:21,209 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.39 vs. limit=9.9705
2024-10-08 22:52:22,746 WARNING [optim.py:503] Scaling gradients by 0.08286923915147781, model_norm_threshold=127308904005632.0
2024-10-08 22:52:22,904 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.802e+29, grad_sumsq=2.556e+31, orig_rms_sq=2.270e-02
2024-10-08 22:52:29,181 WARNING [optim.py:503] Scaling gradients by 0.02245747111737728, model_norm_threshold=127308904005632.0
2024-10-08 22:52:29,340 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.961e+30, grad_sumsq=1.978e+33, orig_rms_sq=4.530e-03
2024-10-08 22:52:30,559 WARNING [optim.py:503] Scaling gradients by 0.03346840292215347, model_norm_threshold=127308904005632.0
2024-10-08 22:52:30,715 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.437e+30, grad_sumsq=1.181e+33, orig_rms_sq=4.604e-03
2024-10-08 22:52:31,988 WARNING [optim.py:503] Scaling gradients by 0.002619281644001603, model_norm_threshold=127308904005632.0
2024-10-08 22:52:32,145 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.429e+32, grad_sumsq=2.367e+34, orig_rms_sq=2.294e-02
2024-10-08 22:52:32,789 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.08 vs. limit=8.7365
2024-10-08 22:52:35,239 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.88 vs. limit=9.972999999999999
2024-10-08 22:52:37,066 WARNING [optim.py:503] Scaling gradients by 0.007423995528370142, model_norm_threshold=127308904005632.0
2024-10-08 22:52:37,222 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.410e+31, grad_sumsq=4.071e+33, orig_rms_sq=2.311e-02
2024-10-08 22:52:40,041 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=3297.3333333333335, ans=0.3454375
2024-10-08 22:52:40,993 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.916e+11 1.212e+13 8.320e+13 2.198e+15 9.259e+17, threshold=1.664e+14, percent-clipped=48.0
2024-10-08 22:52:43,119 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2.whitening_limit, batch_count=3300.6666666666665, ans=6.650333333333333
2024-10-08 22:52:43,564 INFO [train.py:1152] Epoch 2, batch 3050, loss[loss=0.7645, ctc_loss=1.039, attn_decoder_loss=0.6958, over 4749.00 frames. ], tot_loss[loss=0.7564, ctc_loss=1.041, attn_decoder_loss=0.6853, over 966712.08 frames. ], batch size: 19, lr: 3.43e-02,
2024-10-08 22:52:43,645 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=3300.6666666666665, ans=0.076225
2024-10-08 22:52:44,876 WARNING [optim.py:503] Scaling gradients by 0.008002367801964283, model_norm_threshold=166397569138688.0
2024-10-08 22:52:45,034 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.14, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.186e+31, grad_sumsq=1.343e+34, orig_rms_sq=4.606e-03
2024-10-08 22:52:49,831 WARNING [optim.py:503] Scaling gradients by 0.01575685292482376, model_norm_threshold=166397569138688.0
2024-10-08 22:52:49,988 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.212e+31, grad_sumsq=1.379e+33, orig_rms_sq=2.330e-02
2024-10-08 22:52:50,617 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.12 vs. limit=9.9755
2024-10-08 22:52:53,693 WARNING [optim.py:503] Scaling gradients by 0.01865272969007492, model_norm_threshold=166397569138688.0
2024-10-08 22:52:53,852 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.639e+31, grad_sumsq=7.034e+32, orig_rms_sq=2.330e-02
2024-10-08 22:52:55,679 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.47 vs. limit=8.739
2024-10-08 22:52:56,569 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=3304.0, ans=0.035
2024-10-08 22:52:59,154 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=3304.0, ans=0.7843600000000001
2024-10-08 22:53:02,585 WARNING [optim.py:503] Scaling gradients by 0.024800680577754974, model_norm_threshold=166397569138688.0
2024-10-08 22:53:02,743 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.004e+31, grad_sumsq=4.453e+32, orig_rms_sq=2.255e-02
2024-10-08 22:53:03,957 WARNING [optim.py:503] Scaling gradients by 0.005047101993113756, model_norm_threshold=166397569138688.0
2024-10-08 22:53:04,115 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.966e+32, grad_sumsq=8.835e+34, orig_rms_sq=4.489e-03
2024-10-08 22:53:04,788 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=7.55 vs. limit=5.3216
2024-10-08 22:53:08,682 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.whiten.whitening_limit, batch_count=3307.3333333333335, ans=8.74025
2024-10-08 22:53:11,630 WARNING [optim.py:503] Scaling gradients by 0.029431743547320366, model_norm_threshold=166397569138688.0
2024-10-08 22:53:11,786 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.165e+30, grad_sumsq=2.702e+32, orig_rms_sq=2.282e-02
2024-10-08 22:53:14,283 WARNING [optim.py:503] Scaling gradients by 0.0005611252854578197, model_norm_threshold=166397569138688.0
2024-10-08 22:53:14,438 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.58, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.137e+34, grad_sumsq=1.134e+37, orig_rms_sq=4.531e-03
2024-10-08 22:53:14,654 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=3307.3333333333335, ans=0.34496875
2024-10-08 22:53:15,624 WARNING [optim.py:503] Scaling gradients by 0.016944289207458496, model_norm_threshold=166397569138688.0
2024-10-08 22:53:15,781 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.127e+31, grad_sumsq=9.322e+32, orig_rms_sq=2.282e-02
2024-10-08 22:53:18,322 WARNING [optim.py:503] Scaling gradients by 4.255474777892232e-05, model_norm_threshold=166397569138688.0
2024-10-08 22:53:18,481 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.072e+36, grad_sumsq=1.771e+38, orig_rms_sq=2.300e-02
2024-10-08 22:53:25,732 WARNING [optim.py:503] Scaling gradients by 0.05384509637951851, model_norm_threshold=166397569138688.0
2024-10-08 22:53:25,889 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.029e+30, grad_sumsq=8.883e+31, orig_rms_sq=2.284e-02
2024-10-08 22:53:28,407 WARNING [optim.py:503] Scaling gradients by 0.002126232720911503, model_norm_threshold=166397569138688.0
2024-10-08 22:53:28,566 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.097e+33, grad_sumsq=9.275e+34, orig_rms_sq=2.261e-02
2024-10-08 22:53:30,966 WARNING [optim.py:503] Scaling gradients by 0.009037535637617111, model_norm_threshold=166397569138688.0
2024-10-08 22:53:31,121 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.922e+31, grad_sumsq=2.619e+33, orig_rms_sq=2.261e-02
2024-10-08 22:53:33,724 WARNING [optim.py:503] Scaling gradients by 0.00673564663156867, model_norm_threshold=166397569138688.0
2024-10-08 22:53:33,879 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.489e+32, grad_sumsq=5.333e+32, orig_rms_sq=2.791e-01
2024-10-08 22:53:34,093 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=3314.0, ans=0.24971000000000002
2024-10-08 22:53:35,298 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=3314.0, ans=0.34465625
2024-10-08 22:53:36,353 WARNING [optim.py:503] Scaling gradients by 0.001969330944120884, model_norm_threshold=166397569138688.0
2024-10-08 22:53:36,511 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.667e+33, grad_sumsq=7.393e+34, orig_rms_sq=2.255e-02
2024-10-08 22:53:37,734 WARNING [optim.py:503] Scaling gradients by 0.0008009184384718537, model_norm_threshold=166397569138688.0
2024-10-08 22:53:37,890 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.005e+34, grad_sumsq=2.181e+36, orig_rms_sq=4.608e-03
2024-10-08 22:53:42,882 WARNING [optim.py:503] Scaling gradients by 0.08983316272497177, model_norm_threshold=166397569138688.0
2024-10-08 22:53:43,039 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.537e+30, grad_sumsq=6.737e+31, orig_rms_sq=2.281e-02
2024-10-08 22:53:46,730 WARNING [optim.py:503] Scaling gradients by 0.041760772466659546, model_norm_threshold=166397569138688.0
2024-10-08 22:53:48,181 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.658e+30, grad_sumsq=2.042e+32, orig_rms_sq=2.281e-02
2024-10-08 22:53:49,439 INFO [train.py:1152] Epoch 2, batch 3100, loss[loss=0.7504, ctc_loss=1.076, attn_decoder_loss=0.669, over 4817.00 frames. ], tot_loss[loss=0.7579, ctc_loss=1.044, attn_decoder_loss=0.6865, over 966447.38 frames. ], batch size: 38, lr: 3.42e-02,
2024-10-08 22:53:50,685 WARNING [optim.py:503] Scaling gradients by 0.019559821113944054, model_norm_threshold=166397569138688.0
2024-10-08 22:53:50,843 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.499e+31, grad_sumsq=6.536e+32, orig_rms_sq=2.294e-02
2024-10-08 22:53:52,018 WARNING [optim.py:503] Scaling gradients by 0.005472700111567974, model_norm_threshold=166397569138688.0
2024-10-08 22:53:52,174 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.905e+32, grad_sumsq=8.305e+33, orig_rms_sq=2.294e-02
2024-10-08 22:53:53,366 WARNING [optim.py:503] Scaling gradients by 0.08207562565803528, model_norm_threshold=166397569138688.0
2024-10-08 22:53:53,522 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.220e+30, grad_sumsq=2.354e+29, orig_rms_sq=5.184e+00
2024-10-08 22:54:05,221 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=3320.6666666666665, ans=0.03962291666666667
2024-10-08 22:54:07,441 WARNING [optim.py:503] Scaling gradients by 0.005911275744438171, model_norm_threshold=166397569138688.0
2024-10-08 22:54:07,596 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.006e+32, grad_sumsq=8.784e+33, orig_rms_sq=2.283e-02
2024-10-08 22:54:12,543 WARNING [optim.py:503] Scaling gradients by 0.017331790179014206, model_norm_threshold=166397569138688.0
2024-10-08 22:54:12,700 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.588e+31, grad_sumsq=6.930e+32, orig_rms_sq=2.292e-02
2024-10-08 22:54:14,658 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=27.53 vs. limit=9.993
2024-10-08 22:54:15,093 WARNING [optim.py:503] Scaling gradients by 0.0093443114310503, model_norm_threshold=166397569138688.0
2024-10-08 22:54:15,250 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.757e+31, grad_sumsq=2.946e+33, orig_rms_sq=2.294e-02
2024-10-08 22:54:16,440 WARNING [optim.py:503] Scaling gradients by 0.012310870923101902, model_norm_threshold=166397569138688.0
2024-10-08 22:54:16,597 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.66, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.202e+32, grad_sumsq=2.729e+34, orig_rms_sq=4.405e-03
2024-10-08 22:54:22,435 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.61 vs. limit=8.7465
2024-10-08 22:54:23,029 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=3324.0, ans=0.3441875
2024-10-08 22:54:34,186 WARNING [optim.py:503] Scaling gradients by 0.08272827416658401, model_norm_threshold=166397569138688.0
2024-10-08 22:54:34,342 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.646e+29, grad_sumsq=3.818e+31, orig_rms_sq=2.264e-02
2024-10-08 22:54:38,215 WARNING [optim.py:503] Scaling gradients by 0.014022710733115673, model_norm_threshold=166397569138688.0
2024-10-08 22:54:38,375 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.252e+31, grad_sumsq=2.563e+30, orig_rms_sq=8.787e+00
2024-10-08 22:54:39,552 WARNING [optim.py:503] Scaling gradients by 0.020444480702280998, model_norm_threshold=166397569138688.0
2024-10-08 22:54:39,706 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.329e+31, grad_sumsq=5.193e+30, orig_rms_sq=2.560e+00
2024-10-08 22:54:39,896 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=3330.6666666666665, ans=0.7834266666666667
2024-10-08 22:54:40,685 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.20 vs. limit=8.749
2024-10-08 22:54:42,355 WARNING [optim.py:503] Scaling gradients by 0.0649077370762825, model_norm_threshold=166397569138688.0
2024-10-08 22:54:42,511 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.813e+30, grad_sumsq=4.098e+32, orig_rms_sq=4.425e-03
2024-10-08 22:54:47,331 WARNING [optim.py:503] Scaling gradients by 0.03313069045543671, model_norm_threshold=166397569138688.0
2024-10-08 22:54:47,489 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.311e+30, grad_sumsq=1.861e+30, orig_rms_sq=5.003e+00
2024-10-08 22:54:50,543 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=3330.6666666666665, ans=0.26669333333333334
2024-10-08 22:54:51,550 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.347e+11 3.916e+13 2.516e+14 3.090e+15 3.910e+18, threshold=5.031e+14, percent-clipped=51.0
2024-10-08 22:54:52,151 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.61 vs. limit=8.749
2024-10-08 22:54:54,090 INFO [train.py:1152] Epoch 2, batch 3150, loss[loss=0.7338, ctc_loss=1.021, attn_decoder_loss=0.662, over 4796.00 frames. ], tot_loss[loss=0.7571, ctc_loss=1.043, attn_decoder_loss=0.6856, over 966706.57 frames. ], batch size: 40, lr: 3.42e-02,
2024-10-08 22:54:55,210 WARNING [optim.py:503] Scaling gradients by 0.004418207798153162, model_norm_threshold=503130622001152.0
2024-10-08 22:54:55,368 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.853e+33, grad_sumsq=1.722e+35, orig_rms_sq=2.237e-02
2024-10-08 22:55:01,465 WARNING [optim.py:503] Scaling gradients by 0.015078376047313213, model_norm_threshold=503130622001152.0
2024-10-08 22:55:01,621 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.083e+32, grad_sumsq=4.585e+34, orig_rms_sq=4.544e-03
2024-10-08 22:55:04,113 WARNING [optim.py:503] Scaling gradients by 0.024481311440467834, model_norm_threshold=503130622001152.0
2024-10-08 22:55:04,271 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.466e+31, grad_sumsq=4.067e+31, orig_rms_sq=2.327e+00
2024-10-08 22:55:06,468 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.54 vs. limit=10.003
2024-10-08 22:55:08,059 WARNING [optim.py:503] Scaling gradients by 0.0438423790037632, model_norm_threshold=503130622001152.0
2024-10-08 22:55:08,213 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.462e+31, grad_sumsq=9.861e+33, orig_rms_sq=4.525e-03
2024-10-08 22:55:13,972 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.88 vs. limit=10.003
2024-10-08 22:55:26,910 WARNING [optim.py:503] Scaling gradients by 0.000823544804006815, model_norm_threshold=503130622001152.0
2024-10-08 22:55:27,069 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.188e+34, grad_sumsq=3.551e+36, orig_rms_sq=2.306e-02
2024-10-08 22:55:30,715 WARNING [optim.py:503] Scaling gradients by 0.0012929060030728579, model_norm_threshold=503130622001152.0
2024-10-08 22:55:30,872 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.975e+34, grad_sumsq=1.290e+36, orig_rms_sq=2.306e-02
2024-10-08 22:55:41,028 WARNING [optim.py:503] Scaling gradients by 0.012348217889666557, model_norm_threshold=503130622001152.0
2024-10-08 22:55:41,185 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.633e+32, grad_sumsq=1.965e+34, orig_rms_sq=2.358e-02
2024-10-08 22:55:47,426 WARNING [optim.py:503] Scaling gradients by 0.017507260665297508, model_norm_threshold=503130622001152.0
2024-10-08 22:55:47,584 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.178e+32, grad_sumsq=1.372e+34, orig_rms_sq=2.316e-02
2024-10-08 22:55:49,390 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.38 vs. limit=3.5021
2024-10-08 22:55:51,446 WARNING [optim.py:503] Scaling gradients by 0.0010323423193767667, model_norm_threshold=503130622001152.0
2024-10-08 22:55:51,600 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.043e+34, grad_sumsq=2.609e+36, orig_rms_sq=2.316e-02
2024-10-08 22:55:52,789 WARNING [optim.py:503] Scaling gradients by 0.021299371495842934, model_norm_threshold=503130622001152.0
2024-10-08 22:55:52,946 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.149e+32, grad_sumsq=5.011e+33, orig_rms_sq=2.294e-02
2024-10-08 22:55:55,539 WARNING [optim.py:503] Scaling gradients by 0.008340837433934212, model_norm_threshold=503130622001152.0
2024-10-08 22:55:55,693 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.520e+32, grad_sumsq=3.279e+34, orig_rms_sq=2.294e-02
2024-10-08 22:55:55,866 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=3347.3333333333335, ans=0.34309375
2024-10-08 22:55:58,398 INFO [train.py:1152] Epoch 2, batch 3200, loss[loss=0.8073, ctc_loss=1.088, attn_decoder_loss=0.737, over 4758.00 frames. ], tot_loss[loss=0.7577, ctc_loss=1.042, attn_decoder_loss=0.6866, over 967163.60 frames. ], batch size: 20, lr: 3.41e-02,
2024-10-08 22:55:58,537 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=3350.6666666666665, ans=0.25026000000000004
2024-10-08 22:56:00,649 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.75 vs. limit=10.013
2024-10-08 22:56:02,797 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=40.08 vs. limit=10.013
2024-10-08 22:56:02,935 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.84 vs. limit=10.013
2024-10-08 22:56:04,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=3350.6666666666665, ans=0.08116666666666666
2024-10-08 22:56:07,366 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=3350.6666666666665, ans=0.25026000000000004
2024-10-08 22:56:09,568 WARNING [optim.py:503] Scaling gradients by 0.020183978602290154, model_norm_threshold=503130622001152.0
2024-10-08 22:56:09,723 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.302e+31, grad_sumsq=4.106e+33, orig_rms_sq=2.265e-02
2024-10-08 22:56:11,067 WARNING [optim.py:503] Scaling gradients by 0.07847641408443451, model_norm_threshold=503130622001152.0
2024-10-08 22:56:11,224 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.277e+30, grad_sumsq=4.095e+32, orig_rms_sq=2.265e-02
2024-10-08 22:56:13,948 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=3354.0, ans=0.07422499999999999
2024-10-08 22:56:15,098 WARNING [optim.py:503] Scaling gradients by 0.006556435022503138, model_norm_threshold=503130622001152.0
2024-10-08 22:56:15,251 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.064e+33, grad_sumsq=9.032e+34, orig_rms_sq=2.285e-02
2024-10-08 22:56:16,422 WARNING [optim.py:503] Scaling gradients by 0.004241734743118286, model_norm_threshold=503130622001152.0
2024-10-08 22:56:16,577 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.648e+33, grad_sumsq=2.909e+32, orig_rms_sq=9.106e+00
2024-10-08 22:56:17,782 WARNING [optim.py:503] Scaling gradients by 0.08339148014783859, model_norm_threshold=503130622001152.0
2024-10-08 22:56:17,939 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.305e+30, grad_sumsq=8.023e+29, orig_rms_sq=9.106e+00
2024-10-08 22:56:19,172 WARNING [optim.py:503] Scaling gradients by 0.025327417999505997, model_norm_threshold=503130622001152.0
2024-10-08 22:56:19,328 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.099e+32, grad_sumsq=4.770e+33, orig_rms_sq=2.303e-02
2024-10-08 22:56:22,969 WARNING [optim.py:503] Scaling gradients by 0.0015482513699680567, model_norm_threshold=503130622001152.0
2024-10-08 22:56:23,125 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.056e+34, grad_sumsq=3.370e+33, orig_rms_sq=9.066e+00
2024-10-08 22:56:24,361 WARNING [optim.py:503] Scaling gradients by 0.01239527203142643, model_norm_threshold=503130622001152.0
2024-10-08 22:56:24,517 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.445e+32, grad_sumsq=1.073e+34, orig_rms_sq=2.279e-02
2024-10-08 22:56:26,415 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.45 vs. limit=5.839333333333333
2024-10-08 22:56:34,472 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=13.64 vs. limit=10.018
2024-10-08 22:56:35,207 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=19.43 vs. limit=8.759
2024-10-08 22:56:35,765 WARNING [optim.py:503] Scaling gradients by 0.025833820924162865, model_norm_threshold=503130622001152.0
2024-10-08 22:56:35,920 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.917e+31, grad_sumsq=1.897e+34, orig_rms_sq=4.173e-03
2024-10-08 22:56:40,884 WARNING [optim.py:503] Scaling gradients by 0.001583015895448625, model_norm_threshold=503130622001152.0
2024-10-08 22:56:41,040 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.612e+34, grad_sumsq=8.509e+36, orig_rms_sq=4.245e-03
2024-10-08 22:56:43,632 WARNING [optim.py:503] Scaling gradients by 0.05680673196911812, model_norm_threshold=503130622001152.0
2024-10-08 22:56:43,790 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.955e+31, grad_sumsq=2.161e+30, orig_rms_sq=9.046e+00
2024-10-08 22:56:46,349 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=3360.6666666666665, ans=0.0609625
2024-10-08 22:56:49,950 WARNING [optim.py:503] Scaling gradients by 0.06651075184345245, model_norm_threshold=503130622001152.0
2024-10-08 22:56:50,112 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.623e+31, grad_sumsq=7.118e+32, orig_rms_sq=2.280e-02
2024-10-08 22:56:52,231 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.62 vs. limit=10.023
2024-10-08 22:56:52,532 WARNING [optim.py:503] Scaling gradients by 0.09220478683710098, model_norm_threshold=503130622001152.0
2024-10-08 22:56:52,703 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.551e+30, grad_sumsq=4.189e+32, orig_rms_sq=2.280e-02
2024-10-08 22:56:54,962 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=14.91 vs. limit=10.023
2024-10-08 22:56:58,751 WARNING [optim.py:503] Scaling gradients by 0.023676708340644836, model_norm_threshold=503130622001152.0
2024-10-08 22:56:58,908 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.224e+32, grad_sumsq=5.315e+33, orig_rms_sq=2.302e-02
2024-10-08 22:57:00,222 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.915e+11 2.766e+13 1.869e+14 5.457e+15 6.109e+17, threshold=3.737e+14, percent-clipped=42.0
2024-10-08 22:57:02,831 INFO [train.py:1152] Epoch 2, batch 3250, loss[loss=0.8083, ctc_loss=1.122, attn_decoder_loss=0.7299, over 4860.00 frames. ], tot_loss[loss=0.7581, ctc_loss=1.042, attn_decoder_loss=0.687, over 967266.74 frames. ], batch size: 24, lr: 3.41e-02,
2024-10-08 22:57:04,011 WARNING [optim.py:503] Scaling gradients by 0.0026159053668379784, model_norm_threshold=373744799318016.0
2024-10-08 22:57:04,165 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.458e+33, grad_sumsq=1.908e+35, orig_rms_sq=2.336e-02
2024-10-08 22:57:05,601 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=3367.3333333333335, ans=0.06058749999999999
2024-10-08 22:57:05,660 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3367.3333333333335, ans=0.26632666666666666
2024-10-08 22:57:06,631 WARNING [optim.py:503] Scaling gradients by 0.05044108256697655, model_norm_threshold=373744799318016.0
2024-10-08 22:57:06,787 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.983e+31, grad_sumsq=8.448e+32, orig_rms_sq=2.348e-02
2024-10-08 22:57:10,415 WARNING [optim.py:503] Scaling gradients by 0.003909357823431492, model_norm_threshold=373744799318016.0
2024-10-08 22:57:10,570 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.783e+33, grad_sumsq=1.178e+35, orig_rms_sq=2.363e-02
2024-10-08 22:57:15,631 WARNING [optim.py:503] Scaling gradients by 0.01597365364432335, model_norm_threshold=373744799318016.0
2024-10-08 22:57:15,790 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.360e+32, grad_sumsq=2.669e+31, orig_rms_sq=5.094e+00
2024-10-08 22:57:18,785 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten.whitening_limit, batch_count=3370.6666666666665, ans=8.764
2024-10-08 22:57:23,368 WARNING [optim.py:503] Scaling gradients by 0.0009954343549907207, model_norm_threshold=373744799318016.0
2024-10-08 22:57:23,520 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.302e+34, grad_sumsq=1.387e+36, orig_rms_sq=2.381e-02
2024-10-08 22:57:27,817 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.61 vs. limit=10.0305
2024-10-08 22:57:28,378 WARNING [optim.py:503] Scaling gradients by 0.003631055820733309, model_norm_threshold=373744799318016.0
2024-10-08 22:57:28,536 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.073e+33, grad_sumsq=1.709e+35, orig_rms_sq=2.383e-02
2024-10-08 22:57:29,694 WARNING [optim.py:503] Scaling gradients by 0.0031584068201482296, model_norm_threshold=373744799318016.0
2024-10-08 22:57:29,849 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.348e+33, grad_sumsq=1.825e+35, orig_rms_sq=2.383e-02
2024-10-08 22:57:32,569 WARNING [optim.py:503] Scaling gradients by 0.07568376511335373, model_norm_threshold=373744799318016.0
2024-10-08 22:57:32,725 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.53, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.281e+31, grad_sumsq=3.122e+33, orig_rms_sq=4.104e-03
2024-10-08 22:57:36,056 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=47.49 vs. limit=8.76525
2024-10-08 22:57:36,527 WARNING [optim.py:503] Scaling gradients by 0.0033500390127301216, model_norm_threshold=373744799318016.0
2024-10-08 22:57:36,683 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.861e+33, grad_sumsq=9.419e+35, orig_rms_sq=4.099e-03
2024-10-08 22:57:37,872 WARNING [optim.py:503] Scaling gradients by 0.00035387580282986164, model_norm_threshold=373744799318016.0
2024-10-08 22:57:38,027 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.780e+35, grad_sumsq=1.178e+37, orig_rms_sq=2.360e-02
2024-10-08 22:57:39,316 WARNING [optim.py:503] Scaling gradients by 0.0812242403626442, model_norm_threshold=373744799318016.0
2024-10-08 22:57:39,470 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.481e+30, grad_sumsq=1.475e+32, orig_rms_sq=2.360e-02
2024-10-08 22:57:39,636 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=3374.0, ans=0.34184375
2024-10-08 22:57:42,012 WARNING [optim.py:503] Scaling gradients by 0.006725378334522247, model_norm_threshold=373744799318016.0
2024-10-08 22:57:42,167 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.797e+32, grad_sumsq=1.669e+35, orig_rms_sq=4.072e-03
2024-10-08 22:57:42,785 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.63 vs. limit=3.5066
2024-10-08 22:57:47,207 WARNING [optim.py:503] Scaling gradients by 0.003955535124987364, model_norm_threshold=373744799318016.0
2024-10-08 22:57:47,364 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.352e+33, grad_sumsq=1.014e+35, orig_rms_sq=2.321e-02
2024-10-08 22:57:50,548 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.88 vs. limit=5.844333333333333
2024-10-08 22:57:52,702 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=3377.3333333333335, ans=0.02400999999999999
2024-10-08 22:57:52,713 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=3377.3333333333335, ans=0.06002499999999997
2024-10-08 22:57:54,991 WARNING [optim.py:503] Scaling gradients by 0.01572035625576973, model_norm_threshold=373744799318016.0
2024-10-08 22:57:55,146 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.354e+32, grad_sumsq=3.273e+34, orig_rms_sq=4.138e-03
2024-10-08 22:57:58,608 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.39 vs. limit=6.690333333333333
2024-10-08 22:57:59,516 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=5.55 vs. limit=5.352266666666667
2024-10-08 22:58:00,214 WARNING [optim.py:503] Scaling gradients by 0.014870181679725647, model_norm_threshold=373744799318016.0
2024-10-08 22:58:00,369 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.376e+32, grad_sumsq=5.999e+33, orig_rms_sq=2.294e-02
2024-10-08 22:58:03,682 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=15.94 vs. limit=5.845166666666667
2024-10-08 22:58:07,985 INFO [train.py:1152] Epoch 2, batch 3300, loss[loss=0.793, ctc_loss=1.062, attn_decoder_loss=0.7257, over 4840.00 frames. ], tot_loss[loss=0.7558, ctc_loss=1.039, attn_decoder_loss=0.685, over 967749.25 frames. ], batch size: 43, lr: 3.40e-02,
2024-10-08 22:58:15,416 WARNING [optim.py:503] Scaling gradients by 0.019417446106672287, model_norm_threshold=373744799318016.0
2024-10-08 22:58:15,573 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.815e+31, grad_sumsq=3.394e+33, orig_rms_sq=2.303e-02
2024-10-08 22:58:17,996 WARNING [optim.py:503] Scaling gradients by 0.03813314437866211, model_norm_threshold=373744799318016.0
2024-10-08 22:58:18,151 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.312e+31, grad_sumsq=1.003e+33, orig_rms_sq=2.305e-02
2024-10-08 22:58:19,605 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=3387.3333333333335, ans=0.023784999999999987
2024-10-08 22:58:19,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=3387.3333333333335, ans=0.34121875
2024-10-08 22:58:20,162 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=16.00 vs. limit=8.77025
2024-10-08 22:58:20,578 WARNING [optim.py:503] Scaling gradients by 0.003163685090839863, model_norm_threshold=373744799318016.0
2024-10-08 22:58:20,735 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.368e+33, grad_sumsq=1.461e+35, orig_rms_sq=2.305e-02
2024-10-08 22:58:25,486 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=20.03 vs. limit=8.77025
2024-10-08 22:58:28,266 WARNING [optim.py:503] Scaling gradients by 0.001258975826203823, model_norm_threshold=373744799318016.0
2024-10-08 22:58:28,423 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.583e+34, grad_sumsq=1.118e+36, orig_rms_sq=2.310e-02
2024-10-08 22:58:36,459 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=14.85 vs. limit=5.847666666666667
2024-10-08 22:58:38,059 WARNING [optim.py:503] Scaling gradients by 0.09464213997125626, model_norm_threshold=373744799318016.0
2024-10-08 22:58:38,217 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.599e+30, grad_sumsq=1.553e+32, orig_rms_sq=2.317e-02
2024-10-08 22:58:39,373 WARNING [optim.py:503] Scaling gradients by 0.025136329233646393, model_norm_threshold=373744799318016.0
2024-10-08 22:58:39,529 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.612e+31, grad_sumsq=2.854e+33, orig_rms_sq=2.317e-02
2024-10-08 22:58:41,646 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.38 vs. limit=5.847666666666667
2024-10-08 22:58:44,581 WARNING [optim.py:503] Scaling gradients by 0.014224570244550705, model_norm_threshold=373744799318016.0
2024-10-08 22:58:44,737 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.760e+32, grad_sumsq=4.475e+34, orig_rms_sq=3.932e-03
2024-10-08 22:58:46,986 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.51 vs. limit=10.0455
2024-10-08 22:58:49,722 WARNING [optim.py:503] Scaling gradients by 0.008998701348900795, model_norm_threshold=373744799318016.0
2024-10-08 22:58:49,877 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.859e+32, grad_sumsq=1.687e+34, orig_rms_sq=2.288e-02
2024-10-08 22:58:51,800 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=49.95 vs. limit=8.77275
2024-10-08 22:58:55,584 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=46.86 vs. limit=8.77275
2024-10-08 22:58:56,981 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=17.53 vs. limit=8.77275
2024-10-08 22:58:57,928 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.69 vs. limit=10.048
2024-10-08 22:58:59,844 WARNING [optim.py:503] Scaling gradients by 0.02570873312652111, model_norm_threshold=373744799318016.0
2024-10-08 22:59:00,001 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.272e+31, grad_sumsq=1.914e+33, orig_rms_sq=2.232e-02
2024-10-08 22:59:04,849 WARNING [optim.py:503] Scaling gradients by 0.02061433158814907, model_norm_threshold=373744799318016.0
2024-10-08 22:59:05,004 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.136e+32, grad_sumsq=5.115e+33, orig_rms_sq=2.222e-02
2024-10-08 22:59:07,324 WARNING [optim.py:503] Scaling gradients by 0.0001453153818147257, model_norm_threshold=373744799318016.0
2024-10-08 22:59:07,480 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.371e+36, grad_sumsq=6.170e+37, orig_rms_sq=2.222e-02
2024-10-08 22:59:10,282 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.569e+11 3.581e+13 2.492e+14 4.601e+15 2.572e+18, threshold=4.984e+14, percent-clipped=43.0
2024-10-08 22:59:12,882 INFO [train.py:1152] Epoch 2, batch 3350, loss[loss=0.768, ctc_loss=1.062, attn_decoder_loss=0.6945, over 4799.00 frames. ], tot_loss[loss=0.7561, ctc_loss=1.04, attn_decoder_loss=0.6851, over 967044.17 frames. ], batch size: 40, lr: 3.40e-02,
2024-10-08 22:59:15,321 WARNING [optim.py:503] Scaling gradients by 0.0015021899016574025, model_norm_threshold=498376328085504.0
2024-10-08 22:59:15,478 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.878e+34, grad_sumsq=3.668e+33, orig_rms_sq=5.118e+00
2024-10-08 22:59:24,040 WARNING [optim.py:503] Scaling gradients by 0.02741001360118389, model_norm_threshold=498376328085504.0
2024-10-08 22:59:24,196 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.847e+31, grad_sumsq=1.338e+31, orig_rms_sq=5.117e+00
2024-10-08 22:59:27,473 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.66 vs. limit=6.702
2024-10-08 22:59:31,284 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=27.92 vs. limit=10.053
2024-10-08 22:59:33,110 WARNING [optim.py:503] Scaling gradients by 0.00965916272252798, model_norm_threshold=498376328085504.0
2024-10-08 22:59:33,265 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.623e+32, grad_sumsq=3.603e+34, orig_rms_sq=2.116e-02
2024-10-08 22:59:35,738 WARNING [optim.py:503] Scaling gradients by 0.0002219380985479802, model_norm_threshold=498376328085504.0
2024-10-08 22:59:35,895 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.373e+36, grad_sumsq=1.125e+38, orig_rms_sq=2.109e-02
2024-10-08 22:59:36,576 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=15.07 vs. limit=6.702
2024-10-08 22:59:36,726 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=8.68 vs. limit=5.851
2024-10-08 22:59:37,267 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.max_abs, batch_count=3407.3333333333335, ans=7.129583333333334
2024-10-08 22:59:40,207 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=27.71 vs. limit=10.0555
2024-10-08 22:59:41,815 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.01 vs. limit=6.703666666666667
2024-10-08 22:59:44,139 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.74 vs. limit=10.0555
2024-10-08 22:59:46,934 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=18.34 vs. limit=10.0555
2024-10-08 22:59:52,236 WARNING [optim.py:503] Scaling gradients by 0.007110093254595995, model_norm_threshold=498376328085504.0
2024-10-08 22:59:52,392 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.689e+33, grad_sumsq=7.981e+34, orig_rms_sq=2.116e-02
2024-10-08 22:59:57,435 WARNING [optim.py:503] Scaling gradients by 0.0702122375369072, model_norm_threshold=498376328085504.0
2024-10-08 22:59:57,590 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.700e+30, grad_sumsq=1.815e+30, orig_rms_sq=5.344e+00
2024-10-08 22:59:59,947 WARNING [optim.py:503] Scaling gradients by 0.010809997096657753, model_norm_threshold=498376328085504.0
2024-10-08 23:00:00,103 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.178e+32, grad_sumsq=2.417e+34, orig_rms_sq=2.142e-02
2024-10-08 23:00:01,940 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=29.79 vs. limit=8.779
2024-10-08 23:00:02,602 WARNING [optim.py:503] Scaling gradients by 0.04702478274703026, model_norm_threshold=498376328085504.0
2024-10-08 23:00:02,757 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.700e+31, grad_sumsq=6.710e+33, orig_rms_sq=4.023e-03
2024-10-08 23:00:07,188 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=58.38 vs. limit=8.78025
2024-10-08 23:00:07,675 WARNING [optim.py:503] Scaling gradients by 0.00027563993353396654, model_norm_threshold=498376328085504.0
2024-10-08 23:00:07,832 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.433e+35, grad_sumsq=2.913e+37, orig_rms_sq=2.209e-02
2024-10-08 23:00:09,006 WARNING [optim.py:503] Scaling gradients by 0.011938302777707577, model_norm_threshold=498376328085504.0
2024-10-08 23:00:09,163 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.916e+32, grad_sumsq=9.704e+34, orig_rms_sq=4.036e-03
2024-10-08 23:00:14,140 WARNING [optim.py:503] Scaling gradients by 0.007171626668423414, model_norm_threshold=498376328085504.0
2024-10-08 23:00:14,294 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.380e+33, grad_sumsq=6.154e+34, orig_rms_sq=2.243e-02
2024-10-08 23:00:15,029 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=27.34 vs. limit=8.78025
2024-10-08 23:00:16,884 INFO [train.py:1152] Epoch 2, batch 3400, loss[loss=0.7654, ctc_loss=1.059, attn_decoder_loss=0.692, over 4959.00 frames. ], tot_loss[loss=0.7549, ctc_loss=1.039, attn_decoder_loss=0.6837, over 966834.69 frames. ], batch size: 19, lr: 3.39e-02,
2024-10-08 23:00:18,671 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=34.79 vs. limit=8.7815
2024-10-08 23:00:19,318 WARNING [optim.py:503] Scaling gradients by 6.514730193885043e-05, model_norm_threshold=498376328085504.0
2024-10-08 23:00:19,472 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.481e+37, grad_sumsq=inf, orig_rms_sq=4.091e-03
2024-10-08 23:00:22,013 WARNING [optim.py:503] Scaling gradients by 0.00265153544023633, model_norm_threshold=498376328085504.0
2024-10-08 23:00:22,169 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.446e+34, grad_sumsq=3.521e+36, orig_rms_sq=4.108e-03
2024-10-08 23:00:25,261 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.17 vs. limit=8.7815
2024-10-08 23:00:25,992 WARNING [optim.py:503] Scaling gradients by 0.0939316377043724, model_norm_threshold=498376328085504.0
2024-10-08 23:00:26,146 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.289e+30, grad_sumsq=4.036e+32, orig_rms_sq=2.302e-02
2024-10-08 23:00:28,669 WARNING [optim.py:503] Scaling gradients by 0.00010860688053071499, model_norm_threshold=498376328085504.0
2024-10-08 23:00:28,826 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.934e+36, grad_sumsq=1.710e+38, orig_rms_sq=2.300e-02
2024-10-08 23:00:29,499 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.53 vs. limit=3.5131
2024-10-08 23:00:30,714 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=36.68 vs. limit=10.0655
2024-10-08 23:00:42,206 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=32.61 vs. limit=8.784
2024-10-08 23:00:43,782 WARNING [optim.py:503] Scaling gradients by 0.0974743515253067, model_norm_threshold=498376328085504.0
2024-10-08 23:00:43,939 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.774e+30, grad_sumsq=2.542e+32, orig_rms_sq=2.271e-02
2024-10-08 23:00:44,134 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=3424.0, ans=0.05740000000000001
2024-10-08 23:00:58,643 WARNING [optim.py:503] Scaling gradients by 0.03522470220923424, model_norm_threshold=498376328085504.0
2024-10-08 23:00:58,799 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.236e+31, grad_sumsq=2.773e+33, orig_rms_sq=2.249e-02
2024-10-08 23:01:01,285 WARNING [optim.py:503] Scaling gradients by 0.05479083210229874, model_norm_threshold=498376328085504.0
2024-10-08 23:01:01,441 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.299e+31, grad_sumsq=5.776e+32, orig_rms_sq=2.249e-02
2024-10-08 23:01:05,128 WARNING [optim.py:503] Scaling gradients by 0.0003461623564362526, model_norm_threshold=498376328085504.0
2024-10-08 23:01:05,287 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.129e+35, grad_sumsq=2.712e+37, orig_rms_sq=2.260e-02
2024-10-08 23:01:05,983 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=33.39 vs. limit=10.0705
2024-10-08 23:01:07,828 WARNING [optim.py:503] Scaling gradients by 0.07599574327468872, model_norm_threshold=498376328085504.0
2024-10-08 23:01:07,985 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.110e+30, grad_sumsq=3.559e+32, orig_rms_sq=2.279e-02
2024-10-08 23:01:11,204 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.77 vs. limit=10.073
2024-10-08 23:01:13,113 WARNING [optim.py:503] Scaling gradients by 0.011074732057750225, model_norm_threshold=498376328085504.0
2024-10-08 23:01:13,268 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.029e+32, grad_sumsq=2.635e+34, orig_rms_sq=2.288e-02
2024-10-08 23:01:18,323 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.114e+11 3.211e+13 3.070e+14 3.374e+15 7.650e+18, threshold=6.140e+14, percent-clipped=44.0
2024-10-08 23:01:18,324 WARNING [optim.py:503] Scaling gradients by 0.07191586494445801, model_norm_threshold=613981412655104.0
2024-10-08 23:01:18,480 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.370e+31, grad_sumsq=1.033e+33, orig_rms_sq=2.293e-02
2024-10-08 23:01:19,724 WARNING [optim.py:503] Scaling gradients by 0.018098747357726097, model_norm_threshold=613981412655104.0
2024-10-08 23:01:19,881 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.720e+32, grad_sumsq=1.186e+34, orig_rms_sq=2.293e-02
2024-10-08 23:01:21,110 WARNING [optim.py:503] Scaling gradients by 0.001735551399178803, model_norm_threshold=613981412655104.0
2024-10-08 23:01:21,266 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.638e+34, grad_sumsq=6.248e+36, orig_rms_sq=4.222e-03
2024-10-08 23:01:21,324 INFO [train.py:1152] Epoch 2, batch 3450, loss[loss=0.7819, ctc_loss=1.094, attn_decoder_loss=0.7037, over 4836.00 frames. ], tot_loss[loss=0.7545, ctc_loss=1.04, attn_decoder_loss=0.683, over 967029.08 frames. ], batch size: 43, lr: 3.39e-02,
2024-10-08 23:01:22,432 WARNING [optim.py:503] Scaling gradients by 0.0833413228392601, model_norm_threshold=613981412655104.0
2024-10-08 23:01:22,593 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.498e+31, grad_sumsq=6.533e+32, orig_rms_sq=2.293e-02
2024-10-08 23:01:23,562 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.57 vs. limit=10.0755
2024-10-08 23:01:23,907 WARNING [optim.py:503] Scaling gradients by 0.004754250403493643, model_norm_threshold=613981412655104.0
2024-10-08 23:01:24,063 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.335e+33, grad_sumsq=4.762e+32, orig_rms_sq=9.103e+00
2024-10-08 23:01:27,914 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=3434.0, ans=0.33903125
2024-10-08 23:01:29,552 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=16.40 vs. limit=8.787749999999999
2024-10-08 23:01:31,380 WARNING [optim.py:503] Scaling gradients by 0.07851212471723557, model_norm_threshold=613981412655104.0
2024-10-08 23:01:31,535 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.824e+31, grad_sumsq=6.952e+33, orig_rms_sq=4.062e-03
2024-10-08 23:01:32,713 WARNING [optim.py:503] Scaling gradients by 0.0019041151972487569, model_norm_threshold=613981412655104.0
2024-10-08 23:01:32,873 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.672e+34, grad_sumsq=1.172e+36, orig_rms_sq=2.280e-02
2024-10-08 23:01:33,536 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.58 vs. limit=5.859333333333334
2024-10-08 23:01:37,429 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=32.56 vs. limit=8.789
2024-10-08 23:01:40,377 WARNING [optim.py:503] Scaling gradients by 0.09865672141313553, model_norm_threshold=613981412655104.0
2024-10-08 23:01:40,532 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.126e+31, grad_sumsq=4.882e+32, orig_rms_sq=2.307e-02
2024-10-08 23:01:46,851 WARNING [optim.py:503] Scaling gradients by 0.04395876079797745, model_norm_threshold=613981412655104.0
2024-10-08 23:01:47,006 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.606e+31, grad_sumsq=2.119e+34, orig_rms_sq=4.062e-03
2024-10-08 23:01:49,426 WARNING [optim.py:503] Scaling gradients by 0.005490509793162346, model_norm_threshold=613981412655104.0
2024-10-08 23:01:49,583 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.61, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.570e+33, grad_sumsq=1.815e+36, orig_rms_sq=4.171e-03
2024-10-08 23:01:50,860 WARNING [optim.py:503] Scaling gradients by 0.03639790043234825, model_norm_threshold=613981412655104.0
2024-10-08 23:01:51,017 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.532e+32, grad_sumsq=3.673e+34, orig_rms_sq=4.171e-03
2024-10-08 23:01:52,216 WARNING [optim.py:503] Scaling gradients by 0.00039704699884168804, model_norm_threshold=613981412655104.0
2024-10-08 23:01:52,373 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.115e+36, grad_sumsq=2.673e+38, orig_rms_sq=4.171e-03
2024-10-08 23:01:54,750 WARNING [optim.py:503] Scaling gradients by 0.002849145559594035, model_norm_threshold=613981412655104.0
2024-10-08 23:01:54,907 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.469e+34, grad_sumsq=6.253e+35, orig_rms_sq=2.350e-02
2024-10-08 23:01:57,268 WARNING [optim.py:503] Scaling gradients by 0.013479438610374928, model_norm_threshold=613981412655104.0
2024-10-08 23:01:57,424 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.220e+32, grad_sumsq=2.222e+34, orig_rms_sq=2.350e-02
2024-10-08 23:01:58,668 WARNING [optim.py:503] Scaling gradients by 0.08844615519046783, model_norm_threshold=613981412655104.0
2024-10-08 23:01:58,824 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.644e+31, grad_sumsq=6.997e+32, orig_rms_sq=2.350e-02
2024-10-08 23:02:01,304 WARNING [optim.py:503] Scaling gradients by 0.0, model_norm_threshold=613981412655104.0
2024-10-08 23:02:01,341 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out_norm.log_scale with proportion 0.00, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.043e+33, grad_sumsq=3.043e+35, orig_rms_sq=1.000e-02
2024-10-08 23:02:02,476 WARNING [optim.py:503] Scaling gradients by 0.022310741245746613, model_norm_threshold=613981412655104.0
2024-10-08 23:02:02,635 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.110e+32, grad_sumsq=3.792e+31, orig_rms_sq=5.564e+00
2024-10-08 23:02:07,121 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=57.01 vs. limit=8.7915
2024-10-08 23:02:07,660 WARNING [optim.py:503] Scaling gradients by 0.0, model_norm_threshold=613981412655104.0
2024-10-08 23:02:07,696 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out_norm.log_scale with proportion 0.00, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.278e+36, grad_sumsq=1.278e+38, orig_rms_sq=1.000e-02
2024-10-08 23:02:09,715 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.74 vs. limit=3.5166
2024-10-08 23:02:10,219 WARNING [optim.py:503] Scaling gradients by 0.0040304651483893394, model_norm_threshold=613981412655104.0
2024-10-08 23:02:10,377 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.175e+34, grad_sumsq=4.961e+35, orig_rms_sq=2.369e-02
2024-10-08 23:02:12,510 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.66 vs. limit=8.79275
2024-10-08 23:02:12,894 WARNING [optim.py:503] Scaling gradients by 0.0030046356841921806, model_norm_threshold=613981412655104.0
2024-10-08 23:02:13,051 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.874e+33, grad_sumsq=3.324e+35, orig_rms_sq=2.369e-02
2024-10-08 23:02:14,307 WARNING [optim.py:503] Scaling gradients by 0.0071586985141038895, model_norm_threshold=613981412655104.0
2024-10-08 23:02:14,465 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.875e+33, grad_sumsq=3.342e+32, orig_rms_sq=5.610e+00
2024-10-08 23:02:15,558 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.91 vs. limit=5.861833333333333
2024-10-08 23:02:17,139 WARNING [optim.py:503] Scaling gradients by 0.0626969188451767, model_norm_threshold=613981412655104.0
2024-10-08 23:02:17,301 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.944e+31, grad_sumsq=1.244e+33, orig_rms_sq=2.367e-02
2024-10-08 23:02:20,430 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.14 vs. limit=10.0855
2024-10-08 23:02:21,105 WARNING [optim.py:503] Scaling gradients by 0.07856227457523346, model_norm_threshold=613981412655104.0
2024-10-08 23:02:21,262 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.216e+31, grad_sumsq=9.571e+32, orig_rms_sq=2.315e-02
2024-10-08 23:02:26,338 WARNING [optim.py:503] Scaling gradients by 0.0003609940758906305, model_norm_threshold=613981412655104.0
2024-10-08 23:02:26,493 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.606e+35, grad_sumsq=1.921e+38, orig_rms_sq=4.481e-03
2024-10-08 23:02:26,551 INFO [train.py:1152] Epoch 2, batch 3500, loss[loss=0.757, ctc_loss=1.047, attn_decoder_loss=0.6845, over 4883.00 frames. ], tot_loss[loss=0.7531, ctc_loss=1.038, attn_decoder_loss=0.6819, over 967387.59 frames. ], batch size: 19, lr: 3.38e-02,
2024-10-08 23:02:28,393 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=21.68 vs. limit=8.794
2024-10-08 23:02:28,640 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.20 vs. limit=3.5176
2024-10-08 23:02:28,965 WARNING [optim.py:503] Scaling gradients by 0.0030312794260680676, model_norm_threshold=613981412655104.0
2024-10-08 23:02:29,121 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.108e+34, grad_sumsq=4.846e+35, orig_rms_sq=2.287e-02
2024-10-08 23:02:34,153 WARNING [optim.py:503] Scaling gradients by 0.009015130810439587, model_norm_threshold=613981412655104.0
2024-10-08 23:02:34,311 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.334e+33, grad_sumsq=5.862e+34, orig_rms_sq=2.275e-02
2024-10-08 23:02:35,143 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.02 vs. limit=5.862666666666667
2024-10-08 23:02:36,620 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.09 vs. limit=6.725333333333333
2024-10-08 23:02:44,392 WARNING [optim.py:503] Scaling gradients by 0.032093651592731476, model_norm_threshold=613981412655104.0
2024-10-08 23:02:44,547 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.820e+31, grad_sumsq=1.224e+31, orig_rms_sq=5.570e+00
2024-10-08 23:02:49,602 WARNING [optim.py:503] Scaling gradients by 0.002820543246343732, model_norm_threshold=613981412655104.0
2024-10-08 23:02:49,758 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.773e+34, grad_sumsq=7.811e+35, orig_rms_sq=2.270e-02
2024-10-08 23:02:50,911 WARNING [optim.py:503] Scaling gradients by 0.0026339064352214336, model_norm_threshold=613981412655104.0
2024-10-08 23:02:51,068 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.397e+34, grad_sumsq=6.154e+35, orig_rms_sq=2.270e-02
2024-10-08 23:02:51,719 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.83 vs. limit=10.093
2024-10-08 23:02:54,733 WARNING [optim.py:503] Scaling gradients by 0.06578323245048523, model_norm_threshold=613981412655104.0
2024-10-08 23:02:54,889 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.226e+31, grad_sumsq=1.430e+33, orig_rms_sq=2.256e-02
2024-10-08 23:02:58,552 WARNING [optim.py:503] Scaling gradients by 0.002656162716448307, model_norm_threshold=613981412655104.0
2024-10-08 23:02:58,711 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.312e+33, grad_sumsq=4.156e+35, orig_rms_sq=2.241e-02
2024-10-08 23:03:04,829 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.39 vs. limit=10.0955
2024-10-08 23:03:04,994 WARNING [optim.py:503] Scaling gradients by 0.03872508928179741, model_norm_threshold=613981412655104.0
2024-10-08 23:03:05,150 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.057e+31, grad_sumsq=2.252e+33, orig_rms_sq=2.245e-02
2024-10-08 23:03:07,575 WARNING [optim.py:503] Scaling gradients by 0.04334869980812073, model_norm_threshold=613981412655104.0
2024-10-08 23:03:07,732 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.613e+31, grad_sumsq=2.498e+33, orig_rms_sq=2.247e-02
2024-10-08 23:03:12,857 WARNING [optim.py:503] Scaling gradients by 0.0024393685162067413, model_norm_threshold=613981412655104.0
2024-10-08 23:03:13,013 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.018e+34, grad_sumsq=8.991e+35, orig_rms_sq=2.245e-02
2024-10-08 23:03:13,245 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=3460.6666666666665, ans=0.7788766666666667
2024-10-08 23:03:13,854 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.60 vs. limit=5.865166666666667
2024-10-08 23:03:20,506 WARNING [optim.py:503] Scaling gradients by 0.006783257704228163, model_norm_threshold=613981412655104.0
2024-10-08 23:03:20,663 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.938e+33, grad_sumsq=8.585e+34, orig_rms_sq=2.258e-02
2024-10-08 23:03:22,583 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=31.31 vs. limit=10.097999999999999
2024-10-08 23:03:23,125 WARNING [optim.py:503] Scaling gradients by 7.702669245190918e-05, model_norm_threshold=613981412655104.0
2024-10-08 23:03:23,284 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.759e+37, grad_sumsq=inf, orig_rms_sq=2.259e-02
2024-10-08 23:03:25,796 WARNING [optim.py:503] Scaling gradients by 0.044358618557453156, model_norm_threshold=613981412655104.0
2024-10-08 23:03:25,951 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.342e+31, grad_sumsq=1.447e+34, orig_rms_sq=4.384e-03
2024-10-08 23:03:27,116 WARNING [optim.py:503] Scaling gradients by 0.0008307037642225623, model_norm_threshold=613981412655104.0
2024-10-08 23:03:27,273 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.729e+35, grad_sumsq=3.943e+37, orig_rms_sq=4.384e-03
2024-10-08 23:03:28,787 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.516e+11 1.430e+14 9.601e+14 2.752e+16 inf, threshold=1.920e+15, percent-clipped=55.0
2024-10-08 23:03:28,787 WARNING [optim.py:503] Scaling gradients by 0.0026006095577031374, model_norm_threshold=1920195455090688.0
2024-10-08 23:03:28,942 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.739e+34, grad_sumsq=1.233e+34, orig_rms_sq=7.901e+00
2024-10-08 23:03:31,517 INFO [train.py:1152] Epoch 2, batch 3550, loss[loss=0.7135, ctc_loss=1.008, attn_decoder_loss=0.6399, over 4795.00 frames. ], tot_loss[loss=0.7551, ctc_loss=1.039, attn_decoder_loss=0.684, over 967440.72 frames. ], batch size: 29, lr: 3.37e-02,
2024-10-08 23:03:34,654 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.71 vs. limit=3.5201000000000002
2024-10-08 23:03:34,830 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.78 vs. limit=5.866833333333333
2024-10-08 23:03:38,252 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=29.72 vs. limit=10.1005
2024-10-08 23:03:42,673 WARNING [optim.py:503] Scaling gradients by 0.003721818095073104, model_norm_threshold=1920195455090688.0
2024-10-08 23:03:42,829 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.566e+34, grad_sumsq=3.326e+36, orig_rms_sq=2.275e-02
2024-10-08 23:03:44,343 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=3470.6666666666665, ans=0.3373125
2024-10-08 23:03:48,817 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=15.89 vs. limit=8.8015
2024-10-08 23:03:49,163 WARNING [optim.py:503] Scaling gradients by 0.0005144233000464737, model_norm_threshold=1920195455090688.0
2024-10-08 23:03:49,322 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.234e+36, grad_sumsq=2.803e+38, orig_rms_sq=2.224e-02
2024-10-08 23:03:50,500 WARNING [optim.py:503] Scaling gradients by 0.009648746810853481, model_norm_threshold=1920195455090688.0
2024-10-08 23:03:50,655 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.108e+34, grad_sumsq=4.982e+35, orig_rms_sq=2.224e-02
2024-10-08 23:03:53,216 WARNING [optim.py:503] Scaling gradients by 0.00024144725466612726, model_norm_threshold=1920195455090688.0
2024-10-08 23:03:53,371 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.044e+37, grad_sumsq=1.864e+36, orig_rms_sq=5.599e+00
2024-10-08 23:03:59,528 WARNING [optim.py:503] Scaling gradients by 0.00204956135712564, model_norm_threshold=1920195455090688.0
2024-10-08 23:03:59,685 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.940e+35, grad_sumsq=3.582e+34, orig_rms_sq=5.415e+00
2024-10-08 23:03:59,944 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3474.0, ans=0.26526
2024-10-08 23:04:09,484 WARNING [optim.py:503] Scaling gradients by 0.02023390866816044, model_norm_threshold=1920195455090688.0
2024-10-08 23:04:09,639 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.340e+33, grad_sumsq=7.954e+32, orig_rms_sq=5.456e+00
2024-10-08 23:04:12,001 WARNING [optim.py:503] Scaling gradients by 0.09442201256752014, model_norm_threshold=1920195455090688.0
2024-10-08 23:04:12,156 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.645e+31, grad_sumsq=1.789e+34, orig_rms_sq=4.272e-03
2024-10-08 23:04:12,636 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten.whitening_limit, batch_count=3477.3333333333335, ans=8.804
2024-10-08 23:04:14,551 WARNING [optim.py:503] Scaling gradients by 0.00014310612459667027, model_norm_threshold=1920195455090688.0
2024-10-08 23:04:14,716 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.400e+37, grad_sumsq=inf, orig_rms_sq=2.229e-02
2024-10-08 23:04:15,925 WARNING [optim.py:503] Scaling gradients by 0.002623127307742834, model_norm_threshold=1920195455090688.0
2024-10-08 23:04:16,083 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.246e+35, grad_sumsq=5.563e+36, orig_rms_sq=2.240e-02
2024-10-08 23:04:18,535 WARNING [optim.py:503] Scaling gradients by 0.07896872609853745, model_norm_threshold=1920195455090688.0
2024-10-08 23:04:18,693 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.67, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.946e+32, grad_sumsq=9.373e+34, orig_rms_sq=4.210e-03
2024-10-08 23:04:19,708 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.96 vs. limit=5.8693333333333335
2024-10-08 23:04:23,727 WARNING [optim.py:503] Scaling gradients by 0.02111135423183441, model_norm_threshold=1920195455090688.0
2024-10-08 23:04:23,882 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.077e+33, grad_sumsq=1.353e+35, orig_rms_sq=2.275e-02
2024-10-08 23:04:25,065 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=12.85 vs. limit=4.696133333333333
2024-10-08 23:04:30,306 WARNING [optim.py:503] Scaling gradients by 0.0012645836686715484, model_norm_threshold=1920195455090688.0
2024-10-08 23:04:30,461 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.341e+35, grad_sumsq=2.756e+37, orig_rms_sq=2.301e-02
2024-10-08 23:04:32,438 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.13 vs. limit=10.1105
2024-10-08 23:04:34,809 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=28.81 vs. limit=8.805250000000001
2024-10-08 23:04:36,799 INFO [train.py:1152] Epoch 2, batch 3600, loss[loss=0.7825, ctc_loss=1.077, attn_decoder_loss=0.7089, over 4925.00 frames. ], tot_loss[loss=0.7557, ctc_loss=1.04, attn_decoder_loss=0.6846, over 967477.28 frames. ], batch size: 20, lr: 3.37e-02,
2024-10-08 23:04:37,918 WARNING [optim.py:503] Scaling gradients by 0.003050785278901458, model_norm_threshold=1920195455090688.0
2024-10-08 23:04:38,078 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.913e+34, grad_sumsq=3.918e+36, orig_rms_sq=2.275e-02
2024-10-08 23:04:39,257 WARNING [optim.py:503] Scaling gradients by 0.09964174032211304, model_norm_threshold=1920195455090688.0
2024-10-08 23:04:39,414 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.433e+32, grad_sumsq=6.299e+33, orig_rms_sq=2.275e-02
2024-10-08 23:04:40,610 WARNING [optim.py:503] Scaling gradients by 0.0005770265124738216, model_norm_threshold=1920195455090688.0
2024-10-08 23:04:40,766 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.746e+36, grad_sumsq=1.647e+38, orig_rms_sq=2.275e-02
2024-10-08 23:04:44,807 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=3484.0, ans=0.33668750000000003
2024-10-08 23:04:53,356 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.22 vs. limit=5.871833333333333
2024-10-08 23:05:01,092 WARNING [optim.py:503] Scaling gradients by 0.013462280854582787, model_norm_threshold=1920195455090688.0
2024-10-08 23:05:01,249 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.509e+33, grad_sumsq=2.405e+35, orig_rms_sq=2.291e-02
2024-10-08 23:05:06,750 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=16.41 vs. limit=8.809
2024-10-08 23:05:08,281 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.51 vs. limit=10.118
2024-10-08 23:05:10,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=3490.6666666666665, ans=0.336375
2024-10-08 23:05:17,415 WARNING [optim.py:503] Scaling gradients by 0.00037663851981051266, model_norm_threshold=1920195455090688.0
2024-10-08 23:05:17,571 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.796e+36, grad_sumsq=inf, orig_rms_sq=4.272e-03
2024-10-08 23:05:22,967 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.61 vs. limit=3.5241
2024-10-08 23:05:26,819 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten.whitening_limit, batch_count=3497.3333333333335, ans=8.8115
2024-10-08 23:05:28,061 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=24.74 vs. limit=10.123000000000001
2024-10-08 23:05:31,816 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.12 vs. limit=8.8115
2024-10-08 23:05:32,381 WARNING [optim.py:503] Scaling gradients by 0.011006750166416168, model_norm_threshold=1920195455090688.0
2024-10-08 23:05:32,537 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.045e+34, grad_sumsq=4.384e+35, orig_rms_sq=2.385e-02
2024-10-08 23:05:35,731 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.24 vs. limit=6.748666666666667
2024-10-08 23:05:37,157 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=7.04 vs. limit=5.398933333333334
2024-10-08 23:05:37,544 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.754e+12 6.845e+13 6.216e+14 9.422e+15 1.342e+19, threshold=1.243e+15, percent-clipped=37.0
2024-10-08 23:05:40,099 INFO [train.py:1152] Epoch 2, batch 3650, loss[loss=0.7862, ctc_loss=1.094, attn_decoder_loss=0.7091, over 4851.00 frames. ], tot_loss[loss=0.7559, ctc_loss=1.04, attn_decoder_loss=0.6849, over 967979.47 frames. ], batch size: 31, lr: 3.36e-02,
2024-10-08 23:05:48,625 WARNING [optim.py:503] Scaling gradients by 0.022556470707058907, model_norm_threshold=1243143118782464.0
2024-10-08 23:05:48,780 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.562e+32, grad_sumsq=2.843e+34, orig_rms_sq=2.308e-02
2024-10-08 23:05:49,963 WARNING [optim.py:503] Scaling gradients by 0.05314681679010391, model_norm_threshold=1243143118782464.0
2024-10-08 23:05:50,130 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.094e+32, grad_sumsq=4.738e+33, orig_rms_sq=2.308e-02
2024-10-08 23:06:00,254 WARNING [optim.py:503] Scaling gradients by 0.015397467650473118, model_norm_threshold=1243143118782464.0
2024-10-08 23:06:00,409 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.345e+33, grad_sumsq=6.039e+34, orig_rms_sq=2.227e-02
2024-10-08 23:06:02,346 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=56.34 vs. limit=8.814
2024-10-08 23:06:03,986 WARNING [optim.py:503] Scaling gradients by 0.04257489740848541, model_norm_threshold=1243143118782464.0
2024-10-08 23:06:04,147 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.751e+32, grad_sumsq=7.020e+34, orig_rms_sq=3.918e-03
2024-10-08 23:06:10,372 WARNING [optim.py:503] Scaling gradients by 0.013665017671883106, model_norm_threshold=1243143118782464.0
2024-10-08 23:06:10,531 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.361e+33, grad_sumsq=1.091e+35, orig_rms_sq=2.163e-02
2024-10-08 23:06:14,338 WARNING [optim.py:503] Scaling gradients by 0.022785913199186325, model_norm_threshold=1243143118782464.0
2024-10-08 23:06:14,493 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.221e+33, grad_sumsq=5.662e+34, orig_rms_sq=2.156e-02
2024-10-08 23:06:23,337 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.max_abs, batch_count=3510.6666666666665, ans=7.194166666666666
2024-10-08 23:06:24,175 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.87 vs. limit=10.133
2024-10-08 23:06:25,612 WARNING [optim.py:503] Scaling gradients by 0.00011604934115894139, model_norm_threshold=1243143118782464.0
2024-10-08 23:06:25,769 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.235e+37, grad_sumsq=inf, orig_rms_sq=2.221e-02
2024-10-08 23:06:26,974 WARNING [optim.py:503] Scaling gradients by 0.013346069492399693, model_norm_threshold=1243143118782464.0
2024-10-08 23:06:27,129 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.722e+33, grad_sumsq=1.225e+35, orig_rms_sq=2.221e-02
2024-10-08 23:06:33,633 WARNING [optim.py:503] Scaling gradients by 0.09330961853265762, model_norm_threshold=1243143118782464.0
2024-10-08 23:06:33,788 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.linear_pos.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.585e+31, grad_sumsq=5.918e+30, orig_rms_sq=7.748e+00
2024-10-08 23:06:36,234 WARNING [optim.py:503] Scaling gradients by 0.0, model_norm_threshold=1243143118782464.0
2024-10-08 23:06:36,271 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out_norm.log_scale with proportion 0.00, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.241e+34, grad_sumsq=1.241e+36, orig_rms_sq=1.000e-02
2024-10-08 23:06:37,424 WARNING [optim.py:503] Scaling gradients by 0.006068630144000053, model_norm_threshold=1243143118782464.0
2024-10-08 23:06:37,579 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.600e+33, grad_sumsq=1.509e+33, orig_rms_sq=5.700e+00
2024-10-08 23:06:43,373 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=30.63 vs. limit=8.818999999999999
2024-10-08 23:06:43,955 INFO [train.py:1152] Epoch 2, batch 3700, loss[loss=0.7485, ctc_loss=1.069, attn_decoder_loss=0.6684, over 4853.00 frames. ], tot_loss[loss=0.7571, ctc_loss=1.043, attn_decoder_loss=0.6857, over 967367.50 frames. ], batch size: 24, lr: 3.36e-02,
2024-10-08 23:06:46,427 WARNING [optim.py:503] Scaling gradients by 0.03802682086825371, model_norm_threshold=1243143118782464.0
2024-10-08 23:06:46,586 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.644e+32, grad_sumsq=1.146e+34, orig_rms_sq=2.307e-02
2024-10-08 23:06:47,242 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.80 vs. limit=3.5276
2024-10-08 23:06:48,057 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=3517.3333333333335, ans=0.335125
2024-10-08 23:06:48,989 WARNING [optim.py:503] Scaling gradients by 0.014583933167159557, model_norm_threshold=1243143118782464.0
2024-10-08 23:06:49,145 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.524e+33, grad_sumsq=1.086e+35, orig_rms_sq=2.323e-02
2024-10-08 23:06:51,455 WARNING [optim.py:503] Scaling gradients by 0.06057603657245636, model_norm_threshold=1243143118782464.0
2024-10-08 23:06:51,611 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.388e+32, grad_sumsq=3.457e+34, orig_rms_sq=4.015e-03
2024-10-08 23:06:52,841 WARNING [optim.py:503] Scaling gradients by 0.000912513118237257, model_norm_threshold=1243143118782464.0
2024-10-08 23:06:52,999 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.852e+35, grad_sumsq=3.380e+37, orig_rms_sq=2.323e-02
2024-10-08 23:06:54,375 WARNING [optim.py:503] Scaling gradients by 0.01912187598645687, model_norm_threshold=1243143118782464.0
2024-10-08 23:06:54,531 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.173e+33, grad_sumsq=5.016e+34, orig_rms_sq=2.338e-02
2024-10-08 23:06:57,123 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=3520.6666666666665, ans=0.06797500000000001
2024-10-08 23:06:58,506 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=3520.6666666666665, ans=0.33496875000000004
2024-10-08 23:07:00,861 WARNING [optim.py:503] Scaling gradients by 0.00500644464045763, model_norm_threshold=1243143118782464.0
2024-10-08 23:07:01,031 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.620e+34, grad_sumsq=6.994e+35, orig_rms_sq=2.316e-02
2024-10-08 23:07:02,797 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.50 vs. limit=8.82025
2024-10-08 23:07:04,635 WARNING [optim.py:503] Scaling gradients by 7.760192238492891e-05, model_norm_threshold=1243143118782464.0
2024-10-08 23:07:04,791 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.834e+37, grad_sumsq=inf, orig_rms_sq=2.279e-02
2024-10-08 23:07:05,466 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=14.65 vs. limit=5.880166666666667
2024-10-08 23:07:09,374 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=37.27 vs. limit=8.8215
2024-10-08 23:07:09,819 WARNING [optim.py:503] Scaling gradients by 0.0024102400057017803, model_norm_threshold=1243143118782464.0
2024-10-08 23:07:09,976 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.383e+34, grad_sumsq=1.133e+37, orig_rms_sq=3.869e-03
2024-10-08 23:07:10,582 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.79 vs. limit=6.7620000000000005
2024-10-08 23:07:11,287 WARNING [optim.py:503] Scaling gradients by 0.015353725291788578, model_norm_threshold=1243143118782464.0
2024-10-08 23:07:11,478 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.220e+33, grad_sumsq=5.447e+34, orig_rms_sq=2.240e-02
2024-10-08 23:07:12,641 WARNING [optim.py:503] Scaling gradients by 0.012026887387037277, model_norm_threshold=1243143118782464.0
2024-10-08 23:07:12,795 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.263e+33, grad_sumsq=5.849e+35, orig_rms_sq=3.869e-03
2024-10-08 23:07:15,256 WARNING [optim.py:503] Scaling gradients by 0.003759278915822506, model_norm_threshold=1243143118782464.0
2024-10-08 23:07:15,413 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.218e+34, grad_sumsq=8.420e+36, orig_rms_sq=3.821e-03
2024-10-08 23:07:18,497 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=29.11 vs. limit=8.8215
2024-10-08 23:07:21,606 WARNING [optim.py:503] Scaling gradients by 0.04439333453774452, model_norm_threshold=1243143118782464.0
2024-10-08 23:07:21,764 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.648e+32, grad_sumsq=4.308e+34, orig_rms_sq=3.827e-03
2024-10-08 23:07:28,195 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=3527.3333333333335, ans=0.33465625
2024-10-08 23:07:28,817 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.98 vs. limit=8.82275
2024-10-08 23:07:32,896 WARNING [optim.py:503] Scaling gradients by 0.0011914914939552546, model_norm_threshold=1243143118782464.0
2024-10-08 23:07:33,051 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.087e+35, grad_sumsq=9.466e+36, orig_rms_sq=2.205e-02
2024-10-08 23:07:36,701 WARNING [optim.py:503] Scaling gradients by 0.08389405161142349, model_norm_threshold=1243143118782464.0
2024-10-08 23:07:36,856 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.743e+31, grad_sumsq=3.042e+33, orig_rms_sq=2.216e-02
2024-10-08 23:07:38,069 WARNING [optim.py:503] Scaling gradients by 0.046178728342056274, model_norm_threshold=1243143118782464.0
2024-10-08 23:07:38,225 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.610e+32, grad_sumsq=1.178e+34, orig_rms_sq=2.216e-02
2024-10-08 23:07:38,362 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=3530.6666666666665, ans=0.020559999999999995
2024-10-08 23:07:38,424 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=3530.6666666666665, ans=0.0586666666666667
2024-10-08 23:07:46,198 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.451e+12 1.073e+14 1.426e+15 1.482e+16 inf, threshold=2.853e+15, percent-clipped=51.0
2024-10-08 23:07:48,700 INFO [train.py:1152] Epoch 2, batch 3750, loss[loss=0.7125, ctc_loss=0.9625, attn_decoder_loss=0.65, over 4959.00 frames. ], tot_loss[loss=0.7568, ctc_loss=1.042, attn_decoder_loss=0.6854, over 967811.19 frames. ], batch size: 19, lr: 3.35e-02,
2024-10-08 23:07:50,691 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=28.49 vs. limit=10.150500000000001
2024-10-08 23:07:53,662 WARNING [optim.py:503] Scaling gradients by 0.0004797921283170581, model_norm_threshold=2852796734898176.0
2024-10-08 23:07:53,819 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.716e+36, grad_sumsq=3.003e+38, orig_rms_sq=2.237e-02
2024-10-08 23:08:00,976 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=16.85 vs. limit=8.8265
2024-10-08 23:08:01,150 WARNING [optim.py:503] Scaling gradients by 0.0009994114516302943, model_norm_threshold=2852796734898176.0
2024-10-08 23:08:01,308 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.778e+36, grad_sumsq=1.977e+35, orig_rms_sq=8.992e+00
2024-10-08 23:08:01,481 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=3537.3333333333335, ans=0.33418749999999997
2024-10-08 23:08:02,524 WARNING [optim.py:503] Scaling gradients by 0.09684640914201736, model_norm_threshold=2852796734898176.0
2024-10-08 23:08:02,680 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.895e+32, grad_sumsq=3.251e+31, orig_rms_sq=5.830e+00
2024-10-08 23:08:07,096 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=14.88 vs. limit=8.8265
2024-10-08 23:08:09,192 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=3537.3333333333335, ans=0.7761933333333334
2024-10-08 23:08:10,478 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=3537.3333333333335, ans=0.05783333333333329
2024-10-08 23:08:11,742 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=3537.3333333333335, ans=0.020409999999999984
2024-10-08 23:08:14,101 WARNING [optim.py:503] Scaling gradients by 0.0723990947008133, model_norm_threshold=2852796734898176.0
2024-10-08 23:08:14,257 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.271e+32, grad_sumsq=7.819e+34, orig_rms_sq=4.184e-03
2024-10-08 23:08:17,527 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.74 vs. limit=6.770333333333333
2024-10-08 23:08:18,852 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=17.55 vs. limit=10.1555
2024-10-08 23:08:19,150 WARNING [optim.py:503] Scaling gradients by 0.01679944060742855, model_norm_threshold=2852796734898176.0
2024-10-08 23:08:19,306 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.254e+34, grad_sumsq=2.146e+33, orig_rms_sq=5.846e+00
2024-10-08 23:08:23,788 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=3540.6666666666665, ans=10.1555
2024-10-08 23:08:27,428 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=23.75 vs. limit=10.158
2024-10-08 23:08:28,381 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=3544.0, ans=0.33387500000000003
2024-10-08 23:08:29,431 WARNING [optim.py:503] Scaling gradients by 0.05993879958987236, model_norm_threshold=2852796734898176.0
2024-10-08 23:08:29,589 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.352e+32, grad_sumsq=2.335e+34, orig_rms_sq=2.292e-02
2024-10-08 23:08:29,831 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=3544.0, ans=0.06709999999999999
2024-10-08 23:08:30,773 WARNING [optim.py:503] Scaling gradients by 0.006946322042495012, model_norm_threshold=2852796734898176.0
2024-10-08 23:08:30,929 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.852e+34, grad_sumsq=1.244e+36, orig_rms_sq=2.292e-02
2024-10-08 23:08:31,607 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=8.24 vs. limit=5.4176
2024-10-08 23:08:32,104 WARNING [optim.py:503] Scaling gradients by 0.02438635751605034, model_norm_threshold=2852796734898176.0
2024-10-08 23:08:32,258 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.344e+33, grad_sumsq=1.866e+35, orig_rms_sq=2.328e-02
2024-10-08 23:08:36,015 WARNING [optim.py:503] Scaling gradients by 0.0030800316017121077, model_norm_threshold=2852796734898176.0
2024-10-08 23:08:36,171 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.852e+35, grad_sumsq=1.225e+37, orig_rms_sq=2.328e-02
2024-10-08 23:08:43,562 WARNING [optim.py:503] Scaling gradients by 0.0, model_norm_threshold=2852796734898176.0
2024-10-08 23:08:43,598 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out_norm.log_scale with proportion 0.00, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.208e+35, grad_sumsq=7.208e+37, orig_rms_sq=1.000e-02
2024-10-08 23:08:44,783 WARNING [optim.py:503] Scaling gradients by 0.020147118717432022, model_norm_threshold=2852796734898176.0
2024-10-08 23:08:44,939 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.890e+33, grad_sumsq=2.483e+35, orig_rms_sq=2.373e-02
2024-10-08 23:08:52,511 WARNING [optim.py:503] Scaling gradients by 0.03620396926999092, model_norm_threshold=2852796734898176.0
2024-10-08 23:08:52,668 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.211e+33, grad_sumsq=5.338e+35, orig_rms_sq=4.143e-03
2024-10-08 23:08:52,726 INFO [train.py:1152] Epoch 2, batch 3800, loss[loss=0.7513, ctc_loss=1.045, attn_decoder_loss=0.678, over 4774.00 frames. ], tot_loss[loss=0.755, ctc_loss=1.04, attn_decoder_loss=0.6838, over 967504.90 frames. ], batch size: 26, lr: 3.35e-02,
2024-10-08 23:08:55,563 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.50 vs. limit=8.8315
2024-10-08 23:08:56,546 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=3550.6666666666665, ans=0.3335625
2024-10-08 23:09:00,073 WARNING [optim.py:503] Scaling gradients by 0.007481634616851807, model_norm_threshold=2852796734898176.0
2024-10-08 23:09:00,231 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.040e+34, grad_sumsq=9.954e+36, orig_rms_sq=4.059e-03
2024-10-08 23:09:01,493 WARNING [optim.py:503] Scaling gradients by 0.005933361127972603, model_norm_threshold=2852796734898176.0
2024-10-08 23:09:01,652 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.431e+34, grad_sumsq=2.273e+36, orig_rms_sq=2.390e-02
2024-10-08 23:09:03,983 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.87 vs. limit=8.8315
2024-10-08 23:09:04,280 WARNING [optim.py:503] Scaling gradients by 0.08600297570228577, model_norm_threshold=2852796734898176.0
2024-10-08 23:09:04,435 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.703e+32, grad_sumsq=1.556e+34, orig_rms_sq=2.380e-02
2024-10-08 23:09:09,396 WARNING [optim.py:503] Scaling gradients by 0.020784959197044373, model_norm_threshold=2852796734898176.0
2024-10-08 23:09:09,552 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.764e+33, grad_sumsq=1.448e+36, orig_rms_sq=3.981e-03
2024-10-08 23:09:12,110 WARNING [optim.py:503] Scaling gradients by 0.0985746830701828, model_norm_threshold=2852796734898176.0
2024-10-08 23:09:12,267 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.673e+32, grad_sumsq=1.131e+34, orig_rms_sq=2.364e-02
2024-10-08 23:09:18,212 WARNING [optim.py:503] Scaling gradients by 0.06906973570585251, model_norm_threshold=2852796734898176.0
2024-10-08 23:09:18,371 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.667e+32, grad_sumsq=2.397e+34, orig_rms_sq=2.364e-02
2024-10-08 23:09:19,611 WARNING [optim.py:503] Scaling gradients by 0.0002684685750864446, model_norm_threshold=2852796734898176.0
2024-10-08 23:09:19,765 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.360e+37, grad_sumsq=3.721e+36, orig_rms_sq=9.030e+00
2024-10-08 23:09:20,951 WARNING [optim.py:503] Scaling gradients by 0.02250136062502861, model_norm_threshold=2852796734898176.0
2024-10-08 23:09:21,109 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.038e+33, grad_sumsq=1.693e+35, orig_rms_sq=2.385e-02
2024-10-08 23:09:23,525 WARNING [optim.py:503] Scaling gradients by 0.0012124860659241676, model_norm_threshold=2852796734898176.0
2024-10-08 23:09:23,683 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.220e+36, grad_sumsq=5.114e+37, orig_rms_sq=2.385e-02
2024-10-08 23:09:27,418 WARNING [optim.py:503] Scaling gradients by 0.004106726497411728, model_norm_threshold=2852796734898176.0
2024-10-08 23:09:27,577 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.206e+35, grad_sumsq=4.981e+36, orig_rms_sq=2.422e-02
2024-10-08 23:09:28,170 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.24 vs. limit=10.168
2024-10-08 23:09:30,359 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=3557.3333333333335, ans=0.019960000000000006
2024-10-08 23:09:31,387 WARNING [optim.py:503] Scaling gradients by 0.025241896510124207, model_norm_threshold=2852796734898176.0
2024-10-08 23:09:31,544 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.870e+33, grad_sumsq=1.589e+35, orig_rms_sq=2.435e-02
2024-10-08 23:09:33,538 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=35.91 vs. limit=10.1705
2024-10-08 23:09:34,798 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.65 vs. limit=3.5341
2024-10-08 23:09:40,143 WARNING [optim.py:503] Scaling gradients by 0.010132519528269768, model_norm_threshold=2852796734898176.0
2024-10-08 23:09:40,298 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.864e+34, grad_sumsq=3.171e+33, orig_rms_sq=9.033e+00
2024-10-08 23:09:40,509 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.min_abs, batch_count=3560.6666666666665, ans=0.25341
2024-10-08 23:09:42,760 WARNING [optim.py:503] Scaling gradients by 0.0025913200806826353, model_norm_threshold=2852796734898176.0
2024-10-08 23:09:42,914 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.942e+35, grad_sumsq=2.018e+37, orig_rms_sq=2.449e-02
2024-10-08 23:09:49,163 WARNING [optim.py:503] Scaling gradients by 0.0023581560235470533, model_norm_threshold=2852796734898176.0
2024-10-08 23:09:49,319 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.827e+35, grad_sumsq=2.398e+37, orig_rms_sq=2.430e-02
2024-10-08 23:09:50,221 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=25.87 vs. limit=10.173
2024-10-08 23:09:52,049 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=3564.0, ans=0.3329375
2024-10-08 23:09:52,698 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=15.96 vs. limit=8.836500000000001
2024-10-08 23:09:53,860 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=6.14 vs. limit=5.4256
2024-10-08 23:09:55,635 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.465e+11 1.740e+14 1.049e+15 2.946e+16 inf, threshold=2.098e+15, percent-clipped=46.0
2024-10-08 23:09:58,169 INFO [train.py:1152] Epoch 2, batch 3850, loss[loss=0.7253, ctc_loss=0.995, attn_decoder_loss=0.6579, over 4821.00 frames. ], tot_loss[loss=0.7535, ctc_loss=1.038, attn_decoder_loss=0.6825, over 967557.35 frames. ], batch size: 38, lr: 3.34e-02,
2024-10-08 23:10:10,246 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=5.64 vs. limit=5.428266666666667
2024-10-08 23:10:11,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.32 vs. limit=10.178
2024-10-08 23:10:23,050 WARNING [optim.py:503] Scaling gradients by 0.03923185169696808, model_norm_threshold=2097712627449856.0
2024-10-08 23:10:23,206 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.895e+32, grad_sumsq=2.864e+34, orig_rms_sq=2.407e-02
2024-10-08 23:10:23,933 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=56.96 vs. limit=8.84025
2024-10-08 23:10:23,943 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=35.45 vs. limit=10.1805
2024-10-08 23:10:24,234 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.90 vs. limit=10.1805
2024-10-08 23:10:26,916 WARNING [optim.py:503] Scaling gradients by 0.020281879231333733, model_norm_threshold=2097712627449856.0
2024-10-08 23:10:27,073 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.190e+33, grad_sumsq=4.314e+32, orig_rms_sq=5.075e+00
2024-10-08 23:10:28,288 WARNING [optim.py:503] Scaling gradients by 0.004204873461276293, model_norm_threshold=2097712627449856.0
2024-10-08 23:10:28,443 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.878e+34, grad_sumsq=2.437e+36, orig_rms_sq=2.412e-02
2024-10-08 23:10:29,944 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=3574.0, ans=0.065975
2024-10-08 23:10:30,913 WARNING [optim.py:503] Scaling gradients by 0.0552840456366539, model_norm_threshold=2097712627449856.0
2024-10-08 23:10:31,068 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.073e+32, grad_sumsq=1.266e+34, orig_rms_sq=2.428e-02
2024-10-08 23:10:32,963 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.71 vs. limit=3.5361000000000002
2024-10-08 23:10:34,107 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.09 vs. limit=10.1805
2024-10-08 23:10:39,168 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=23.74 vs. limit=8.8415
2024-10-08 23:10:43,778 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3577.3333333333335, ans=0.26422666666666667
2024-10-08 23:10:45,446 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.11 vs. limit=5.894333333333334
2024-10-08 23:10:46,995 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.12 vs. limit=5.894333333333334
2024-10-08 23:10:52,391 WARNING [optim.py:503] Scaling gradients by 0.0007451234851032495, model_norm_threshold=2097712627449856.0
2024-10-08 23:10:52,547 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.389e+36, grad_sumsq=9.575e+37, orig_rms_sq=2.496e-02
2024-10-08 23:10:58,059 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=31.81 vs. limit=10.185500000000001
2024-10-08 23:10:58,343 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.84 vs. limit=10.185500000000001
2024-10-08 23:11:01,187 WARNING [optim.py:503] Scaling gradients by 0.0024891996290534735, model_norm_threshold=2097712627449856.0
2024-10-08 23:11:01,344 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.375e+35, grad_sumsq=1.341e+37, orig_rms_sq=2.516e-02
2024-10-08 23:11:01,402 INFO [train.py:1152] Epoch 2, batch 3900, loss[loss=0.7562, ctc_loss=1.037, attn_decoder_loss=0.6859, over 4751.00 frames. ], tot_loss[loss=0.7541, ctc_loss=1.038, attn_decoder_loss=0.6832, over 967006.82 frames. ], batch size: 26, lr: 3.34e-02,
2024-10-08 23:11:03,733 WARNING [optim.py:503] Scaling gradients by 0.00024953525280579925, model_norm_threshold=2097712627449856.0
2024-10-08 23:11:03,890 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.747e+37, grad_sumsq=inf, orig_rms_sq=2.516e-02
2024-10-08 23:11:07,582 WARNING [optim.py:503] Scaling gradients by 0.003901991294696927, model_norm_threshold=2097712627449856.0
2024-10-08 23:11:07,739 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.852e+34, grad_sumsq=3.505e+36, orig_rms_sq=2.525e-02
2024-10-08 23:11:08,957 WARNING [optim.py:503] Scaling gradients by 0.0006591247511096299, model_norm_threshold=2097712627449856.0
2024-10-08 23:11:09,113 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.353e+36, grad_sumsq=9.319e+37, orig_rms_sq=2.525e-02
2024-10-08 23:11:12,780 WARNING [optim.py:503] Scaling gradients by 0.000420435331761837, model_norm_threshold=2097712627449856.0
2024-10-08 23:11:12,936 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.503e+36, grad_sumsq=inf, orig_rms_sq=2.495e-02
2024-10-08 23:11:14,875 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.81 vs. limit=10.1905
2024-10-08 23:11:18,274 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=16.62 vs. limit=8.84525
2024-10-08 23:11:20,384 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=3587.3333333333335, ans=0.33184375
2024-10-08 23:11:25,708 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=32.39 vs. limit=8.8465
2024-10-08 23:11:29,175 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=3590.6666666666665, ans=0.3316875
2024-10-08 23:11:33,960 WARNING [optim.py:503] Scaling gradients by 0.0053866151720285416, model_norm_threshold=2097712627449856.0
2024-10-08 23:11:34,121 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.971e+34, grad_sumsq=1.219e+36, orig_rms_sq=2.436e-02
2024-10-08 23:11:35,519 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=3590.6666666666665, ans=0.3316875
2024-10-08 23:11:35,582 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=3590.6666666666665, ans=0.06535000000000002
2024-10-08 23:11:46,151 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=18.79 vs. limit=8.84775
2024-10-08 23:11:46,852 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=3594.0, ans=0.06522499999999998
2024-10-08 23:11:50,209 WARNING [optim.py:503] Scaling gradients by 0.00048382458044216037, model_norm_threshold=2097712627449856.0
2024-10-08 23:11:50,365 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.314e+36, grad_sumsq=1.756e+38, orig_rms_sq=2.456e-02
2024-10-08 23:11:54,619 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.63 vs. limit=5.438933333333333
2024-10-08 23:12:01,544 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.028e+11 4.249e+13 4.759e+14 4.175e+15 8.406e+18, threshold=9.519e+14, percent-clipped=32.0
2024-10-08 23:12:04,102 INFO [train.py:1152] Epoch 2, batch 3950, loss[loss=0.8024, ctc_loss=1.096, attn_decoder_loss=0.7289, over 4837.00 frames. ], tot_loss[loss=0.7497, ctc_loss=1.032, attn_decoder_loss=0.679, over 967330.14 frames. ], batch size: 36, lr: 3.33e-02,
2024-10-08 23:12:08,914 WARNING [optim.py:503] Scaling gradients by 0.042053427547216415, model_norm_threshold=951858705203200.0
2024-10-08 23:12:09,071 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.381e+32, grad_sumsq=5.661e+33, orig_rms_sq=2.439e-02
2024-10-08 23:12:14,099 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=3600.6666666666665, ans=0.33121875
2024-10-08 23:12:14,102 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=3600.6666666666665, ans=0.33121875
2024-10-08 23:12:14,124 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3600.6666666666665, ans=0.2639933333333333
2024-10-08 23:12:22,597 WARNING [optim.py:503] Scaling gradients by 0.0020940257236361504, model_norm_threshold=951858705203200.0
2024-10-08 23:12:22,757 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.523e+34, grad_sumsq=2.286e+36, orig_rms_sq=2.416e-02
2024-10-08 23:12:23,946 WARNING [optim.py:503] Scaling gradients by 0.04958370327949524, model_norm_threshold=951858705203200.0
2024-10-08 23:12:24,104 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.369e+32, grad_sumsq=5.669e+33, orig_rms_sq=2.416e-02
2024-10-08 23:12:27,400 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.83 vs. limit=6.802
2024-10-08 23:12:29,451 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=3607.3333333333335, ans=0.04908333333333331
2024-10-08 23:12:30,149 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.01 vs. limit=3.5411
2024-10-08 23:12:34,164 WARNING [optim.py:503] Scaling gradients by 0.030054109171032906, model_norm_threshold=951858705203200.0
2024-10-08 23:12:34,320 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.847e+32, grad_sumsq=1.579e+34, orig_rms_sq=2.436e-02
2024-10-08 23:12:38,246 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=3607.3333333333335, ans=0.33090625
2024-10-08 23:12:39,226 WARNING [optim.py:503] Scaling gradients by 0.0011492412304505706, model_norm_threshold=951858705203200.0
2024-10-08 23:12:39,382 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.085e+35, grad_sumsq=8.524e+36, orig_rms_sq=2.446e-02
2024-10-08 23:12:41,570 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=21.33 vs. limit=8.854
2024-10-08 23:12:44,113 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=22.37 vs. limit=8.854
2024-10-08 23:12:44,415 WARNING [optim.py:503] Scaling gradients by 0.0008815668988972902, model_norm_threshold=951858705203200.0
2024-10-08 23:12:44,573 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.586e+35, grad_sumsq=1.872e+37, orig_rms_sq=2.450e-02
2024-10-08 23:12:48,563 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=3610.6666666666665, ans=0.01876
2024-10-08 23:12:51,838 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=18.24 vs. limit=10.208
2024-10-08 23:12:53,598 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=3614.0, ans=0.06447499999999998
2024-10-08 23:12:56,070 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=3614.0, ans=0.77351
2024-10-08 23:12:56,858 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module2.whiten, num_groups=1, num_channels=192, metric=3.10 vs. limit=8.85525
2024-10-08 23:12:57,098 WARNING [optim.py:503] Scaling gradients by 0.025580711662769318, model_norm_threshold=951858705203200.0
2024-10-08 23:12:57,255 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.662e+32, grad_sumsq=1.900e+34, orig_rms_sq=2.454e-02
2024-10-08 23:13:01,685 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=12.09 vs. limit=8.85525
2024-10-08 23:13:07,052 WARNING [optim.py:503] Scaling gradients by 0.07764269411563873, model_norm_threshold=951858705203200.0
2024-10-08 23:13:07,206 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.456e+31, grad_sumsq=1.408e+33, orig_rms_sq=2.454e-02
2024-10-08 23:13:07,264 INFO [train.py:1152] Epoch 2, batch 4000, loss[loss=0.7322, ctc_loss=1.007, attn_decoder_loss=0.6635, over 4815.00 frames. ], tot_loss[loss=0.7502, ctc_loss=1.034, attn_decoder_loss=0.6792, over 967288.92 frames. ], batch size: 19, lr: 3.33e-02,
2024-10-08 23:13:08,379 WARNING [optim.py:503] Scaling gradients by 0.05168982222676277, model_norm_threshold=951858705203200.0
2024-10-08 23:13:08,537 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.033e+32, grad_sumsq=4.210e+33, orig_rms_sq=2.454e-02
2024-10-08 23:13:12,205 WARNING [optim.py:503] Scaling gradients by 0.09581901133060455, model_norm_threshold=951858705203200.0
2024-10-08 23:13:12,361 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.001e+31, grad_sumsq=1.223e+33, orig_rms_sq=2.455e-02
2024-10-08 23:13:13,519 WARNING [optim.py:503] Scaling gradients by 0.014303850010037422, model_norm_threshold=951858705203200.0
2024-10-08 23:13:13,675 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.640e+33, grad_sumsq=6.681e+34, orig_rms_sq=2.455e-02
2024-10-08 23:13:13,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=3617.3333333333335, ans=0.06434999999999999
2024-10-08 23:13:15,128 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=9.051e+05
2024-10-08 23:13:16,148 WARNING [optim.py:503] Scaling gradients by 0.01951288804411888, model_norm_threshold=951858705203200.0
2024-10-08 23:13:16,304 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.284e+32, grad_sumsq=1.745e+34, orig_rms_sq=2.455e-02
2024-10-08 23:13:17,613 WARNING [optim.py:503] Scaling gradients by 0.08307570964097977, model_norm_threshold=951858705203200.0
2024-10-08 23:13:17,769 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.035e+31, grad_sumsq=1.337e+34, orig_rms_sq=3.765e-03
2024-10-08 23:13:23,887 WARNING [optim.py:503] Scaling gradients by 0.09341911971569061, model_norm_threshold=951858705203200.0
2024-10-08 23:13:24,042 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.623e+31, grad_sumsq=1.066e+33, orig_rms_sq=2.460e-02
2024-10-08 23:13:26,467 WARNING [optim.py:503] Scaling gradients by 0.01096391025930643, model_norm_threshold=951858705203200.0
2024-10-08 23:13:26,624 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.821e+33, grad_sumsq=1.147e+35, orig_rms_sq=2.460e-02
2024-10-08 23:13:27,248 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.79 vs. limit=5.448266666666667
2024-10-08 23:13:27,957 WARNING [optim.py:503] Scaling gradients by 0.08753850311040878, model_norm_threshold=951858705203200.0
2024-10-08 23:13:28,112 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.748e+31, grad_sumsq=1.522e+33, orig_rms_sq=2.462e-02
2024-10-08 23:13:29,000 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.73 vs. limit=10.2155
2024-10-08 23:13:30,100 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=15.92 vs. limit=8.85775
2024-10-08 23:13:33,158 WARNING [optim.py:503] Scaling gradients by 0.004705165512859821, model_norm_threshold=951858705203200.0
2024-10-08 23:13:33,314 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.060e+34, grad_sumsq=4.304e+35, orig_rms_sq=2.463e-02
2024-10-08 23:13:36,636 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.24 vs. limit=8.859
2024-10-08 23:13:40,682 WARNING [optim.py:503] Scaling gradients by 0.0006558192544616759, model_norm_threshold=951858705203200.0
2024-10-08 23:13:40,838 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.614e+35, grad_sumsq=2.685e+37, orig_rms_sq=2.463e-02
2024-10-08 23:13:44,459 WARNING [optim.py:503] Scaling gradients by 0.008122727274894714, model_norm_threshold=951858705203200.0
2024-10-08 23:13:44,614 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.841e+33, grad_sumsq=9.927e+35, orig_rms_sq=3.870e-03
2024-10-08 23:13:46,017 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=3627.3333333333335, ans=0.7730433333333333
2024-10-08 23:13:49,422 WARNING [optim.py:503] Scaling gradients by 0.0005402882234193385, model_norm_threshold=951858705203200.0
2024-10-08 23:13:49,580 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.068e+35, grad_sumsq=3.688e+37, orig_rms_sq=2.459e-02
2024-10-08 23:13:50,854 WARNING [optim.py:503] Scaling gradients by 0.07514026761054993, model_norm_threshold=951858705203200.0
2024-10-08 23:13:51,013 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.590e+31, grad_sumsq=1.867e+33, orig_rms_sq=2.459e-02
2024-10-08 23:13:52,569 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.max_positive, batch_count=3627.3333333333335, ans=0.7862733333333334
2024-10-08 23:13:53,576 WARNING [optim.py:503] Scaling gradients by 0.05871112272143364, model_norm_threshold=951858705203200.0
2024-10-08 23:13:53,733 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.133e+31, grad_sumsq=3.715e+33, orig_rms_sq=2.459e-02
2024-10-08 23:13:58,745 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=3630.6666666666665, ans=0.26369333333333334
2024-10-08 23:13:59,749 WARNING [optim.py:503] Scaling gradients by 0.00017163038137368858, model_norm_threshold=951858705203200.0
2024-10-08 23:13:59,905 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.872e+36, grad_sumsq=3.210e+38, orig_rms_sq=2.453e-02
2024-10-08 23:14:03,517 WARNING [optim.py:503] Scaling gradients by 0.003554548369720578, model_norm_threshold=951858705203200.0
2024-10-08 23:14:03,672 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.272e+34, grad_sumsq=9.277e+35, orig_rms_sq=2.449e-02
2024-10-08 23:14:05,781 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.48 vs. limit=5.907666666666667
2024-10-08 23:14:09,580 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.644e+12 8.339e+13 3.004e+14 9.128e+15 5.546e+18, threshold=6.007e+14, percent-clipped=37.0
2024-10-08 23:14:10,389 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=15.82 vs. limit=8.8615
2024-10-08 23:14:11,027 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=3634.0, ans=0.018234999999999987
2024-10-08 23:14:12,007 INFO [train.py:1152] Epoch 2, batch 4050, loss[loss=0.8203, ctc_loss=1.123, attn_decoder_loss=0.7445, over 4768.00 frames. ], tot_loss[loss=0.7485, ctc_loss=1.032, attn_decoder_loss=0.6776, over 967673.41 frames. ], batch size: 53, lr: 3.32e-02,
2024-10-08 23:14:13,129 WARNING [optim.py:503] Scaling gradients by 0.037652637809515, model_norm_threshold=600702044864512.0
2024-10-08 23:14:13,286 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.526e+31, grad_sumsq=3.057e+33, orig_rms_sq=2.462e-02
2024-10-08 23:14:13,973 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.11 vs. limit=10.2255
2024-10-08 23:14:14,500 WARNING [optim.py:503] Scaling gradients by 0.004858727566897869, model_norm_threshold=600702044864512.0
2024-10-08 23:14:14,654 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.077e+33, grad_sumsq=1.250e+35, orig_rms_sq=2.461e-02
2024-10-08 23:14:15,865 WARNING [optim.py:503] Scaling gradients by 0.0735478550195694, model_norm_threshold=600702044864512.0
2024-10-08 23:14:16,020 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.958e+31, grad_sumsq=7.955e+32, orig_rms_sq=2.461e-02
2024-10-08 23:14:18,534 WARNING [optim.py:503] Scaling gradients by 0.04671410843729973, model_norm_threshold=600702044864512.0
2024-10-08 23:14:18,691 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.309e+31, grad_sumsq=2.563e+33, orig_rms_sq=2.461e-02
2024-10-08 23:14:22,363 WARNING [optim.py:503] Scaling gradients by 0.00015533360419794917, model_norm_threshold=600702044864512.0
2024-10-08 23:14:22,527 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.569e+36, grad_sumsq=1.861e+38, orig_rms_sq=2.455e-02
2024-10-08 23:14:24,859 WARNING [optim.py:503] Scaling gradients by 0.01026055309921503, model_norm_threshold=600702044864512.0
2024-10-08 23:14:25,015 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.830e+32, grad_sumsq=4.012e+34, orig_rms_sq=2.450e-02
2024-10-08 23:14:32,246 WARNING [optim.py:503] Scaling gradients by 0.0002677852753549814, model_norm_threshold=600702044864512.0
2024-10-08 23:14:32,404 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.548e+36, grad_sumsq=6.305e+37, orig_rms_sq=2.454e-02
2024-10-08 23:14:33,619 WARNING [optim.py:503] Scaling gradients by 0.0051553621888160706, model_norm_threshold=600702044864512.0
2024-10-08 23:14:33,776 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.363e+33, grad_sumsq=6.393e+33, orig_rms_sq=3.696e-01
2024-10-08 23:14:38,165 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=17.38 vs. limit=8.86525
2024-10-08 23:14:38,793 WARNING [optim.py:503] Scaling gradients by 0.03283611312508583, model_norm_threshold=600702044864512.0
2024-10-08 23:14:38,950 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.750e+31, grad_sumsq=3.552e+33, orig_rms_sq=2.463e-02
2024-10-08 23:14:42,102 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.89 vs. limit=10.2305
2024-10-08 23:14:44,576 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=25.10 vs. limit=10.2305
2024-10-08 23:14:44,686 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=14.82 vs. limit=8.86525
2024-10-08 23:14:45,058 WARNING [optim.py:503] Scaling gradients by 0.058167777955532074, model_norm_threshold=600702044864512.0
2024-10-08 23:14:45,216 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.657e+31, grad_sumsq=1.486e+33, orig_rms_sq=2.461e-02
2024-10-08 23:14:49,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=3644.0, ans=0.77246
2024-10-08 23:14:58,693 WARNING [optim.py:503] Scaling gradients by 0.041445571929216385, model_norm_threshold=600702044864512.0
2024-10-08 23:14:58,849 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.373e+31, grad_sumsq=1.785e+33, orig_rms_sq=2.450e-02
2024-10-08 23:15:03,337 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=23.53 vs. limit=8.867750000000001
2024-10-08 23:15:04,008 WARNING [optim.py:503] Scaling gradients by 0.07125924527645111, model_norm_threshold=600702044864512.0
2024-10-08 23:15:04,164 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.918e+31, grad_sumsq=1.195e+33, orig_rms_sq=2.442e-02
2024-10-08 23:15:05,411 WARNING [optim.py:503] Scaling gradients by 0.006342126522213221, model_norm_threshold=600702044864512.0
2024-10-08 23:15:05,567 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.665e+33, grad_sumsq=7.220e+35, orig_rms_sq=3.691e-03
2024-10-08 23:15:06,258 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=31.33 vs. limit=10.2355
2024-10-08 23:15:06,748 WARNING [optim.py:503] Scaling gradients by 0.0004108662542421371, model_norm_threshold=600702044864512.0
2024-10-08 23:15:06,905 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.228e+35, grad_sumsq=3.375e+37, orig_rms_sq=2.438e-02
2024-10-08 23:15:08,074 WARNING [optim.py:503] Scaling gradients by 0.0004300984146539122, model_norm_threshold=600702044864512.0
2024-10-08 23:15:08,230 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.345e+35, grad_sumsq=2.192e+37, orig_rms_sq=2.438e-02
2024-10-08 23:15:09,145 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.40 vs. limit=10.2355
2024-10-08 23:15:14,490 WARNING [optim.py:503] Scaling gradients by 0.0008240963215939701, model_norm_threshold=600702044864512.0
2024-10-08 23:15:14,647 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.618e+35, grad_sumsq=6.666e+36, orig_rms_sq=2.427e-02
2024-10-08 23:15:16,040 INFO [train.py:1152] Epoch 2, batch 4100, loss[loss=0.7808, ctc_loss=1.076, attn_decoder_loss=0.7071, over 4868.00 frames. ], tot_loss[loss=0.7499, ctc_loss=1.032, attn_decoder_loss=0.6795, over 967077.11 frames. ], batch size: 31, lr: 3.32e-02,
2024-10-08 23:15:22,850 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.min_positive, batch_count=3650.6666666666665, ans=0.03859166666666667
2024-10-08 23:15:23,295 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=13.18 vs. limit=6.825333333333333
2024-10-08 23:15:23,803 WARNING [optim.py:503] Scaling gradients by 0.010121641680598259, model_norm_threshold=600702044864512.0
2024-10-08 23:15:23,964 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.600e+33, grad_sumsq=6.524e+34, orig_rms_sq=2.453e-02
2024-10-08 23:15:25,438 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=3650.6666666666665, ans=0.32887500000000003
2024-10-08 23:15:26,103 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=7.98 vs. limit=5.460266666666667
2024-10-08 23:15:26,423 WARNING [optim.py:503] Scaling gradients by 0.00026896450435742736, model_norm_threshold=600702044864512.0
2024-10-08 23:15:26,579 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.541e+36, grad_sumsq=6.286e+37, orig_rms_sq=2.452e-02
2024-10-08 23:15:27,767 WARNING [optim.py:503] Scaling gradients by 0.0130180474370718, model_norm_threshold=600702044864512.0
2024-10-08 23:15:27,923 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.364e+32, grad_sumsq=3.411e+34, orig_rms_sq=2.452e-02
2024-10-08 23:15:32,503 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=7.59 vs. limit=5.4616
2024-10-08 23:15:33,547 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=22.70 vs. limit=8.87025
2024-10-08 23:15:33,583 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=19.08 vs. limit=8.87025
2024-10-08 23:15:34,216 WARNING [optim.py:503] Scaling gradients by 0.022375689819455147, model_norm_threshold=600702044864512.0
2024-10-08 23:15:34,374 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.535e+32, grad_sumsq=6.357e+33, orig_rms_sq=2.415e-02
2024-10-08 23:15:40,035 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.28 vs. limit=6.827
2024-10-08 23:15:41,222 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.88 vs. limit=3.5486
2024-10-08 23:15:43,072 WARNING [optim.py:503] Scaling gradients by 0.004913246724754572, model_norm_threshold=600702044864512.0
2024-10-08 23:15:43,228 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.973e+33, grad_sumsq=1.669e+35, orig_rms_sq=2.380e-02
2024-10-08 23:15:45,917 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3657.3333333333335, ans=0.26342666666666664
2024-10-08 23:15:48,219 WARNING [optim.py:503] Scaling gradients by 0.07101786136627197, model_norm_threshold=600702044864512.0
2024-10-08 23:15:48,376 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.551e+31, grad_sumsq=4.197e+33, orig_rms_sq=3.695e-03
2024-10-08 23:15:48,985 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.81 vs. limit=10.243
2024-10-08 23:15:53,377 WARNING [optim.py:503] Scaling gradients by 0.0024990758392959833, model_norm_threshold=600702044864512.0
2024-10-08 23:15:53,535 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.397e+34, grad_sumsq=5.850e+35, orig_rms_sq=2.388e-02
2024-10-08 23:15:54,140 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=24.66 vs. limit=8.87275
2024-10-08 23:15:57,166 WARNING [optim.py:503] Scaling gradients by 0.0031949884723871946, model_norm_threshold=600702044864512.0
2024-10-08 23:15:57,324 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.104e+33, grad_sumsq=2.239e+34, orig_rms_sq=3.620e-01
2024-10-08 23:15:58,634 WARNING [optim.py:503] Scaling gradients by 0.07925587147474289, model_norm_threshold=600702044864512.0
2024-10-08 23:15:58,788 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.001e+31, grad_sumsq=1.116e+30, orig_rms_sq=8.968e+00
2024-10-08 23:15:58,960 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=3660.6666666666665, ans=0.062725
2024-10-08 23:15:59,511 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=25.15 vs. limit=10.2455
2024-10-08 23:16:07,418 WARNING [optim.py:503] Scaling gradients by 0.008554319851100445, model_norm_threshold=600702044864512.0
2024-10-08 23:16:07,574 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.155e+33, grad_sumsq=2.979e+35, orig_rms_sq=3.879e-03
2024-10-08 23:16:08,328 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=3664.0, ans=10.248000000000001
2024-10-08 23:16:08,924 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=3664.0, ans=0.04199999999999998
2024-10-08 23:16:12,492 WARNING [optim.py:503] Scaling gradients by 0.038980185985565186, model_norm_threshold=600702044864512.0
2024-10-08 23:16:12,647 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.569e+31, grad_sumsq=1.179e+34, orig_rms_sq=3.875e-03
2024-10-08 23:16:17,979 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.314e+11 2.617e+13 4.033e+14 8.430e+15 3.867e+18, threshold=8.065e+14, percent-clipped=44.0
2024-10-08 23:16:18,765 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=20.83 vs. limit=5.916
2024-10-08 23:16:20,594 INFO [train.py:1152] Epoch 2, batch 4150, loss[loss=0.7784, ctc_loss=1.08, attn_decoder_loss=0.703, over 4749.00 frames. ], tot_loss[loss=0.7517, ctc_loss=1.032, attn_decoder_loss=0.6816, over 967122.98 frames. ], batch size: 20, lr: 3.31e-02,
2024-10-08 23:16:22,998 WARNING [optim.py:503] Scaling gradients by 0.025368254631757736, model_norm_threshold=806530500788224.0
2024-10-08 23:16:23,155 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.555e+32, grad_sumsq=1.421e+34, orig_rms_sq=2.502e-02
2024-10-08 23:16:28,174 WARNING [optim.py:503] Scaling gradients by 0.0016013215063139796, model_norm_threshold=806530500788224.0
2024-10-08 23:16:28,331 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.049e+34, grad_sumsq=2.448e+37, orig_rms_sq=3.696e-03
2024-10-08 23:16:29,518 WARNING [optim.py:503] Scaling gradients by 0.059432607144117355, model_norm_threshold=806530500788224.0
2024-10-08 23:16:29,675 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.142e+31, grad_sumsq=2.460e+33, orig_rms_sq=2.496e-02
2024-10-08 23:16:30,272 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=34.28 vs. limit=8.87525
2024-10-08 23:16:31,144 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=3667.3333333333335, ans=0.041583333333333306
2024-10-08 23:16:31,594 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.66 vs. limit=8.87525
2024-10-08 23:16:32,302 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=3670.6666666666665, ans=0.041166666666666685
2024-10-08 23:16:35,839 WARNING [optim.py:503] Scaling gradients by 0.0013872758718207479, model_norm_threshold=806530500788224.0
2024-10-08 23:16:35,995 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.974e+34, grad_sumsq=3.217e+36, orig_rms_sq=2.479e-02
2024-10-08 23:16:37,189 WARNING [optim.py:503] Scaling gradients by 0.00399037916213274, model_norm_threshold=806530500788224.0
2024-10-08 23:16:37,343 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.148e+34, grad_sumsq=4.632e+35, orig_rms_sq=2.479e-02
2024-10-08 23:16:38,574 WARNING [optim.py:503] Scaling gradients by 0.001982488203793764, model_norm_threshold=806530500788224.0
2024-10-08 23:16:38,730 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.247e+34, grad_sumsq=2.145e+36, orig_rms_sq=2.446e-02
2024-10-08 23:16:50,105 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=3674.0, ans=0.32778125
2024-10-08 23:16:51,732 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=nan vs. limit=8.87775
2024-10-08 23:17:14,422 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=3680.6666666666665, ans=0.32746875
2024-10-08 23:17:21,070 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=nan vs. limit=8.88025
2024-10-08 23:17:22,903 INFO [train.py:1152] Epoch 2, batch 4200, loss[loss=nan, ctc_loss=nan, attn_decoder_loss=nan, over 4830.00 frames. ], tot_loss[loss=0.63, ctc_loss=0.8648, attn_decoder_loss=0.5713, over 967292.55 frames. ], batch size: 31, lr: 3.31e-02,
2024-10-08 23:17:23,428 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=nan vs. limit=8.881499999999999
2024-10-08 23:17:28,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=3684.0, ans=0.3273125
2024-10-08 23:17:29,195 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=3684.0, ans=0.04277500000000001
2024-10-08 23:17:29,725 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=nan vs. limit=5.4736
2024-10-08 23:17:29,885 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten.whitening_limit, batch_count=3684.0, ans=10.263
2024-10-08 23:17:40,666 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=3687.3333333333335, ans=0.017034999999999995
2024-10-08 23:17:43,085 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=3687.3333333333335, ans=0.7709433333333333
2024-10-08 23:17:51,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=nan vs. limit=8.884
2024-10-08 23:17:54,627 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=3690.6666666666665, ans=0.327
2024-10-08 23:17:56,514 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=nan vs. limit=8.884
2024-10-08 23:18:07,270 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=3694.0, ans=0.32684375
2024-10-08 23:18:07,774 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=nan vs. limit=8.88525
2024-10-08 23:18:11,463 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=nan vs. limit=8.88525
2024-10-08 23:18:11,767 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten.whitening_limit, batch_count=3694.0, ans=8.88525
2024-10-08 23:18:12,921 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys.whitening_limit, batch_count=3697.3333333333335, ans=3.5545999999999998
2024-10-08 23:18:16,591 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=nan vs. limit=5.478933333333334
2024-10-08 23:18:21,594 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=nan vs. limit=8.8865
2024-10-08 23:18:21,619 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=nan vs. limit=8.8865
2024-10-08 23:18:23,521 INFO [train.py:1079] Caught exception: Too many grads were not finite.
2024-10-08 23:18:23,522 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_True/bad-model-0.pt
2024-10-08 23:18:25,032 INFO [train.py:1467] Saving batch to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_True/batch-c33f4584-b23b-c1d8-493c-d01609de8895.pt
2024-10-08 23:18:25,039 INFO [train.py:1473] features shape: torch.Size([20, 979, 80])
2024-10-08 23:18:25,040 INFO [train.py:1477] num tokens: 986
Traceback (most recent call last):
  File "/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/zipformer/train.py", line 1553, in <module>
  File "/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/zipformer/train.py", line 1543, in main
    mp.spawn(run, args=(world_size, args), nprocs=world_size, join=True)
  File "/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/zipformer/train.py", line 1410, in run
    params.cur_epoch = epoch
  File "/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/zipformer/train.py", line 1075, in train_one_epoch
    scheduler.step_batch(params.batch_idx_train)
  File "/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 315, in step
    return optimizer.step(*args, **kwargs)
  File "/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/zipformer/optim.py", line 345, in step
    clipping_scale = self._get_clipping_scale(group, batches)
  File "/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/zipformer/optim.py", line 473, in _get_clipping_scale
    raise RuntimeError("Too many grads were not finite")
RuntimeError: Too many grads were not finite
