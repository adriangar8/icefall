2024-10-08 19:12:42,823 INFO [train.py:1230] Training started
2024-10-08 19:12:42,824 INFO [train.py:1240] Device: cuda:0
2024-10-08 19:12:42,826 INFO [train.py:1271] Using dtype=torch.float32
2024-10-08 19:12:42,826 INFO [train.py:1272] Use AMP=False
2024-10-08 19:12:42,826 INFO [train.py:1274] {'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 50, 'reset_interval': 200, 'valid_interval': 3000, 'feature_dim': 80, 'subsampling_factor': 4, 'ignore_id': -1, 'label_smoothing': 0.1, 'warm_step': 2000, 'env_info': {'k2-version': '1.24.3', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': 'e400fa3b456faf8afe0ee5bfe572946b4921a3db', 'k2-git-date': 'Sat Jul 15 04:21:50 2023', 'lhotse-version': '1.25.0', 'torch-version': '2.0.1+cu118', 'torch-cuda-available': True, 'torch-cuda-version': '11.8', 'python-version': '3.9', 'icefall-git-branch': 'master', 'icefall-git-sha1': 'cabeaf7f-dirty', 'icefall-git-date': 'Thu Oct 3 12:53:52 2024', 'icefall-path': '/mnt/ahogpu_ldisk2/adriang/icefall', 'k2-path': '/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/k2/__init__.py', 'lhotse-path': '/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/lhotse/__init__.py', 'hostname': 'ahogpu', 'IP address': '192.168.1.130'}, 'world_size': 1, 'master_port': 12354, 'tensorboard': True, 'num_epochs': 30, 'start_epoch': 1, 'start_batch': 0, 'exp_dir': PosixPath('zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_False'), 'bpe_model': '/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/data/lang_bpe_256/bpe.model', 'base_lr': 0.045, 'lr_batches': 7500, 'lr_epochs': 3.5, 'ref_duration': 600, 'context_size': 2, 'prune_range': 5, 'lm_scale': 0.25, 'am_scale': 0.0, 'simple_loss_scale': 0.5, 'ctc_loss_scale': 0.2, 'attention_decoder_loss_scale': 0.8, 'seed': 42, 'print_diagnostics': False, 'inf_check': False, 'save_every_n': 4000, 'keep_last_k': 30, 'average_period': 200, 'use_bf16': False, 'use_fp16': False, 'num_encoder_layers': '2,2,3,4,3,2', 'downsampling_factor': '1,2,4,8,4,2', 'feedforward_dim': '512,768,1024,1536,1024,768', 'num_heads': '4,4,4,8,4,4', 'encoder_dim': '192,256,384,512,384,256', 'query_head_dim': '32', 'value_head_dim': '12', 'pos_head_dim': '4', 'pos_dim': 48, 'encoder_unmasked_dim': '192,192,256,256,256,192', 'cnn_module_kernel': '31,31,15,15,15,31', 'decoder_dim': 512, 'joiner_dim': 512, 'attention_decoder_dim': 512, 'attention_decoder_num_layers': 6, 'attention_decoder_attention_dim': 512, 'attention_decoder_num_heads': 8, 'attention_decoder_feedforward_dim': 2048, 'causal': False, 'chunk_size': '16,32,64,-1', 'left_context_frames': '64,128,256,-1', 'use_transducer': False, 'use_ctc': True, 'use_attention_decoder': True, 'manifest_dir': PosixPath('/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/data/fbank'), 'max_duration': 200.0, 'bucketing_sampler': True, 'num_buckets': 30, 'concatenate_cuts': False, 'duration_factor': 1.0, 'gap': 1.0, 'on_the_fly_feats': False, 'shuffle': True, 'drop_last': True, 'return_cuts': True, 'num_workers': 2, 'enable_spec_aug': True, 'spec_aug_time_warp_factor': 80, 'enable_musan': True, 'input_strategy': 'PrecomputedFeatures', 'blank_id': 0, 'sos_id': 1, 'eos_id': 1, 'vocab_size': 256, 'dtype': torch.float32, 'use_autocast': False}
2024-10-08 19:12:42,826 INFO [train.py:1276] About to create model
2024-10-08 19:12:43,391 INFO [train.py:1280] Number of model parameters: 89612023
2024-10-08 19:12:45,030 INFO [train.py:1298] Using single GPU
2024-10-08 19:12:45,046 INFO [custom_asr_data_module.py:394] About to load train cuts
2024-10-08 19:12:45,047 INFO [custom_asr_data_module.py:204] Enable MUSAN
2024-10-08 19:12:45,047 INFO [custom_asr_data_module.py:205] About to get Musan cuts
2024-10-08 19:12:46,518 INFO [custom_asr_data_module.py:234] Enable SpecAugment
2024-10-08 19:12:46,518 INFO [custom_asr_data_module.py:235] Time warp factor: 80
2024-10-08 19:12:46,518 INFO [custom_asr_data_module.py:245] Num frame mask: 10
2024-10-08 19:12:46,518 INFO [custom_asr_data_module.py:260] About to create train dataset
2024-10-08 19:12:46,518 INFO [custom_asr_data_module.py:287] Using DynamicBucketingSampler.
2024-10-08 19:12:46,890 INFO [custom_asr_data_module.py:304] About to create train dataloader
2024-10-08 19:12:46,891 INFO [custom_asr_data_module.py:402] About to load valid cuts
2024-10-08 19:12:46,891 INFO [custom_asr_data_module.py:338] About to create dev dataset
2024-10-08 19:12:46,912 INFO [custom_asr_data_module.py:355] About to create dev dataloader
2024-10-08 19:12:46,912 INFO [train.py:1490] Sanity check -- see if any of the batches in epoch 1 would cause OOM.
/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
2024-10-08 19:13:18,970 INFO [scaling.py:1024] Whitening: name=None, num_groups=1, num_channels=512, metric=113.70 vs. limit=4.0
2024-10-08 19:13:19,220 INFO [scaling.py:1024] Whitening: name=None, num_groups=1, num_channels=192, metric=35.48 vs. limit=7.5
2024-10-08 19:13:19,297 INFO [train.py:1518] Maximum memory allocated so far is 5154MB
2024-10-08 19:13:20,196 INFO [train.py:1518] Maximum memory allocated so far is 5154MB
2024-10-08 19:13:21,220 INFO [train.py:1518] Maximum memory allocated so far is 5154MB
2024-10-08 19:13:22,519 INFO [train.py:1518] Maximum memory allocated so far is 5154MB
2024-10-08 19:13:23,554 INFO [train.py:1518] Maximum memory allocated so far is 5154MB
2024-10-08 19:13:24,645 INFO [train.py:1518] Maximum memory allocated so far is 5168MB
2024-10-08 19:13:37,166 INFO [train.py:1152] Epoch 1, batch 0, loss[loss=7.75, ctc_loss=4.29, attn_decoder_loss=8.615, over 4853.00 frames. ], tot_loss[loss=7.75, ctc_loss=4.29, attn_decoder_loss=8.615, over 4853.00 frames. ], batch size: 19, lr: 2.25e-02,
2024-10-08 19:13:37,167 INFO [train.py:1175] Computing validation loss
2024-10-08 19:13:44,364 INFO [train.py:1184] Epoch 1, validation: loss=7.449, ctc_loss=4.22, attn_decoder_loss=8.256, over 90464.00 frames.
2024-10-08 19:13:44,365 INFO [train.py:1185] Maximum memory allocated so far is 5168MB
2024-10-08 19:13:57,415 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.351e+02 8.521e+02 1.038e+03 1.119e+03 1.272e+03, threshold=4.154e+03, percent-clipped=0.0
2024-10-08 19:14:11,609 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.426e+02 8.351e+02 8.849e+02 1.087e+03 1.285e+03, threshold=3.540e+03, percent-clipped=0.0
2024-10-08 19:14:13,936 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=29.55 vs. limit=7.5025
2024-10-08 19:14:15,623 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=15.75 vs. limit=7.5025
2024-10-08 19:14:16,864 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=30.59 vs. limit=7.5025
2024-10-08 19:14:28,980 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=18.72 vs. limit=7.50375
2024-10-08 19:14:40,696 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=30.63 vs. limit=7.505
2024-10-08 19:14:41,260 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.446e+02 6.301e+02 7.540e+02 8.849e+02 1.285e+03, threshold=3.016e+03, percent-clipped=0.0
2024-10-08 19:14:43,102 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=13.333333333333334, ans=5.008333333333334
2024-10-08 19:14:48,101 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=13.57 vs. limit=5.003333333333333
2024-10-08 19:14:53,123 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=25.93 vs. limit=5.006666666666667
2024-10-08 19:14:54,230 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=34.95 vs. limit=7.505
2024-10-08 19:14:54,356 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=27.80 vs. limit=7.51
2024-10-08 19:14:56,615 INFO [train.py:1152] Epoch 1, batch 50, loss[loss=2.086, ctc_loss=1.162, attn_decoder_loss=2.318, over 4912.00 frames. ], tot_loss[loss=3.542, ctc_loss=1.715, attn_decoder_loss=3.999, over 217782.93 frames. ], batch size: 19, lr: 2.48e-02,
2024-10-08 19:15:06,777 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=49.18 vs. limit=7.50625
2024-10-08 19:15:10,665 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer_ff3.min_abs, batch_count=20.0, ans=0.001
2024-10-08 19:15:15,734 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=39.85 vs. limit=5.01
2024-10-08 19:15:24,077 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=10.40 vs. limit=5.005
2024-10-08 19:15:25,193 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=33.94 vs. limit=7.5075
2024-10-08 19:15:29,100 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=23.333333333333332, ans=0.49890625
2024-10-08 19:15:40,159 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=60.72 vs. limit=7.5175
2024-10-08 19:15:40,251 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=18.21 vs. limit=4.009333333333333
2024-10-08 19:15:44,960 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=29.81 vs. limit=7.51
2024-10-08 19:15:54,171 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=28.21 vs. limit=5.006666666666667
2024-10-08 19:15:55,858 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=39.81 vs. limit=7.51
2024-10-08 19:16:08,581 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=127.78 vs. limit=4.006
2024-10-08 19:16:12,672 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=80.45 vs. limit=7.5125
2024-10-08 19:16:13,378 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.192e+02 4.157e+02 5.113e+02 7.131e+02 1.285e+03, threshold=1.023e+03, percent-clipped=0.0
2024-10-08 19:16:13,441 INFO [train.py:1152] Epoch 1, batch 100, loss[loss=1.394, ctc_loss=1.031, attn_decoder_loss=1.485, over 4752.00 frames. ], tot_loss[loss=2.562, ctc_loss=1.393, attn_decoder_loss=2.854, over 383572.91 frames. ], batch size: 19, lr: 2.70e-02,
2024-10-08 19:16:21,247 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=33.333333333333336, ans=0.4984375
2024-10-08 19:16:24,882 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.02 vs. limit=3.005
2024-10-08 19:16:25,830 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=33.333333333333336, ans=0.19875
2024-10-08 19:16:37,856 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module1.whiten, num_groups=1, num_channels=192, metric=51.29 vs. limit=7.51375
2024-10-08 19:16:42,058 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=41.52 vs. limit=5.0183333333333335
2024-10-08 19:17:02,717 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten.whitening_limit, batch_count=43.333333333333336, ans=7.51625
2024-10-08 19:17:08,130 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer_ff2.min_abs, batch_count=43.333333333333336, ans=0.0010833333333333335
2024-10-08 19:17:14,787 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=66.64 vs. limit=7.5175
2024-10-08 19:17:16,496 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=25.19 vs. limit=4.018666666666666
2024-10-08 19:17:19,565 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=12.46 vs. limit=5.011666666666667
2024-10-08 19:17:19,794 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=103.99 vs. limit=7.5175
2024-10-08 19:17:21,372 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=54.63 vs. limit=7.5175
2024-10-08 19:17:26,788 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.prob, batch_count=46.666666666666664, ans=0.4978125
2024-10-08 19:17:31,407 INFO [train.py:1152] Epoch 1, batch 150, loss[loss=1.221, ctc_loss=1.06, attn_decoder_loss=1.262, over 4910.00 frames. ], tot_loss[loss=2.051, ctc_loss=1.272, attn_decoder_loss=2.246, over 513480.80 frames. ], batch size: 19, lr: 2.93e-02,
2024-10-08 19:17:31,529 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=50.0, ans=0.2495
2024-10-08 19:17:38,588 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=61.18 vs. limit=7.51875
2024-10-08 19:17:43,117 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=242.62 vs. limit=5.025
2024-10-08 19:17:44,541 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=56.78 vs. limit=7.51875
2024-10-08 19:17:49,166 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=65.30 vs. limit=7.52
2024-10-08 19:17:49,261 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=40.47 vs. limit=7.52
2024-10-08 19:18:00,274 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=22.20 vs. limit=7.52
2024-10-08 19:18:21,119 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=60.0, ans=0.19775
2024-10-08 19:18:23,152 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=101.37 vs. limit=7.545
2024-10-08 19:18:26,537 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=38.68 vs. limit=5.015
2024-10-08 19:18:27,704 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=93.51 vs. limit=7.5225
2024-10-08 19:18:41,853 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.05 vs. limit=3.0095
2024-10-08 19:18:42,925 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=63.333333333333336, ans=0.29936666666666667
2024-10-08 19:18:49,309 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.534e+02 1.914e+02 2.287e+02 2.805e+02 3.567e+02, threshold=4.575e+02, percent-clipped=0.0
2024-10-08 19:18:49,360 INFO [train.py:1152] Epoch 1, batch 200, loss[loss=1.123, ctc_loss=1.157, attn_decoder_loss=1.115, over 4760.00 frames. ], tot_loss[loss=1.737, ctc_loss=1.215, attn_decoder_loss=1.867, over 613838.26 frames. ], batch size: 45, lr: 3.15e-02,
2024-10-08 19:18:51,782 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=95.94 vs. limit=7.525
2024-10-08 19:18:55,182 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=66.66666666666667, ans=0.496875
2024-10-08 19:19:01,299 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=66.66666666666667, ans=0.496875
2024-10-08 19:19:03,488 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=36.55 vs. limit=7.5525
2024-10-08 19:19:03,873 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module2.whiten, num_groups=1, num_channels=192, metric=27.68 vs. limit=7.52625
2024-10-08 19:19:04,407 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=70.0, ans=0.2993
2024-10-08 19:19:08,058 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=512, metric=49.78 vs. limit=7.52625
2024-10-08 19:19:17,501 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=26.24 vs. limit=7.52625
2024-10-08 19:19:33,210 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=84.16 vs. limit=7.5275
2024-10-08 19:19:34,538 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=32.66 vs. limit=7.52875
2024-10-08 19:19:36,184 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=19.72 vs. limit=7.5575
2024-10-08 19:19:50,694 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=80.0, ans=0.04975
2024-10-08 19:19:58,976 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=35.60 vs. limit=7.56
2024-10-08 19:19:59,100 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=15.73 vs. limit=4.032
2024-10-08 19:20:05,784 INFO [train.py:1152] Epoch 1, batch 250, loss[loss=1.062, ctc_loss=1.158, attn_decoder_loss=1.038, over 4819.00 frames. ], tot_loss[loss=1.517, ctc_loss=1.176, attn_decoder_loss=1.602, over 692578.36 frames. ], batch size: 38, lr: 3.38e-02,
2024-10-08 19:20:09,715 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=19.45 vs. limit=7.53125
2024-10-08 19:20:33,865 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=86.66666666666667, ans=0.48916666666666664
2024-10-08 19:20:40,951 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=125.72 vs. limit=7.53375
2024-10-08 19:20:47,312 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=40.16 vs. limit=7.5675
2024-10-08 19:20:50,311 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=72.51 vs. limit=7.53375
2024-10-08 19:20:52,333 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=7.31 vs. limit=4.037333333333334
2024-10-08 19:20:52,918 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=93.33333333333333, ans=0.29906666666666665
2024-10-08 19:20:53,002 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=93.33333333333333, ans=0.09941666666666667
2024-10-08 19:20:53,745 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=204.11 vs. limit=5.046666666666667
2024-10-08 19:20:56,933 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=15.77 vs. limit=7.57
2024-10-08 19:20:59,150 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=93.33333333333333, ans=0.495625
2024-10-08 19:21:06,838 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=96.66666666666667, ans=0.49546875
2024-10-08 19:21:13,041 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=96.66666666666667, ans=0.09782500000000001
2024-10-08 19:21:23,857 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.625e+01 1.042e+02 1.193e+02 1.331e+02 1.674e+02, threshold=2.386e+02, percent-clipped=0.0
2024-10-08 19:21:23,919 INFO [train.py:1152] Epoch 1, batch 300, loss[loss=1.049, ctc_loss=1.212, attn_decoder_loss=1.009, over 4773.00 frames. ], tot_loss[loss=1.362, ctc_loss=1.151, attn_decoder_loss=1.415, over 753007.62 frames. ], batch size: 32, lr: 3.60e-02,
2024-10-08 19:21:24,784 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=116.36 vs. limit=7.5375
2024-10-08 19:21:26,182 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=80.96 vs. limit=7.5375
2024-10-08 19:21:35,029 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=100.0, ans=0.099375
2024-10-08 19:21:54,440 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=74.62 vs. limit=7.54
2024-10-08 19:22:10,257 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=23.42 vs. limit=5.055
2024-10-08 19:22:16,566 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=51.56 vs. limit=7.5825
2024-10-08 19:22:16,818 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=10.34 vs. limit=5.0275
2024-10-08 19:22:21,250 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=127.13 vs. limit=7.54125
2024-10-08 19:22:21,379 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=45.69 vs. limit=7.54125
2024-10-08 19:22:21,541 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=17.93 vs. limit=7.54125
2024-10-08 19:22:22,327 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=110.0, ans=0.097525
2024-10-08 19:22:23,086 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=65.23 vs. limit=7.54125
2024-10-08 19:22:24,669 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=34.74 vs. limit=7.54125
2024-10-08 19:22:26,079 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=69.31 vs. limit=7.5425
2024-10-08 19:22:26,110 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=30.11 vs. limit=7.585
2024-10-08 19:22:27,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=26.71 vs. limit=7.5425
2024-10-08 19:22:30,866 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=12.53 vs. limit=5.028333333333333
2024-10-08 19:22:35,538 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=141.83 vs. limit=7.5425
2024-10-08 19:22:36,650 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=113.33333333333333, ans=0.29886666666666667
2024-10-08 19:22:39,705 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=113.33333333333333, ans=0.29886666666666667
2024-10-08 19:22:41,839 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=109.79 vs. limit=5.058333333333334
2024-10-08 19:22:42,066 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=20.34 vs. limit=7.54375
2024-10-08 19:22:42,755 INFO [train.py:1152] Epoch 1, batch 350, loss[loss=0.9347, ctc_loss=1.05, attn_decoder_loss=0.9059, over 4883.00 frames. ], tot_loss[loss=1.248, ctc_loss=1.134, attn_decoder_loss=1.276, over 800374.59 frames. ], batch size: 19, lr: 3.83e-02,
2024-10-08 19:22:48,248 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=61.35 vs. limit=7.54375
2024-10-08 19:22:53,797 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=116.66666666666667, ans=5.072916666666667
2024-10-08 19:23:00,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=22.23 vs. limit=7.545
2024-10-08 19:23:06,989 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=88.76 vs. limit=7.59
2024-10-08 19:23:10,379 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=35.00 vs. limit=7.59
2024-10-08 19:23:15,162 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=12.87 vs. limit=5.030833333333334
2024-10-08 19:23:17,706 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=123.33333333333333, ans=0.19537500000000002
2024-10-08 19:23:17,707 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=123.33333333333333, ans=0.49421875
2024-10-08 19:23:23,148 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=12.40 vs. limit=5.030833333333334
2024-10-08 19:23:24,875 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=14.52 vs. limit=7.54625
2024-10-08 19:23:27,276 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=123.33333333333333, ans=0.2987666666666667
2024-10-08 19:23:30,432 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=126.66666666666667, ans=0.2987333333333333
2024-10-08 19:23:34,419 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=13.77 vs. limit=5.031666666666666
2024-10-08 19:23:40,636 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=25.40 vs. limit=7.595
2024-10-08 19:23:49,274 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=19.41 vs. limit=7.5975
2024-10-08 19:23:51,435 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=130.0, ans=0.89545
2024-10-08 19:24:00,093 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.47 vs. limit=5.065
2024-10-08 19:24:01,181 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=133.33333333333334, ans=0.48333333333333334
2024-10-08 19:24:02,389 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.005e+01 6.951e+01 7.782e+01 8.321e+01 1.097e+02, threshold=1.556e+02, percent-clipped=0.0
2024-10-08 19:24:02,443 INFO [train.py:1152] Epoch 1, batch 400, loss[loss=0.904, ctc_loss=1.126, attn_decoder_loss=0.8485, over 4874.00 frames. ], tot_loss[loss=1.162, ctc_loss=1.123, attn_decoder_loss=1.171, over 837078.96 frames. ], batch size: 22, lr: 4.05e-02,
2024-10-08 19:24:08,633 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=133.33333333333334, ans=0.49375
2024-10-08 19:24:09,299 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=32.54 vs. limit=7.55
2024-10-08 19:24:10,807 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=130.51 vs. limit=7.55
2024-10-08 19:24:11,205 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=83.64 vs. limit=7.55
2024-10-08 19:24:12,338 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.84 vs. limit=5.033333333333333
2024-10-08 19:24:32,251 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=140.0, ans=0.4934375
2024-10-08 19:24:39,242 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.87 vs. limit=5.035
2024-10-08 19:24:42,428 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=7.27 vs. limit=4.056
2024-10-08 19:24:47,226 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=59.95 vs. limit=7.5525
2024-10-08 19:24:52,260 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=31.13 vs. limit=7.6075
2024-10-08 19:24:55,087 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=58.30 vs. limit=7.6075
2024-10-08 19:24:56,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=143.33333333333334, ans=0.49328125
2024-10-08 19:24:58,614 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=26.08 vs. limit=7.6075
2024-10-08 19:25:07,327 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=146.66666666666666, ans=0.2985333333333333
2024-10-08 19:25:11,222 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=26.45 vs. limit=7.555
2024-10-08 19:25:16,720 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.max_abs, batch_count=146.66666666666666, ans=5.091666666666667
2024-10-08 19:25:18,804 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=51.74 vs. limit=5.073333333333333
2024-10-08 19:25:21,385 INFO [train.py:1152] Epoch 1, batch 450, loss[loss=0.8881, ctc_loss=1.134, attn_decoder_loss=0.8268, over 4873.00 frames. ], tot_loss[loss=1.091, ctc_loss=1.113, attn_decoder_loss=1.086, over 865679.54 frames. ], batch size: 23, lr: 4.28e-02,
2024-10-08 19:25:22,082 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=85.55 vs. limit=7.6125
2024-10-08 19:25:29,490 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=150.0, ans=0.19437500000000002
2024-10-08 19:25:34,724 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=61.82 vs. limit=5.075
2024-10-08 19:25:49,463 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=157.79 vs. limit=7.5575
2024-10-08 19:25:51,983 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=156.66666666666666, ans=0.29843333333333333
2024-10-08 19:26:00,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=49.42 vs. limit=7.55875
2024-10-08 19:26:03,228 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=156.66666666666666, ans=0.49265625
2024-10-08 19:26:09,062 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=8.13 vs. limit=7.56
2024-10-08 19:26:10,765 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=2.99 vs. limit=4.064
2024-10-08 19:26:12,070 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=115.14 vs. limit=7.56
2024-10-08 19:26:16,245 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=160.0, ans=0.4925
2024-10-08 19:26:18,419 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=8.37 vs. limit=4.064
2024-10-08 19:26:33,970 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=163.33333333333334, ans=0.29836666666666667
2024-10-08 19:26:41,685 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.403e+01 5.436e+01 5.880e+01 6.582e+01 9.590e+01, threshold=1.176e+02, percent-clipped=0.0
2024-10-08 19:26:41,748 INFO [train.py:1152] Epoch 1, batch 500, loss[loss=0.8517, ctc_loss=1.059, attn_decoder_loss=0.7999, over 4806.00 frames. ], tot_loss[loss=1.039, ctc_loss=1.106, attn_decoder_loss=1.022, over 888220.59 frames. ], batch size: 34, lr: 4.49e-02,
2024-10-08 19:26:42,821 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=25.93 vs. limit=5.083333333333333
2024-10-08 19:26:47,433 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=17.94 vs. limit=5.083333333333333
2024-10-08 19:26:47,704 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=34.71 vs. limit=7.5625
2024-10-08 19:26:53,313 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=166.66666666666666, ans=0.4791666666666667
2024-10-08 19:27:11,306 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=103.57 vs. limit=7.56375
2024-10-08 19:27:11,800 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=3.82 vs. limit=5.0425
2024-10-08 19:27:17,955 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=104.43 vs. limit=7.565
2024-10-08 19:27:17,991 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=9.28 vs. limit=7.565
2024-10-08 19:27:35,433 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=23.40 vs. limit=7.6325
2024-10-08 19:27:52,782 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten.whitening_limit, batch_count=180.0, ans=7.5675
2024-10-08 19:27:56,020 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=185.31 vs. limit=7.5675
2024-10-08 19:27:56,207 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=82.20 vs. limit=5.09
2024-10-08 19:28:01,731 INFO [train.py:1152] Epoch 1, batch 550, loss[loss=0.8779, ctc_loss=1.125, attn_decoder_loss=0.8161, over 4821.00 frames. ], tot_loss[loss=0.9956, ctc_loss=1.099, attn_decoder_loss=0.9697, over 905711.34 frames. ], batch size: 40, lr: 4.49e-02,
2024-10-08 19:28:14,103 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2.whitening_limit, batch_count=183.33333333333334, ans=5.091666666666667
2024-10-08 19:28:14,232 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.49 vs. limit=5.091666666666667
2024-10-08 19:28:29,875 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.13 vs. limit=5.046666666666667
2024-10-08 19:28:33,773 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=190.0, ans=0.49109375
2024-10-08 19:28:39,421 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=16.40 vs. limit=7.57125
2024-10-08 19:28:43,684 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=20.33 vs. limit=7.57125
2024-10-08 19:28:44,113 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=16.63 vs. limit=7.6425
2024-10-08 19:28:54,401 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=193.33333333333334, ans=0.09879166666666667
2024-10-08 19:28:55,952 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=193.33333333333334, ans=0.4909375
2024-10-08 19:29:07,236 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=196.66666666666666, ans=0.2980333333333333
2024-10-08 19:29:09,292 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=161.23 vs. limit=7.57375
2024-10-08 19:29:17,750 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=59.70 vs. limit=7.57375
2024-10-08 19:29:20,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=200.0, ans=0.23875
2024-10-08 19:29:21,879 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.067e+01 4.860e+01 5.250e+01 5.832e+01 9.005e+01, threshold=1.050e+02, percent-clipped=0.0
2024-10-08 19:29:21,935 INFO [train.py:1152] Epoch 1, batch 600, loss[loss=0.8826, ctc_loss=1.122, attn_decoder_loss=0.8228, over 4834.00 frames. ], tot_loss[loss=0.9596, ctc_loss=1.092, attn_decoder_loss=0.9264, over 919511.50 frames. ], batch size: 38, lr: 4.49e-02,
2024-10-08 19:29:22,107 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=200.0, ans=0.475
2024-10-08 19:29:24,442 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=22.99 vs. limit=7.65
2024-10-08 19:29:36,989 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=199.80 vs. limit=7.57625
2024-10-08 19:29:47,273 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=203.33333333333334, ans=0.19237500000000002
2024-10-08 19:30:04,001 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=22.41 vs. limit=7.5775
2024-10-08 19:30:15,190 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=25.42 vs. limit=7.57875
2024-10-08 19:30:16,326 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=210.0, ans=0.5
2024-10-08 19:30:18,850 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=47.36 vs. limit=7.57875
2024-10-08 19:30:22,785 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=210.0, ans=0.47375
2024-10-08 19:30:41,058 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=9.99 vs. limit=4.086666666666667
2024-10-08 19:30:41,809 INFO [train.py:1152] Epoch 1, batch 650, loss[loss=0.8309, ctc_loss=1.047, attn_decoder_loss=0.7769, over 4844.00 frames. ], tot_loss[loss=0.9324, ctc_loss=1.088, attn_decoder_loss=0.8935, over 930350.51 frames. ], batch size: 21, lr: 4.49e-02,
2024-10-08 19:30:41,944 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=216.66666666666666, ans=0.09864583333333334
2024-10-08 19:30:44,365 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=24.40 vs. limit=7.58125
2024-10-08 19:30:55,312 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=12.71 vs. limit=5.108333333333333
2024-10-08 19:31:01,884 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=3.42 vs. limit=4.088
2024-10-08 19:31:05,753 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=220.0, ans=0.4725
2024-10-08 19:31:09,363 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=42.08 vs. limit=5.11
2024-10-08 19:31:16,239 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=17.74 vs. limit=5.111666666666666
2024-10-08 19:31:16,959 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=223.33333333333334, ans=0.2977666666666667
2024-10-08 19:31:22,269 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=79.87 vs. limit=7.58375
2024-10-08 19:31:26,628 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=223.33333333333334, ans=0.19162500000000002
2024-10-08 19:31:37,652 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=226.66666666666666, ans=0.1915
2024-10-08 19:31:41,597 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=512, metric=16.47 vs. limit=7.585
2024-10-08 19:31:51,343 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.52 vs. limit=7.6725
2024-10-08 19:31:56,371 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.16 vs. limit=7.6725
2024-10-08 19:32:01,405 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.922e+01 4.503e+01 4.861e+01 5.601e+01 8.703e+01, threshold=9.722e+01, percent-clipped=0.0
2024-10-08 19:32:01,466 INFO [train.py:1152] Epoch 1, batch 700, loss[loss=0.7881, ctc_loss=0.9963, attn_decoder_loss=0.7361, over 4758.00 frames. ], tot_loss[loss=0.9108, ctc_loss=1.086, attn_decoder_loss=0.8671, over 938206.07 frames. ], batch size: 19, lr: 4.49e-02,
2024-10-08 19:32:15,725 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=236.66666666666666, ans=0.48890625
2024-10-08 19:32:23,615 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=236.66666666666666, ans=0.48890625
2024-10-08 19:32:24,213 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=12.77 vs. limit=7.58875
2024-10-08 19:32:31,093 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=9.60 vs. limit=7.58875
2024-10-08 19:32:41,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer1.prob, batch_count=240.0, ans=0.48875
2024-10-08 19:32:41,055 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=240.0, ans=0.191
2024-10-08 19:32:44,693 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=19.95 vs. limit=7.59
2024-10-08 19:32:59,060 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.87 vs. limit=7.6825
2024-10-08 19:33:01,026 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.53 vs. limit=7.6825
2024-10-08 19:33:02,207 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=26.03 vs. limit=7.6825
2024-10-08 19:33:04,098 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=6.81 vs. limit=7.5925
2024-10-08 19:33:14,539 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=4.754e+00
2024-10-08 19:33:20,517 INFO [train.py:1152] Epoch 1, batch 750, loss[loss=0.7473, ctc_loss=0.994, attn_decoder_loss=0.6856, over 4880.00 frames. ], tot_loss[loss=0.8888, ctc_loss=1.078, attn_decoder_loss=0.8414, over 944974.19 frames. ], batch size: 22, lr: 4.49e-02,
2024-10-08 19:33:27,074 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=250.0, ans=0.19062500000000002
2024-10-08 19:33:27,479 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=73.54 vs. limit=7.59375
2024-10-08 19:33:30,189 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=250.0, ans=0.2975
2024-10-08 19:33:32,174 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=169.87 vs. limit=7.59375
2024-10-08 19:33:34,838 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=253.33333333333334, ans=0.5
2024-10-08 19:33:35,259 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=46.54 vs. limit=7.595
2024-10-08 19:33:41,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=253.33333333333334, ans=0.1905
2024-10-08 19:33:41,693 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten.whitening_limit, batch_count=253.33333333333334, ans=7.595
2024-10-08 19:33:45,847 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=253.33333333333334, ans=0.1905
2024-10-08 19:33:48,283 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.80 vs. limit=5.0633333333333335
2024-10-08 19:33:51,991 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer1.prob, batch_count=256.6666666666667, ans=0.48796875
2024-10-08 19:33:53,596 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=256.6666666666667, ans=0.46791666666666665
2024-10-08 19:33:59,091 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=47.99 vs. limit=7.59625
2024-10-08 19:34:00,456 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=4.10 vs. limit=4.102666666666667
2024-10-08 19:34:10,387 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.59 vs. limit=7.5975
2024-10-08 19:34:14,914 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.51 vs. limit=5.065
2024-10-08 19:34:30,732 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=20.22 vs. limit=7.59875
2024-10-08 19:34:39,779 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.557e+01 4.630e+01 5.178e+01 5.947e+01 1.016e+02, threshold=1.036e+02, percent-clipped=1.0
2024-10-08 19:34:39,836 INFO [train.py:1152] Epoch 1, batch 800, loss[loss=0.7691, ctc_loss=0.9787, attn_decoder_loss=0.7168, over 4855.00 frames. ], tot_loss[loss=0.8725, ctc_loss=1.075, attn_decoder_loss=0.8219, over 949819.77 frames. ], batch size: 19, lr: 4.49e-02,
2024-10-08 19:34:51,777 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.53 vs. limit=7.7
2024-10-08 19:34:59,335 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.43 vs. limit=5.135
2024-10-08 19:35:08,717 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=121.38 vs. limit=7.60125
2024-10-08 19:35:13,594 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=12.27 vs. limit=7.6025
2024-10-08 19:35:19,170 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=273.3333333333333, ans=0.4871875
2024-10-08 19:35:28,775 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=276.6666666666667, ans=0.48703125
2024-10-08 19:35:35,571 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=92.07 vs. limit=7.60375
2024-10-08 19:35:35,772 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=10.31 vs. limit=7.60375
2024-10-08 19:36:00,236 INFO [train.py:1152] Epoch 1, batch 850, loss[loss=0.7461, ctc_loss=0.982, attn_decoder_loss=0.6871, over 4793.00 frames. ], tot_loss[loss=0.8595, ctc_loss=1.07, attn_decoder_loss=0.8068, over 954090.52 frames. ], batch size: 29, lr: 4.49e-02,
2024-10-08 19:36:02,029 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten.whitening_limit, batch_count=283.3333333333333, ans=7.60625
2024-10-08 19:36:09,336 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=6.49 vs. limit=7.60625
2024-10-08 19:36:19,216 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=286.6666666666667, ans=0.4865625
2024-10-08 19:36:19,860 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.11 vs. limit=7.715
2024-10-08 19:36:27,588 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=61.57 vs. limit=7.6075
2024-10-08 19:36:30,477 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=116.92 vs. limit=7.60875
2024-10-08 19:36:40,772 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=290.0, ans=0.46375
2024-10-08 19:36:41,427 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=93.60 vs. limit=7.60875
2024-10-08 19:36:46,889 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=3.340e+01
2024-10-08 19:36:47,792 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=66.54 vs. limit=7.61
2024-10-08 19:36:52,241 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=11.80 vs. limit=7.61
2024-10-08 19:36:59,673 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=296.6666666666667, ans=0.48609375
2024-10-08 19:37:01,616 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=61.48 vs. limit=7.61125
2024-10-08 19:37:01,909 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=5.62 vs. limit=5.148333333333333
2024-10-08 19:37:03,678 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.98 vs. limit=7.7225
2024-10-08 19:37:12,326 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=296.6666666666667, ans=0.48609375
2024-10-08 19:37:14,474 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=10.44 vs. limit=7.61125
2024-10-08 19:37:16,307 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.98 vs. limit=7.6125
2024-10-08 19:37:16,783 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.455e+01 4.880e+01 5.554e+01 6.426e+01 1.473e+02, threshold=1.111e+02, percent-clipped=4.0
2024-10-08 19:37:16,845 INFO [train.py:1152] Epoch 1, batch 900, loss[loss=0.7838, ctc_loss=1.003, attn_decoder_loss=0.7289, over 4854.00 frames. ], tot_loss[loss=0.8487, ctc_loss=1.065, attn_decoder_loss=0.7947, over 956897.83 frames. ], batch size: 19, lr: 4.48e-02,
2024-10-08 19:37:25,565 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=6.91 vs. limit=4.12
2024-10-08 19:37:38,331 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.34 vs. limit=5.151666666666666
2024-10-08 19:37:40,925 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=303.3333333333333, ans=0.2329375
2024-10-08 19:38:03,131 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=310.0, ans=0.88915
2024-10-08 19:38:17,725 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=9.92 vs. limit=7.61625
2024-10-08 19:38:26,202 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=49.57 vs. limit=7.6175
2024-10-08 19:38:29,101 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.45 vs. limit=5.156666666666666
2024-10-08 19:38:36,304 INFO [train.py:1152] Epoch 1, batch 950, loss[loss=0.8334, ctc_loss=1.07, attn_decoder_loss=0.7743, over 4817.00 frames. ], tot_loss[loss=0.8405, ctc_loss=1.062, attn_decoder_loss=0.7851, over 958663.64 frames. ], batch size: 19, lr: 4.48e-02,
2024-10-08 19:38:40,323 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=71.40 vs. limit=7.61875
2024-10-08 19:38:50,741 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=320.0, ans=0.485
2024-10-08 19:38:55,424 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=320.0, ans=0.2968
2024-10-08 19:38:58,527 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=320.0, ans=0.188
2024-10-08 19:39:14,877 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=125.84 vs. limit=7.62125
2024-10-08 19:39:18,953 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=323.3333333333333, ans=0.48484375
2024-10-08 19:39:30,926 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=21.95 vs. limit=7.745
2024-10-08 19:39:43,726 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=8.92 vs. limit=7.62375
2024-10-08 19:39:55,712 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.868e+01 4.733e+01 5.449e+01 6.459e+01 1.109e+02, threshold=1.090e+02, percent-clipped=0.0
2024-10-08 19:39:55,769 INFO [train.py:1152] Epoch 1, batch 1000, loss[loss=0.7411, ctc_loss=0.976, attn_decoder_loss=0.6823, over 4940.00 frames. ], tot_loss[loss=0.8352, ctc_loss=1.059, attn_decoder_loss=0.7792, over 960466.08 frames. ], batch size: 20, lr: 4.48e-02,
2024-10-08 19:39:59,933 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=16.81 vs. limit=7.625
2024-10-08 19:40:09,032 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=26.96 vs. limit=7.625
2024-10-08 19:40:20,408 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=7.01 vs. limit=4.134666666666667
2024-10-08 19:40:33,438 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=30.56 vs. limit=7.6275
2024-10-08 19:40:36,525 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.14 vs. limit=5.17
2024-10-08 19:40:45,539 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=343.3333333333333, ans=0.48390625
2024-10-08 19:40:54,267 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.14 vs. limit=7.7575
2024-10-08 19:41:00,444 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.79 vs. limit=5.173333333333334
2024-10-08 19:41:14,060 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=350.0, ans=0.48359375
2024-10-08 19:41:15,497 INFO [train.py:1152] Epoch 1, batch 1050, loss[loss=0.7711, ctc_loss=0.9636, attn_decoder_loss=0.723, over 4792.00 frames. ], tot_loss[loss=0.8267, ctc_loss=1.051, attn_decoder_loss=0.7707, over 962579.73 frames. ], batch size: 25, lr: 4.48e-02,
2024-10-08 19:41:26,021 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten.whitening_limit, batch_count=350.0, ans=7.63125
2024-10-08 19:41:26,975 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=350.0, ans=0.186875
2024-10-08 19:41:33,747 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.58 vs. limit=5.088333333333333
2024-10-08 19:41:36,856 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=49.78 vs. limit=7.6325
2024-10-08 19:41:44,361 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=353.3333333333333, ans=0.4834375
2024-10-08 19:41:53,056 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=2.62 vs. limit=3.0535
2024-10-08 19:41:58,563 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=356.6666666666667, ans=0.48328125
2024-10-08 19:42:00,776 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=356.6666666666667, ans=7.7675
2024-10-08 19:42:14,110 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=12.92 vs. limit=7.635
2024-10-08 19:42:16,952 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.94 vs. limit=5.18
2024-10-08 19:42:21,930 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=46.06 vs. limit=7.63625
2024-10-08 19:42:27,634 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=363.3333333333333, ans=0.091825
2024-10-08 19:42:35,463 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.655e+01 4.698e+01 5.289e+01 6.176e+01 1.146e+02, threshold=1.058e+02, percent-clipped=2.0
2024-10-08 19:42:35,525 INFO [train.py:1152] Epoch 1, batch 1100, loss[loss=0.764, ctc_loss=0.9615, attn_decoder_loss=0.7147, over 4863.00 frames. ], tot_loss[loss=0.8197, ctc_loss=1.042, attn_decoder_loss=0.7641, over 964036.58 frames. ], batch size: 20, lr: 4.48e-02,
2024-10-08 19:42:56,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=370.0, ans=0.48265625
2024-10-08 19:43:01,469 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=370.0, ans=0.48265625
2024-10-08 19:43:07,120 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.97 vs. limit=7.64
2024-10-08 19:43:25,833 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=5.14 vs. limit=4.150666666666667
2024-10-08 19:43:27,692 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=23.92 vs. limit=7.64125
2024-10-08 19:43:29,160 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.32 vs. limit=5.094166666666666
2024-10-08 19:43:38,730 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.11 vs. limit=3.057
2024-10-08 19:43:43,435 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=28.28 vs. limit=7.6425
2024-10-08 19:43:45,149 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.10 vs. limit=5.095
2024-10-08 19:43:50,416 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=8.55 vs. limit=7.785
2024-10-08 19:43:55,693 INFO [train.py:1152] Epoch 1, batch 1150, loss[loss=0.8265, ctc_loss=1.051, attn_decoder_loss=0.7704, over 4850.00 frames. ], tot_loss[loss=0.8155, ctc_loss=1.035, attn_decoder_loss=0.7607, over 964294.11 frames. ], batch size: 20, lr: 4.47e-02,
2024-10-08 19:44:00,035 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.25 vs. limit=4.153333333333333
2024-10-08 19:44:04,788 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.39 vs. limit=5.095833333333333
2024-10-08 19:44:12,023 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=386.6666666666667, ans=0.1855
2024-10-08 19:44:12,406 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=8.46 vs. limit=7.645
2024-10-08 19:44:15,740 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=8.50 vs. limit=7.645
2024-10-08 19:44:16,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=386.6666666666667, ans=0.2961333333333333
2024-10-08 19:44:18,465 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=386.6666666666667, ans=0.2961333333333333
2024-10-08 19:44:21,674 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=386.6666666666667, ans=0.09758333333333334
2024-10-08 19:44:22,207 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.40 vs. limit=3.058
2024-10-08 19:44:26,486 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=390.0, ans=0.48171875
2024-10-08 19:44:33,602 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=22.07 vs. limit=7.64625
2024-10-08 19:44:42,922 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=29.04 vs. limit=7.795
2024-10-08 19:44:44,876 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.54 vs. limit=4.157333333333334
2024-10-08 19:44:50,924 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.64 vs. limit=5.098333333333334
2024-10-08 19:44:52,864 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.87 vs. limit=7.795
2024-10-08 19:44:59,026 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.30 vs. limit=5.198333333333333
2024-10-08 19:45:04,856 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=396.6666666666667, ans=0.185125
2024-10-08 19:45:08,156 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=396.6666666666667, ans=0.48140625
2024-10-08 19:45:16,205 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.212e+01 5.022e+01 6.212e+01 7.524e+01 1.364e+02, threshold=1.242e+02, percent-clipped=3.0
2024-10-08 19:45:16,261 INFO [train.py:1152] Epoch 1, batch 1200, loss[loss=0.7828, ctc_loss=1.021, attn_decoder_loss=0.7233, over 4810.00 frames. ], tot_loss[loss=0.8096, ctc_loss=1.025, attn_decoder_loss=0.7557, over 964164.94 frames. ], batch size: 25, lr: 4.47e-02,
2024-10-08 19:45:16,357 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=400.0, ans=0.45
2024-10-08 19:45:17,837 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=400.0, ans=0.45
2024-10-08 19:45:23,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=30.12 vs. limit=7.8
2024-10-08 19:45:24,900 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=19.24 vs. limit=7.65
2024-10-08 19:45:38,597 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=403.3333333333333, ans=0.48109375
2024-10-08 19:45:42,701 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=19.71 vs. limit=7.65125
2024-10-08 19:45:58,258 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=21.73 vs. limit=7.6525
2024-10-08 19:46:01,001 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=406.6666666666667, ans=0.18475
2024-10-08 19:46:08,955 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=410.0, ans=0.2959
2024-10-08 19:46:18,611 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.prob, batch_count=413.3333333333333, ans=0.480625
2024-10-08 19:46:36,069 INFO [train.py:1152] Epoch 1, batch 1250, loss[loss=0.7548, ctc_loss=0.9235, attn_decoder_loss=0.7126, over 4754.00 frames. ], tot_loss[loss=0.8051, ctc_loss=1.016, attn_decoder_loss=0.7523, over 964201.34 frames. ], batch size: 32, lr: 4.47e-02,
2024-10-08 19:46:44,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=416.6666666666667, ans=0.29583333333333334
2024-10-08 19:46:56,591 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.04 vs. limit=5.105
2024-10-08 19:47:05,435 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=420.0, ans=0.2063
2024-10-08 19:47:08,782 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=423.3333333333333, ans=0.8851833333333333
2024-10-08 19:47:11,983 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=423.3333333333333, ans=0.44708333333333333
2024-10-08 19:47:12,553 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=40.31 vs. limit=7.65875
2024-10-08 19:47:14,126 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=22.51 vs. limit=7.65875
2024-10-08 19:47:16,773 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=423.3333333333333, ans=0.8851833333333333
2024-10-08 19:47:18,365 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=423.3333333333333, ans=0.184125
2024-10-08 19:47:19,338 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=11.25 vs. limit=7.65875
2024-10-08 19:47:26,934 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=426.6666666666667, ans=0.48
2024-10-08 19:47:29,861 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer_na.min_abs, batch_count=426.6666666666667, ans=0.005706666666666667
2024-10-08 19:47:32,263 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=25.13 vs. limit=7.66
2024-10-08 19:47:38,145 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=426.6666666666667, ans=0.48
2024-10-08 19:47:46,355 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.max_abs, batch_count=430.0, ans=5.26875
2024-10-08 19:47:57,197 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.116e+01 4.810e+01 5.631e+01 6.991e+01 1.294e+02, threshold=1.126e+02, percent-clipped=2.0
2024-10-08 19:47:57,262 INFO [train.py:1152] Epoch 1, batch 1300, loss[loss=0.8287, ctc_loss=1.013, attn_decoder_loss=0.7824, over 4845.00 frames. ], tot_loss[loss=0.7972, ctc_loss=1.001, attn_decoder_loss=0.7464, over 965501.64 frames. ], batch size: 43, lr: 4.47e-02,
2024-10-08 19:47:59,850 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.30 vs. limit=7.825
2024-10-08 19:48:06,094 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=8.69 vs. limit=7.6625
2024-10-08 19:48:10,110 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=433.3333333333333, ans=0.44583333333333336
2024-10-08 19:48:13,723 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=26.99 vs. limit=7.8275
2024-10-08 19:48:14,078 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=21.64 vs. limit=7.66375
2024-10-08 19:48:15,422 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=33.17 vs. limit=7.66375
2024-10-08 19:48:20,320 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.17 vs. limit=3.0655
2024-10-08 19:48:23,567 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=19.27 vs. limit=7.66375
2024-10-08 19:48:26,912 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=14.35 vs. limit=7.66375
2024-10-08 19:48:30,329 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=8.32 vs. limit=7.83
2024-10-08 19:48:42,017 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=440.0, ans=0.479375
2024-10-08 19:48:47,287 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=4.83 vs. limit=4.177333333333333
2024-10-08 19:48:48,916 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=19.05 vs. limit=7.66625
2024-10-08 19:48:52,615 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.15 vs. limit=7.8325
2024-10-08 19:49:00,159 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=24.43 vs. limit=7.835
2024-10-08 19:49:02,920 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=1.508e-01
2024-10-08 19:49:11,956 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=10.14 vs. limit=7.6675
2024-10-08 19:49:14,945 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.whiten.whitening_limit, batch_count=446.6666666666667, ans=4.1786666666666665
2024-10-08 19:49:17,265 INFO [train.py:1152] Epoch 1, batch 1350, loss[loss=0.7786, ctc_loss=0.9266, attn_decoder_loss=0.7416, over 4829.00 frames. ], tot_loss[loss=0.7898, ctc_loss=0.9847, attn_decoder_loss=0.7411, over 966236.50 frames. ], batch size: 21, lr: 4.46e-02,
2024-10-08 19:49:20,864 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 19:49:21,873 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=9.90 vs. limit=7.66875
2024-10-08 19:49:23,145 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=5.09 vs. limit=4.18
2024-10-08 19:49:29,413 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=22.58 vs. limit=7.66875
2024-10-08 19:49:29,463 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=25.29 vs. limit=7.66875
2024-10-08 19:49:32,880 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=13.22 vs. limit=7.67
2024-10-08 19:49:37,496 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.15 vs. limit=5.226666666666667
2024-10-08 19:49:39,338 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.65 vs. limit=5.113333333333333
2024-10-08 19:49:40,522 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=26.96 vs. limit=7.84
2024-10-08 19:49:40,610 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.34 vs. limit=5.113333333333333
2024-10-08 19:49:44,719 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=453.3333333333333, ans=0.183
2024-10-08 19:49:47,150 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.60 vs. limit=7.84
2024-10-08 19:49:58,543 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.05 vs. limit=7.67125
2024-10-08 19:50:04,749 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=24.28 vs. limit=7.6725
2024-10-08 19:50:07,948 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten.whitening_limit, batch_count=460.0, ans=7.6725
2024-10-08 19:50:32,489 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=18.15 vs. limit=7.67375
2024-10-08 19:50:32,573 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=16.13 vs. limit=7.67375
2024-10-08 19:50:38,337 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.096e+01 5.241e+01 6.263e+01 8.402e+01 1.823e+02, threshold=1.253e+02, percent-clipped=13.0
2024-10-08 19:50:38,393 INFO [train.py:1152] Epoch 1, batch 1400, loss[loss=0.7498, ctc_loss=0.8913, attn_decoder_loss=0.7144, over 4940.00 frames. ], tot_loss[loss=0.7857, ctc_loss=0.9732, attn_decoder_loss=0.7388, over 966729.80 frames. ], batch size: 19, lr: 4.46e-02,
2024-10-08 19:50:38,565 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=466.6666666666667, ans=0.7546666666666667
2024-10-08 19:50:41,044 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=8.95 vs. limit=7.675
2024-10-08 19:50:42,595 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.51 vs. limit=7.85
2024-10-08 19:50:44,791 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=466.6666666666667, ans=0.44166666666666665
2024-10-08 19:50:47,241 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=21.20 vs. limit=7.675
2024-10-08 19:50:59,419 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=470.0, ans=0.2235625
2024-10-08 19:51:11,413 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.87 vs. limit=5.118333333333333
2024-10-08 19:51:35,259 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=476.6666666666667, ans=0.29523333333333335
2024-10-08 19:51:36,043 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.01 vs. limit=5.119166666666667
2024-10-08 19:51:56,336 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=480.0, ans=0.29519999999999996
2024-10-08 19:51:56,394 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=480.0, ans=5.3
2024-10-08 19:51:59,458 INFO [train.py:1152] Epoch 1, batch 1450, loss[loss=0.7287, ctc_loss=0.8938, attn_decoder_loss=0.6874, over 4798.00 frames. ], tot_loss[loss=0.7798, ctc_loss=0.9574, attn_decoder_loss=0.7354, over 966641.08 frames. ], batch size: 34, lr: 4.46e-02,
2024-10-08 19:52:03,699 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=16.78 vs. limit=7.68125
2024-10-08 19:52:06,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=483.3333333333333, ans=0.47734375
2024-10-08 19:52:22,305 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=486.6666666666667, ans=0.4771875
2024-10-08 19:52:22,977 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=27.46 vs. limit=7.865
2024-10-08 19:52:24,507 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=5.19 vs. limit=4.1946666666666665
2024-10-08 19:52:24,624 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.17 vs. limit=3.073
2024-10-08 19:52:43,458 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=490.0, ans=0.47703125
2024-10-08 19:52:53,815 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=12.35 vs. limit=5.246666666666667
2024-10-08 19:53:14,594 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=496.6666666666667, ans=0.2950333333333333
2024-10-08 19:53:20,879 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.392e+01 5.431e+01 6.175e+01 7.435e+01 1.421e+02, threshold=1.235e+02, percent-clipped=1.0
2024-10-08 19:53:20,944 INFO [train.py:1152] Epoch 1, batch 1500, loss[loss=0.7099, ctc_loss=0.837, attn_decoder_loss=0.6781, over 4743.00 frames. ], tot_loss[loss=0.7759, ctc_loss=0.9436, attn_decoder_loss=0.7339, over 966360.75 frames. ], batch size: 26, lr: 4.46e-02,
2024-10-08 19:53:25,417 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=2.19 vs. limit=4.2
2024-10-08 19:53:26,795 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.77 vs. limit=5.125
2024-10-08 19:53:53,164 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.61 vs. limit=7.88
2024-10-08 19:53:54,189 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=12.74 vs. limit=7.69
2024-10-08 19:53:56,020 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=27.22 vs. limit=7.88
2024-10-08 19:53:56,300 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.96 vs. limit=7.88
2024-10-08 19:53:56,349 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.80 vs. limit=5.126666666666667
2024-10-08 19:54:04,355 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=23.46 vs. limit=7.69
2024-10-08 19:54:05,469 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=506.6666666666667, ans=0.2949333333333333
2024-10-08 19:54:28,737 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.64 vs. limit=3.077
2024-10-08 19:54:28,793 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.99 vs. limit=4.205333333333333
2024-10-08 19:54:38,675 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=32.47 vs. limit=7.885
2024-10-08 19:54:42,883 INFO [train.py:1152] Epoch 1, batch 1550, loss[loss=0.7215, ctc_loss=0.8415, attn_decoder_loss=0.6915, over 4857.00 frames. ], tot_loss[loss=0.7699, ctc_loss=0.9296, attn_decoder_loss=0.73, over 966180.64 frames. ], batch size: 31, lr: 4.45e-02,
2024-10-08 19:54:51,856 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=27.86 vs. limit=7.8875
2024-10-08 19:54:58,423 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=28.55 vs. limit=7.89
2024-10-08 19:55:04,452 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=520.0, ans=0.048375
2024-10-08 19:55:10,018 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.19 vs. limit=5.26
2024-10-08 19:55:23,092 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=25.10 vs. limit=7.8925
2024-10-08 19:55:28,997 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=523.3333333333334, ans=5.327083333333333
2024-10-08 19:55:34,341 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=11.62 vs. limit=7.6975
2024-10-08 19:55:44,417 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=31.18 vs. limit=7.895
2024-10-08 19:55:48,520 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=530.0, ans=0.47515625
2024-10-08 19:55:52,838 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.93 vs. limit=7.69875
2024-10-08 19:56:04,846 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.132e+01 5.455e+01 6.543e+01 8.368e+01 1.812e+02, threshold=1.309e+02, percent-clipped=8.0
2024-10-08 19:56:04,905 INFO [train.py:1152] Epoch 1, batch 1600, loss[loss=0.7633, ctc_loss=0.8786, attn_decoder_loss=0.7345, over 4810.00 frames. ], tot_loss[loss=0.7619, ctc_loss=0.9116, attn_decoder_loss=0.7245, over 966378.52 frames. ], batch size: 25, lr: 4.45e-02,
2024-10-08 19:56:06,462 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=533.3333333333334, ans=0.22
2024-10-08 19:56:08,124 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=533.3333333333334, ans=0.475
2024-10-08 19:56:11,462 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=533.3333333333334, ans=0.09666666666666668
2024-10-08 19:56:22,865 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=536.6666666666666, ans=0.087925
2024-10-08 19:56:38,239 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=21.12 vs. limit=7.7025
2024-10-08 19:56:39,193 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=540.0, ans=0.08785
2024-10-08 19:56:41,545 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=8.06 vs. limit=7.7025
2024-10-08 19:56:42,910 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=26.49 vs. limit=7.905
2024-10-08 19:56:47,252 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=540.0, ans=0.4746875
2024-10-08 19:56:53,153 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.29 vs. limit=5.135833333333333
2024-10-08 19:56:55,341 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=543.3333333333334, ans=0.8809833333333333
2024-10-08 19:56:58,168 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.63 vs. limit=7.9075
2024-10-08 19:56:58,707 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=543.3333333333334, ans=0.47453125
2024-10-08 19:57:00,819 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.44 vs. limit=7.70375
2024-10-08 19:57:02,668 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=23.81 vs. limit=7.70375
2024-10-08 19:57:17,306 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=14.31 vs. limit=7.705
2024-10-08 19:57:23,629 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.67 vs. limit=5.136666666666667
2024-10-08 19:57:26,141 INFO [train.py:1152] Epoch 1, batch 1650, loss[loss=0.7532, ctc_loss=0.8709, attn_decoder_loss=0.7237, over 4792.00 frames. ], tot_loss[loss=0.7546, ctc_loss=0.8966, attn_decoder_loss=0.7191, over 966732.12 frames. ], batch size: 29, lr: 4.45e-02,
2024-10-08 19:57:27,817 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=550.0, ans=0.43125
2024-10-08 19:57:32,849 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=550.0, ans=0.2945
2024-10-08 19:57:36,629 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=26.72 vs. limit=7.9125
2024-10-08 19:57:39,241 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=550.0, ans=0.2945
2024-10-08 19:57:47,999 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=21.51 vs. limit=7.915
2024-10-08 19:57:50,132 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=7.63 vs. limit=7.7075
2024-10-08 19:57:57,449 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=556.6666666666666, ans=0.09652083333333333
2024-10-08 19:58:14,239 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=12.47 vs. limit=5.28
2024-10-08 19:58:32,003 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.48 vs. limit=5.281666666666666
2024-10-08 19:58:46,525 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=566.6666666666666, ans=0.29433333333333334
2024-10-08 19:58:47,910 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.291e+01 5.877e+01 6.444e+01 8.359e+01 1.533e+02, threshold=1.289e+02, percent-clipped=1.0
2024-10-08 19:58:47,974 INFO [train.py:1152] Epoch 1, batch 1700, loss[loss=0.6912, ctc_loss=0.815, attn_decoder_loss=0.6603, over 4940.00 frames. ], tot_loss[loss=0.7487, ctc_loss=0.883, attn_decoder_loss=0.7152, over 966952.97 frames. ], batch size: 19, lr: 4.44e-02,
2024-10-08 19:58:52,925 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=566.6666666666666, ans=0.09645833333333334
2024-10-08 19:59:08,864 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=570.0, ans=0.47328125
2024-10-08 19:59:10,518 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=570.0, ans=0.88005
2024-10-08 19:59:11,179 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=8.06 vs. limit=7.71375
2024-10-08 19:59:13,030 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.59 vs. limit=7.9275
2024-10-08 19:59:19,129 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.52 vs. limit=5.1433333333333335
2024-10-08 19:59:19,378 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten.whitening_limit, batch_count=573.3333333333334, ans=7.93
2024-10-08 19:59:19,379 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.56 vs. limit=7.93
2024-10-08 19:59:29,909 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=573.3333333333334, ans=0.8799333333333333
2024-10-08 19:59:35,428 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=18.09 vs. limit=7.9325
2024-10-08 19:59:47,368 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=576.6666666666666, ans=0.178375
2024-10-08 19:59:50,017 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=13.06 vs. limit=7.9325
2024-10-08 20:00:05,658 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=11.11 vs. limit=5.29
2024-10-08 20:00:08,284 INFO [train.py:1152] Epoch 1, batch 1750, loss[loss=0.6632, ctc_loss=0.7114, attn_decoder_loss=0.6511, over 4959.00 frames. ], tot_loss[loss=0.7381, ctc_loss=0.8653, attn_decoder_loss=0.7064, over 967116.99 frames. ], batch size: 19, lr: 4.44e-02,
2024-10-08 20:00:18,669 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=34.97 vs. limit=7.9375
2024-10-08 20:00:28,690 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=18.54 vs. limit=7.94
2024-10-08 20:00:48,062 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=37.34 vs. limit=7.9425
2024-10-08 20:00:52,093 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=590.0, ans=0.47234375
2024-10-08 20:00:54,267 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.27 vs. limit=7.72125
2024-10-08 20:01:05,182 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=593.3333333333334, ans=0.4721875
2024-10-08 20:01:06,362 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.42 vs. limit=5.148333333333333
2024-10-08 20:01:09,174 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.83 vs. limit=7.7225
2024-10-08 20:01:12,203 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=6.43 vs. limit=4.238666666666667
2024-10-08 20:01:21,313 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.min_positive, batch_count=596.6666666666666, ans=0.24403333333333332
2024-10-08 20:01:29,327 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.535e+01 6.387e+01 7.340e+01 8.846e+01 1.706e+02, threshold=1.468e+02, percent-clipped=4.0
2024-10-08 20:01:29,384 INFO [train.py:1152] Epoch 1, batch 1800, loss[loss=0.7107, ctc_loss=0.8305, attn_decoder_loss=0.6808, over 4864.00 frames. ], tot_loss[loss=0.732, ctc_loss=0.8547, attn_decoder_loss=0.7014, over 967907.35 frames. ], batch size: 23, lr: 4.44e-02,
2024-10-08 20:01:42,303 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=600.0, ans=0.471875
2024-10-08 20:01:46,454 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.70 vs. limit=7.72625
2024-10-08 20:01:52,203 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=603.3333333333334, ans=0.29396666666666665
2024-10-08 20:01:56,158 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=34.94 vs. limit=7.9525
2024-10-08 20:01:58,760 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=603.3333333333334, ans=0.8788833333333333
2024-10-08 20:02:02,086 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=606.6666666666666, ans=0.4715625
2024-10-08 20:02:09,457 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=16.38 vs. limit=7.955
2024-10-08 20:02:17,414 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.96 vs. limit=4.244
2024-10-08 20:02:50,215 INFO [train.py:1152] Epoch 1, batch 1850, loss[loss=0.6785, ctc_loss=0.7991, attn_decoder_loss=0.6484, over 4737.00 frames. ], tot_loss[loss=0.7235, ctc_loss=0.8401, attn_decoder_loss=0.6943, over 968162.53 frames. ], batch size: 26, lr: 4.43e-02,
2024-10-08 20:02:53,612 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=616.6666666666666, ans=0.176875
2024-10-08 20:02:56,146 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=25.92 vs. limit=7.9625
2024-10-08 20:02:57,276 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=26.99 vs. limit=7.9625
2024-10-08 20:02:57,563 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.12 vs. limit=7.9625
2024-10-08 20:03:02,328 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=25.27 vs. limit=7.73125
2024-10-08 20:03:04,831 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=620.0, ans=0.4225
2024-10-08 20:03:05,672 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=9.43 vs. limit=7.7325
2024-10-08 20:03:26,618 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.74 vs. limit=4.249333333333333
2024-10-08 20:03:34,656 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=26.57 vs. limit=7.9675
2024-10-08 20:03:45,396 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=626.6666666666666, ans=0.7562666666666666
2024-10-08 20:03:52,487 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=14.17 vs. limit=7.735
2024-10-08 20:03:58,830 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=32.46 vs. limit=7.9725
2024-10-08 20:04:07,200 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=6.68 vs. limit=7.73625
2024-10-08 20:04:11,190 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.596e+01 6.582e+01 7.641e+01 8.884e+01 1.330e+02, threshold=1.528e+02, percent-clipped=0.0
2024-10-08 20:04:11,253 INFO [train.py:1152] Epoch 1, batch 1900, loss[loss=0.6975, ctc_loss=0.804, attn_decoder_loss=0.6709, over 4773.00 frames. ], tot_loss[loss=0.7173, ctc_loss=0.8313, attn_decoder_loss=0.6888, over 967894.55 frames. ], batch size: 29, lr: 4.43e-02,
2024-10-08 20:04:12,214 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=9.68 vs. limit=7.7375
2024-10-08 20:04:31,542 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.03 vs. limit=5.159166666666667
2024-10-08 20:04:42,931 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.25 vs. limit=7.98
2024-10-08 20:04:43,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=640.0, ans=0.5
2024-10-08 20:05:00,379 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=73.82 vs. limit=7.9825
2024-10-08 20:05:07,775 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=643.3333333333334, ans=0.46984375
2024-10-08 20:05:12,555 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=643.3333333333334, ans=0.20965
2024-10-08 20:05:18,888 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=646.6666666666666, ans=0.4191666666666667
2024-10-08 20:05:19,660 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=16.82 vs. limit=7.7425
2024-10-08 20:05:21,129 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=11.08 vs. limit=7.7425
2024-10-08 20:05:28,603 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=646.6666666666666, ans=0.4696875
2024-10-08 20:05:31,704 INFO [train.py:1152] Epoch 1, batch 1950, loss[loss=0.6295, ctc_loss=0.6842, attn_decoder_loss=0.6158, over 4863.00 frames. ], tot_loss[loss=0.7094, ctc_loss=0.8199, attn_decoder_loss=0.6818, over 966894.31 frames. ], batch size: 20, lr: 4.43e-02,
2024-10-08 20:05:32,692 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.83 vs. limit=5.1625
2024-10-08 20:05:38,845 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.64 vs. limit=7.74375
2024-10-08 20:05:56,622 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=6.00 vs. limit=4.261333333333333
2024-10-08 20:05:59,078 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=653.3333333333334, ans=0.41833333333333333
2024-10-08 20:05:59,103 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=653.3333333333334, ans=0.8771333333333333
2024-10-08 20:06:02,374 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=656.6666666666666, ans=0.2934333333333333
2024-10-08 20:06:06,553 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.95 vs. limit=5.328333333333333
2024-10-08 20:06:13,461 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=656.6666666666666, ans=0.2934333333333333
2024-10-08 20:06:15,157 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=656.6666666666666, ans=0.46921875
2024-10-08 20:06:18,375 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=660.0, ans=0.4690625
2024-10-08 20:06:19,012 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=27.85 vs. limit=7.995
2024-10-08 20:06:23,722 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.18 vs. limit=7.7475
2024-10-08 20:06:33,691 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=22.57 vs. limit=7.7475
2024-10-08 20:06:33,743 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.32 vs. limit=7.995
2024-10-08 20:06:34,592 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=663.3333333333334, ans=0.29336666666666666
2024-10-08 20:06:35,278 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=41.97 vs. limit=7.74875
2024-10-08 20:06:41,156 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=663.3333333333334, ans=0.46890625
2024-10-08 20:06:52,598 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.574e+01 6.938e+01 8.333e+01 1.025e+02 1.660e+02, threshold=1.667e+02, percent-clipped=2.0
2024-10-08 20:06:52,656 INFO [train.py:1152] Epoch 1, batch 2000, loss[loss=0.5914, ctc_loss=0.6412, attn_decoder_loss=0.579, over 4959.00 frames. ], tot_loss[loss=0.7013, ctc_loss=0.8093, attn_decoder_loss=0.6743, over 966732.32 frames. ], batch size: 19, lr: 4.42e-02,
2024-10-08 20:07:07,024 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=670.0, ans=0.41625
2024-10-08 20:07:09,236 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=9.33 vs. limit=7.75125
2024-10-08 20:07:15,728 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=16.86 vs. limit=7.75125
2024-10-08 20:07:17,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.43 vs. limit=5.1675
2024-10-08 20:07:26,963 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.73 vs. limit=7.7525
2024-10-08 20:07:28,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=673.3333333333334, ans=0.08485000000000001
2024-10-08 20:07:33,445 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.38 vs. limit=5.168333333333333
2024-10-08 20:07:34,421 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer_na.min_abs, batch_count=673.3333333333334, ans=0.006693333333333333
2024-10-08 20:07:43,482 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=18.07 vs. limit=7.75375
2024-10-08 20:08:12,917 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.11 vs. limit=3.1025
2024-10-08 20:08:13,484 INFO [train.py:1152] Epoch 1, batch 2050, loss[loss=0.6182, ctc_loss=0.6906, attn_decoder_loss=0.6001, over 4914.00 frames. ], tot_loss[loss=0.6912, ctc_loss=0.7955, attn_decoder_loss=0.6652, over 967008.64 frames. ], batch size: 19, lr: 4.42e-02,
2024-10-08 20:08:33,730 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=34.55 vs. limit=8.015
2024-10-08 20:08:39,611 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=686.6666666666666, ans=0.4678125
2024-10-08 20:08:50,479 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.65 vs. limit=5.1725
2024-10-08 20:08:52,183 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.82 vs. limit=5.1725
2024-10-08 20:08:53,377 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=52.19 vs. limit=8.0175
2024-10-08 20:08:59,939 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.45 vs. limit=5.1725
2024-10-08 20:09:01,822 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn1.whiten.whitening_limit, batch_count=693.3333333333334, ans=8.02
2024-10-08 20:09:28,311 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=696.6666666666666, ans=0.5
2024-10-08 20:09:30,593 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.71 vs. limit=7.76125
2024-10-08 20:09:33,879 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=15.02 vs. limit=8.025
2024-10-08 20:09:34,313 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.462e+01 7.425e+01 8.739e+01 1.046e+02 1.639e+02, threshold=1.748e+02, percent-clipped=0.0
2024-10-08 20:09:34,376 INFO [train.py:1152] Epoch 1, batch 2100, loss[loss=0.6472, ctc_loss=0.7292, attn_decoder_loss=0.6266, over 4841.00 frames. ], tot_loss[loss=0.6806, ctc_loss=0.7814, attn_decoder_loss=0.6554, over 967171.22 frames. ], batch size: 21, lr: 4.42e-02,
2024-10-08 20:09:37,721 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=700.0, ans=0.4671875
2024-10-08 20:09:44,538 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=5.60 vs. limit=4.28
2024-10-08 20:09:54,251 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=7.06 vs. limit=4.281333333333333
2024-10-08 20:09:59,359 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.35 vs. limit=8.0275
2024-10-08 20:09:59,954 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=703.3333333333334, ans=0.29296666666666665
2024-10-08 20:10:02,345 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=24.15 vs. limit=8.0275
2024-10-08 20:10:07,983 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 20:10:31,207 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=13.59 vs. limit=7.76625
2024-10-08 20:10:42,180 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=29.40 vs. limit=8.035
2024-10-08 20:10:47,215 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=14.17 vs. limit=7.7675
2024-10-08 20:10:54,338 INFO [train.py:1152] Epoch 1, batch 2150, loss[loss=0.5988, ctc_loss=0.7012, attn_decoder_loss=0.5732, over 4862.00 frames. ], tot_loss[loss=0.6725, ctc_loss=0.7708, attn_decoder_loss=0.6479, over 968006.36 frames. ], batch size: 20, lr: 4.41e-02,
2024-10-08 20:10:55,190 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=58.53 vs. limit=8.0375
2024-10-08 20:11:04,690 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=12.31 vs. limit=5.358333333333333
2024-10-08 20:11:29,991 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=38.47 vs. limit=8.0425
2024-10-08 20:11:31,006 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=723.3333333333334, ans=0.8746833333333334
2024-10-08 20:11:35,185 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.19 vs. limit=5.180833333333333
2024-10-08 20:11:54,880 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.max_abs, batch_count=726.6666666666666, ans=5.454166666666667
2024-10-08 20:12:02,928 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=730.0, ans=0.46578125
2024-10-08 20:12:06,595 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.07 vs. limit=7.77375
2024-10-08 20:12:13,222 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.47 vs. limit=7.775
2024-10-08 20:12:14,071 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.825e+01 7.555e+01 8.968e+01 1.099e+02 1.761e+02, threshold=1.794e+02, percent-clipped=1.0
2024-10-08 20:12:14,130 INFO [train.py:1152] Epoch 1, batch 2200, loss[loss=0.6519, ctc_loss=0.7477, attn_decoder_loss=0.628, over 4741.00 frames. ], tot_loss[loss=0.6633, ctc_loss=0.7607, attn_decoder_loss=0.639, over 967775.49 frames. ], batch size: 26, lr: 4.41e-02,
2024-10-08 20:12:16,350 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=23.70 vs. limit=5.366666666666667
2024-10-08 20:12:17,884 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=5.35 vs. limit=7.775
2024-10-08 20:12:19,879 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.55 vs. limit=8.05
2024-10-08 20:12:30,710 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.53 vs. limit=5.368333333333333
2024-10-08 20:12:33,094 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=736.6666666666666, ans=0.8742166666666666
2024-10-08 20:12:36,800 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=31.70 vs. limit=8.0525
2024-10-08 20:12:37,856 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 20:12:38,387 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.30 vs. limit=7.77625
2024-10-08 20:12:38,612 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.70 vs. limit=7.77625
2024-10-08 20:12:55,464 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=740.0, ans=0.4653125
2024-10-08 20:13:00,321 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=743.3333333333334, ans=0.46515625
2024-10-08 20:13:01,914 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=743.3333333333334, ans=0.46515625
2024-10-08 20:13:05,127 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=743.3333333333334, ans=0.8739833333333333
2024-10-08 20:13:07,623 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.21 vs. limit=5.185833333333333
2024-10-08 20:13:10,108 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=743.3333333333334, ans=0.46515625
2024-10-08 20:13:28,000 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=95.02 vs. limit=8.06
2024-10-08 20:13:33,912 INFO [train.py:1152] Epoch 1, batch 2250, loss[loss=0.6791, ctc_loss=0.7604, attn_decoder_loss=0.6588, over 4870.00 frames. ], tot_loss[loss=0.6551, ctc_loss=0.7516, attn_decoder_loss=0.631, over 967682.50 frames. ], batch size: 22, lr: 4.40e-02,
2024-10-08 20:13:34,755 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=34.62 vs. limit=8.0625
2024-10-08 20:13:37,245 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=750.0, ans=0.46484375
2024-10-08 20:13:40,985 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=14.94 vs. limit=5.375
2024-10-08 20:13:41,216 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=14.78 vs. limit=7.78125
2024-10-08 20:13:44,515 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.54 vs. limit=5.1875
2024-10-08 20:13:53,350 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=753.3333333333334, ans=0.29246666666666665
2024-10-08 20:14:04,419 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=756.6666666666666, ans=0.047635416666666666
2024-10-08 20:14:12,542 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=756.6666666666666, ans=0.8735166666666667
2024-10-08 20:14:13,187 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.54 vs. limit=7.78375
2024-10-08 20:14:14,981 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.49 vs. limit=5.378333333333333
2024-10-08 20:14:17,986 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=6.86 vs. limit=4.302666666666667
2024-10-08 20:14:18,985 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 20:14:38,337 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=763.3333333333334, ans=0.46421875
2024-10-08 20:14:43,749 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.99 vs. limit=7.78625
2024-10-08 20:14:48,083 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=763.3333333333334, ans=0.5
2024-10-08 20:14:49,786 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=0.000e+00
2024-10-08 20:14:54,322 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.329e+01 7.703e+01 9.104e+01 1.028e+02 1.534e+02, threshold=1.821e+02, percent-clipped=0.0
2024-10-08 20:14:54,390 INFO [train.py:1152] Epoch 1, batch 2300, loss[loss=0.6249, ctc_loss=0.6863, attn_decoder_loss=0.6095, over 4883.00 frames. ], tot_loss[loss=0.645, ctc_loss=0.7392, attn_decoder_loss=0.6214, over 968174.42 frames. ], batch size: 19, lr: 4.40e-02,
2024-10-08 20:14:55,350 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.74 vs. limit=5.383333333333333
2024-10-08 20:15:02,600 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=766.6666666666666, ans=0.04760416666666667
2024-10-08 20:15:16,201 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.20 vs. limit=7.78875
2024-10-08 20:15:24,312 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.35 vs. limit=7.78875
2024-10-08 20:15:53,588 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.63 vs. limit=5.194166666666667
2024-10-08 20:15:58,986 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=780.0, ans=0.4634375
2024-10-08 20:15:59,065 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=780.0, ans=0.17075
2024-10-08 20:16:06,300 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.46 vs. limit=7.7925
2024-10-08 20:16:10,486 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=780.0, ans=0.17075
2024-10-08 20:16:15,222 INFO [train.py:1152] Epoch 1, batch 2350, loss[loss=0.6149, ctc_loss=0.7221, attn_decoder_loss=0.5881, over 4865.00 frames. ], tot_loss[loss=0.6362, ctc_loss=0.7306, attn_decoder_loss=0.6126, over 968126.48 frames. ], batch size: 23, lr: 4.40e-02,
2024-10-08 20:16:16,937 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=783.3333333333334, ans=0.21175000000000002
2024-10-08 20:16:31,611 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=786.6666666666666, ans=0.8724666666666667
2024-10-08 20:16:34,786 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=786.6666666666666, ans=0.21180000000000002
2024-10-08 20:16:47,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=790.0, ans=0.29209999999999997
2024-10-08 20:17:00,420 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=790.0, ans=0.46296875
2024-10-08 20:17:13,222 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=793.3333333333334, ans=0.4628125
2024-10-08 20:17:13,315 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=793.3333333333334, ans=0.29206666666666664
2024-10-08 20:17:16,408 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=793.3333333333334, ans=0.205375
2024-10-08 20:17:21,958 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=33.86 vs. limit=8.0975
2024-10-08 20:17:36,000 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.268e+01 7.789e+01 8.947e+01 1.052e+02 1.555e+02, threshold=1.789e+02, percent-clipped=0.0
2024-10-08 20:17:36,056 INFO [train.py:1152] Epoch 1, batch 2400, loss[loss=0.6705, ctc_loss=0.7489, attn_decoder_loss=0.6508, over 4749.00 frames. ], tot_loss[loss=0.6289, ctc_loss=0.7233, attn_decoder_loss=0.6053, over 967387.13 frames. ], batch size: 19, lr: 4.39e-02,
2024-10-08 20:17:36,586 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=15.94 vs. limit=5.4
2024-10-08 20:18:06,232 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.max_abs, batch_count=806.6666666666666, ans=5.504166666666666
2024-10-08 20:18:08,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=25.07 vs. limit=5.403333333333333
2024-10-08 20:18:15,229 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=20.96 vs. limit=8.105
2024-10-08 20:18:25,802 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys.whitening_limit, batch_count=810.0, ans=3.1215
2024-10-08 20:18:30,170 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=810.0, ans=0.2044375
2024-10-08 20:18:31,797 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer_ff2.min_abs, batch_count=810.0, ans=0.02025
2024-10-08 20:18:47,310 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.34 vs. limit=3.122
2024-10-08 20:18:49,557 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 20:18:55,729 INFO [train.py:1152] Epoch 1, batch 2450, loss[loss=0.586, ctc_loss=0.7064, attn_decoder_loss=0.5559, over 4865.00 frames. ], tot_loss[loss=0.6219, ctc_loss=0.7162, attn_decoder_loss=0.5984, over 966635.17 frames. ], batch size: 22, lr: 4.39e-02,
2024-10-08 20:19:03,994 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=59.30 vs. limit=8.1125
2024-10-08 20:19:05,371 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=34.60 vs. limit=8.1125
2024-10-08 20:19:07,443 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=816.6666666666666, ans=0.081625
2024-10-08 20:19:19,262 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.36 vs. limit=7.8075
2024-10-08 20:19:23,657 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.02 vs. limit=7.8075
2024-10-08 20:19:31,846 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.36 vs. limit=5.2058333333333335
2024-10-08 20:19:32,031 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=24.59 vs. limit=8.1175
2024-10-08 20:19:39,934 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.35 vs. limit=5.411666666666667
2024-10-08 20:19:54,957 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=826.6666666666666, ans=0.7582666666666666
2024-10-08 20:20:02,941 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=830.0, ans=0.39625
2024-10-08 20:20:06,550 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=6.62 vs. limit=7.81125
2024-10-08 20:20:15,444 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.387e+01 8.204e+01 9.217e+01 1.059e+02 1.671e+02, threshold=1.843e+02, percent-clipped=0.0
2024-10-08 20:20:15,509 INFO [train.py:1152] Epoch 1, batch 2500, loss[loss=0.628, ctc_loss=0.7317, attn_decoder_loss=0.6021, over 4740.00 frames. ], tot_loss[loss=0.6148, ctc_loss=0.7079, attn_decoder_loss=0.5915, over 966280.93 frames. ], batch size: 26, lr: 4.38e-02,
2024-10-08 20:20:30,065 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=836.6666666666666, ans=0.16862500000000002
2024-10-08 20:20:36,517 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=836.6666666666666, ans=0.2029375
2024-10-08 20:20:59,390 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.14 vs. limit=7.815
2024-10-08 20:21:01,184 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.00 vs. limit=5.21
2024-10-08 20:21:05,279 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=843.3333333333334, ans=0.081025
2024-10-08 20:21:07,529 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.17 vs. limit=7.81625
2024-10-08 20:21:15,694 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1.whitening_limit, batch_count=843.3333333333334, ans=5.210833333333333
2024-10-08 20:21:33,824 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=13.25 vs. limit=8.135
2024-10-08 20:21:35,859 INFO [train.py:1152] Epoch 1, batch 2550, loss[loss=0.5266, ctc_loss=0.5708, attn_decoder_loss=0.5155, over 4959.00 frames. ], tot_loss[loss=0.6077, ctc_loss=0.6993, attn_decoder_loss=0.5848, over 966882.73 frames. ], batch size: 19, lr: 4.38e-02,
2024-10-08 20:21:36,830 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.24 vs. limit=5.425
2024-10-08 20:21:37,647 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=850.0, ans=0.16812500000000002
2024-10-08 20:21:40,768 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=850.0, ans=0.46015625
2024-10-08 20:21:54,643 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=17.12 vs. limit=8.14
2024-10-08 20:21:57,406 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.56 vs. limit=7.82
2024-10-08 20:22:02,453 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=12.19 vs. limit=5.426666666666667
2024-10-08 20:22:04,652 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=853.3333333333334, ans=0.3933333333333333
2024-10-08 20:22:05,499 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.32 vs. limit=3.128
2024-10-08 20:22:06,291 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=856.6666666666666, ans=0.45984375
2024-10-08 20:22:07,975 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=856.6666666666666, ans=0.45984375
2024-10-08 20:22:11,791 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=70.22 vs. limit=8.1425
2024-10-08 20:22:12,724 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=856.6666666666666, ans=0.45984375
2024-10-08 20:22:15,845 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=856.6666666666666, ans=0.167875
2024-10-08 20:22:18,032 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.51 vs. limit=7.82125
2024-10-08 20:22:23,915 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=860.0, ans=0.201625
2024-10-08 20:22:25,478 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=860.0, ans=0.2914
2024-10-08 20:22:27,073 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=860.0, ans=0.0473125
2024-10-08 20:22:29,594 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.51 vs. limit=8.145
2024-10-08 20:22:42,026 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=52.42 vs. limit=8.1475
2024-10-08 20:22:45,623 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=7.50 vs. limit=7.82375
2024-10-08 20:22:49,511 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=863.3333333333334, ans=0.047302083333333335
2024-10-08 20:22:55,897 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.619e+01 8.326e+01 9.882e+01 1.183e+02 2.059e+02, threshold=1.976e+02, percent-clipped=1.0
2024-10-08 20:22:55,954 INFO [train.py:1152] Epoch 1, batch 2600, loss[loss=0.6018, ctc_loss=0.6768, attn_decoder_loss=0.5831, over 4859.00 frames. ], tot_loss[loss=0.6012, ctc_loss=0.6921, attn_decoder_loss=0.5785, over 966370.44 frames. ], batch size: 20, lr: 4.37e-02,
2024-10-08 20:22:59,582 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.50 vs. limit=4.346666666666667
2024-10-08 20:23:09,390 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=25.01 vs. limit=8.15
2024-10-08 20:23:10,784 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.28 vs. limit=5.2175
2024-10-08 20:23:19,756 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=870.0, ans=0.45921875
2024-10-08 20:23:21,896 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.35 vs. limit=7.82625
2024-10-08 20:23:22,994 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 20:23:24,459 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=870.0, ans=0.7587
2024-10-08 20:23:58,586 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=38.11 vs. limit=8.16
2024-10-08 20:24:08,104 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=49.14 vs. limit=8.16
2024-10-08 20:24:15,405 INFO [train.py:1152] Epoch 1, batch 2650, loss[loss=0.5672, ctc_loss=0.6854, attn_decoder_loss=0.5377, over 4827.00 frames. ], tot_loss[loss=0.5982, ctc_loss=0.687, attn_decoder_loss=0.576, over 966102.73 frames. ], batch size: 38, lr: 4.37e-02,
2024-10-08 20:24:17,717 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=32.67 vs. limit=8.1625
2024-10-08 20:24:21,093 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=13.97 vs. limit=7.83125
2024-10-08 20:24:22,025 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer_ff3.min_abs, batch_count=883.3333333333334, ans=0.044166666666666674
2024-10-08 20:24:25,232 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=883.3333333333334, ans=0.38958333333333334
2024-10-08 20:24:26,077 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.76 vs. limit=5.441666666666666
2024-10-08 20:24:28,496 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=883.3333333333334, ans=0.45859375
2024-10-08 20:24:30,997 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.16 vs. limit=8.165
2024-10-08 20:24:33,203 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=886.6666666666666, ans=0.38916666666666666
2024-10-08 20:24:40,246 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.30 vs. limit=7.8325
2024-10-08 20:24:45,806 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=7.07 vs. limit=7.8325
2024-10-08 20:24:49,053 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.07 vs. limit=3.1335
2024-10-08 20:24:59,171 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=890.0, ans=0.45828125
2024-10-08 20:25:00,714 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=890.0, ans=0.7589
2024-10-08 20:25:08,171 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.22 vs. limit=8.17
2024-10-08 20:25:08,839 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=893.3333333333334, ans=0.04720833333333334
2024-10-08 20:25:19,091 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=38.52 vs. limit=8.1725
2024-10-08 20:25:20,833 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=17.99 vs. limit=8.1725
2024-10-08 20:25:22,520 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.10 vs. limit=5.448333333333333
2024-10-08 20:25:27,804 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=17.85 vs. limit=8.1725
2024-10-08 20:25:36,117 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.696e+01 8.405e+01 9.485e+01 1.184e+02 2.281e+02, threshold=1.897e+02, percent-clipped=2.0
2024-10-08 20:25:36,181 INFO [train.py:1152] Epoch 1, batch 2700, loss[loss=0.5776, ctc_loss=0.6823, attn_decoder_loss=0.5515, over 4854.00 frames. ], tot_loss[loss=0.5904, ctc_loss=0.6795, attn_decoder_loss=0.5681, over 966383.78 frames. ], batch size: 28, lr: 4.36e-02,
2024-10-08 20:25:41,764 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=25.33 vs. limit=8.175
2024-10-08 20:25:43,341 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=4.56 vs. limit=7.8375
2024-10-08 20:25:43,376 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=21.76 vs. limit=7.8375
2024-10-08 20:25:47,585 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=900.0, ans=0.16625
2024-10-08 20:25:54,158 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=903.3333333333334, ans=0.166125
2024-10-08 20:26:34,952 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=910.0, ans=8.1825
2024-10-08 20:26:39,158 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=913.3333333333334, ans=0.38583333333333336
2024-10-08 20:26:56,819 INFO [train.py:1152] Epoch 1, batch 2750, loss[loss=0.5584, ctc_loss=0.6153, attn_decoder_loss=0.5442, over 4799.00 frames. ], tot_loss[loss=0.5858, ctc_loss=0.6718, attn_decoder_loss=0.5644, over 967020.74 frames. ], batch size: 19, lr: 4.36e-02,
2024-10-08 20:27:07,789 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.17 vs. limit=8.1875
2024-10-08 20:27:16,246 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=920.0, ans=0.1655
2024-10-08 20:27:20,405 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.51 vs. limit=8.19
2024-10-08 20:27:26,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.25 vs. limit=5.23
2024-10-08 20:27:26,595 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.74 vs. limit=7.845
2024-10-08 20:27:44,502 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten.whitening_limit, batch_count=926.6666666666666, ans=7.8475
2024-10-08 20:27:54,089 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.37 vs. limit=5.463333333333333
2024-10-08 20:27:58,189 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=926.6666666666666, ans=0.07915
2024-10-08 20:28:00,282 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=37.77 vs. limit=8.1975
2024-10-08 20:28:00,653 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.52 vs. limit=5.465
2024-10-08 20:28:05,289 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=22.34 vs. limit=8.1975
2024-10-08 20:28:14,082 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=930.0, ans=0.079075
2024-10-08 20:28:16,920 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.17 vs. limit=3.14
2024-10-08 20:28:17,348 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.706e+01 8.500e+01 9.765e+01 1.203e+02 2.106e+02, threshold=1.953e+02, percent-clipped=1.0
2024-10-08 20:28:17,407 INFO [train.py:1152] Epoch 1, batch 2800, loss[loss=0.5647, ctc_loss=0.702, attn_decoder_loss=0.5304, over 4771.00 frames. ], tot_loss[loss=0.5811, ctc_loss=0.6665, attn_decoder_loss=0.5598, over 967098.21 frames. ], batch size: 53, lr: 4.36e-02,
2024-10-08 20:28:17,585 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=933.3333333333334, ans=0.45625
2024-10-08 20:28:25,952 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.70 vs. limit=7.85
2024-10-08 20:28:37,271 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=936.6666666666666, ans=0.45609375
2024-10-08 20:28:39,771 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=936.6666666666666, ans=0.164875
2024-10-08 20:28:41,807 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.16 vs. limit=3.1405
2024-10-08 20:28:41,962 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=12.97 vs. limit=5.468333333333334
2024-10-08 20:28:52,904 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.14 vs. limit=5.235
2024-10-08 20:28:55,885 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=28.41 vs. limit=8.205
2024-10-08 20:28:56,146 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.91 vs. limit=7.8525
2024-10-08 20:28:56,942 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=940.0, ans=0.07885
2024-10-08 20:29:01,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=940.0, ans=0.07885
2024-10-08 20:29:05,512 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=6.23 vs. limit=4.3773333333333335
2024-10-08 20:29:22,407 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=23.23 vs. limit=8.21
2024-10-08 20:29:23,515 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=946.6666666666666, ans=0.5
2024-10-08 20:29:23,548 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=946.6666666666666, ans=0.0787
2024-10-08 20:29:25,865 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.33 vs. limit=7.855
2024-10-08 20:29:26,868 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=946.6666666666666, ans=0.09408333333333334
2024-10-08 20:29:32,192 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=39.66 vs. limit=8.21
2024-10-08 20:29:36,114 INFO [train.py:1152] Epoch 1, batch 2850, loss[loss=0.567, ctc_loss=0.6093, attn_decoder_loss=0.5565, over 4932.00 frames. ], tot_loss[loss=0.5772, ctc_loss=0.6631, attn_decoder_loss=0.5558, over 966900.70 frames. ], batch size: 20, lr: 4.35e-02,
2024-10-08 20:29:37,827 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=950.0, ans=0.047031250000000004
2024-10-08 20:29:37,830 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=950.0, ans=0.45546875
2024-10-08 20:29:39,439 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=950.0, ans=0.45546875
2024-10-08 20:29:41,000 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=950.0, ans=0.078625
2024-10-08 20:29:52,734 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=31.91 vs. limit=8.215
2024-10-08 20:29:56,164 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.51 vs. limit=7.8575
2024-10-08 20:29:59,777 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.79 vs. limit=8.215
2024-10-08 20:30:00,377 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=953.3333333333334, ans=0.07855000000000001
2024-10-08 20:30:08,333 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=956.6666666666666, ans=0.8665166666666667
2024-10-08 20:30:15,242 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=41.79 vs. limit=8.2175
2024-10-08 20:30:22,606 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=960.0, ans=0.455
2024-10-08 20:30:28,147 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten.whitening_limit, batch_count=960.0, ans=7.86
2024-10-08 20:30:32,746 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=40.77 vs. limit=8.22
2024-10-08 20:30:39,543 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.16 vs. limit=5.4816666666666665
2024-10-08 20:30:52,305 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.42 vs. limit=8.2225
2024-10-08 20:30:55,814 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.756e+01 8.497e+01 1.031e+02 1.186e+02 2.156e+02, threshold=2.062e+02, percent-clipped=2.0
2024-10-08 20:30:55,879 INFO [train.py:1152] Epoch 1, batch 2900, loss[loss=0.4902, ctc_loss=0.5313, attn_decoder_loss=0.4799, over 4746.00 frames. ], tot_loss[loss=0.5705, ctc_loss=0.6553, attn_decoder_loss=0.5493, over 965979.15 frames. ], batch size: 20, lr: 4.35e-02,
2024-10-08 20:31:03,248 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=3.96 vs. limit=4.386666666666667
2024-10-08 20:31:05,544 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=966.6666666666666, ans=0.4546875
2024-10-08 20:31:18,911 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.73 vs. limit=5.2425
2024-10-08 20:31:20,348 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=29.11 vs. limit=8.2275
2024-10-08 20:31:26,366 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=973.3333333333334, ans=0.8659333333333333
2024-10-08 20:31:28,768 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=18.40 vs. limit=8.23
2024-10-08 20:31:29,378 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=973.3333333333334, ans=0.04695833333333334
2024-10-08 20:31:32,694 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=973.3333333333334, ans=0.8659333333333333
2024-10-08 20:31:34,973 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.42 vs. limit=7.865
2024-10-08 20:31:36,524 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.38 vs. limit=7.865
2024-10-08 20:31:50,731 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=12.15 vs. limit=5.488333333333333
2024-10-08 20:31:56,580 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=976.6666666666666, ans=0.09389583333333335
2024-10-08 20:32:03,709 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=17.91 vs. limit=8.235
2024-10-08 20:32:06,115 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=980.0, ans=0.7598
2024-10-08 20:32:15,569 INFO [train.py:1152] Epoch 1, batch 2950, loss[loss=0.5485, ctc_loss=0.5964, attn_decoder_loss=0.5365, over 4798.00 frames. ], tot_loss[loss=0.5658, ctc_loss=0.6491, attn_decoder_loss=0.5449, over 966555.58 frames. ], batch size: 19, lr: 4.34e-02,
2024-10-08 20:32:16,419 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=14.85 vs. limit=7.86875
2024-10-08 20:32:25,387 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=983.3333333333334, ans=0.45390625
2024-10-08 20:32:29,421 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.24 vs. limit=8.2375
2024-10-08 20:32:40,202 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=52.69 vs. limit=8.24
2024-10-08 20:32:45,516 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.25 vs. limit=7.87
2024-10-08 20:32:46,268 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=990.0, ans=0.45359375
2024-10-08 20:32:49,359 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=990.0, ans=0.45359375
2024-10-08 20:32:52,558 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=990.0, ans=0.046906250000000003
2024-10-08 20:32:56,650 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.59 vs. limit=8.2425
2024-10-08 20:32:58,104 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=42.65 vs. limit=8.2425
2024-10-08 20:33:00,737 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=990.0, ans=0.45359375
2024-10-08 20:33:04,001 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=993.3333333333334, ans=0.24006666666666668
2024-10-08 20:33:34,714 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=1000.0, ans=0.865
2024-10-08 20:33:35,936 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.131e+01 8.672e+01 9.991e+01 1.158e+02 1.664e+02, threshold=1.998e+02, percent-clipped=0.0
2024-10-08 20:33:35,992 INFO [train.py:1152] Epoch 1, batch 3000, loss[loss=0.5621, ctc_loss=0.6075, attn_decoder_loss=0.5507, over 4849.00 frames. ], tot_loss[loss=0.5606, ctc_loss=0.6426, attn_decoder_loss=0.5401, over 967211.69 frames. ], batch size: 21, lr: 4.34e-02,
2024-10-08 20:33:35,993 INFO [train.py:1175] Computing validation loss
2024-10-08 20:33:44,941 INFO [train.py:1184] Epoch 1, validation: loss=0.4534, ctc_loss=0.4595, attn_decoder_loss=0.4519, over 90464.00 frames.
2024-10-08 20:33:44,941 INFO [train.py:1185] Maximum memory allocated so far is 6613MB
2024-10-08 20:33:46,737 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=1000.0, ans=0.1625
2024-10-08 20:33:47,521 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.66 vs. limit=5.25
2024-10-08 20:33:58,562 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.36 vs. limit=8.25
2024-10-08 20:34:07,726 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=21.45 vs. limit=8.2525
2024-10-08 20:34:17,319 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.89 vs. limit=8.255
2024-10-08 20:34:21,956 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.67 vs. limit=7.8775
2024-10-08 20:34:24,439 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 20:34:29,730 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=6.67 vs. limit=7.8775
2024-10-08 20:34:33,285 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.39 vs. limit=7.87875
2024-10-08 20:34:45,724 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.56 vs. limit=5.2525
2024-10-08 20:34:55,022 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=27.52 vs. limit=8.26
2024-10-08 20:34:57,965 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.39 vs. limit=5.253333333333333
2024-10-08 20:34:59,120 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=1013.3333333333334, ans=0.2152
2024-10-08 20:35:00,622 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=1013.3333333333334, ans=0.5
2024-10-08 20:35:02,109 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=1016.6666666666666, ans=0.09364583333333334
2024-10-08 20:35:03,521 INFO [train.py:1152] Epoch 1, batch 3050, loss[loss=0.5398, ctc_loss=0.6187, attn_decoder_loss=0.5201, over 4750.00 frames. ], tot_loss[loss=0.5571, ctc_loss=0.6382, attn_decoder_loss=0.5368, over 966736.06 frames. ], batch size: 19, lr: 4.33e-02,
2024-10-08 20:35:04,657 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.47 vs. limit=5.254166666666666
2024-10-08 20:35:05,775 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.07 vs. limit=7.88125
2024-10-08 20:35:16,220 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=1016.6666666666666, ans=0.161875
2024-10-08 20:35:17,761 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=1020.0, ans=0.4521875
2024-10-08 20:35:18,317 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=72.62 vs. limit=8.265
2024-10-08 20:35:18,378 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.34 vs. limit=5.255
2024-10-08 20:35:27,436 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 20:35:33,643 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=1023.3333333333334, ans=0.45203125
2024-10-08 20:35:34,261 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.97 vs. limit=4.4093333333333335
2024-10-08 20:35:34,366 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.67 vs. limit=7.88375
2024-10-08 20:35:35,888 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.99 vs. limit=7.88375
2024-10-08 20:35:36,346 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.08 vs. limit=5.511666666666667
2024-10-08 20:35:42,268 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=6.41 vs. limit=7.88375
2024-10-08 20:35:51,549 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=18.09 vs. limit=7.885
2024-10-08 20:35:55,081 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.45 vs. limit=5.256666666666667
2024-10-08 20:36:00,523 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=1026.6666666666667, ans=0.451875
2024-10-08 20:36:08,555 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=1030.0, ans=0.45171875
2024-10-08 20:36:14,916 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=1030.0, ans=0.076825
2024-10-08 20:36:21,215 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=1033.3333333333333, ans=0.37083333333333335
2024-10-08 20:36:22,450 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.189e+01 8.678e+01 9.666e+01 1.170e+02 1.975e+02, threshold=1.933e+02, percent-clipped=0.0
2024-10-08 20:36:22,514 INFO [train.py:1152] Epoch 1, batch 3100, loss[loss=0.5221, ctc_loss=0.6146, attn_decoder_loss=0.499, over 4807.00 frames. ], tot_loss[loss=0.5513, ctc_loss=0.6301, attn_decoder_loss=0.5316, over 966426.09 frames. ], batch size: 38, lr: 4.33e-02,
2024-10-08 20:36:33,201 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=4.55 vs. limit=7.8875
2024-10-08 20:36:44,470 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.44 vs. limit=5.259166666666666
2024-10-08 20:36:46,605 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1036.6666666666667, ans=0.45140625
2024-10-08 20:36:59,331 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=1040.0, ans=0.45125
2024-10-08 20:37:13,584 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=1043.3333333333333, ans=0.45109375
2024-10-08 20:37:15,812 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=79.30 vs. limit=8.2825
2024-10-08 20:37:20,519 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.91 vs. limit=5.260833333333333
2024-10-08 20:37:37,747 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=28.46 vs. limit=8.285
2024-10-08 20:37:42,492 INFO [train.py:1152] Epoch 1, batch 3150, loss[loss=0.559, ctc_loss=0.6701, attn_decoder_loss=0.5312, over 4800.00 frames. ], tot_loss[loss=0.545, ctc_loss=0.6227, attn_decoder_loss=0.5256, over 966760.36 frames. ], batch size: 40, lr: 4.32e-02,
2024-10-08 20:37:44,696 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=10.04 vs. limit=7.89375
2024-10-08 20:37:45,837 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=1050.0, ans=0.8632500000000001
2024-10-08 20:37:48,021 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.43 vs. limit=4.42
2024-10-08 20:38:00,484 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.07 vs. limit=7.895
2024-10-08 20:38:12,521 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=1056.6666666666667, ans=0.160375
2024-10-08 20:38:25,151 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=1056.6666666666667, ans=0.8630166666666667
2024-10-08 20:38:41,420 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=5.41 vs. limit=7.8975
2024-10-08 20:38:51,947 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=1063.3333333333333, ans=0.45015625
2024-10-08 20:38:54,361 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.14 vs. limit=7.89875
2024-10-08 20:39:01,666 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.664e+01 8.674e+01 1.002e+02 1.167e+02 2.099e+02, threshold=2.004e+02, percent-clipped=1.0
2024-10-08 20:39:01,723 INFO [train.py:1152] Epoch 1, batch 3200, loss[loss=0.5666, ctc_loss=0.6249, attn_decoder_loss=0.5521, over 4743.00 frames. ], tot_loss[loss=0.5408, ctc_loss=0.6169, attn_decoder_loss=0.5217, over 967211.05 frames. ], batch size: 20, lr: 4.32e-02,
2024-10-08 20:39:19,056 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer_ff2.min_abs, batch_count=1070.0, ans=0.02675
2024-10-08 20:39:21,454 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.69 vs. limit=5.2675
2024-10-08 20:39:49,169 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1076.6666666666667, ans=0.28923333333333334
2024-10-08 20:39:50,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=1076.6666666666667, ans=0.8623166666666667
2024-10-08 20:39:53,905 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=1076.6666666666667, ans=0.44953125
2024-10-08 20:39:57,951 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.35 vs. limit=5.269166666666667
2024-10-08 20:40:04,623 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.14 vs. limit=5.54
2024-10-08 20:40:15,172 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.31 vs. limit=5.54
2024-10-08 20:40:20,832 INFO [train.py:1152] Epoch 1, batch 3250, loss[loss=0.5262, ctc_loss=0.607, attn_decoder_loss=0.506, over 4855.00 frames. ], tot_loss[loss=0.5369, ctc_loss=0.612, attn_decoder_loss=0.5181, over 967283.25 frames. ], batch size: 24, lr: 4.31e-02,
2024-10-08 20:40:30,601 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=1083.3333333333333, ans=0.8620833333333333
2024-10-08 20:40:51,050 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=1090.0, ans=0.075475
2024-10-08 20:40:51,119 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=1090.0, ans=0.36375
2024-10-08 20:40:51,122 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=1090.0, ans=0.04659375
2024-10-08 20:41:09,344 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=7.93 vs. limit=7.91
2024-10-08 20:41:12,616 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.59 vs. limit=5.273333333333333
2024-10-08 20:41:28,262 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.84 vs. limit=5.274166666666667
2024-10-08 20:41:40,195 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.433e+01 8.962e+01 1.020e+02 1.195e+02 1.733e+02, threshold=2.040e+02, percent-clipped=0.0
2024-10-08 20:41:40,259 INFO [train.py:1152] Epoch 1, batch 3300, loss[loss=0.5523, ctc_loss=0.6395, attn_decoder_loss=0.5306, over 4840.00 frames. ], tot_loss[loss=0.5326, ctc_loss=0.6057, attn_decoder_loss=0.5143, over 967719.24 frames. ], batch size: 43, lr: 4.31e-02,
2024-10-08 20:41:48,136 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=1100.0, ans=0.07525
2024-10-08 20:41:50,543 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.30 vs. limit=3.165
2024-10-08 20:41:54,636 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=1103.3333333333333, ans=0.1879375
2024-10-08 20:41:59,414 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=1103.3333333333333, ans=0.1879375
2024-10-08 20:42:07,932 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.25 vs. limit=4.441333333333334
2024-10-08 20:42:10,023 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=7.17 vs. limit=7.91375
2024-10-08 20:42:10,539 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=1106.6666666666667, ans=0.1585
2024-10-08 20:42:13,656 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=1106.6666666666667, ans=0.0751
2024-10-08 20:42:14,424 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.63 vs. limit=7.915
2024-10-08 20:42:15,182 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=1106.6666666666667, ans=0.0751
2024-10-08 20:42:24,218 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=7.06 vs. limit=7.915
2024-10-08 20:42:29,558 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=1110.0, ans=0.1875625
2024-10-08 20:42:31,720 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.72 vs. limit=7.91625
2024-10-08 20:42:35,730 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=1110.0, ans=0.04653125
2024-10-08 20:42:39,592 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.89 vs. limit=7.91625
2024-10-08 20:42:46,103 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.31 vs. limit=7.9175
2024-10-08 20:42:53,902 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.82 vs. limit=4.445333333333333
2024-10-08 20:42:58,798 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=20.06 vs. limit=8.3375
2024-10-08 20:42:59,471 INFO [train.py:1152] Epoch 1, batch 3350, loss[loss=0.5707, ctc_loss=0.6719, attn_decoder_loss=0.5454, over 4802.00 frames. ], tot_loss[loss=0.5305, ctc_loss=0.6051, attn_decoder_loss=0.5119, over 966981.25 frames. ], batch size: 40, lr: 4.30e-02,
2024-10-08 20:43:02,702 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=1116.6666666666667, ans=0.36041666666666666
2024-10-08 20:43:03,649 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.13 vs. limit=7.91875
2024-10-08 20:43:16,708 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer_na.min_abs, batch_count=1120.0, ans=0.008480000000000001
2024-10-08 20:43:29,333 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=1123.3333333333333, ans=0.44734375
2024-10-08 20:43:46,081 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=63.29 vs. limit=8.345
2024-10-08 20:44:00,418 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=26.80 vs. limit=8.345
2024-10-08 20:44:19,145 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.168e+01 9.213e+01 1.042e+02 1.183e+02 1.585e+02, threshold=2.084e+02, percent-clipped=0.0
2024-10-08 20:44:19,202 INFO [train.py:1152] Epoch 1, batch 3400, loss[loss=0.4857, ctc_loss=0.5313, attn_decoder_loss=0.4743, over 4959.00 frames. ], tot_loss[loss=0.5271, ctc_loss=0.6007, attn_decoder_loss=0.5088, over 966766.51 frames. ], batch size: 19, lr: 4.29e-02,
2024-10-08 20:44:19,978 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=34.10 vs. limit=8.35
2024-10-08 20:44:27,701 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.36 vs. limit=5.283333333333333
2024-10-08 20:44:30,309 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1133.3333333333333, ans=0.2886666666666667
2024-10-08 20:44:36,251 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.03 vs. limit=8.3525
2024-10-08 20:44:41,046 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=4.34 vs. limit=4.227333333333333
2024-10-08 20:44:44,717 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=1136.6666666666667, ans=0.04644791666666667
2024-10-08 20:44:44,747 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.min_abs, batch_count=1136.6666666666667, ans=0.21705000000000002
2024-10-08 20:45:07,176 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1143.3333333333333, ans=0.28856666666666664
2024-10-08 20:45:23,262 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=1146.6666666666667, ans=0.09283333333333334
2024-10-08 20:45:28,718 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.71 vs. limit=5.573333333333333
2024-10-08 20:45:29,747 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=1146.6666666666667, ans=0.5
2024-10-08 20:45:39,221 INFO [train.py:1152] Epoch 1, batch 3450, loss[loss=0.5787, ctc_loss=0.6557, attn_decoder_loss=0.5595, over 4854.00 frames. ], tot_loss[loss=0.5253, ctc_loss=0.5987, attn_decoder_loss=0.5069, over 967096.41 frames. ], batch size: 43, lr: 4.29e-02,
2024-10-08 20:45:39,963 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=44.31 vs. limit=8.3625
2024-10-08 20:45:43,983 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=1150.0, ans=0.44609375
2024-10-08 20:45:49,654 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.59 vs. limit=5.575
2024-10-08 20:45:52,054 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=1150.0, ans=0.44609375
2024-10-08 20:45:54,776 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.82 vs. limit=5.288333333333333
2024-10-08 20:45:56,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.75 vs. limit=5.288333333333333
2024-10-08 20:45:56,908 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1153.3333333333333, ans=0.28846666666666665
2024-10-08 20:46:09,130 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.59 vs. limit=8.365
2024-10-08 20:46:13,483 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=15.09 vs. limit=5.578333333333333
2024-10-08 20:46:16,592 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.76 vs. limit=5.289166666666667
2024-10-08 20:46:17,574 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=1156.6666666666667, ans=0.44578125
2024-10-08 20:46:19,104 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=1156.6666666666667, ans=0.2884333333333333
2024-10-08 20:46:25,614 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=1160.0, ans=0.445625
2024-10-08 20:46:27,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=1160.0, ans=0.09275
2024-10-08 20:46:28,798 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=1160.0, ans=0.445625
2024-10-08 20:46:53,652 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.85 vs. limit=8.3725
2024-10-08 20:46:53,838 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.67 vs. limit=8.3725
2024-10-08 20:46:59,069 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.654e+01 8.504e+01 9.861e+01 1.190e+02 1.916e+02, threshold=1.972e+02, percent-clipped=0.0
2024-10-08 20:46:59,134 INFO [train.py:1152] Epoch 1, batch 3500, loss[loss=0.477, ctc_loss=0.5253, attn_decoder_loss=0.465, over 4883.00 frames. ], tot_loss[loss=0.5199, ctc_loss=0.5917, attn_decoder_loss=0.5019, over 967403.75 frames. ], batch size: 19, lr: 4.28e-02,
2024-10-08 20:47:00,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=1166.6666666666667, ans=0.4453125
2024-10-08 20:47:06,653 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.10 vs. limit=7.9375
2024-10-08 20:47:40,239 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.75 vs. limit=4.469333333333333
2024-10-08 20:47:46,096 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=1176.6666666666667, ans=0.7617666666666667
2024-10-08 20:47:50,062 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.24 vs. limit=7.94125
2024-10-08 20:48:01,879 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=1180.0, ans=0.09262500000000001
2024-10-08 20:48:08,187 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=1180.0, ans=0.4446875
2024-10-08 20:48:08,221 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=1180.0, ans=0.07345
2024-10-08 20:48:13,786 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.32 vs. limit=3.177
2024-10-08 20:48:16,139 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=1180.0, ans=0.15575
2024-10-08 20:48:19,210 INFO [train.py:1152] Epoch 1, batch 3550, loss[loss=0.4677, ctc_loss=0.5399, attn_decoder_loss=0.4496, over 4792.00 frames. ], tot_loss[loss=0.516, ctc_loss=0.5867, attn_decoder_loss=0.4984, over 967338.54 frames. ], batch size: 29, lr: 4.28e-02,
2024-10-08 20:48:22,643 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=1183.3333333333333, ans=0.44453125
2024-10-08 20:48:23,293 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.28 vs. limit=5.295833333333333
2024-10-08 20:48:25,736 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=1183.3333333333333, ans=0.44453125
2024-10-08 20:48:25,845 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=1183.3333333333333, ans=0.07337500000000001
2024-10-08 20:48:27,374 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=1183.3333333333333, ans=0.44453125
2024-10-08 20:48:29,104 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=1183.3333333333333, ans=0.046302083333333334
2024-10-08 20:48:42,509 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=7.58 vs. limit=7.945
2024-10-08 20:48:42,663 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.22 vs. limit=3.178
2024-10-08 20:48:44,230 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.42 vs. limit=4.474666666666667
2024-10-08 20:48:51,442 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=1190.0, ans=0.21785000000000002
2024-10-08 20:48:51,992 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.60 vs. limit=7.94625
2024-10-08 20:48:53,267 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=1190.0, ans=0.155375
2024-10-08 20:48:54,763 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=1190.0, ans=0.155375
2024-10-08 20:49:07,851 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=1193.3333333333333, ans=0.2179
2024-10-08 20:49:12,571 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=1193.3333333333333, ans=0.8582333333333334
2024-10-08 20:49:12,586 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1193.3333333333333, ans=0.28806666666666664
2024-10-08 20:49:14,712 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=13.21 vs. limit=5.596666666666667
2024-10-08 20:49:35,295 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=1196.6666666666667, ans=5.747916666666667
2024-10-08 20:49:40,169 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.824e+01 8.898e+01 1.024e+02 1.260e+02 2.000e+02, threshold=2.049e+02, percent-clipped=2.0
2024-10-08 20:49:40,226 INFO [train.py:1152] Epoch 1, batch 3600, loss[loss=0.4599, ctc_loss=0.4917, attn_decoder_loss=0.452, over 4946.00 frames. ], tot_loss[loss=0.5145, ctc_loss=0.5847, attn_decoder_loss=0.497, over 967405.82 frames. ], batch size: 20, lr: 4.27e-02,
2024-10-08 20:49:42,531 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.13 vs. limit=7.95
2024-10-08 20:49:43,586 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1200.0, ans=0.288
2024-10-08 20:49:46,729 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=1200.0, ans=0.44375
2024-10-08 20:49:52,391 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.36 vs. limit=3.18
2024-10-08 20:49:52,487 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=33.30 vs. limit=8.4
2024-10-08 20:49:54,926 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=1203.3333333333333, ans=0.1823125
2024-10-08 20:49:58,923 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.39 vs. limit=8.4025
2024-10-08 20:50:03,098 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=1203.3333333333333, ans=0.154875
2024-10-08 20:50:16,059 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1206.6666666666667, ans=0.2879333333333333
2024-10-08 20:50:22,440 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1206.6666666666667, ans=0.2879333333333333
2024-10-08 20:50:24,148 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1206.6666666666667, ans=0.2879333333333333
2024-10-08 20:50:32,365 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=1210.0, ans=5.75625
2024-10-08 20:50:37,149 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=1210.0, ans=0.44328125
2024-10-08 20:50:58,028 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=1213.3333333333333, ans=0.443125
2024-10-08 20:51:01,146 INFO [train.py:1152] Epoch 1, batch 3650, loss[loss=0.4708, ctc_loss=0.5642, attn_decoder_loss=0.4475, over 4842.00 frames. ], tot_loss[loss=0.5115, ctc_loss=0.581, attn_decoder_loss=0.4941, over 967903.82 frames. ], batch size: 31, lr: 4.27e-02,
2024-10-08 20:51:06,959 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.82 vs. limit=7.95625
2024-10-08 20:51:29,643 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=50.30 vs. limit=8.415
2024-10-08 20:51:33,855 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=1223.3333333333333, ans=0.44265625
2024-10-08 20:51:43,626 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=1223.3333333333333, ans=0.44265625
2024-10-08 20:51:44,035 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=18.53 vs. limit=8.4175
2024-10-08 20:51:45,853 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.35 vs. limit=3.1835
2024-10-08 20:51:52,352 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=26.69 vs. limit=8.42
2024-10-08 20:52:01,392 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=1226.6666666666667, ans=0.2184
2024-10-08 20:52:01,393 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=1226.6666666666667, ans=0.4425
2024-10-08 20:52:04,635 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=1230.0, ans=0.44234375
2024-10-08 20:52:05,230 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.91 vs. limit=5.3075
2024-10-08 20:52:06,856 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=31.36 vs. limit=8.4225
2024-10-08 20:52:07,861 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1230.0, ans=0.2877
2024-10-08 20:52:08,591 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.02 vs. limit=7.96125
2024-10-08 20:52:10,341 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=27.74 vs. limit=8.4225
2024-10-08 20:52:22,223 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.618e+01 8.463e+01 9.890e+01 1.137e+02 1.747e+02, threshold=1.978e+02, percent-clipped=0.0
2024-10-08 20:52:22,290 INFO [train.py:1152] Epoch 1, batch 3700, loss[loss=0.548, ctc_loss=0.6166, attn_decoder_loss=0.5309, over 4827.00 frames. ], tot_loss[loss=0.5085, ctc_loss=0.5773, attn_decoder_loss=0.4913, over 967394.99 frames. ], batch size: 24, lr: 4.26e-02,
2024-10-08 20:52:30,465 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=1233.3333333333333, ans=0.4421875
2024-10-08 20:52:35,231 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=1233.3333333333333, ans=0.4421875
2024-10-08 20:52:36,885 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=1236.6666666666667, ans=0.44203125
2024-10-08 20:52:37,559 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=22.71 vs. limit=8.4275
2024-10-08 20:52:38,340 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=1236.6666666666667, ans=0.44203125
2024-10-08 20:52:43,926 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.50 vs. limit=4.494666666666666
2024-10-08 20:52:44,913 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=1236.6666666666667, ans=0.8567166666666667
2024-10-08 20:52:45,560 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.51 vs. limit=7.96375
2024-10-08 20:52:46,523 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=1236.6666666666667, ans=0.1804375
2024-10-08 20:52:48,188 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 20:52:53,690 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.40 vs. limit=7.965
2024-10-08 20:52:59,424 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1240.0, ans=0.28759999999999997
2024-10-08 20:53:00,491 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=16.41 vs. limit=8.43
2024-10-08 20:53:11,398 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.29 vs. limit=7.96625
2024-10-08 20:53:18,619 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=1243.3333333333333, ans=0.153375
2024-10-08 20:53:42,451 INFO [train.py:1152] Epoch 1, batch 3750, loss[loss=0.4449, ctc_loss=0.4755, attn_decoder_loss=0.4373, over 4959.00 frames. ], tot_loss[loss=0.5041, ctc_loss=0.5715, attn_decoder_loss=0.4872, over 967724.70 frames. ], batch size: 19, lr: 4.26e-02,
2024-10-08 20:53:54,146 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=1250.0, ans=0.44140625
2024-10-08 20:54:03,058 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.38 vs. limit=5.3133333333333335
2024-10-08 20:54:09,027 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=36.45 vs. limit=8.44
2024-10-08 20:54:11,738 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=1253.3333333333333, ans=0.8561333333333334
2024-10-08 20:54:19,034 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.38 vs. limit=3.1885
2024-10-08 20:54:37,619 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=1260.0, ans=0.4409375
2024-10-08 20:54:41,597 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.94 vs. limit=5.63
2024-10-08 20:54:48,796 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 20:54:50,248 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=1263.3333333333333, ans=0.44078125
2024-10-08 20:54:56,592 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=1263.3333333333333, ans=0.34208333333333335
2024-10-08 20:55:03,184 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.985e+01 9.451e+01 1.077e+02 1.190e+02 3.164e+02, threshold=2.153e+02, percent-clipped=4.0
2024-10-08 20:55:03,243 INFO [train.py:1152] Epoch 1, batch 3800, loss[loss=0.4893, ctc_loss=0.5552, attn_decoder_loss=0.4728, over 4760.00 frames. ], tot_loss[loss=0.5028, ctc_loss=0.5698, attn_decoder_loss=0.486, over 967570.03 frames. ], batch size: 26, lr: 4.25e-02,
2024-10-08 20:55:07,548 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=4.73 vs. limit=4.253333333333333
2024-10-08 20:55:14,014 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=4.18 vs. limit=5.0
2024-10-08 20:55:17,667 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=1270.0, ans=0.44046875
2024-10-08 20:55:21,857 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.23 vs. limit=5.635
2024-10-08 20:55:22,601 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=1270.0, ans=0.152375
2024-10-08 20:55:24,100 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=1270.0, ans=0.071425
2024-10-08 20:55:28,260 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=24.71 vs. limit=8.4525
2024-10-08 20:55:33,077 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.21 vs. limit=7.97625
2024-10-08 20:55:38,019 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.40 vs. limit=5.636666666666667
2024-10-08 20:55:43,417 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=1273.3333333333333, ans=0.4403125
2024-10-08 20:55:44,311 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.64 vs. limit=5.318333333333333
2024-10-08 20:55:47,382 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten.whitening_limit, batch_count=1273.3333333333333, ans=7.9775
2024-10-08 20:55:57,825 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=1276.6666666666667, ans=0.1781875
2024-10-08 20:55:57,827 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=1276.6666666666667, ans=0.44015625
2024-10-08 20:55:59,517 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 20:55:59,648 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=1276.6666666666667, ans=0.44015625
2024-10-08 20:56:11,649 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=6.63 vs. limit=7.98
2024-10-08 20:56:13,961 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.max_abs, batch_count=1280.0, ans=5.8
2024-10-08 20:56:23,269 INFO [train.py:1152] Epoch 1, batch 3850, loss[loss=0.522, ctc_loss=0.6008, attn_decoder_loss=0.5023, over 4830.00 frames. ], tot_loss[loss=0.5006, ctc_loss=0.5667, attn_decoder_loss=0.4841, over 967529.49 frames. ], batch size: 38, lr: 4.24e-02,
2024-10-08 20:56:26,673 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=1283.3333333333333, ans=0.2871666666666667
2024-10-08 20:56:42,712 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.min_positive, batch_count=1286.6666666666667, ans=0.23713333333333333
2024-10-08 20:56:45,245 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.54 vs. limit=5.321666666666666
2024-10-08 20:56:55,709 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=1290.0, ans=0.33875
2024-10-08 20:57:02,062 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=1290.0, ans=0.43953125
2024-10-08 20:57:13,304 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=1293.3333333333333, ans=0.439375
2024-10-08 20:57:33,207 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=1296.6666666666667, ans=8.4725
2024-10-08 20:57:33,800 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=4.81 vs. limit=4.259333333333333
2024-10-08 20:57:43,652 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.794e+01 9.119e+01 9.868e+01 1.228e+02 2.008e+02, threshold=1.974e+02, percent-clipped=0.0
2024-10-08 20:57:43,719 INFO [train.py:1152] Epoch 1, batch 3900, loss[loss=0.474, ctc_loss=0.5438, attn_decoder_loss=0.4566, over 4743.00 frames. ], tot_loss[loss=0.498, ctc_loss=0.5636, attn_decoder_loss=0.4816, over 967015.45 frames. ], batch size: 26, lr: 4.24e-02,
2024-10-08 20:58:13,299 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.94 vs. limit=7.98875
2024-10-08 20:58:14,330 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=1306.6666666666667, ans=0.151
2024-10-08 20:58:20,693 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=1306.6666666666667, ans=0.151
2024-10-08 20:58:28,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.max_abs, batch_count=1306.6666666666667, ans=5.816666666666666
2024-10-08 20:58:33,588 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=1310.0, ans=0.150875
2024-10-08 20:58:40,034 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1310.0, ans=0.2869
2024-10-08 20:58:48,284 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=1313.3333333333333, ans=0.4384375
2024-10-08 20:59:01,975 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.22 vs. limit=5.656666666666666
2024-10-08 20:59:04,070 INFO [train.py:1152] Epoch 1, batch 3950, loss[loss=0.466, ctc_loss=0.5551, attn_decoder_loss=0.4438, over 4839.00 frames. ], tot_loss[loss=0.4932, ctc_loss=0.5581, attn_decoder_loss=0.477, over 967247.74 frames. ], batch size: 36, lr: 4.23e-02,
2024-10-08 20:59:35,679 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.01 vs. limit=5.3308333333333335
2024-10-08 20:59:37,144 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=26.01 vs. limit=5.661666666666667
2024-10-08 20:59:50,044 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.57 vs. limit=8.4925
2024-10-08 20:59:50,970 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=1326.6666666666667, ans=0.4378125
2024-10-08 20:59:57,319 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=1326.6666666666667, ans=0.04585416666666667
2024-10-08 20:59:58,196 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.27 vs. limit=7.9975
2024-10-08 21:00:03,735 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=1326.6666666666667, ans=0.4378125
2024-10-08 21:00:18,839 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.48 vs. limit=5.665
2024-10-08 21:00:23,179 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_False/checkpoint-4000.pt
2024-10-08 21:00:25,871 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.103e+01 8.792e+01 9.761e+01 1.198e+02 1.794e+02, threshold=1.952e+02, percent-clipped=0.0
2024-10-08 21:00:25,919 INFO [train.py:1152] Epoch 1, batch 4000, loss[loss=0.4351, ctc_loss=0.4604, attn_decoder_loss=0.4288, over 4815.00 frames. ], tot_loss[loss=0.4918, ctc_loss=0.5554, attn_decoder_loss=0.4759, over 967198.32 frames. ], batch size: 19, lr: 4.23e-02,
2024-10-08 21:00:32,143 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=18.36 vs. limit=8.5
2024-10-08 21:00:38,107 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=1333.3333333333333, ans=0.15
2024-10-08 21:00:39,664 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=1336.6666666666667, ans=0.8532166666666667
2024-10-08 21:00:45,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=1336.6666666666667, ans=0.149875
2024-10-08 21:00:48,252 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.23 vs. limit=3.2005
2024-10-08 21:00:56,291 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.92 vs. limit=5.67
2024-10-08 21:01:16,977 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=37.82 vs. limit=8.5075
2024-10-08 21:01:29,082 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1346.6666666666667, ans=0.2865333333333333
2024-10-08 21:01:30,610 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=1346.6666666666667, ans=0.0697
2024-10-08 21:01:35,476 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=1346.6666666666667, ans=0.5
2024-10-08 21:01:44,804 INFO [train.py:1152] Epoch 1, batch 4050, loss[loss=0.5849, ctc_loss=0.6888, attn_decoder_loss=0.5589, over 4777.00 frames. ], tot_loss[loss=0.4919, ctc_loss=0.5557, attn_decoder_loss=0.4759, over 967549.96 frames. ], batch size: 53, lr: 4.22e-02,
2024-10-08 21:01:44,973 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=1350.0, ans=0.149375
2024-10-08 21:01:46,784 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 21:01:49,316 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=4.54 vs. limit=4.27
2024-10-08 21:02:00,327 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.93 vs. limit=8.0075
2024-10-08 21:02:03,657 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=7.62 vs. limit=8.0075
2024-10-08 21:02:11,092 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.53 vs. limit=5.338333333333333
2024-10-08 21:02:19,749 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=1356.6666666666667, ans=0.43640625
2024-10-08 21:02:19,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=1356.6666666666667, ans=0.43640625
2024-10-08 21:02:35,921 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.31 vs. limit=3.204
2024-10-08 21:02:40,117 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1360.0, ans=0.2864
2024-10-08 21:02:46,456 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=1363.3333333333333, ans=0.069325
2024-10-08 21:02:59,097 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=1363.3333333333333, ans=0.1873891666666667
2024-10-08 21:02:59,318 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=1363.3333333333333, ans=0.045739583333333333
2024-10-08 21:02:59,774 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.18 vs. limit=5.681666666666667
2024-10-08 21:03:00,839 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=1363.3333333333333, ans=0.148875
2024-10-08 21:03:03,826 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.201e+01 9.273e+01 1.076e+02 1.222e+02 2.088e+02, threshold=2.152e+02, percent-clipped=2.0
2024-10-08 21:03:03,890 INFO [train.py:1152] Epoch 1, batch 4100, loss[loss=0.4161, ctc_loss=0.4828, attn_decoder_loss=0.3994, over 4861.00 frames. ], tot_loss[loss=0.4915, ctc_loss=0.5562, attn_decoder_loss=0.4753, over 966936.24 frames. ], batch size: 31, lr: 4.22e-02,
2024-10-08 21:03:12,646 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=24.57 vs. limit=8.525
2024-10-08 21:03:15,906 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=7.77 vs. limit=8.0125
2024-10-08 21:03:17,455 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten.whitening_limit, batch_count=1366.6666666666667, ans=8.0125
2024-10-08 21:03:22,381 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.30 vs. limit=8.5275
2024-10-08 21:03:38,400 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.51 vs. limit=4.549333333333333
2024-10-08 21:03:42,461 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=1373.3333333333333, ans=0.435625
2024-10-08 21:03:43,064 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.12 vs. limit=8.015
2024-10-08 21:03:46,355 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.97 vs. limit=8.015
2024-10-08 21:03:50,376 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=1376.6666666666667, ans=0.43546875
2024-10-08 21:03:54,201 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.00 vs. limit=8.01625
2024-10-08 21:03:59,048 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.90 vs. limit=8.01625
2024-10-08 21:04:02,109 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=29.93 vs. limit=8.5325
2024-10-08 21:04:21,611 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.29 vs. limit=3.207
2024-10-08 21:04:23,830 INFO [train.py:1152] Epoch 1, batch 4150, loss[loss=0.5138, ctc_loss=0.5728, attn_decoder_loss=0.4991, over 4749.00 frames. ], tot_loss[loss=0.4884, ctc_loss=0.5518, attn_decoder_loss=0.4726, over 967084.83 frames. ], batch size: 20, lr: 4.21e-02,
2024-10-08 21:04:36,300 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=14.54 vs. limit=8.5375
2024-10-08 21:04:41,672 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=1386.6666666666667, ans=0.09133333333333334
2024-10-08 21:04:46,545 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=1386.6666666666667, ans=0.7638666666666667
2024-10-08 21:04:55,436 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.12 vs. limit=3.2085
2024-10-08 21:04:59,381 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=1390.0, ans=0.43484375
2024-10-08 21:04:59,811 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.59 vs. limit=4.556
2024-10-08 21:04:59,878 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.53 vs. limit=5.695
2024-10-08 21:05:05,266 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.26 vs. limit=3.2085
2024-10-08 21:05:05,712 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=1390.0, ans=0.32625000000000004
2024-10-08 21:05:33,309 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=29.65 vs. limit=8.5475
2024-10-08 21:05:35,204 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.24 vs. limit=5.349166666666667
2024-10-08 21:05:38,104 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=135.25 vs. limit=8.5475
2024-10-08 21:05:43,229 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=25.33 vs. limit=8.55
2024-10-08 21:05:43,759 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.092e+01 8.864e+01 1.059e+02 1.259e+02 1.857e+02, threshold=2.117e+02, percent-clipped=0.0
2024-10-08 21:05:43,819 INFO [train.py:1152] Epoch 1, batch 4200, loss[loss=0.4516, ctc_loss=0.4945, attn_decoder_loss=0.4409, over 4858.00 frames. ], tot_loss[loss=0.4866, ctc_loss=0.5493, attn_decoder_loss=0.471, over 967267.35 frames. ], batch size: 31, lr: 4.20e-02,
2024-10-08 21:05:47,063 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=1400.0, ans=0.851
2024-10-08 21:05:52,668 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.38 vs. limit=5.35
2024-10-08 21:05:56,728 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=1400.0, ans=0.851
2024-10-08 21:06:05,841 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.87 vs. limit=8.02625
2024-10-08 21:06:36,021 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.69 vs. limit=5.705
2024-10-08 21:06:37,846 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.56 vs. limit=5.3525
2024-10-08 21:06:41,102 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.16 vs. limit=8.02875
2024-10-08 21:06:47,238 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=13.10 vs. limit=5.706666666666667
2024-10-08 21:06:56,205 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=1413.3333333333333, ans=0.035
2024-10-08 21:07:00,254 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.30 vs. limit=3.212
2024-10-08 21:07:04,055 INFO [train.py:1152] Epoch 1, batch 4250, loss[loss=0.4776, ctc_loss=0.5304, attn_decoder_loss=0.4644, over 4756.00 frames. ], tot_loss[loss=0.4823, ctc_loss=0.5429, attn_decoder_loss=0.4672, over 967210.84 frames. ], batch size: 19, lr: 4.20e-02,
2024-10-08 21:07:13,122 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.70 vs. limit=5.708333333333333
2024-10-08 21:07:20,291 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=1420.0, ans=0.43343750000000003
2024-10-08 21:07:31,441 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=1420.0, ans=0.8503000000000001
2024-10-08 21:07:35,688 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.49 vs. limit=8.567499999999999
2024-10-08 21:07:49,816 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=18.18 vs. limit=8.567499999999999
2024-10-08 21:07:56,133 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.55 vs. limit=3.214
2024-10-08 21:07:57,132 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=1426.6666666666667, ans=0.433125
2024-10-08 21:08:11,684 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=1430.0, ans=0.067825
2024-10-08 21:08:19,798 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 21:08:21,307 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=1430.0, ans=0.146375
2024-10-08 21:08:24,306 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.118e+01 8.854e+01 9.948e+01 1.118e+02 1.661e+02, threshold=1.990e+02, percent-clipped=0.0
2024-10-08 21:08:24,370 INFO [train.py:1152] Epoch 1, batch 4300, loss[loss=0.4366, ctc_loss=0.5225, attn_decoder_loss=0.4151, over 4849.00 frames. ], tot_loss[loss=0.4802, ctc_loss=0.5412, attn_decoder_loss=0.465, over 967391.96 frames. ], batch size: 21, lr: 4.19e-02,
2024-10-08 21:08:33,636 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=5.94 vs. limit=8.0375
2024-10-08 21:08:57,944 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=1440.0, ans=0.8496
2024-10-08 21:09:02,104 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.32 vs. limit=8.04
2024-10-08 21:09:09,666 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=46.09 vs. limit=8.58
2024-10-08 21:09:21,815 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=1443.3333333333333, ans=0.43234375
2024-10-08 21:09:24,817 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.76 vs. limit=5.721666666666667
2024-10-08 21:09:28,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=1446.6666666666667, ans=0.06745000000000001
2024-10-08 21:09:39,909 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.90 vs. limit=8.0425
2024-10-08 21:09:42,267 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=1450.0, ans=0.06737499999999999
2024-10-08 21:09:43,658 INFO [train.py:1152] Epoch 1, batch 4350, loss[loss=0.4219, ctc_loss=0.454, attn_decoder_loss=0.4139, over 4851.00 frames. ], tot_loss[loss=0.4769, ctc_loss=0.5372, attn_decoder_loss=0.4618, over 966230.05 frames. ], batch size: 21, lr: 4.19e-02,
2024-10-08 21:09:54,755 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=1450.0, ans=0.43203125
2024-10-08 21:10:02,578 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer_ff3.min_abs, batch_count=1453.3333333333333, ans=0.07266666666666667
2024-10-08 21:10:04,257 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1453.3333333333333, ans=0.28546666666666665
2024-10-08 21:10:15,373 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=1456.6666666666667, ans=0.43171875
2024-10-08 21:10:41,329 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.43 vs. limit=3.219
2024-10-08 21:10:41,503 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.01 vs. limit=5.73
2024-10-08 21:10:50,706 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.whiten.whitening_limit, batch_count=1463.3333333333333, ans=4.585333333333333
2024-10-08 21:10:55,671 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.17 vs. limit=5.7316666666666665
2024-10-08 21:11:02,802 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.526e+01 9.202e+01 1.022e+02 1.163e+02 1.951e+02, threshold=2.043e+02, percent-clipped=0.0
2024-10-08 21:11:02,861 INFO [train.py:1152] Epoch 1, batch 4400, loss[loss=0.4688, ctc_loss=0.511, attn_decoder_loss=0.4582, over 4735.00 frames. ], tot_loss[loss=0.4764, ctc_loss=0.5366, attn_decoder_loss=0.4613, over 965771.58 frames. ], batch size: 26, lr: 4.18e-02,
2024-10-08 21:11:11,199 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=22.26 vs. limit=8.6
2024-10-08 21:11:15,525 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=1466.6666666666667, ans=0.31666666666666665
2024-10-08 21:11:17,034 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=1470.0, ans=0.06692500000000001
2024-10-08 21:11:34,881 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=35.34 vs. limit=8.605
2024-10-08 21:11:43,735 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=1473.3333333333333, ans=0.06684999999999999
2024-10-08 21:12:08,205 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.72 vs. limit=3.222
2024-10-08 21:12:13,967 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1480.0, ans=0.2852
2024-10-08 21:12:17,900 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.92 vs. limit=8.61
2024-10-08 21:12:21,703 INFO [train.py:1152] Epoch 1, batch 4450, loss[loss=0.4166, ctc_loss=0.449, attn_decoder_loss=0.4085, over 4883.00 frames. ], tot_loss[loss=0.4746, ctc_loss=0.5346, attn_decoder_loss=0.4596, over 966090.40 frames. ], batch size: 19, lr: 4.17e-02,
2024-10-08 21:12:28,868 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=39.82 vs. limit=8.6125
2024-10-08 21:12:54,176 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.08 vs. limit=8.6175
2024-10-08 21:12:54,188 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.36 vs. limit=3.2235
2024-10-08 21:13:05,515 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=5.79 vs. limit=5.745
2024-10-08 21:13:07,022 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.54 vs. limit=5.3725
2024-10-08 21:13:20,010 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.43 vs. limit=8.620000000000001
2024-10-08 21:13:32,858 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.22 vs. limit=5.748333333333333
2024-10-08 21:13:39,946 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=1500.0, ans=0.4296875
2024-10-08 21:13:41,257 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.165e+01 8.997e+01 1.055e+02 1.254e+02 2.034e+02, threshold=2.111e+02, percent-clipped=0.0
2024-10-08 21:13:41,323 INFO [train.py:1152] Epoch 1, batch 4500, loss[loss=0.4319, ctc_loss=0.4876, attn_decoder_loss=0.418, over 4861.00 frames. ], tot_loss[loss=0.4735, ctc_loss=0.5325, attn_decoder_loss=0.4588, over 966208.89 frames. ], batch size: 28, lr: 4.17e-02,
2024-10-08 21:14:02,830 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.83 vs. limit=8.06375
2024-10-08 21:14:05,516 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=1503.3333333333333, ans=0.7650333333333333
2024-10-08 21:14:07,791 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.81 vs. limit=5.3758333333333335
2024-10-08 21:14:21,901 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=25.04 vs. limit=8.629999999999999
2024-10-08 21:14:22,044 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.00 vs. limit=5.753333333333334
2024-10-08 21:14:31,336 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=27.04 vs. limit=8.6325
2024-10-08 21:14:32,902 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=5.07 vs. limit=8.06625
2024-10-08 21:14:38,567 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=1510.0, ans=0.42921875
2024-10-08 21:14:39,230 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.54 vs. limit=5.755
2024-10-08 21:14:45,332 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.87 vs. limit=8.0675
2024-10-08 21:14:51,269 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=1513.3333333333333, ans=0.4290625
2024-10-08 21:14:54,844 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.50 vs. limit=3.227
2024-10-08 21:14:56,679 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.81 vs. limit=8.0675
2024-10-08 21:14:58,314 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.51 vs. limit=4.605333333333333
2024-10-08 21:15:00,042 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.81 vs. limit=8.6375
2024-10-08 21:15:00,602 INFO [train.py:1152] Epoch 1, batch 4550, loss[loss=0.4489, ctc_loss=0.4757, attn_decoder_loss=0.4422, over 4846.00 frames. ], tot_loss[loss=0.4729, ctc_loss=0.5319, attn_decoder_loss=0.4582, over 966014.55 frames. ], batch size: 20, lr: 4.16e-02,
2024-10-08 21:15:00,849 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=1516.6666666666667, ans=0.5
2024-10-08 21:15:02,411 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=1516.6666666666667, ans=0.143125
2024-10-08 21:15:05,580 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=1516.6666666666667, ans=0.035
2024-10-08 21:15:06,504 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=24.24 vs. limit=8.6375
2024-10-08 21:15:11,176 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.99 vs. limit=8.06875
2024-10-08 21:15:39,626 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.62 vs. limit=8.07125
2024-10-08 21:16:05,601 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=1530.0, ans=8.6475
2024-10-08 21:16:07,991 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff2_skip_rate, batch_count=1530.0, ans=0.065575
2024-10-08 21:16:12,287 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=5.11 vs. limit=4.306
2024-10-08 21:16:17,257 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.27 vs. limit=3.2295
2024-10-08 21:16:19,589 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=1533.3333333333333, ans=0.428125
2024-10-08 21:16:20,666 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.281e+01 9.349e+01 1.066e+02 1.255e+02 1.941e+02, threshold=2.132e+02, percent-clipped=0.0
2024-10-08 21:16:20,716 INFO [train.py:1152] Epoch 1, batch 4600, loss[loss=0.5453, ctc_loss=0.6466, attn_decoder_loss=0.5199, over 4767.00 frames. ], tot_loss[loss=0.4703, ctc_loss=0.5287, attn_decoder_loss=0.4557, over 966350.88 frames. ], batch size: 45, lr: 4.15e-02,
2024-10-08 21:16:34,161 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.24 vs. limit=4.613333333333333
2024-10-08 21:16:42,391 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.97 vs. limit=5.384166666666666
2024-10-08 21:16:54,222 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=1540.0, ans=0.14225
2024-10-08 21:16:57,903 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.07 vs. limit=8.0775
2024-10-08 21:17:07,563 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.56 vs. limit=8.6575
2024-10-08 21:17:17,003 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=43.53 vs. limit=8.6575
2024-10-08 21:17:17,036 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=21.12 vs. limit=8.6575
2024-10-08 21:17:20,163 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.46 vs. limit=4.617333333333333
2024-10-08 21:17:36,244 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.81 vs. limit=8.66
2024-10-08 21:17:37,066 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=1546.6666666666667, ans=0.4275
2024-10-08 21:17:40,033 INFO [train.py:1152] Epoch 1, batch 4650, loss[loss=0.4912, ctc_loss=0.5526, attn_decoder_loss=0.4758, over 4824.00 frames. ], tot_loss[loss=0.4702, ctc_loss=0.5282, attn_decoder_loss=0.4557, over 965794.19 frames. ], batch size: 36, lr: 4.15e-02,
2024-10-08 21:17:47,037 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=20.65 vs. limit=8.6625
2024-10-08 21:17:53,776 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.22 vs. limit=5.3875
2024-10-08 21:18:16,659 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1556.6666666666667, ans=0.2844333333333333
2024-10-08 21:18:25,823 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=3.17 vs. limit=4.6226666666666665
2024-10-08 21:18:33,491 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.03 vs. limit=8.085
2024-10-08 21:18:33,939 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.82 vs. limit=5.78
2024-10-08 21:18:42,964 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.78 vs. limit=5.781666666666666
2024-10-08 21:18:43,884 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=1563.3333333333333, ans=0.141375
2024-10-08 21:18:44,525 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=14.13 vs. limit=8.08625
2024-10-08 21:18:59,264 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=38.05 vs. limit=8.675
2024-10-08 21:18:59,872 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.386e+01 9.574e+01 1.090e+02 1.237e+02 2.307e+02, threshold=2.179e+02, percent-clipped=1.0
2024-10-08 21:18:59,940 INFO [train.py:1152] Epoch 1, batch 4700, loss[loss=0.4669, ctc_loss=0.5053, attn_decoder_loss=0.4574, over 4940.00 frames. ], tot_loss[loss=0.4696, ctc_loss=0.5266, attn_decoder_loss=0.4553, over 965687.48 frames. ], batch size: 19, lr: 4.14e-02,
2024-10-08 21:19:05,478 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=11.45 vs. limit=5.783333333333333
2024-10-08 21:19:07,536 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.14 vs. limit=8.675
2024-10-08 21:19:09,806 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=1566.6666666666667, ans=0.14125
2024-10-08 21:19:21,816 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.01 vs. limit=8.6775
2024-10-08 21:19:23,287 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=21.00 vs. limit=8.6775
2024-10-08 21:19:28,522 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=8.94 vs. limit=8.6775
2024-10-08 21:19:36,294 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.57 vs. limit=8.09
2024-10-08 21:19:52,310 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.15 vs. limit=5.788333333333333
2024-10-08 21:20:20,336 INFO [train.py:1152] Epoch 1, batch 4750, loss[loss=0.4861, ctc_loss=0.5728, attn_decoder_loss=0.4644, over 4769.00 frames. ], tot_loss[loss=0.4695, ctc_loss=0.5275, attn_decoder_loss=0.455, over 965640.65 frames. ], batch size: 45, lr: 4.14e-02,
2024-10-08 21:20:32,134 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=28.05 vs. limit=8.6875
2024-10-08 21:21:00,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=1590.0, ans=0.04503125
2024-10-08 21:21:00,547 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=1590.0, ans=0.42546875
2024-10-08 21:21:16,898 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.75 vs. limit=5.796666666666667
2024-10-08 21:21:17,966 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=1593.3333333333333, ans=0.160375
2024-10-08 21:21:17,990 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=1593.3333333333333, ans=0.22390000000000002
2024-10-08 21:21:20,114 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=21.16 vs. limit=8.695
2024-10-08 21:21:37,181 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=1596.6666666666667, ans=0.42515625
2024-10-08 21:21:40,369 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.020e+01 8.942e+01 1.024e+02 1.219e+02 2.043e+02, threshold=2.047e+02, percent-clipped=0.0
2024-10-08 21:21:40,429 INFO [train.py:1152] Epoch 1, batch 4800, loss[loss=0.4125, ctc_loss=0.4393, attn_decoder_loss=0.4057, over 4873.00 frames. ], tot_loss[loss=0.4661, ctc_loss=0.5232, attn_decoder_loss=0.4518, over 965918.29 frames. ], batch size: 22, lr: 4.13e-02,
2024-10-08 21:21:43,114 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.14 vs. limit=5.8
2024-10-08 21:21:49,229 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=34.48 vs. limit=8.7
2024-10-08 21:21:59,821 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=1603.3333333333333, ans=0.2995833333333333
2024-10-08 21:22:01,392 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1603.3333333333333, ans=0.28396666666666665
2024-10-08 21:22:15,859 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=1606.6666666666667, ans=0.15962500000000002
2024-10-08 21:22:17,918 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=16.52 vs. limit=5.803333333333334
2024-10-08 21:22:18,964 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=1606.6666666666667, ans=0.06384999999999999
2024-10-08 21:22:21,156 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=45.01 vs. limit=8.705
2024-10-08 21:22:28,482 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=1610.0, ans=0.04496875
2024-10-08 21:22:35,282 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.74 vs. limit=5.4025
2024-10-08 21:22:40,284 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.17 vs. limit=3.2415
2024-10-08 21:22:40,528 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.62 vs. limit=8.7075
2024-10-08 21:22:41,195 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=1610.0, ans=0.139625
2024-10-08 21:22:46,868 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.03 vs. limit=8.71
2024-10-08 21:22:56,402 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=4.72 vs. limit=8.105
2024-10-08 21:22:57,192 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=1613.3333333333333, ans=0.8435333333333334
2024-10-08 21:22:59,972 INFO [train.py:1152] Epoch 1, batch 4850, loss[loss=0.4178, ctc_loss=0.4663, attn_decoder_loss=0.4056, over 4848.00 frames. ], tot_loss[loss=0.4627, ctc_loss=0.5188, attn_decoder_loss=0.4487, over 966613.53 frames. ], batch size: 28, lr: 4.12e-02,
2024-10-08 21:23:19,335 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=1620.0, ans=0.4240625
2024-10-08 21:23:27,311 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=1620.0, ans=0.4240625
2024-10-08 21:23:29,596 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.19 vs. limit=4.648
2024-10-08 21:23:35,310 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=1623.3333333333333, ans=0.42390625
2024-10-08 21:23:57,537 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=1626.6666666666667, ans=0.139
2024-10-08 21:24:19,648 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.374e+01 9.040e+01 1.041e+02 1.223e+02 2.823e+02, threshold=2.082e+02, percent-clipped=3.0
2024-10-08 21:24:19,712 INFO [train.py:1152] Epoch 1, batch 4900, loss[loss=0.4032, ctc_loss=0.4343, attn_decoder_loss=0.3954, over 4854.00 frames. ], tot_loss[loss=0.4617, ctc_loss=0.5187, attn_decoder_loss=0.4474, over 967195.10 frames. ], batch size: 21, lr: 4.12e-02,
2024-10-08 21:24:27,700 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1633.3333333333333, ans=0.2836666666666667
2024-10-08 21:24:28,286 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=4.29 vs. limit=8.1125
2024-10-08 21:24:31,842 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.61 vs. limit=5.408333333333333
2024-10-08 21:24:37,886 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.97 vs. limit=5.409166666666667
2024-10-08 21:24:38,002 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=33.14 vs. limit=8.7275
2024-10-08 21:24:54,988 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=1640.0, ans=0.0631
2024-10-08 21:24:59,640 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=1640.0, ans=0.423125
2024-10-08 21:25:22,395 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.14 vs. limit=8.1175
2024-10-08 21:25:26,503 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=1646.6666666666667, ans=0.4228125
2024-10-08 21:25:39,001 INFO [train.py:1152] Epoch 1, batch 4950, loss[loss=0.4946, ctc_loss=0.6075, attn_decoder_loss=0.4664, over 4775.00 frames. ], tot_loss[loss=0.4618, ctc_loss=0.5191, attn_decoder_loss=0.4475, over 966685.70 frames. ], batch size: 53, lr: 4.11e-02,
2024-10-08 21:25:49,518 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.21 vs. limit=5.4125
2024-10-08 21:25:54,710 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.26 vs. limit=5.413333333333333
2024-10-08 21:26:07,946 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=1653.3333333333333, ans=0.4225
2024-10-08 21:26:14,989 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.59 vs. limit=5.828333333333333
2024-10-08 21:26:23,935 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=1656.6666666666667, ans=0.42234375
2024-10-08 21:26:39,938 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.min_abs, batch_count=1660.0, ans=0.22490000000000002
2024-10-08 21:26:56,026 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=1663.3333333333333, ans=0.04480208333333334
2024-10-08 21:26:59,134 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.268e+01 8.989e+01 1.040e+02 1.230e+02 2.459e+02, threshold=2.080e+02, percent-clipped=1.0
2024-10-08 21:26:59,193 INFO [train.py:1152] Epoch 1, batch 5000, loss[loss=0.4684, ctc_loss=0.5244, attn_decoder_loss=0.4544, over 4786.00 frames. ], tot_loss[loss=0.4579, ctc_loss=0.514, attn_decoder_loss=0.4439, over 967691.25 frames. ], batch size: 29, lr: 4.10e-02,
2024-10-08 21:27:00,930 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1666.6666666666667, ans=0.2833333333333333
2024-10-08 21:27:02,516 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=1666.6666666666667, ans=0.421875
2024-10-08 21:27:04,706 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.95 vs. limit=5.416666666666667
2024-10-08 21:27:14,705 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.39 vs. limit=3.2505
2024-10-08 21:27:18,420 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=1670.0, ans=0.137375
2024-10-08 21:27:22,056 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=1670.0, ans=8.7525
2024-10-08 21:27:35,145 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.18 vs. limit=5.418333333333333
2024-10-08 21:27:51,163 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.75 vs. limit=8.12875
2024-10-08 21:27:51,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=1676.6666666666667, ans=0.42140625
2024-10-08 21:27:54,085 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.45 vs. limit=3.2515
2024-10-08 21:28:02,048 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.45 vs. limit=5.84
2024-10-08 21:28:06,752 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.90 vs. limit=5.42
2024-10-08 21:28:08,609 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=27.46 vs. limit=8.76
2024-10-08 21:28:17,674 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=1683.3333333333333, ans=0.13687500000000002
2024-10-08 21:28:18,957 INFO [train.py:1152] Epoch 1, batch 5050, loss[loss=0.425, ctc_loss=0.4601, attn_decoder_loss=0.4162, over 4854.00 frames. ], tot_loss[loss=0.4539, ctc_loss=0.5083, attn_decoder_loss=0.4402, over 968668.79 frames. ], batch size: 19, lr: 4.10e-02,
2024-10-08 21:28:28,666 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=1683.3333333333333, ans=0.42109375
2024-10-08 21:28:39,006 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.82 vs. limit=5.843333333333334
2024-10-08 21:28:39,594 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=1686.6666666666667, ans=0.4209375
2024-10-08 21:28:39,625 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=1686.6666666666667, ans=0.4209375
2024-10-08 21:28:47,403 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=1686.6666666666667, ans=0.4209375
2024-10-08 21:28:49,007 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=1690.0, ans=0.28875
2024-10-08 21:28:53,648 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=1690.0, ans=0.035
2024-10-08 21:28:58,486 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=1690.0, ans=0.42078125
2024-10-08 21:29:00,842 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.27 vs. limit=8.7675
2024-10-08 21:29:05,387 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=4.77 vs. limit=4.677333333333333
2024-10-08 21:29:19,424 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=46.76 vs. limit=8.77
2024-10-08 21:29:34,625 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=1696.6666666666667, ans=0.42046875
2024-10-08 21:29:37,535 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.754e+01 8.922e+01 1.028e+02 1.151e+02 1.789e+02, threshold=2.056e+02, percent-clipped=0.0
2024-10-08 21:29:37,598 INFO [train.py:1152] Epoch 1, batch 5100, loss[loss=0.4389, ctc_loss=0.4778, attn_decoder_loss=0.4292, over 4816.00 frames. ], tot_loss[loss=0.4564, ctc_loss=0.5123, attn_decoder_loss=0.4425, over 967968.84 frames. ], batch size: 19, lr: 4.09e-02,
2024-10-08 21:29:45,750 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=1700.0, ans=0.8405
2024-10-08 21:29:50,620 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1700.0, ans=0.283
2024-10-08 21:29:54,325 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=35.65 vs. limit=8.7775
2024-10-08 21:30:11,016 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=1706.6666666666667, ans=6.066666666666666
2024-10-08 21:30:16,368 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.62 vs. limit=8.14
2024-10-08 21:30:21,921 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1706.6666666666667, ans=0.2829333333333333
2024-10-08 21:30:40,917 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=1713.3333333333333, ans=0.061450000000000005
2024-10-08 21:30:47,877 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.37 vs. limit=4.685333333333333
2024-10-08 21:30:56,560 INFO [train.py:1152] Epoch 1, batch 5150, loss[loss=0.4584, ctc_loss=0.519, attn_decoder_loss=0.4432, over 4814.00 frames. ], tot_loss[loss=0.454, ctc_loss=0.5105, attn_decoder_loss=0.4399, over 968096.19 frames. ], batch size: 36, lr: 4.09e-02,
2024-10-08 21:31:05,459 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.83 vs. limit=8.7875
2024-10-08 21:31:11,852 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.42 vs. limit=5.86
2024-10-08 21:31:14,692 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=17.41 vs. limit=8.79
2024-10-08 21:31:15,519 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=1720.0, ans=0.419375
2024-10-08 21:31:20,300 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=1720.0, ans=0.0613
2024-10-08 21:31:20,667 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=20.60 vs. limit=8.79
2024-10-08 21:31:24,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=1720.0, ans=0.8398
2024-10-08 21:31:28,074 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.prob, batch_count=1723.3333333333333, ans=0.41921875
2024-10-08 21:31:33,319 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=21.46 vs. limit=8.7925
2024-10-08 21:31:36,133 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 21:31:45,557 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=1726.6666666666667, ans=0.4190625
2024-10-08 21:31:46,266 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.45 vs. limit=8.795
2024-10-08 21:32:10,873 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.min_positive, batch_count=1730.0, ans=0.0891875
2024-10-08 21:32:10,898 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=1730.0, ans=0.135125
2024-10-08 21:32:15,508 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.844e+01 8.780e+01 1.012e+02 1.210e+02 1.968e+02, threshold=2.024e+02, percent-clipped=0.0
2024-10-08 21:32:15,568 INFO [train.py:1152] Epoch 1, batch 5200, loss[loss=0.4136, ctc_loss=0.456, attn_decoder_loss=0.4029, over 4793.00 frames. ], tot_loss[loss=0.4518, ctc_loss=0.507, attn_decoder_loss=0.438, over 967730.18 frames. ], batch size: 29, lr: 4.08e-02,
2024-10-08 21:32:19,303 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.35 vs. limit=3.26
2024-10-08 21:32:26,752 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1733.3333333333333, ans=0.41875
2024-10-08 21:32:31,870 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=7.37 vs. limit=8.15125
2024-10-08 21:32:39,769 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=30.62 vs. limit=8.8025
2024-10-08 21:32:42,504 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer_ff2.min_abs, batch_count=1736.6666666666667, ans=0.04341666666666667
2024-10-08 21:32:50,840 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.24 vs. limit=5.435
2024-10-08 21:32:52,969 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=7.57 vs. limit=8.1525
2024-10-08 21:32:54,326 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.50 vs. limit=8.1525
2024-10-08 21:33:25,148 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=1746.6666666666667, ans=0.41812499999999997
2024-10-08 21:33:29,236 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.36 vs. limit=5.873333333333333
2024-10-08 21:33:33,006 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=1750.0, ans=0.134375
2024-10-08 21:33:34,431 INFO [train.py:1152] Epoch 1, batch 5250, loss[loss=0.4871, ctc_loss=0.5403, attn_decoder_loss=0.4738, over 4857.00 frames. ], tot_loss[loss=0.45, ctc_loss=0.5037, attn_decoder_loss=0.4366, over 967808.47 frames. ], batch size: 20, lr: 4.07e-02,
2024-10-08 21:33:38,253 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.06 vs. limit=8.15625
2024-10-08 21:33:40,939 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=1750.0, ans=0.134375
2024-10-08 21:33:43,016 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=55.86 vs. limit=8.8125
2024-10-08 21:33:46,347 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.27 vs. limit=8.15625
2024-10-08 21:33:55,745 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.22 vs. limit=4.701333333333333
2024-10-08 21:34:04,032 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=16.05 vs. limit=8.815
2024-10-08 21:34:14,087 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=1756.6666666666667, ans=0.1511875
2024-10-08 21:34:22,011 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=1760.0, ans=0.134
2024-10-08 21:34:25,680 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=29.26 vs. limit=8.82
2024-10-08 21:34:33,643 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.77 vs. limit=8.82
2024-10-08 21:34:53,320 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.358e+01 9.162e+01 1.038e+02 1.261e+02 2.191e+02, threshold=2.077e+02, percent-clipped=3.0
2024-10-08 21:34:53,384 INFO [train.py:1152] Epoch 1, batch 5300, loss[loss=0.4381, ctc_loss=0.4841, attn_decoder_loss=0.4266, over 4825.00 frames. ], tot_loss[loss=0.4505, ctc_loss=0.5042, attn_decoder_loss=0.4371, over 967887.53 frames. ], batch size: 38, lr: 4.07e-02,
2024-10-08 21:34:54,393 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.01 vs. limit=8.825
2024-10-08 21:35:01,377 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=1766.6666666666667, ans=0.4171875
2024-10-08 21:35:11,121 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1770.0, ans=0.2823
2024-10-08 21:35:22,010 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=1770.0, ans=0.060175
2024-10-08 21:35:25,738 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=34.13 vs. limit=8.83
2024-10-08 21:35:27,224 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=20.43 vs. limit=8.83
2024-10-08 21:35:31,578 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 21:35:37,755 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=1773.3333333333333, ans=0.416875
2024-10-08 21:35:41,525 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.47 vs. limit=8.16625
2024-10-08 21:35:47,779 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.75 vs. limit=5.444166666666667
2024-10-08 21:35:49,557 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.91 vs. limit=5.888333333333334
2024-10-08 21:35:55,128 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.min_abs, batch_count=1780.0, ans=0.2267
2024-10-08 21:36:08,431 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.98 vs. limit=5.89
2024-10-08 21:36:12,319 INFO [train.py:1152] Epoch 1, batch 5350, loss[loss=0.4298, ctc_loss=0.4966, attn_decoder_loss=0.4131, over 4978.00 frames. ], tot_loss[loss=0.4486, ctc_loss=0.502, attn_decoder_loss=0.4352, over 967293.95 frames. ], batch size: 19, lr: 4.06e-02,
2024-10-08 21:36:13,547 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module2.whiten, num_groups=1, num_channels=192, metric=3.85 vs. limit=8.16875
2024-10-08 21:36:17,387 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=1783.3333333333333, ans=0.133125
2024-10-08 21:36:21,132 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.39 vs. limit=3.2675
2024-10-08 21:36:22,479 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=19.79 vs. limit=8.8375
2024-10-08 21:36:27,176 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.85 vs. limit=5.8933333333333335
2024-10-08 21:36:33,698 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.57 vs. limit=8.17
2024-10-08 21:36:39,214 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=1786.6666666666667, ans=0.08883333333333333
2024-10-08 21:36:42,265 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=1790.0, ans=0.41609375
2024-10-08 21:36:44,484 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.49 vs. limit=8.17125
2024-10-08 21:36:58,084 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=1793.3333333333333, ans=0.4159375
2024-10-08 21:37:02,833 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=1793.3333333333333, ans=0.4159375
2024-10-08 21:37:06,906 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.20 vs. limit=5.8966666666666665
2024-10-08 21:37:12,252 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=1793.3333333333333, ans=0.4159375
2024-10-08 21:37:17,173 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=0.000e+00
2024-10-08 21:37:31,136 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.643e+01 9.491e+01 1.092e+02 1.260e+02 2.275e+02, threshold=2.184e+02, percent-clipped=1.0
2024-10-08 21:37:31,196 INFO [train.py:1152] Epoch 1, batch 5400, loss[loss=0.4813, ctc_loss=0.5551, attn_decoder_loss=0.4629, over 4773.00 frames. ], tot_loss[loss=0.4509, ctc_loss=0.5044, attn_decoder_loss=0.4375, over 966532.54 frames. ], batch size: 49, lr: 4.05e-02,
2024-10-08 21:37:35,294 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.14 vs. limit=8.85
2024-10-08 21:37:39,909 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=18.05 vs. limit=8.85
2024-10-08 21:37:41,690 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=5.68 vs. limit=8.175
2024-10-08 21:37:49,228 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.66 vs. limit=8.8525
2024-10-08 21:37:50,193 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=1803.3333333333333, ans=0.132375
2024-10-08 21:37:57,524 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.27 vs. limit=5.450833333333334
2024-10-08 21:38:00,368 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.42 vs. limit=5.450833333333334
2024-10-08 21:38:05,182 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten.whitening_limit, batch_count=1806.6666666666667, ans=8.1775
2024-10-08 21:38:11,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.09 vs. limit=8.855
2024-10-08 21:38:16,187 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=28.07 vs. limit=8.855
2024-10-08 21:38:20,260 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=1810.0, ans=0.1481875
2024-10-08 21:38:25,694 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.60 vs. limit=8.8575
2024-10-08 21:38:30,704 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.05 vs. limit=8.8575
2024-10-08 21:38:32,192 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.94 vs. limit=5.4525
2024-10-08 21:38:47,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=1813.3333333333333, ans=0.0592
2024-10-08 21:38:48,198 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=9.37 vs. limit=8.86
2024-10-08 21:38:50,279 INFO [train.py:1152] Epoch 1, batch 5450, loss[loss=0.4009, ctc_loss=0.4339, attn_decoder_loss=0.3927, over 4940.00 frames. ], tot_loss[loss=0.4483, ctc_loss=0.5004, attn_decoder_loss=0.4353, over 967208.69 frames. ], batch size: 19, lr: 4.05e-02,
2024-10-08 21:38:50,430 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1816.6666666666667, ans=0.2818333333333333
2024-10-08 21:38:53,039 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.59 vs. limit=5.454166666666667
2024-10-08 21:38:56,832 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=1816.6666666666667, ans=0.131875
2024-10-08 21:39:11,885 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=1820.0, ans=8.865
2024-10-08 21:39:12,657 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=1820.0, ans=0.044312500000000005
2024-10-08 21:39:26,312 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.66 vs. limit=8.8675
2024-10-08 21:39:32,328 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=31.15 vs. limit=8.8675
2024-10-08 21:39:35,269 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=1823.3333333333333, ans=6.139583333333333
2024-10-08 21:39:40,024 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=1826.6666666666667, ans=0.414375
2024-10-08 21:39:57,371 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=1830.0, ans=0.41421874999999997
2024-10-08 21:40:09,791 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.408e+01 9.001e+01 9.887e+01 1.177e+02 2.394e+02, threshold=1.977e+02, percent-clipped=1.0
2024-10-08 21:40:09,859 INFO [train.py:1152] Epoch 1, batch 5500, loss[loss=0.4542, ctc_loss=0.5246, attn_decoder_loss=0.4365, over 4811.00 frames. ], tot_loss[loss=0.4474, ctc_loss=0.4989, attn_decoder_loss=0.4345, over 967472.79 frames. ], batch size: 49, lr: 4.04e-02,
2024-10-08 21:40:17,765 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=1833.3333333333333, ans=0.4140625
2024-10-08 21:40:34,887 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.58 vs. limit=3.2755
2024-10-08 21:40:52,534 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.11 vs. limit=5.92
2024-10-08 21:41:22,014 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.58 vs. limit=8.885
2024-10-08 21:41:24,439 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=1846.6666666666667, ans=0.4134375
2024-10-08 21:41:29,047 INFO [train.py:1152] Epoch 1, batch 5550, loss[loss=0.4044, ctc_loss=0.4183, attn_decoder_loss=0.4009, over 4799.00 frames. ], tot_loss[loss=0.4465, ctc_loss=0.4969, attn_decoder_loss=0.4339, over 967087.24 frames. ], batch size: 19, lr: 4.03e-02,
2024-10-08 21:41:32,381 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=1850.0, ans=6.15625
2024-10-08 21:41:36,262 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2.whitening_limit, batch_count=1850.0, ans=5.925
2024-10-08 21:41:39,452 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=8.02 vs. limit=8.19375
2024-10-08 21:42:02,696 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=1856.6666666666667, ans=0.41296875
2024-10-08 21:42:28,996 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.67 vs. limit=8.1975
2024-10-08 21:42:32,833 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=1863.3333333333333, ans=0.41265625
2024-10-08 21:42:33,237 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.93 vs. limit=8.8975
2024-10-08 21:42:34,809 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=19.68 vs. limit=8.8975
2024-10-08 21:42:43,889 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1863.3333333333333, ans=0.28136666666666665
2024-10-08 21:42:48,638 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.360e+01 9.281e+01 1.055e+02 1.213e+02 2.502e+02, threshold=2.110e+02, percent-clipped=3.0
2024-10-08 21:42:48,699 INFO [train.py:1152] Epoch 1, batch 5600, loss[loss=0.4339, ctc_loss=0.4928, attn_decoder_loss=0.4192, over 4862.00 frames. ], tot_loss[loss=0.4447, ctc_loss=0.4941, attn_decoder_loss=0.4323, over 967173.94 frames. ], batch size: 28, lr: 4.03e-02,
2024-10-08 21:42:54,127 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.97 vs. limit=5.466666666666667
2024-10-08 21:42:55,658 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.68 vs. limit=8.2
2024-10-08 21:42:59,033 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=8.20 vs. limit=8.2
2024-10-08 21:43:01,598 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=1866.6666666666667, ans=0.8346666666666667
2024-10-08 21:43:03,588 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.22 vs. limit=3.2805
2024-10-08 21:43:08,052 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=1870.0, ans=0.1448125
2024-10-08 21:43:20,817 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1873.3333333333333, ans=0.28126666666666666
2024-10-08 21:43:28,168 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.14 vs. limit=8.905
2024-10-08 21:43:40,100 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=1876.6666666666667, ans=0.08827083333333334
2024-10-08 21:43:48,686 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.42 vs. limit=3.2815
2024-10-08 21:44:06,251 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.94 vs. limit=5.47
2024-10-08 21:44:07,093 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=1883.3333333333333, ans=0.129375
2024-10-08 21:44:08,559 INFO [train.py:1152] Epoch 1, batch 5650, loss[loss=0.4658, ctc_loss=0.5302, attn_decoder_loss=0.4497, over 4739.00 frames. ], tot_loss[loss=0.4406, ctc_loss=0.4892, attn_decoder_loss=0.4284, over 967022.70 frames. ], batch size: 45, lr: 4.02e-02,
2024-10-08 21:44:15,490 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.74 vs. limit=5.470833333333333
2024-10-08 21:44:18,937 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=16.83 vs. limit=8.9125
2024-10-08 21:44:19,176 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.35 vs. limit=3.2824999999999998
2024-10-08 21:44:28,552 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=30.47 vs. limit=8.915
2024-10-08 21:44:41,450 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=14.56 vs. limit=8.9175
2024-10-08 21:44:47,486 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=4.93 vs. limit=4.756
2024-10-08 21:44:53,867 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.39 vs. limit=8.20875
2024-10-08 21:45:09,143 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 21:45:17,167 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=1896.6666666666667, ans=0.2629166666666667
2024-10-08 21:45:24,351 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=4.38 vs. limit=8.21125
2024-10-08 21:45:27,716 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.90 vs. limit=8.925
2024-10-08 21:45:28,215 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.560e+01 8.944e+01 1.014e+02 1.180e+02 2.096e+02, threshold=2.028e+02, percent-clipped=0.0
2024-10-08 21:45:28,281 INFO [train.py:1152] Epoch 1, batch 5700, loss[loss=0.455, ctc_loss=0.5161, attn_decoder_loss=0.4397, over 4888.00 frames. ], tot_loss[loss=0.4376, ctc_loss=0.4856, attn_decoder_loss=0.4256, over 966420.61 frames. ], batch size: 22, lr: 4.02e-02,
2024-10-08 21:45:31,624 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1900.0, ans=0.28099999999999997
2024-10-08 21:45:41,813 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=4.95 vs. limit=4.76
2024-10-08 21:45:49,133 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1903.3333333333333, ans=0.28096666666666664
2024-10-08 21:45:49,201 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=1903.3333333333333, ans=0.41078125
2024-10-08 21:46:00,975 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.17 vs. limit=5.953333333333333
2024-10-08 21:46:05,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=1906.6666666666667, ans=0.410625
2024-10-08 21:46:09,170 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=12.30 vs. limit=5.953333333333333
2024-10-08 21:46:39,965 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.68 vs. limit=8.935
2024-10-08 21:46:41,842 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.49 vs. limit=5.956666666666667
2024-10-08 21:46:48,751 INFO [train.py:1152] Epoch 1, batch 5750, loss[loss=0.52, ctc_loss=0.6296, attn_decoder_loss=0.4926, over 4851.00 frames. ], tot_loss[loss=0.4392, ctc_loss=0.4874, attn_decoder_loss=0.4271, over 966852.46 frames. ], batch size: 43, lr: 4.01e-02,
2024-10-08 21:46:58,617 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=1916.6666666666667, ans=0.128125
2024-10-08 21:47:12,950 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=1920.0, ans=0.0568
2024-10-08 21:47:13,531 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=12.69 vs. limit=5.96
2024-10-08 21:47:25,027 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.93 vs. limit=8.942499999999999
2024-10-08 21:47:29,563 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=35.95 vs. limit=8.942499999999999
2024-10-08 21:47:30,685 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=1923.3333333333333, ans=0.40984375
2024-10-08 21:47:41,868 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=1926.6666666666667, ans=0.25916666666666666
2024-10-08 21:47:46,592 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.min_positive, batch_count=1926.6666666666667, ans=0.043979166666666666
2024-10-08 21:47:51,317 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=1930.0, ans=0.1414375
2024-10-08 21:48:03,032 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.09 vs. limit=5.965
2024-10-08 21:48:08,258 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.40 vs. limit=8.225
2024-10-08 21:48:08,611 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.392e+01 9.619e+01 1.102e+02 1.269e+02 2.371e+02, threshold=2.203e+02, percent-clipped=2.0
2024-10-08 21:48:08,670 INFO [train.py:1152] Epoch 1, batch 5800, loss[loss=0.4819, ctc_loss=0.5652, attn_decoder_loss=0.4611, over 4846.00 frames. ], tot_loss[loss=0.4375, ctc_loss=0.4849, attn_decoder_loss=0.4257, over 966144.94 frames. ], batch size: 43, lr: 4.00e-02,
2024-10-08 21:48:13,567 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=2.688e-03
2024-10-08 21:48:20,583 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=4.93 vs. limit=4.773333333333333
2024-10-08 21:48:42,581 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=27.62 vs. limit=8.955
2024-10-08 21:48:45,642 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.24 vs. limit=5.485
2024-10-08 21:48:54,707 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=1943.3333333333333, ans=0.127125
2024-10-08 21:49:01,008 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=1943.3333333333333, ans=0.8319833333333333
2024-10-08 21:49:03,384 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.00 vs. limit=5.971666666666667
2024-10-08 21:49:13,978 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.min_abs, batch_count=1946.6666666666667, ans=0.22920000000000001
2024-10-08 21:49:25,707 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=38.39 vs. limit=8.96
2024-10-08 21:49:28,119 INFO [train.py:1152] Epoch 1, batch 5850, loss[loss=0.4717, ctc_loss=0.5415, attn_decoder_loss=0.4542, over 4733.00 frames. ], tot_loss[loss=0.4377, ctc_loss=0.4845, attn_decoder_loss=0.4259, over 966531.89 frames. ], batch size: 45, lr: 4.00e-02,
2024-10-08 21:49:28,295 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=1950.0, ans=0.40859375
2024-10-08 21:49:33,573 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.51 vs. limit=5.975
2024-10-08 21:49:40,286 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.09 vs. limit=5.975
2024-10-08 21:49:43,119 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=28.25 vs. limit=8.965
2024-10-08 21:49:52,814 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.99 vs. limit=5.488333333333333
2024-10-08 21:49:56,534 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.06 vs. limit=5.976666666666667
2024-10-08 21:50:20,012 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.81 vs. limit=8.97
2024-10-08 21:50:30,633 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=1963.3333333333333, ans=0.2545833333333334
2024-10-08 21:50:32,970 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.70 vs. limit=8.9725
2024-10-08 21:50:47,934 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.229e+01 9.274e+01 1.022e+02 1.175e+02 1.979e+02, threshold=2.043e+02, percent-clipped=0.0
2024-10-08 21:50:48,000 INFO [train.py:1152] Epoch 1, batch 5900, loss[loss=0.4468, ctc_loss=0.4944, attn_decoder_loss=0.435, over 4808.00 frames. ], tot_loss[loss=0.435, ctc_loss=0.4805, attn_decoder_loss=0.4236, over 966715.50 frames. ], batch size: 34, lr: 3.99e-02,
2024-10-08 21:50:51,894 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.34 vs. limit=4.786666666666667
2024-10-08 21:51:04,266 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=1970.0, ans=0.40765625
2024-10-08 21:51:41,192 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=1976.6666666666667, ans=0.40734375
2024-10-08 21:51:41,784 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=31.93 vs. limit=8.9825
2024-10-08 21:51:46,572 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.12 vs. limit=8.9825
2024-10-08 21:51:57,870 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.37 vs. limit=3.297
2024-10-08 21:52:01,827 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=1980.0, ans=0.12575
2024-10-08 21:52:08,048 INFO [train.py:1152] Epoch 1, batch 5950, loss[loss=0.4271, ctc_loss=0.4696, attn_decoder_loss=0.4164, over 4823.00 frames. ], tot_loss[loss=0.4336, ctc_loss=0.478, attn_decoder_loss=0.4225, over 966052.83 frames. ], batch size: 34, lr: 3.98e-02,
2024-10-08 21:52:09,137 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=15.89 vs. limit=8.9875
2024-10-08 21:52:18,668 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.84 vs. limit=8.9875
2024-10-08 21:52:32,985 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.06 vs. limit=5.993333333333333
2024-10-08 21:52:35,697 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=1986.6666666666667, ans=0.8304666666666667
2024-10-08 21:52:52,657 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=7.42 vs. limit=8.24625
2024-10-08 21:52:58,092 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=1993.3333333333333, ans=0.08754166666666667
2024-10-08 21:53:28,417 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.264e+01 9.380e+01 1.052e+02 1.229e+02 2.557e+02, threshold=2.105e+02, percent-clipped=1.0
2024-10-08 21:53:28,480 INFO [train.py:1152] Epoch 1, batch 6000, loss[loss=0.4834, ctc_loss=0.5717, attn_decoder_loss=0.4613, over 4795.00 frames. ], tot_loss[loss=0.4336, ctc_loss=0.4779, attn_decoder_loss=0.4225, over 966652.71 frames. ], batch size: 49, lr: 3.98e-02,
2024-10-08 21:53:28,480 INFO [train.py:1175] Computing validation loss
2024-10-08 21:53:37,589 INFO [train.py:1184] Epoch 1, validation: loss=0.3233, ctc_loss=0.2612, attn_decoder_loss=0.3388, over 90464.00 frames.
2024-10-08 21:53:37,590 INFO [train.py:1185] Maximum memory allocated so far is 6613MB
2024-10-08 21:53:37,836 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=2000.0, ans=0.40625
2024-10-08 21:53:55,545 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.33 vs. limit=9.0025
2024-10-08 21:53:59,833 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2003.3333333333333, ans=0.27996666666666664
2024-10-08 21:54:10,166 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.84 vs. limit=5.501666666666667
2024-10-08 21:54:22,260 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2006.6666666666667, ans=0.4059375
2024-10-08 21:54:23,853 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=2010.0, ans=0.82965
2024-10-08 21:54:27,069 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2010.0, ans=0.40578125
2024-10-08 21:54:31,852 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2010.0, ans=0.2799
2024-10-08 21:54:35,918 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.21 vs. limit=5.5024999999999995
2024-10-08 21:54:45,615 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.28 vs. limit=5.503333333333333
2024-10-08 21:54:47,080 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.62 vs. limit=8.255
2024-10-08 21:54:51,123 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2013.3333333333333, ans=0.405625
2024-10-08 21:54:57,383 INFO [train.py:1152] Epoch 1, batch 6050, loss[loss=0.4284, ctc_loss=0.4347, attn_decoder_loss=0.4269, over 4813.00 frames. ], tot_loss[loss=0.4323, ctc_loss=0.4763, attn_decoder_loss=0.4213, over 966584.47 frames. ], batch size: 19, lr: 3.97e-02,
2024-10-08 21:55:15,834 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.23 vs. limit=9.015
2024-10-08 21:55:17,510 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.24 vs. limit=6.01
2024-10-08 21:55:25,358 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=14.65 vs. limit=6.01
2024-10-08 21:55:38,015 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2.whitening_limit, batch_count=2023.3333333333333, ans=6.011666666666667
2024-10-08 21:55:39,077 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2023.3333333333333, ans=0.27976666666666666
2024-10-08 21:55:41,394 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.17 vs. limit=8.25875
2024-10-08 21:56:17,196 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.728e+01 9.643e+01 1.092e+02 1.375e+02 3.527e+02, threshold=2.184e+02, percent-clipped=3.0
2024-10-08 21:56:17,262 INFO [train.py:1152] Epoch 1, batch 6100, loss[loss=0.4629, ctc_loss=0.521, attn_decoder_loss=0.4483, over 4787.00 frames. ], tot_loss[loss=0.4323, ctc_loss=0.4765, attn_decoder_loss=0.4213, over 966083.82 frames. ], batch size: 34, lr: 3.96e-02,
2024-10-08 21:56:23,777 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=2033.3333333333333, ans=0.12375
2024-10-08 21:56:29,166 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.79 vs. limit=9.025
2024-10-08 21:56:35,321 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=11.06 vs. limit=8.26375
2024-10-08 21:57:05,869 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.32 vs. limit=4.817333333333333
2024-10-08 21:57:13,536 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten.whitening_limit, batch_count=2043.3333333333333, ans=9.0325
2024-10-08 21:57:36,697 INFO [train.py:1152] Epoch 1, batch 6150, loss[loss=0.4548, ctc_loss=0.5118, attn_decoder_loss=0.4406, over 4829.00 frames. ], tot_loss[loss=0.4311, ctc_loss=0.4747, attn_decoder_loss=0.4202, over 966182.97 frames. ], batch size: 43, lr: 3.96e-02,
2024-10-08 21:57:40,701 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.05 vs. limit=5.5125
2024-10-08 21:57:41,723 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=2050.0, ans=0.82825
2024-10-08 21:57:47,225 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.77 vs. limit=9.0375
2024-10-08 21:58:00,897 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=2053.3333333333335, ans=0.123
2024-10-08 21:58:22,235 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=6.00 vs. limit=8.27125
2024-10-08 21:58:32,739 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=2060.0, ans=0.8279000000000001
2024-10-08 21:58:56,853 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.307e+01 9.384e+01 1.089e+02 1.250e+02 2.239e+02, threshold=2.179e+02, percent-clipped=1.0
2024-10-08 21:58:56,911 INFO [train.py:1152] Epoch 1, batch 6200, loss[loss=0.5088, ctc_loss=0.5721, attn_decoder_loss=0.4929, over 4786.00 frames. ], tot_loss[loss=0.4303, ctc_loss=0.4732, attn_decoder_loss=0.4196, over 966399.01 frames. ], batch size: 29, lr: 3.95e-02,
2024-10-08 21:59:30,350 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=2073.3333333333335, ans=6.295833333333333
2024-10-08 21:59:31,836 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2073.3333333333335, ans=0.27926666666666666
2024-10-08 21:59:44,549 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=2076.6666666666665, ans=0.12212500000000001
2024-10-08 21:59:46,233 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=2076.6666666666665, ans=0.8273166666666667
2024-10-08 22:00:05,212 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=2080.0, ans=0.40249999999999997
2024-10-08 22:00:05,922 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=8.88 vs. limit=8.28
2024-10-08 22:00:08,506 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=2080.0, ans=0.8272
2024-10-08 22:00:16,239 INFO [train.py:1152] Epoch 1, batch 6250, loss[loss=0.4329, ctc_loss=0.4853, attn_decoder_loss=0.4198, over 4731.00 frames. ], tot_loss[loss=0.4269, ctc_loss=0.4685, attn_decoder_loss=0.4165, over 966780.96 frames. ], batch size: 26, lr: 3.94e-02,
2024-10-08 22:00:19,653 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=2083.3333333333335, ans=0.1328125
2024-10-08 22:00:21,161 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 22:00:26,655 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.98 vs. limit=6.041666666666667
2024-10-08 22:00:30,849 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=2086.6666666666665, ans=0.12175000000000001
2024-10-08 22:00:31,676 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.81 vs. limit=5.5216666666666665
2024-10-08 22:00:49,869 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=2090.0, ans=0.08182500000000043
2024-10-08 22:00:55,095 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.84 vs. limit=9.067499999999999
2024-10-08 22:01:12,084 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=2093.3333333333335, ans=0.5
2024-10-08 22:01:12,241 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 22:01:12,249 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=2093.3333333333335, ans=0.401875
2024-10-08 22:01:12,794 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=16.72 vs. limit=9.07
2024-10-08 22:01:23,196 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=2096.6666666666665, ans=0.40171875
2024-10-08 22:01:28,652 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.68 vs. limit=5.524166666666667
2024-10-08 22:01:35,711 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.383e+01 9.038e+01 1.019e+02 1.160e+02 3.159e+02, threshold=2.037e+02, percent-clipped=4.0
2024-10-08 22:01:35,777 INFO [train.py:1152] Epoch 1, batch 6300, loss[loss=0.4299, ctc_loss=0.4611, attn_decoder_loss=0.4221, over 4978.00 frames. ], tot_loss[loss=0.4245, ctc_loss=0.4655, attn_decoder_loss=0.4143, over 966447.34 frames. ], batch size: 19, lr: 3.94e-02,
2024-10-08 22:01:38,251 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.49 vs. limit=6.05
2024-10-08 22:01:54,140 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.35 vs. limit=9.0775
2024-10-08 22:02:10,864 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=2106.6666666666665, ans=0.40125
2024-10-08 22:02:42,407 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2113.3333333333335, ans=0.27886666666666665
2024-10-08 22:02:54,924 INFO [train.py:1152] Epoch 1, batch 6350, loss[loss=0.4952, ctc_loss=0.578, attn_decoder_loss=0.4745, over 4817.00 frames. ], tot_loss[loss=0.4218, ctc_loss=0.4616, attn_decoder_loss=0.4118, over 966135.03 frames. ], batch size: 36, lr: 3.93e-02,
2024-10-08 22:03:05,142 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.90 vs. limit=6.058333333333334
2024-10-08 22:03:09,994 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=15.78 vs. limit=6.0600000000000005
2024-10-08 22:03:15,197 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module1.whiten, num_groups=1, num_channels=192, metric=4.60 vs. limit=8.295
2024-10-08 22:03:18,936 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=2120.0, ans=0.400625
2024-10-08 22:03:29,908 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.prob, batch_count=2123.3333333333335, ans=0.40046875
2024-10-08 22:03:34,693 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=2123.3333333333335, ans=0.08672916666666666
2024-10-08 22:03:51,106 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.24 vs. limit=6.0633333333333335
2024-10-08 22:03:52,183 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=7.594e-03
2024-10-08 22:03:56,059 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=4.86 vs. limit=4.850666666666666
2024-10-08 22:03:59,182 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=20.30 vs. limit=9.0975
2024-10-08 22:04:03,344 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=2130.0, ans=0.40015625
2024-10-08 22:04:08,078 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=2130.0, ans=0.40015625
2024-10-08 22:04:08,152 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=2130.0, ans=0.40015625
2024-10-08 22:04:10,332 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.22 vs. limit=9.0975
2024-10-08 22:04:10,430 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.68 vs. limit=8.29875
2024-10-08 22:04:14,421 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.589e+01 9.446e+01 1.046e+02 1.234e+02 1.633e+02, threshold=2.092e+02, percent-clipped=0.0
2024-10-08 22:04:14,482 INFO [train.py:1152] Epoch 1, batch 6400, loss[loss=0.4039, ctc_loss=0.4228, attn_decoder_loss=0.3992, over 4882.00 frames. ], tot_loss[loss=0.4202, ctc_loss=0.4595, attn_decoder_loss=0.4104, over 965893.85 frames. ], batch size: 23, lr: 3.92e-02,
2024-10-08 22:04:15,142 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=25.35 vs. limit=9.1
2024-10-08 22:04:16,619 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=13.60 vs. limit=6.066666666666666
2024-10-08 22:04:23,071 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.16 vs. limit=9.1
2024-10-08 22:04:24,000 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=2133.3333333333335, ans=0.4
2024-10-08 22:04:27,248 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2133.3333333333335, ans=0.4
2024-10-08 22:04:41,049 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.99 vs. limit=9.1025
2024-10-08 22:04:42,470 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.28 vs. limit=3.3205
2024-10-08 22:04:47,142 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.28 vs. limit=8.3025
2024-10-08 22:04:47,189 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.36 vs. limit=3.321
2024-10-08 22:04:48,405 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.38 vs. limit=5.535
2024-10-08 22:04:57,870 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=2140.0, ans=9.105
2024-10-08 22:05:00,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=2143.3333333333335, ans=0.39953125
2024-10-08 22:05:23,359 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.73 vs. limit=9.11
2024-10-08 22:05:27,531 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 22:05:29,786 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.04 vs. limit=4.858666666666666
2024-10-08 22:05:33,576 INFO [train.py:1152] Epoch 1, batch 6450, loss[loss=0.3983, ctc_loss=0.4439, attn_decoder_loss=0.3869, over 4740.00 frames. ], tot_loss[loss=0.4194, ctc_loss=0.4574, attn_decoder_loss=0.4098, over 965238.39 frames. ], batch size: 26, lr: 3.92e-02,
2024-10-08 22:05:44,855 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=2150.0, ans=0.82475
2024-10-08 22:05:51,928 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=49.34 vs. limit=9.115
2024-10-08 22:05:54,449 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=2153.3333333333335, ans=0.11925
2024-10-08 22:05:58,346 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.54 vs. limit=9.115
2024-10-08 22:06:05,541 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=2156.6666666666665, ans=0.2784333333333333
2024-10-08 22:06:11,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.max_abs, batch_count=2156.6666666666665, ans=6.347916666666666
2024-10-08 22:06:15,721 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=19.54 vs. limit=6.078333333333333
2024-10-08 22:06:17,477 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module2.whiten, num_groups=1, num_channels=192, metric=3.05 vs. limit=8.30875
2024-10-08 22:06:18,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=2156.6666666666665, ans=0.39890625
2024-10-08 22:06:48,578 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.95 vs. limit=8.31125
2024-10-08 22:06:48,729 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.17 vs. limit=9.1225
2024-10-08 22:06:50,621 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.97 vs. limit=9.1225
2024-10-08 22:06:52,550 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.420e+01 9.827e+01 1.089e+02 1.290e+02 2.216e+02, threshold=2.179e+02, percent-clipped=1.0
2024-10-08 22:06:52,614 INFO [train.py:1152] Epoch 1, batch 6500, loss[loss=0.4214, ctc_loss=0.4472, attn_decoder_loss=0.415, over 4735.00 frames. ], tot_loss[loss=0.4169, ctc_loss=0.4528, attn_decoder_loss=0.4079, over 964973.97 frames. ], batch size: 26, lr: 3.91e-02,
2024-10-08 22:07:10,911 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.76 vs. limit=9.1275
2024-10-08 22:07:20,326 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=20.22 vs. limit=9.1275
2024-10-08 22:07:22,910 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=2173.3333333333335, ans=0.1185
2024-10-08 22:07:23,907 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.09 vs. limit=9.13
2024-10-08 22:07:32,319 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=2173.3333333333335, ans=0.1185
2024-10-08 22:07:36,412 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.46 vs. limit=9.13
2024-10-08 22:07:44,204 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=34.12 vs. limit=9.1325
2024-10-08 22:07:55,359 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=24.06 vs. limit=9.135
2024-10-08 22:07:58,393 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=24.00 vs. limit=9.135
2024-10-08 22:08:00,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=2180.0, ans=0.3978125
2024-10-08 22:08:00,917 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=2180.0, ans=0.22749999999999998
2024-10-08 22:08:07,224 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2180.0, ans=0.2782
2024-10-08 22:08:11,741 INFO [train.py:1152] Epoch 1, batch 6550, loss[loss=0.3662, ctc_loss=0.3774, attn_decoder_loss=0.3634, over 4978.00 frames. ], tot_loss[loss=0.415, ctc_loss=0.4493, attn_decoder_loss=0.4064, over 964827.84 frames. ], batch size: 19, lr: 3.91e-02,
2024-10-08 22:08:17,509 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.86 vs. limit=8.31875
2024-10-08 22:08:20,580 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.40 vs. limit=9.1375
2024-10-08 22:08:35,648 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=2186.6666666666665, ans=0.050800000000000005
2024-10-08 22:08:40,364 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=2186.6666666666665, ans=0.127
2024-10-08 22:08:53,635 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.90 vs. limit=9.1425
2024-10-08 22:09:16,104 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.92 vs. limit=6.098333333333333
2024-10-08 22:09:22,699 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.94 vs. limit=6.098333333333333
2024-10-08 22:09:31,431 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.525e+01 9.230e+01 1.066e+02 1.244e+02 2.371e+02, threshold=2.132e+02, percent-clipped=2.0
2024-10-08 22:09:31,491 INFO [train.py:1152] Epoch 1, batch 6600, loss[loss=0.4502, ctc_loss=0.502, attn_decoder_loss=0.4372, over 4835.00 frames. ], tot_loss[loss=0.4147, ctc_loss=0.4479, attn_decoder_loss=0.4064, over 965321.62 frames. ], batch size: 23, lr: 3.90e-02,
2024-10-08 22:09:40,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=17.17 vs. limit=8.325
2024-10-08 22:09:53,707 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.63 vs. limit=5.550833333333333
2024-10-08 22:09:53,822 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.83 vs. limit=9.1525
2024-10-08 22:09:55,880 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=2203.3333333333335, ans=0.39671875
2024-10-08 22:09:55,971 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=2203.3333333333335, ans=0.39671875
2024-10-08 22:09:57,524 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=2203.3333333333335, ans=0.39671875
2024-10-08 22:10:01,473 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=20.41 vs. limit=9.1525
2024-10-08 22:10:02,410 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=2206.6666666666665, ans=0.8227666666666666
2024-10-08 22:10:09,005 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 22:10:28,252 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2210.0, ans=0.2779
2024-10-08 22:10:46,082 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=2213.3333333333335, ans=0.1255
2024-10-08 22:10:47,601 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=2213.3333333333335, ans=0.39625
2024-10-08 22:10:52,475 INFO [train.py:1152] Epoch 1, batch 6650, loss[loss=0.4131, ctc_loss=0.428, attn_decoder_loss=0.4094, over 4748.00 frames. ], tot_loss[loss=0.4139, ctc_loss=0.4456, attn_decoder_loss=0.4059, over 967033.23 frames. ], batch size: 20, lr: 3.89e-02,
2024-10-08 22:11:30,473 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.55 vs. limit=6.111666666666666
2024-10-08 22:11:33,627 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.53 vs. limit=5.555833333333333
2024-10-08 22:11:42,965 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.50 vs. limit=9.17
2024-10-08 22:11:52,176 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=2226.6666666666665, ans=0.395625
2024-10-08 22:11:57,807 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.whiten.whitening_limit, batch_count=2230.0, ans=4.892
2024-10-08 22:12:03,610 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2230.0, ans=0.39546875
2024-10-08 22:12:13,267 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.955e+01 9.721e+01 1.099e+02 1.256e+02 2.104e+02, threshold=2.198e+02, percent-clipped=0.0
2024-10-08 22:12:13,334 INFO [train.py:1152] Epoch 1, batch 6700, loss[loss=0.3881, ctc_loss=0.3929, attn_decoder_loss=0.387, over 4938.00 frames. ], tot_loss[loss=0.4106, ctc_loss=0.4401, attn_decoder_loss=0.4032, over 969213.20 frames. ], batch size: 20, lr: 3.89e-02,
2024-10-08 22:12:13,464 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=2233.3333333333335, ans=0.22766666666666666
2024-10-08 22:12:18,433 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=2233.3333333333335, ans=0.3953125
2024-10-08 22:12:22,166 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=32.89 vs. limit=9.175
2024-10-08 22:12:23,766 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.37 vs. limit=3.335
2024-10-08 22:12:28,009 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=2236.6666666666665, ans=0.39515625
2024-10-08 22:12:49,148 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=2240.0, ans=0.11599999999999999
2024-10-08 22:12:50,842 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=2240.0, ans=0.8216
2024-10-08 22:12:53,466 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.74 vs. limit=6.12
2024-10-08 22:13:27,326 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.81 vs. limit=9.185
2024-10-08 22:13:28,874 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.49 vs. limit=9.185
2024-10-08 22:13:31,425 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=2246.6666666666665, ans=0.12362500000000001
2024-10-08 22:13:34,527 INFO [train.py:1152] Epoch 1, batch 6750, loss[loss=0.3564, ctc_loss=0.3773, attn_decoder_loss=0.3511, over 4911.00 frames. ], tot_loss[loss=0.4041, ctc_loss=0.4319, attn_decoder_loss=0.3971, over 972265.95 frames. ], batch size: 19, lr: 3.88e-02,
2024-10-08 22:14:09,612 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.23 vs. limit=9.192499999999999
2024-10-08 22:14:12,898 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.65 vs. limit=9.192499999999999
2024-10-08 22:14:21,427 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.12 vs. limit=8.34625
2024-10-08 22:14:26,216 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=38.74 vs. limit=9.195
2024-10-08 22:14:30,365 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=2260.0, ans=0.8209
2024-10-08 22:14:56,533 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.378e+01 9.547e+01 1.102e+02 1.290e+02 3.596e+02, threshold=2.204e+02, percent-clipped=3.0
2024-10-08 22:14:56,594 INFO [train.py:1152] Epoch 1, batch 6800, loss[loss=0.3749, ctc_loss=0.3934, attn_decoder_loss=0.3703, over 4912.00 frames. ], tot_loss[loss=0.4011, ctc_loss=0.4269, attn_decoder_loss=0.3947, over 974591.32 frames. ], batch size: 19, lr: 3.87e-02,
2024-10-08 22:15:32,834 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=2273.3333333333335, ans=0.21583333333333332
2024-10-08 22:15:44,352 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=2276.6666666666665, ans=0.114625
2024-10-08 22:15:44,412 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=2276.6666666666665, ans=0.04288541666666667
2024-10-08 22:15:51,095 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=2276.6666666666665, ans=0.2154166666666667
2024-10-08 22:15:56,090 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=2276.6666666666665, ans=0.114625
2024-10-08 22:15:56,698 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.29 vs. limit=6.138333333333334
2024-10-08 22:16:02,488 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=2280.0, ans=0.8202
2024-10-08 22:16:06,914 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.38 vs. limit=6.14
2024-10-08 22:16:18,802 INFO [train.py:1152] Epoch 1, batch 6850, loss[loss=0.3882, ctc_loss=0.4186, attn_decoder_loss=0.3806, over 4978.00 frames. ], tot_loss[loss=0.3965, ctc_loss=0.4221, attn_decoder_loss=0.3901, over 978927.87 frames. ], batch size: 19, lr: 3.87e-02,
2024-10-08 22:16:20,495 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_False/epoch-1.pt
2024-10-08 22:16:36,796 INFO [train.py:1152] Epoch 2, batch 0, loss[loss=0.3675, ctc_loss=0.4007, attn_decoder_loss=0.3592, over 4855.00 frames. ], tot_loss[loss=0.3675, ctc_loss=0.4007, attn_decoder_loss=0.3592, over 4855.00 frames. ], batch size: 19, lr: 3.79e-02,
2024-10-08 22:16:36,796 INFO [train.py:1175] Computing validation loss
2024-10-08 22:16:42,274 INFO [zipformer.py:1858] name=encoder.encoders.3.encoder.layers.3.self_attn_weights, attn_weights_entropy = tensor([2.6734, 2.8276, 3.1456, 3.1093, 3.0796, 2.8380, 2.7454, 3.2924],
       device='cuda:0')
2024-10-08 22:16:44,978 INFO [train.py:1184] Epoch 2, validation: loss=0.3289, ctc_loss=0.2709, attn_decoder_loss=0.3434, over 90464.00 frames.
2024-10-08 22:16:44,979 INFO [train.py:1185] Maximum memory allocated so far is 6613MB
2024-10-08 22:16:54,845 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.52 vs. limit=6.1419999999999995
2024-10-08 22:17:00,630 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=21.18 vs. limit=9.2155
2024-10-08 22:17:02,008 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=6.58 vs. limit=8.35775
2024-10-08 22:17:13,863 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=2290.6666666666665, ans=0.04846
2024-10-08 22:17:14,489 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=5.74 vs. limit=6.145333333333333
2024-10-08 22:17:15,935 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.07 vs. limit=8.359
2024-10-08 22:17:25,342 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.55 vs. limit=8.359
2024-10-08 22:17:28,037 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=2290.6666666666665, ans=0.5
2024-10-08 22:17:28,071 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=2290.6666666666665, ans=0.392625
2024-10-08 22:17:32,706 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=2294.0, ans=0.39246875000000003
2024-10-08 22:17:33,546 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.91 vs. limit=8.36025
2024-10-08 22:17:35,080 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.73 vs. limit=9.2205
2024-10-08 22:17:52,500 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.26 vs. limit=4.918933333333333
2024-10-08 22:17:53,876 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.36 vs. limit=6.148666666666667
2024-10-08 22:17:57,259 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=512, metric=7.63 vs. limit=8.3615
2024-10-08 22:17:59,455 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.394e+01 9.011e+01 1.050e+02 1.268e+02 2.499e+02, threshold=2.101e+02, percent-clipped=1.0
2024-10-08 22:18:02,747 INFO [train.py:1152] Epoch 2, batch 50, loss[loss=0.3476, ctc_loss=0.3537, attn_decoder_loss=0.346, over 4909.00 frames. ], tot_loss[loss=0.421, ctc_loss=0.4676, attn_decoder_loss=0.4093, over 217758.01 frames. ], batch size: 19, lr: 3.79e-02,
2024-10-08 22:18:17,899 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.18 vs. limit=9.228
2024-10-08 22:18:20,719 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=2304.0, ans=0.04816
2024-10-08 22:18:21,092 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.84 vs. limit=9.228
2024-10-08 22:18:21,539 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=2.98 vs. limit=4.9216
2024-10-08 22:18:26,625 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=15.76 vs. limit=9.228
2024-10-08 22:18:33,961 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.47 vs. limit=3.3461
2024-10-08 22:18:34,286 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.66 vs. limit=9.2305
2024-10-08 22:18:34,886 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=2307.3333333333335, ans=0.39184375
2024-10-08 22:18:35,672 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.65 vs. limit=8.36525
2024-10-08 22:18:48,278 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.47 vs. limit=5.576833333333333
2024-10-08 22:18:52,424 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=2310.6666666666665, ans=0.11335
2024-10-08 22:19:17,034 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.whiten.whitening_limit, batch_count=2314.0, ans=4.9256
2024-10-08 22:19:22,719 INFO [train.py:1152] Epoch 2, batch 100, loss[loss=0.4219, ctc_loss=0.451, attn_decoder_loss=0.4146, over 4755.00 frames. ], tot_loss[loss=0.4261, ctc_loss=0.471, attn_decoder_loss=0.4149, over 383327.18 frames. ], batch size: 19, lr: 3.78e-02,
2024-10-08 22:19:22,857 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=2317.3333333333335, ans=0.39137500000000003
2024-10-08 22:19:49,017 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.99 vs. limit=4.928266666666667
2024-10-08 22:19:56,431 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=2324.0, ans=0.11284999999999999
2024-10-08 22:20:01,940 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.23 vs. limit=4.9296
2024-10-08 22:20:13,438 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.41 vs. limit=5.581833333333334
2024-10-08 22:20:14,888 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.97 vs. limit=9.2455
2024-10-08 22:20:27,764 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.46 vs. limit=3.3496
2024-10-08 22:20:32,472 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.06 vs. limit=6.165333333333333
2024-10-08 22:20:39,871 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.089e+01 9.566e+01 1.099e+02 1.355e+02 2.656e+02, threshold=2.197e+02, percent-clipped=2.0
2024-10-08 22:20:43,086 INFO [train.py:1152] Epoch 2, batch 150, loss[loss=0.4149, ctc_loss=0.4436, attn_decoder_loss=0.4077, over 4908.00 frames. ], tot_loss[loss=0.4176, ctc_loss=0.4572, attn_decoder_loss=0.4077, over 513287.79 frames. ], batch size: 19, lr: 3.77e-02,
2024-10-08 22:20:54,498 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=2334.0, ans=0.39059375
2024-10-08 22:21:00,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=2337.3333333333335, ans=0.3904375
2024-10-08 22:21:07,629 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.51 vs. limit=3.3506
2024-10-08 22:21:10,292 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=2337.3333333333335, ans=0.3904375
2024-10-08 22:21:11,123 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten.whitening_limit, batch_count=2337.3333333333335, ans=8.3765
2024-10-08 22:21:31,169 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2344.0, ans=0.390125
2024-10-08 22:21:44,855 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.70 vs. limit=9.258
2024-10-08 22:21:53,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.min_positive, batch_count=2347.3333333333335, ans=0.04266458333333334
2024-10-08 22:21:54,605 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.82 vs. limit=5.586833333333334
2024-10-08 22:22:02,726 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.04 vs. limit=9.263
2024-10-08 22:22:03,372 INFO [train.py:1152] Epoch 2, batch 200, loss[loss=0.4914, ctc_loss=0.546, attn_decoder_loss=0.4777, over 4761.00 frames. ], tot_loss[loss=0.4142, ctc_loss=0.4511, attn_decoder_loss=0.405, over 613727.61 frames. ], batch size: 45, lr: 3.77e-02,
2024-10-08 22:22:31,212 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.44 vs. limit=9.2655
2024-10-08 22:22:33,899 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=2357.3333333333335, ans=0.3895
2024-10-08 22:22:42,435 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.42 vs. limit=3.3536
2024-10-08 22:22:42,675 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.95 vs. limit=6.1786666666666665
2024-10-08 22:23:12,285 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=2364.0, ans=0.11134999999999999
2024-10-08 22:23:19,928 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.301e+01 9.281e+01 1.058e+02 1.169e+02 1.755e+02, threshold=2.117e+02, percent-clipped=0.0
2024-10-08 22:23:21,756 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.min_positive, batch_count=2367.3333333333335, ans=0.22632666666666668
2024-10-08 22:23:23,080 INFO [train.py:1152] Epoch 2, batch 250, loss[loss=0.4029, ctc_loss=0.4554, attn_decoder_loss=0.3898, over 4832.00 frames. ], tot_loss[loss=0.4134, ctc_loss=0.4501, attn_decoder_loss=0.4042, over 692427.42 frames. ], batch size: 38, lr: 3.76e-02,
2024-10-08 22:23:25,486 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.89 vs. limit=5.591833333333334
2024-10-08 22:23:36,932 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.51 vs. limit=9.275500000000001
2024-10-08 22:23:43,968 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=2370.6666666666665, ans=0.23556
2024-10-08 22:23:48,548 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=2370.6666666666665, ans=0.04666
2024-10-08 22:23:50,934 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.85 vs. limit=6.185333333333333
2024-10-08 22:23:51,364 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=13.91 vs. limit=9.278
2024-10-08 22:23:59,833 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=2374.0, ans=0.38871875
2024-10-08 22:24:06,256 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=2374.0, ans=0.11097499999999999
2024-10-08 22:24:09,463 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=2377.3333333333335, ans=0.23566
2024-10-08 22:24:14,053 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=2377.3333333333335, ans=0.046509999999999996
2024-10-08 22:24:26,522 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2380.6666666666665, ans=0.38840625
2024-10-08 22:24:42,004 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten.whitening_limit, batch_count=2384.0, ans=8.394
2024-10-08 22:24:42,812 INFO [train.py:1152] Epoch 2, batch 300, loss[loss=0.4047, ctc_loss=0.4411, attn_decoder_loss=0.3956, over 4786.00 frames. ], tot_loss[loss=0.4102, ctc_loss=0.4457, attn_decoder_loss=0.4013, over 752845.68 frames. ], batch size: 32, lr: 3.75e-02,
2024-10-08 22:24:55,519 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=2384.0, ans=0.042550000000000004
2024-10-08 22:25:00,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.33 vs. limit=9.2905
2024-10-08 22:25:08,185 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=2387.3333333333335, ans=0.20158333333333334
2024-10-08 22:25:24,605 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.59 vs. limit=6.195333333333333
2024-10-08 22:25:27,231 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer_ff2.min_abs, batch_count=2390.6666666666665, ans=0.05976666666666666
2024-10-08 22:25:30,449 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=2394.0, ans=0.81621
2024-10-08 22:25:50,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=2397.3333333333335, ans=0.11009999999999999
2024-10-08 22:25:51,661 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.78 vs. limit=6.198666666666667
2024-10-08 22:25:58,764 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.284e+01 8.974e+01 9.964e+01 1.157e+02 2.132e+02, threshold=1.993e+02, percent-clipped=2.0
2024-10-08 22:26:01,854 INFO [train.py:1152] Epoch 2, batch 350, loss[loss=0.3985, ctc_loss=0.404, attn_decoder_loss=0.3971, over 4883.00 frames. ], tot_loss[loss=0.409, ctc_loss=0.4443, attn_decoder_loss=0.4002, over 800170.97 frames. ], batch size: 19, lr: 3.75e-02,
2024-10-08 22:26:08,893 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.35 vs. limit=9.3005
2024-10-08 22:26:10,001 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.min_abs, batch_count=2400.6666666666665, ans=0.23601
2024-10-08 22:26:20,957 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=2404.0, ans=0.10984999999999999
2024-10-08 22:26:31,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=2407.3333333333335, ans=0.5
2024-10-08 22:26:33,474 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=2407.3333333333335, ans=0.8157433333333334
2024-10-08 22:26:59,395 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.98 vs. limit=9.308
2024-10-08 22:27:05,902 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.07 vs. limit=8.40525
2024-10-08 22:27:12,012 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=24.25 vs. limit=9.3105
2024-10-08 22:27:19,465 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2417.3333333333335, ans=0.27582666666666666
2024-10-08 22:27:20,865 INFO [train.py:1152] Epoch 2, batch 400, loss[loss=0.4026, ctc_loss=0.4322, attn_decoder_loss=0.3952, over 4879.00 frames. ], tot_loss[loss=0.4081, ctc_loss=0.4419, attn_decoder_loss=0.3997, over 836930.10 frames. ], batch size: 22, lr: 3.74e-02,
2024-10-08 22:27:27,752 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.80 vs. limit=9.313
2024-10-08 22:27:46,185 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=2420.6666666666665, ans=0.8152766666666666
2024-10-08 22:27:47,738 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2420.6666666666665, ans=0.38653125
2024-10-08 22:27:49,270 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 22:28:04,852 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.91 vs. limit=6.212
2024-10-08 22:28:11,829 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=2427.3333333333335, ans=0.10897499999999999
2024-10-08 22:28:18,560 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.71 vs. limit=9.3205
2024-10-08 22:28:24,489 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.max_positive, batch_count=2430.6666666666665, ans=0.7743066666666667
2024-10-08 22:28:33,981 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=2430.6666666666665, ans=0.22569333333333333
2024-10-08 22:28:37,032 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.337e+01 9.467e+01 1.046e+02 1.168e+02 1.975e+02, threshold=2.092e+02, percent-clipped=0.0
2024-10-08 22:28:38,939 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=2434.0, ans=0.38590625
2024-10-08 22:28:40,287 INFO [train.py:1152] Epoch 2, batch 450, loss[loss=0.4084, ctc_loss=0.4242, attn_decoder_loss=0.4044, over 4871.00 frames. ], tot_loss[loss=0.4074, ctc_loss=0.4396, attn_decoder_loss=0.3993, over 865484.37 frames. ], batch size: 23, lr: 3.73e-02,
2024-10-08 22:28:50,709 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.74 vs. limit=3.3651
2024-10-08 22:28:51,462 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=2434.0, ans=0.10872499999999999
2024-10-08 22:28:52,163 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.33 vs. limit=9.3255
2024-10-08 22:28:53,002 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2434.0, ans=0.27566
2024-10-08 22:29:00,337 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.09 vs. limit=9.328
2024-10-08 22:29:32,045 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=10.61 vs. limit=9.333
2024-10-08 22:29:39,068 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=2444.0, ans=0.5
2024-10-08 22:29:39,102 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2444.0, ans=0.27555999999999997
2024-10-08 22:29:42,836 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=27.14 vs. limit=9.3355
2024-10-08 22:29:47,016 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=2447.3333333333335, ans=0.8143433333333333
2024-10-08 22:29:54,886 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.min_positive, batch_count=2447.3333333333335, ans=0.04235208333333333
2024-10-08 22:29:55,806 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.42 vs. limit=8.41775
2024-10-08 22:29:58,030 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2450.6666666666665, ans=0.2754933333333333
2024-10-08 22:29:59,440 INFO [train.py:1152] Epoch 2, batch 500, loss[loss=0.3819, ctc_loss=0.4005, attn_decoder_loss=0.3773, over 4837.00 frames. ], tot_loss[loss=0.4052, ctc_loss=0.4363, attn_decoder_loss=0.3974, over 888098.43 frames. ], batch size: 34, lr: 3.73e-02,
2024-10-08 22:30:07,451 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=2450.6666666666665, ans=0.385125
2024-10-08 22:30:17,676 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.28 vs. limit=9.3405
2024-10-08 22:30:21,812 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.min_positive, batch_count=2454.0, ans=0.04233125
2024-10-08 22:30:34,592 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=2457.3333333333335, ans=0.08464166666666667
2024-10-08 22:30:38,155 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.48 vs. limit=9.343
2024-10-08 22:30:42,353 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=2457.3333333333335, ans=0.3848125
2024-10-08 22:30:49,151 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.58 vs. limit=9.3455
2024-10-08 22:30:54,841 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=2460.6666666666665, ans=0.38465625000000003
2024-10-08 22:30:56,326 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=2460.6666666666665, ans=0.38465625000000003
2024-10-08 22:30:56,339 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2460.6666666666665, ans=0.2753933333333333
2024-10-08 22:31:10,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=2464.0, ans=0.192
2024-10-08 22:31:15,115 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.725e+01 9.424e+01 1.127e+02 1.276e+02 2.172e+02, threshold=2.255e+02, percent-clipped=1.0
2024-10-08 22:31:16,861 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=2467.3333333333335, ans=0.38434375
2024-10-08 22:31:18,281 INFO [train.py:1152] Epoch 2, batch 550, loss[loss=0.4114, ctc_loss=0.4247, attn_decoder_loss=0.408, over 4790.00 frames. ], tot_loss[loss=0.4062, ctc_loss=0.4377, attn_decoder_loss=0.3984, over 905465.58 frames. ], batch size: 40, lr: 3.72e-02,
2024-10-08 22:31:26,265 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2467.3333333333335, ans=0.38434375
2024-10-08 22:31:30,509 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.12 vs. limit=5.616833333333333
2024-10-08 22:31:48,090 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.min_abs, batch_count=2474.0, ans=0.23711000000000002
2024-10-08 22:31:48,514 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.33 vs. limit=8.42775
2024-10-08 22:31:59,012 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=2474.0, ans=0.38403125
2024-10-08 22:32:14,831 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=2477.3333333333335, ans=0.10709999999999999
2024-10-08 22:32:30,575 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2480.6666666666665, ans=0.27519333333333335
2024-10-08 22:32:36,753 INFO [train.py:1152] Epoch 2, batch 600, loss[loss=0.3863, ctc_loss=0.41, attn_decoder_loss=0.3804, over 4810.00 frames. ], tot_loss[loss=0.4055, ctc_loss=0.4362, attn_decoder_loss=0.3978, over 919259.30 frames. ], batch size: 38, lr: 3.72e-02,
2024-10-08 22:32:38,022 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=6.96 vs. limit=8.4315
2024-10-08 22:32:38,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=2484.0, ans=0.81306
2024-10-08 22:32:39,148 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.55 vs. limit=9.363
2024-10-08 22:32:55,787 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=2487.3333333333335, ans=0.10672499999999999
2024-10-08 22:33:10,229 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=2490.6666666666665, ans=0.38325
2024-10-08 22:33:16,043 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=2490.6666666666665, ans=9.368
2024-10-08 22:33:33,345 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.50 vs. limit=6.247
2024-10-08 22:33:37,859 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.74 vs. limit=5.6235
2024-10-08 22:33:37,972 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.98 vs. limit=9.3705
2024-10-08 22:33:40,436 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2497.3333333333335, ans=0.3829375
2024-10-08 22:33:52,735 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.903e+01 9.224e+01 1.065e+02 1.294e+02 2.528e+02, threshold=2.131e+02, percent-clipped=1.0
2024-10-08 22:33:55,935 INFO [train.py:1152] Epoch 2, batch 650, loss[loss=0.4024, ctc_loss=0.4561, attn_decoder_loss=0.3889, over 4843.00 frames. ], tot_loss[loss=0.4033, ctc_loss=0.4327, attn_decoder_loss=0.3959, over 930188.13 frames. ], batch size: 21, lr: 3.71e-02,
2024-10-08 22:33:59,820 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.34 vs. limit=3.3750999999999998
2024-10-08 22:34:02,995 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.85 vs. limit=9.3755
2024-10-08 22:34:03,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=2500.6666666666665, ans=0.2749933333333333
2024-10-08 22:34:04,769 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.51 vs. limit=6.250333333333334
2024-10-08 22:34:15,633 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.55 vs. limit=9.378
2024-10-08 22:34:20,207 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.92 vs. limit=9.378
2024-10-08 22:34:22,873 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2504.0, ans=0.382625
2024-10-08 22:34:37,050 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=2507.3333333333335, ans=0.38246875
2024-10-08 22:34:37,480 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.59 vs. limit=9.3805
2024-10-08 22:35:02,258 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=2514.0, ans=0.38215625
2024-10-08 22:35:07,859 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.48 vs. limit=3.3771
2024-10-08 22:35:09,117 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.55 vs. limit=8.44275
2024-10-08 22:35:14,542 INFO [train.py:1152] Epoch 2, batch 700, loss[loss=0.429, ctc_loss=0.4468, attn_decoder_loss=0.4246, over 4740.00 frames. ], tot_loss[loss=0.403, ctc_loss=0.4322, attn_decoder_loss=0.3957, over 937971.52 frames. ], batch size: 19, lr: 3.70e-02,
2024-10-08 22:35:21,047 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=2517.3333333333335, ans=0.043359999999999996
2024-10-08 22:35:53,324 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.66 vs. limit=9.393
2024-10-08 22:36:07,447 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.91 vs. limit=9.3955
2024-10-08 22:36:08,710 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=2527.3333333333335, ans=0.38153125
2024-10-08 22:36:12,454 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.24 vs. limit=8.44775
2024-10-08 22:36:22,988 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=2530.6666666666665, ans=0.08418333333333333
2024-10-08 22:36:29,563 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=2530.6666666666665, ans=0.08418333333333333
2024-10-08 22:36:30,853 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.510e+01 9.583e+01 1.099e+02 1.257e+02 2.638e+02, threshold=2.198e+02, percent-clipped=1.0
2024-10-08 22:36:34,054 INFO [train.py:1152] Epoch 2, batch 750, loss[loss=0.388, ctc_loss=0.4205, attn_decoder_loss=0.3798, over 4894.00 frames. ], tot_loss[loss=0.4019, ctc_loss=0.4306, attn_decoder_loss=0.3947, over 944733.25 frames. ], batch size: 22, lr: 3.70e-02,
2024-10-08 22:37:08,957 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2540.6666666666665, ans=0.2745933333333333
2024-10-08 22:37:10,565 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.max_abs, batch_count=2540.6666666666665, ans=6.587916666666667
2024-10-08 22:37:14,740 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=2540.6666666666665, ans=0.18534470360261346
2024-10-08 22:37:16,646 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.34 vs. limit=9.4055
2024-10-08 22:37:17,158 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.50 vs. limit=6.270333333333333
2024-10-08 22:37:27,827 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.82 vs. limit=9.408
2024-10-08 22:37:31,634 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=2544.0, ans=0.04276
2024-10-08 22:37:38,402 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.51 vs. limit=3.3821
2024-10-08 22:37:38,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.97 vs. limit=8.45525
2024-10-08 22:37:39,270 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=2547.3333333333335, ans=0.38059375
2024-10-08 22:37:41,480 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.50 vs. limit=5.018933333333333
2024-10-08 22:37:45,671 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2547.3333333333335, ans=0.27452666666666664
2024-10-08 22:37:53,356 INFO [train.py:1152] Epoch 2, batch 800, loss[loss=0.3501, ctc_loss=0.3717, attn_decoder_loss=0.3447, over 4852.00 frames. ], tot_loss[loss=0.4009, ctc_loss=0.4289, attn_decoder_loss=0.3938, over 949630.50 frames. ], batch size: 19, lr: 3.69e-02,
2024-10-08 22:38:23,154 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.96 vs. limit=5.6385
2024-10-08 22:38:27,837 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2.whitening_limit, batch_count=2557.3333333333335, ans=6.278666666666667
2024-10-08 22:38:30,104 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=2557.3333333333335, ans=0.10614999999999997
2024-10-08 22:38:33,737 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=17.78 vs. limit=6.278666666666667
2024-10-08 22:38:50,475 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=2560.6666666666665, ans=0.37996874999999997
2024-10-08 22:38:52,994 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.60 vs. limit=8.46025
2024-10-08 22:39:09,394 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.297e+01 9.465e+01 1.091e+02 1.260e+02 4.146e+02, threshold=2.182e+02, percent-clipped=3.0
2024-10-08 22:39:12,712 INFO [train.py:1152] Epoch 2, batch 850, loss[loss=0.3903, ctc_loss=0.413, attn_decoder_loss=0.3846, over 4804.00 frames. ], tot_loss[loss=0.3981, ctc_loss=0.4249, attn_decoder_loss=0.3914, over 953989.62 frames. ], batch size: 29, lr: 3.69e-02,
2024-10-08 22:39:15,966 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=2567.3333333333335, ans=0.37965625000000003
2024-10-08 22:39:23,284 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten.whitening_limit, batch_count=2567.3333333333335, ans=9.4255
2024-10-08 22:39:27,938 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.41 vs. limit=3.3856
2024-10-08 22:39:56,046 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=2574.0, ans=0.80991
2024-10-08 22:40:07,592 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=2577.3333333333335, ans=9.433
2024-10-08 22:40:21,520 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=2580.6666666666665, ans=0.5
2024-10-08 22:40:27,160 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.65 vs. limit=9.4355
2024-10-08 22:40:32,475 INFO [train.py:1152] Epoch 2, batch 900, loss[loss=0.3885, ctc_loss=0.401, attn_decoder_loss=0.3854, over 4855.00 frames. ], tot_loss[loss=0.3978, ctc_loss=0.4241, attn_decoder_loss=0.3912, over 956955.00 frames. ], batch size: 19, lr: 3.68e-02,
2024-10-08 22:40:36,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=13.38 vs. limit=8.469
2024-10-08 22:40:46,252 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.35 vs. limit=9.438
2024-10-08 22:40:54,533 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.33 vs. limit=6.293666666666667
2024-10-08 22:41:03,928 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.03 vs. limit=5.036266666666666
2024-10-08 22:41:05,492 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.59 vs. limit=9.443
2024-10-08 22:41:16,649 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.64 vs. limit=5.647666666666667
2024-10-08 22:41:20,891 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=2594.0, ans=0.37840625
2024-10-08 22:41:23,979 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=2594.0, ans=0.37840625
2024-10-08 22:41:31,048 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.48 vs. limit=9.4455
2024-10-08 22:41:49,354 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.782e+01 9.476e+01 1.027e+02 1.160e+02 2.556e+02, threshold=2.053e+02, percent-clipped=3.0
2024-10-08 22:41:52,537 INFO [train.py:1152] Epoch 2, batch 950, loss[loss=0.3549, ctc_loss=0.3568, attn_decoder_loss=0.3545, over 4817.00 frames. ], tot_loss[loss=0.3977, ctc_loss=0.423, attn_decoder_loss=0.3914, over 958786.93 frames. ], batch size: 19, lr: 3.67e-02,
2024-10-08 22:41:52,753 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=2600.6666666666665, ans=0.8089766666666667
2024-10-08 22:42:20,246 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=24.03 vs. limit=6.302
2024-10-08 22:42:34,049 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=2607.3333333333335, ans=0.8087433333333334
2024-10-08 22:42:48,404 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=2610.6666666666665, ans=0.041260000000000005
2024-10-08 22:42:50,102 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=2610.6666666666665, ans=0.041841666666666666
2024-10-08 22:42:50,543 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.66 vs. limit=9.458
2024-10-08 22:42:51,677 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=2610.6666666666665, ans=0.10210000000000001
2024-10-08 22:43:00,620 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.61 vs. limit=3.3921
2024-10-08 22:43:03,217 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=21.73 vs. limit=9.4605
2024-10-08 22:43:12,325 INFO [train.py:1152] Epoch 2, batch 1000, loss[loss=0.3487, ctc_loss=0.3363, attn_decoder_loss=0.3517, over 4946.00 frames. ], tot_loss[loss=0.3979, ctc_loss=0.4234, attn_decoder_loss=0.3916, over 960713.58 frames. ], batch size: 20, lr: 3.67e-02,
2024-10-08 22:43:15,692 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=2617.3333333333335, ans=0.23926000000000003
2024-10-08 22:43:42,163 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.88 vs. limit=5.655166666666666
2024-10-08 22:44:16,114 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=5.48 vs. limit=8.4865
2024-10-08 22:44:16,690 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=2630.6666666666665, ans=0.3766875
2024-10-08 22:44:29,231 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.679e+01 9.453e+01 1.070e+02 1.270e+02 2.384e+02, threshold=2.140e+02, percent-clipped=2.0
2024-10-08 22:44:29,992 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.11 vs. limit=9.473
2024-10-08 22:44:32,503 INFO [train.py:1152] Epoch 2, batch 1050, loss[loss=0.3792, ctc_loss=0.3997, attn_decoder_loss=0.3741, over 4823.00 frames. ], tot_loss[loss=0.3956, ctc_loss=0.4195, attn_decoder_loss=0.3897, over 962809.92 frames. ], batch size: 25, lr: 3.66e-02,
2024-10-08 22:44:37,528 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=2634.0, ans=0.37653125
2024-10-08 22:45:06,286 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=2640.6666666666665, ans=0.37621875
2024-10-08 22:45:15,305 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.36 vs. limit=5.660166666666667
2024-10-08 22:45:19,867 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.52 vs. limit=3.3966
2024-10-08 22:45:42,507 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=13.98 vs. limit=9.4855
2024-10-08 22:45:52,577 INFO [train.py:1152] Epoch 2, batch 1100, loss[loss=0.3747, ctc_loss=0.3851, attn_decoder_loss=0.3721, over 4862.00 frames. ], tot_loss[loss=0.3946, ctc_loss=0.419, attn_decoder_loss=0.3885, over 964135.11 frames. ], batch size: 20, lr: 3.65e-02,
2024-10-08 22:46:00,656 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=2650.6666666666665, ans=0.1686666666666667
2024-10-08 22:46:10,945 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=9.81 vs. limit=8.49525
2024-10-08 22:46:11,910 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2654.0, ans=0.27346
2024-10-08 22:46:19,017 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.38 vs. limit=8.49525
2024-10-08 22:46:37,461 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=2657.3333333333335, ans=0.27342666666666665
2024-10-08 22:46:37,876 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=24.18 vs. limit=9.493
2024-10-08 22:47:02,301 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.66 vs. limit=5.666
2024-10-08 22:47:04,612 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=2664.0, ans=0.375125
2024-10-08 22:47:07,993 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_False/checkpoint-8000.pt
2024-10-08 22:47:10,591 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.140e+01 9.538e+01 1.051e+02 1.159e+02 1.909e+02, threshold=2.103e+02, percent-clipped=0.0
2024-10-08 22:47:13,289 INFO [train.py:1152] Epoch 2, batch 1150, loss[loss=0.3533, ctc_loss=0.355, attn_decoder_loss=0.3528, over 4865.00 frames. ], tot_loss[loss=0.3936, ctc_loss=0.4172, attn_decoder_loss=0.3877, over 964398.55 frames. ], batch size: 20, lr: 3.65e-02,
2024-10-08 22:47:21,113 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=2667.3333333333335, ans=0.04166458333333334
2024-10-08 22:47:24,092 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=2667.3333333333335, ans=0.099975
2024-10-08 22:47:31,834 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=2670.6666666666665, ans=0.09985000000000001
2024-10-08 22:47:41,757 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.75 vs. limit=9.503
2024-10-08 22:47:42,052 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.84 vs. limit=8.5015
2024-10-08 22:47:52,431 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=2674.0, ans=0.37465625
2024-10-08 22:48:00,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=2677.3333333333335, ans=0.3745
2024-10-08 22:48:05,049 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=2677.3333333333335, ans=0.3745
2024-10-08 22:48:05,605 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.00 vs. limit=5.0709333333333335
2024-10-08 22:48:27,902 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=2680.6666666666665, ans=9.5105
2024-10-08 22:48:30,207 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2684.0, ans=0.3741875
2024-10-08 22:48:31,634 INFO [train.py:1152] Epoch 2, batch 1200, loss[loss=0.4063, ctc_loss=0.4424, attn_decoder_loss=0.3972, over 4808.00 frames. ], tot_loss[loss=0.3931, ctc_loss=0.4165, attn_decoder_loss=0.3872, over 964262.33 frames. ], batch size: 25, lr: 3.64e-02,
2024-10-08 22:48:33,376 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=2684.0, ans=0.09935
2024-10-08 22:48:58,941 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=2687.3333333333335, ans=0.37403125000000004
2024-10-08 22:48:59,409 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.44 vs. limit=5.074933333333333
2024-10-08 22:49:01,475 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.42 vs. limit=3.4031000000000002
2024-10-08 22:49:10,771 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.58 vs. limit=9.518
2024-10-08 22:49:19,484 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.22 vs. limit=5.6735
2024-10-08 22:49:38,454 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2697.3333333333335, ans=0.3735625
2024-10-08 22:49:43,091 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=2697.3333333333335, ans=0.3735625
2024-10-08 22:49:47,474 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.219e+01 9.668e+01 1.053e+02 1.207e+02 1.565e+02, threshold=2.107e+02, percent-clipped=0.0
2024-10-08 22:49:50,596 INFO [train.py:1152] Epoch 2, batch 1250, loss[loss=0.4133, ctc_loss=0.437, attn_decoder_loss=0.4074, over 4745.00 frames. ], tot_loss[loss=0.3938, ctc_loss=0.4169, attn_decoder_loss=0.388, over 964185.52 frames. ], batch size: 32, lr: 3.64e-02,
2024-10-08 22:50:12,741 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2704.0, ans=0.27296
2024-10-08 22:50:29,364 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.82 vs. limit=9.5305
2024-10-08 22:50:38,005 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=2710.6666666666665, ans=0.5
2024-10-08 22:50:48,592 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.87 vs. limit=6.355333333333333
2024-10-08 22:50:56,269 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.45 vs. limit=5.6785
2024-10-08 22:51:03,720 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=2714.0, ans=0.37278125
2024-10-08 22:51:07,437 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.57 vs. limit=5.0855999999999995
2024-10-08 22:51:09,797 INFO [train.py:1152] Epoch 2, batch 1300, loss[loss=0.4179, ctc_loss=0.4532, attn_decoder_loss=0.409, over 4826.00 frames. ], tot_loss[loss=0.3938, ctc_loss=0.4152, attn_decoder_loss=0.3884, over 965636.55 frames. ], batch size: 43, lr: 3.63e-02,
2024-10-08 22:51:09,908 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=2717.3333333333335, ans=0.09809999999999999
2024-10-08 22:51:13,011 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=2717.3333333333335, ans=0.8048933333333333
2024-10-08 22:51:33,698 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2720.6666666666665, ans=0.37246875
2024-10-08 22:51:34,064 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.13 vs. limit=8.52025
2024-10-08 22:51:36,727 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=2720.6666666666665, ans=0.37246875
2024-10-08 22:51:38,258 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.min_positive, batch_count=2720.6666666666665, ans=0.08299583333333334
2024-10-08 22:51:39,065 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=25.13 vs. limit=9.5405
2024-10-08 22:51:39,956 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=2724.0, ans=0.03871
2024-10-08 22:51:43,246 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=2724.0, ans=0.15949999999999998
2024-10-08 22:51:46,326 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=2724.0, ans=0.80466
2024-10-08 22:51:46,641 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten.whitening_limit, batch_count=2724.0, ans=9.543
2024-10-08 22:51:54,164 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=2724.0, ans=0.09784999999999999
2024-10-08 22:52:00,707 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=2727.3333333333335, ans=0.5
2024-10-08 22:52:03,831 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=2727.3333333333335, ans=0.37215624999999997
2024-10-08 22:52:12,441 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=19.66 vs. limit=6.365333333333333
2024-10-08 22:52:21,882 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.95 vs. limit=6.365333333333333
2024-10-08 22:52:26,146 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.424e+01 9.214e+01 1.032e+02 1.195e+02 2.286e+02, threshold=2.064e+02, percent-clipped=1.0
2024-10-08 22:52:29,171 INFO [train.py:1152] Epoch 2, batch 1350, loss[loss=0.3953, ctc_loss=0.4251, attn_decoder_loss=0.3878, over 4833.00 frames. ], tot_loss[loss=0.3919, ctc_loss=0.4124, attn_decoder_loss=0.3868, over 966306.21 frames. ], batch size: 21, lr: 3.62e-02,
2024-10-08 22:52:37,857 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.85 vs. limit=5.6835
2024-10-08 22:52:40,536 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=2734.0, ans=0.8043100000000001
2024-10-08 22:52:42,900 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.45 vs. limit=3.4101
2024-10-08 22:52:49,049 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.27 vs. limit=6.368666666666667
2024-10-08 22:52:53,022 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=2737.3333333333335, ans=0.3716875
2024-10-08 22:52:53,048 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=2737.3333333333335, ans=0.03840999999999999
2024-10-08 22:52:53,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2737.3333333333335, ans=0.27262666666666663
2024-10-08 22:52:57,850 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=2737.3333333333335, ans=0.08289166666666667
2024-10-08 22:53:04,930 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.36 vs. limit=9.5555
2024-10-08 22:53:12,780 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.81 vs. limit=6.370333333333333
2024-10-08 22:53:16,890 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=2744.0, ans=0.371375
2024-10-08 22:53:18,486 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=2744.0, ans=0.371375
2024-10-08 22:53:23,663 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1.whitening_limit, batch_count=2744.0, ans=5.686
2024-10-08 22:53:28,807 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.96 vs. limit=6.372
2024-10-08 22:53:31,797 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.12 vs. limit=8.53025
2024-10-08 22:53:34,207 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=2747.3333333333335, ans=0.08282916666666668
2024-10-08 22:53:41,995 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=2747.3333333333335, ans=0.37121875
2024-10-08 22:53:45,913 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.60 vs. limit=9.560500000000001
2024-10-08 22:53:48,215 INFO [train.py:1152] Epoch 2, batch 1400, loss[loss=0.3572, ctc_loss=0.3732, attn_decoder_loss=0.3532, over 4940.00 frames. ], tot_loss[loss=0.391, ctc_loss=0.4115, attn_decoder_loss=0.3859, over 966654.78 frames. ], batch size: 19, lr: 3.62e-02,
2024-10-08 22:53:51,055 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.04 vs. limit=9.563
2024-10-08 22:53:51,976 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.75 vs. limit=3.4126
2024-10-08 22:53:54,728 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=2750.6666666666665, ans=0.37106249999999996
2024-10-08 22:53:57,741 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=2750.6666666666665, ans=0.09685
2024-10-08 22:54:04,250 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=2754.0, ans=0.37090625
2024-10-08 22:54:24,745 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 22:54:24,788 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=2757.3333333333335, ans=0.37075
2024-10-08 22:54:26,234 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=2757.3333333333335, ans=0.037959999999999994
2024-10-08 22:54:34,630 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.36 vs. limit=9.5705
2024-10-08 22:54:34,785 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.45 vs. limit=9.5705
2024-10-08 22:54:37,379 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2760.6666666666665, ans=0.37059375
2024-10-08 22:54:51,350 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=2764.0, ans=0.3704375
2024-10-08 22:54:54,584 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=2764.0, ans=0.09634999999999999
2024-10-08 22:55:03,709 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.381e+01 9.821e+01 1.080e+02 1.188e+02 1.651e+02, threshold=2.159e+02, percent-clipped=0.0
2024-10-08 22:55:06,945 INFO [train.py:1152] Epoch 2, batch 1450, loss[loss=0.3974, ctc_loss=0.448, attn_decoder_loss=0.3848, over 4795.00 frames. ], tot_loss[loss=0.3902, ctc_loss=0.4105, attn_decoder_loss=0.3852, over 966538.71 frames. ], batch size: 34, lr: 3.61e-02,
2024-10-08 22:55:35,523 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=2770.6666666666665, ans=0.09415000000000001
2024-10-08 22:55:44,853 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.prob, batch_count=2774.0, ans=0.36996875
2024-10-08 22:56:00,575 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.8.prob, batch_count=2777.3333333333335, ans=0.3698125
2024-10-08 22:56:00,723 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=2777.3333333333335, ans=0.3698125
2024-10-08 22:56:02,233 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=2777.3333333333335, ans=0.3698125
2024-10-08 22:56:17,459 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.28 vs. limit=6.390333333333333
2024-10-08 22:56:18,073 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=2780.6666666666665, ans=0.095725
2024-10-08 22:56:25,728 INFO [train.py:1152] Epoch 2, batch 1500, loss[loss=0.333, ctc_loss=0.3408, attn_decoder_loss=0.3311, over 4751.00 frames. ], tot_loss[loss=0.3915, ctc_loss=0.4118, attn_decoder_loss=0.3864, over 966209.40 frames. ], batch size: 26, lr: 3.61e-02,
2024-10-08 22:56:36,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=2784.0, ans=0.07
2024-10-08 22:56:52,762 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=2787.3333333333335, ans=0.09547499999999999
2024-10-08 22:57:09,968 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=2790.6666666666665, ans=0.03720999999999999
2024-10-08 22:57:24,140 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=2794.0, ans=0.09522499999999999
2024-10-08 22:57:34,318 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.23 vs. limit=9.597999999999999
2024-10-08 22:57:35,209 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=2797.3333333333335, ans=0.037059999999999996
2024-10-08 22:57:41,437 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.063e+01 9.915e+01 1.125e+02 1.328e+02 2.167e+02, threshold=2.251e+02, percent-clipped=1.0
2024-10-08 22:57:44,454 INFO [train.py:1152] Epoch 2, batch 1550, loss[loss=0.4082, ctc_loss=0.4279, attn_decoder_loss=0.4033, over 4840.00 frames. ], tot_loss[loss=0.3917, ctc_loss=0.4126, attn_decoder_loss=0.3865, over 966004.73 frames. ], batch size: 31, lr: 3.60e-02,
2024-10-08 22:57:46,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=2800.6666666666665, ans=0.36871875
2024-10-08 22:57:58,823 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2804.0, ans=0.27196
2024-10-08 22:58:05,078 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=2804.0, ans=0.80186
2024-10-08 22:58:37,402 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.52 vs. limit=3.4215999999999998
2024-10-08 22:58:43,723 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.97 vs. limit=6.405333333333333
2024-10-08 22:58:52,389 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=2814.0, ans=0.36809375
2024-10-08 22:59:03,318 INFO [train.py:1152] Epoch 2, batch 1600, loss[loss=0.3979, ctc_loss=0.4118, attn_decoder_loss=0.3944, over 4799.00 frames. ], tot_loss[loss=0.3907, ctc_loss=0.4106, attn_decoder_loss=0.3857, over 966251.12 frames. ], batch size: 25, lr: 3.59e-02,
2024-10-08 22:59:07,494 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=6.01 vs. limit=8.5565
2024-10-08 22:59:18,384 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.37 vs. limit=8.55775
2024-10-08 22:59:23,677 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.min_positive, batch_count=2820.6666666666665, ans=0.08237083333333334
2024-10-08 22:59:30,027 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=2820.6666666666665, ans=0.09133750000000002
2024-10-08 22:59:34,793 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=2824.0, ans=0.367625
2024-10-08 22:59:37,906 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2824.0, ans=0.27176
2024-10-08 22:59:40,085 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=21.05 vs. limit=9.618
2024-10-08 22:59:47,866 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=7.58 vs. limit=8.559
2024-10-08 22:59:54,450 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.72 vs. limit=9.6205
2024-10-08 22:59:55,083 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=2827.3333333333335, ans=0.036385
2024-10-08 22:59:55,807 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.53 vs. limit=5.706833333333334
2024-10-08 22:59:59,955 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=2827.3333333333335, ans=0.09397499999999999
2024-10-08 23:00:00,584 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=19.81 vs. limit=6.413666666666667
2024-10-08 23:00:06,818 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.10 vs. limit=9.623000000000001
2024-10-08 23:00:17,274 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=2830.6666666666665, ans=0.09385
2024-10-08 23:00:17,287 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=2830.6666666666665, ans=0.14616666666666667
2024-10-08 23:00:18,494 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.749e+01 9.147e+01 1.006e+02 1.192e+02 1.741e+02, threshold=2.012e+02, percent-clipped=0.0
2024-10-08 23:00:19,341 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=43.65 vs. limit=9.623000000000001
2024-10-08 23:00:21,723 INFO [train.py:1152] Epoch 2, batch 1650, loss[loss=0.4126, ctc_loss=0.4467, attn_decoder_loss=0.4041, over 4780.00 frames. ], tot_loss[loss=0.3892, ctc_loss=0.4092, attn_decoder_loss=0.3842, over 966771.45 frames. ], batch size: 29, lr: 3.59e-02,
2024-10-08 23:00:28,121 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2834.0, ans=0.36715624999999996
2024-10-08 23:00:31,891 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.57 vs. limit=9.6255
2024-10-08 23:00:48,642 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=2837.3333333333335, ans=0.09359999999999999
2024-10-08 23:00:54,790 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.76 vs. limit=9.6305
2024-10-08 23:00:56,028 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.31 vs. limit=9.6305
2024-10-08 23:00:59,204 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.91 vs. limit=6.420333333333334
2024-10-08 23:01:03,673 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.10 vs. limit=9.6305
2024-10-08 23:01:10,200 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.32 vs. limit=3.4266
2024-10-08 23:01:17,241 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.88 vs. limit=9.633
2024-10-08 23:01:22,057 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1.whitening_limit, batch_count=2844.0, ans=5.711
2024-10-08 23:01:40,050 INFO [train.py:1152] Epoch 2, batch 1700, loss[loss=0.3634, ctc_loss=0.3674, attn_decoder_loss=0.3624, over 4940.00 frames. ], tot_loss[loss=0.3876, ctc_loss=0.4061, attn_decoder_loss=0.383, over 966817.97 frames. ], batch size: 19, lr: 3.58e-02,
2024-10-08 23:01:40,215 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=2850.6666666666665, ans=0.366375
2024-10-08 23:01:42,604 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.56 vs. limit=9.638
2024-10-08 23:01:43,986 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.99 vs. limit=6.425333333333333
2024-10-08 23:01:47,098 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=23.03 vs. limit=9.638
2024-10-08 23:01:52,352 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.05 vs. limit=9.638
2024-10-08 23:01:54,621 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=2854.0, ans=0.36621875000000004
2024-10-08 23:01:57,243 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.48 vs. limit=9.6405
2024-10-08 23:02:31,207 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.94 vs. limit=6.4303333333333335
2024-10-08 23:02:33,561 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2860.6666666666665, ans=0.2713933333333333
2024-10-08 23:02:47,733 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2864.0, ans=0.27136
2024-10-08 23:02:55,581 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.586e+01 9.423e+01 1.064e+02 1.243e+02 2.400e+02, threshold=2.129e+02, percent-clipped=1.0
2024-10-08 23:02:58,779 INFO [train.py:1152] Epoch 2, batch 1750, loss[loss=0.3174, ctc_loss=0.2928, attn_decoder_loss=0.3236, over 4959.00 frames. ], tot_loss[loss=0.3854, ctc_loss=0.403, attn_decoder_loss=0.381, over 967075.83 frames. ], batch size: 19, lr: 3.58e-02,
2024-10-08 23:03:47,180 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.46 vs. limit=9.658
2024-10-08 23:03:50,654 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.03 vs. limit=5.719333333333333
2024-10-08 23:04:10,060 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=2880.6666666666665, ans=0.091975
2024-10-08 23:04:17,796 INFO [train.py:1152] Epoch 2, batch 1800, loss[loss=0.3578, ctc_loss=0.3371, attn_decoder_loss=0.363, over 4842.00 frames. ], tot_loss[loss=0.3863, ctc_loss=0.404, attn_decoder_loss=0.3818, over 967791.06 frames. ], batch size: 23, lr: 3.57e-02,
2024-10-08 23:04:23,269 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.95 vs. limit=9.663
2024-10-08 23:04:25,295 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=4.25 vs. limit=8.5815
2024-10-08 23:04:44,973 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=2887.3333333333335, ans=0.08758749999999998
2024-10-08 23:04:44,991 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=2887.3333333333335, ans=0.36465625
2024-10-08 23:04:53,033 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 23:05:07,487 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=2894.0, ans=0.09147499999999999
2024-10-08 23:05:10,605 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=2894.0, ans=0.0872125
2024-10-08 23:05:12,712 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.31 vs. limit=8.58525
2024-10-08 23:05:25,864 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=16.33 vs. limit=9.673
2024-10-08 23:05:28,161 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=2897.3333333333335, ans=0.5
2024-10-08 23:05:34,163 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.635e+01 9.482e+01 1.083e+02 1.222e+02 1.994e+02, threshold=2.165e+02, percent-clipped=0.0
2024-10-08 23:05:37,331 INFO [train.py:1152] Epoch 2, batch 1850, loss[loss=0.4574, ctc_loss=0.5131, attn_decoder_loss=0.4434, over 4736.00 frames. ], tot_loss[loss=0.3874, ctc_loss=0.4053, attn_decoder_loss=0.383, over 967990.35 frames. ], batch size: 26, lr: 3.57e-02,
2024-10-08 23:05:52,377 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.80 vs. limit=9.678
2024-10-08 23:06:09,754 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=25.03 vs. limit=9.6805
2024-10-08 23:06:27,014 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.68 vs. limit=9.683
2024-10-08 23:06:56,306 INFO [train.py:1152] Epoch 2, batch 1900, loss[loss=0.3894, ctc_loss=0.4193, attn_decoder_loss=0.3819, over 4788.00 frames. ], tot_loss[loss=0.3852, ctc_loss=0.4021, attn_decoder_loss=0.381, over 967833.03 frames. ], batch size: 29, lr: 3.56e-02,
2024-10-08 23:06:58,965 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.47 vs. limit=9.688
2024-10-08 23:07:06,730 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.92 vs. limit=5.729333333333333
2024-10-08 23:07:06,761 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.69 vs. limit=9.688
2024-10-08 23:07:14,349 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.73 vs. limit=5.168266666666667
2024-10-08 23:07:18,594 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=2920.6666666666665, ans=0.36309375
2024-10-08 23:07:25,444 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=3.08 vs. limit=8.59525
2024-10-08 23:07:28,658 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.08 vs. limit=5.731
2024-10-08 23:07:31,714 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.73 vs. limit=9.693
2024-10-08 23:07:34,431 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2924.0, ans=0.27076
2024-10-08 23:07:41,705 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.11 vs. limit=9.693
2024-10-08 23:07:45,396 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=2927.3333333333335, ans=0.08170416666666667
2024-10-08 23:07:46,873 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=2927.3333333333335, ans=0.36278125
2024-10-08 23:08:12,377 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.096e+01 9.324e+01 1.046e+02 1.200e+02 1.924e+02, threshold=2.092e+02, percent-clipped=0.0
2024-10-08 23:08:15,458 INFO [train.py:1152] Epoch 2, batch 1950, loss[loss=0.3923, ctc_loss=0.372, attn_decoder_loss=0.3973, over 4866.00 frames. ], tot_loss[loss=0.3848, ctc_loss=0.4014, attn_decoder_loss=0.3807, over 966712.50 frames. ], batch size: 20, lr: 3.55e-02,
2024-10-08 23:08:18,857 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=2934.0, ans=0.03398499999999999
2024-10-08 23:08:19,926 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=8.83 vs. limit=9.7005
2024-10-08 23:08:28,241 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer1.prob, batch_count=2934.0, ans=0.36246875
2024-10-08 23:08:52,790 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.40 vs. limit=8.60275
2024-10-08 23:08:58,144 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=2940.6666666666665, ans=6.837916666666667
2024-10-08 23:09:14,005 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=2944.0, ans=0.362
2024-10-08 23:09:20,954 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=8.00 vs. limit=8.60525
2024-10-08 23:09:21,920 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=2947.3333333333335, ans=0.36184375
2024-10-08 23:09:34,472 INFO [train.py:1152] Epoch 2, batch 2000, loss[loss=0.3698, ctc_loss=0.3914, attn_decoder_loss=0.3644, over 4959.00 frames. ], tot_loss[loss=0.3854, ctc_loss=0.4017, attn_decoder_loss=0.3813, over 966453.06 frames. ], batch size: 19, lr: 3.55e-02,
2024-10-08 23:09:35,589 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.86 vs. limit=6.475333333333333
2024-10-08 23:09:36,213 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.max_abs, batch_count=2950.6666666666665, ans=6.844166666666666
2024-10-08 23:09:48,235 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.65 vs. limit=9.713000000000001
2024-10-08 23:09:49,106 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2954.0, ans=0.27046
2024-10-08 23:10:01,600 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=2954.0, ans=0.36153124999999997
2024-10-08 23:10:12,119 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.39 vs. limit=6.478666666666667
2024-10-08 23:10:17,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=2957.3333333333335, ans=0.1303333333333333
2024-10-08 23:10:28,021 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.86 vs. limit=9.7205
2024-10-08 23:10:30,164 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=2960.6666666666665, ans=0.36121875000000003
2024-10-08 23:10:32,446 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=2960.6666666666665, ans=9.7205
2024-10-08 23:10:38,458 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.30 vs. limit=9.722999999999999
2024-10-08 23:10:45,785 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2964.0, ans=0.3610625
2024-10-08 23:10:50,179 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.978e+01 9.687e+01 1.055e+02 1.203e+02 1.987e+02, threshold=2.110e+02, percent-clipped=0.0
2024-10-08 23:10:53,292 INFO [train.py:1152] Epoch 2, batch 2050, loss[loss=0.3756, ctc_loss=0.3893, attn_decoder_loss=0.3722, over 4914.00 frames. ], tot_loss[loss=0.3837, ctc_loss=0.3997, attn_decoder_loss=0.3797, over 966856.73 frames. ], batch size: 19, lr: 3.54e-02,
2024-10-08 23:11:08,442 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.88 vs. limit=5.188266666666666
2024-10-08 23:11:28,466 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=2974.0, ans=0.088475
2024-10-08 23:11:37,972 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=2974.0, ans=0.088475
2024-10-08 23:11:43,343 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=2977.3333333333335, ans=0.7957933333333334
2024-10-08 23:11:43,456 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=2977.3333333333335, ans=0.36043749999999997
2024-10-08 23:11:46,232 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=2977.3333333333335, ans=0.07
2024-10-08 23:11:52,487 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=2977.3333333333335, ans=0.040695833333333334
2024-10-08 23:11:54,705 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.57 vs. limit=3.4466
2024-10-08 23:12:12,140 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.49 vs. limit=8.619
2024-10-08 23:12:12,787 INFO [train.py:1152] Epoch 2, batch 2100, loss[loss=0.4036, ctc_loss=0.4279, attn_decoder_loss=0.3975, over 4842.00 frames. ], tot_loss[loss=0.3835, ctc_loss=0.3991, attn_decoder_loss=0.3796, over 967073.81 frames. ], batch size: 21, lr: 3.54e-02,
2024-10-08 23:12:26,172 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.75 vs. limit=9.738
2024-10-08 23:12:44,727 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.04 vs. limit=6.495333333333333
2024-10-08 23:13:16,984 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=9.09 vs. limit=8.624
2024-10-08 23:13:19,302 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=2997.3333333333335, ans=0.08126666666666667
2024-10-08 23:13:20,802 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=2997.3333333333335, ans=0.0876
2024-10-08 23:13:20,869 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2997.3333333333335, ans=0.27002666666666664
2024-10-08 23:13:22,504 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2997.3333333333335, ans=0.27002666666666664
2024-10-08 23:13:28,862 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.360e+01 9.600e+01 1.084e+02 1.172e+02 2.679e+02, threshold=2.167e+02, percent-clipped=3.0
2024-10-08 23:13:31,914 INFO [train.py:1152] Epoch 2, batch 2150, loss[loss=0.3768, ctc_loss=0.3857, attn_decoder_loss=0.3746, over 4869.00 frames. ], tot_loss[loss=0.3802, ctc_loss=0.3942, attn_decoder_loss=0.3766, over 967927.80 frames. ], batch size: 20, lr: 3.53e-02,
2024-10-08 23:13:36,823 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=3000.6666666666665, ans=0.7949766666666667
2024-10-08 23:13:39,484 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.31 vs. limit=6.500333333333334
2024-10-08 23:13:44,792 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 23:13:57,577 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=3004.0, ans=0.3591875
2024-10-08 23:13:58,684 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.49 vs. limit=6.502
2024-10-08 23:14:10,252 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=3007.3333333333335, ans=0.26992666666666665
2024-10-08 23:14:24,771 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=3010.6666666666665, ans=0.7946266666666667
2024-10-08 23:14:51,386 INFO [train.py:1152] Epoch 2, batch 2200, loss[loss=0.35, ctc_loss=0.3508, attn_decoder_loss=0.3498, over 4746.00 frames. ], tot_loss[loss=0.3794, ctc_loss=0.3929, attn_decoder_loss=0.376, over 967667.89 frames. ], batch size: 26, lr: 3.52e-02,
2024-10-08 23:14:57,891 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=3017.3333333333335, ans=0.3585625
2024-10-08 23:15:03,155 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.94 vs. limit=9.763
2024-10-08 23:15:09,488 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.88 vs. limit=6.5103333333333335
2024-10-08 23:15:23,589 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=10.98 vs. limit=9.768
2024-10-08 23:15:27,293 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.00 vs. limit=6.5120000000000005
2024-10-08 23:15:39,021 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=3027.3333333333335, ans=0.031885
2024-10-08 23:15:42,985 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.73 vs. limit=9.7705
2024-10-08 23:16:07,359 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.715e+01 9.981e+01 1.111e+02 1.246e+02 2.592e+02, threshold=2.222e+02, percent-clipped=2.0
2024-10-08 23:16:10,760 INFO [train.py:1152] Epoch 2, batch 2250, loss[loss=0.4084, ctc_loss=0.4327, attn_decoder_loss=0.4024, over 4876.00 frames. ], tot_loss[loss=0.3799, ctc_loss=0.3929, attn_decoder_loss=0.3767, over 967733.55 frames. ], batch size: 22, lr: 3.52e-02,
2024-10-08 23:16:11,376 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.63 vs. limit=3.4551
2024-10-08 23:16:13,139 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=31.10 vs. limit=9.775500000000001
2024-10-08 23:16:20,545 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=3034.0, ans=0.35778125
2024-10-08 23:16:26,309 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.46 vs. limit=8.639
2024-10-08 23:16:32,650 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.33 vs. limit=5.759333333333333
2024-10-08 23:16:44,681 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=3040.6666666666665, ans=0.35746875
2024-10-08 23:16:46,228 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=3040.6666666666665, ans=0.24561
2024-10-08 23:16:47,850 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=3040.6666666666665, ans=0.24561
2024-10-08 23:17:02,788 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.44 vs. limit=9.783
2024-10-08 23:17:03,846 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=3044.0, ans=0.1195
2024-10-08 23:17:11,214 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=3.09 vs. limit=5.2176
2024-10-08 23:17:18,919 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=5.35 vs. limit=8.64275
2024-10-08 23:17:30,598 INFO [train.py:1152] Epoch 2, batch 2300, loss[loss=0.3314, ctc_loss=0.3136, attn_decoder_loss=0.3359, over 4883.00 frames. ], tot_loss[loss=0.378, ctc_loss=0.3901, attn_decoder_loss=0.375, over 968263.98 frames. ], batch size: 19, lr: 3.51e-02,
2024-10-08 23:17:32,853 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.25 vs. limit=8.644
2024-10-08 23:17:48,095 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=3054.0, ans=0.79311
2024-10-08 23:17:48,703 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=7.93 vs. limit=8.64525
2024-10-08 23:17:58,154 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.59 vs. limit=5.7635
2024-10-08 23:18:00,281 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.52 vs. limit=3.4581
2024-10-08 23:18:08,596 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=3057.3333333333335, ans=0.3566875
2024-10-08 23:18:15,698 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=36.02 vs. limit=8.6465
2024-10-08 23:18:18,758 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=27.86 vs. limit=9.7955
2024-10-08 23:18:24,364 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=3060.6666666666665, ans=0.035
2024-10-08 23:18:25,954 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer_ff3.min_abs, batch_count=3060.6666666666665, ans=0.15303333333333333
2024-10-08 23:18:33,806 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3064.0, ans=0.26936
2024-10-08 23:18:40,177 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=3064.0, ans=0.79276
2024-10-08 23:18:46,424 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.194e+01 9.336e+01 1.047e+02 1.180e+02 2.820e+02, threshold=2.095e+02, percent-clipped=1.0
2024-10-08 23:18:49,630 INFO [train.py:1152] Epoch 2, batch 2350, loss[loss=0.3888, ctc_loss=0.4038, attn_decoder_loss=0.3851, over 4844.00 frames. ], tot_loss[loss=0.3762, ctc_loss=0.3876, attn_decoder_loss=0.3734, over 968291.76 frames. ], batch size: 23, lr: 3.51e-02,
2024-10-08 23:18:50,404 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=5.48 vs. limit=8.65025
2024-10-08 23:18:59,271 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=3067.3333333333335, ans=0.35621875000000003
2024-10-08 23:19:09,558 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.37 vs. limit=5.228266666666666
2024-10-08 23:19:10,353 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=3070.6666666666665, ans=0.3560625
2024-10-08 23:19:10,362 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3070.6666666666665, ans=0.26929333333333333
2024-10-08 23:19:24,774 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.max_abs, batch_count=3074.0, ans=6.92125
2024-10-08 23:19:24,836 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=3074.0, ans=0.35590625
2024-10-08 23:19:46,203 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.81 vs. limit=3.4616
2024-10-08 23:19:49,590 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=5.41 vs. limit=8.654
2024-10-08 23:20:05,089 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=3080.6666666666665, ans=0.04037291666666667
2024-10-08 23:20:09,829 INFO [train.py:1152] Epoch 2, batch 2400, loss[loss=0.3703, ctc_loss=0.3774, attn_decoder_loss=0.3686, over 4755.00 frames. ], tot_loss[loss=0.3775, ctc_loss=0.3893, attn_decoder_loss=0.3745, over 967510.32 frames. ], batch size: 19, lr: 3.50e-02,
2024-10-08 23:20:11,447 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=3084.0, ans=0.26916
2024-10-08 23:20:17,278 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.38 vs. limit=3.4626
2024-10-08 23:20:18,151 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 23:20:38,795 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=3087.3333333333335, ans=0.030534999999999993
2024-10-08 23:20:49,042 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.53 vs. limit=3.4636
2024-10-08 23:20:57,177 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.71 vs. limit=9.8205
2024-10-08 23:21:02,905 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=3094.0, ans=0.083975
2024-10-08 23:21:24,041 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=3097.3333333333335, ans=0.3548125
2024-10-08 23:21:26,966 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.844e+01 9.717e+01 1.089e+02 1.255e+02 2.497e+02, threshold=2.179e+02, percent-clipped=2.0
2024-10-08 23:21:30,380 INFO [train.py:1152] Epoch 2, batch 2450, loss[loss=0.3612, ctc_loss=0.3625, attn_decoder_loss=0.3609, over 4886.00 frames. ], tot_loss[loss=0.3788, ctc_loss=0.3905, attn_decoder_loss=0.3759, over 966929.13 frames. ], batch size: 22, lr: 3.50e-02,
2024-10-08 23:21:44,159 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.06 vs. limit=9.8255
2024-10-08 23:21:45,329 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=3104.0, ans=0.35450000000000004
2024-10-08 23:21:47,569 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.55 vs. limit=8.664
2024-10-08 23:21:57,952 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=3104.0, ans=0.0806
2024-10-08 23:21:59,130 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=3104.0, ans=0.7913600000000001
2024-10-08 23:22:14,971 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.38 vs. limit=5.776833333333333
2024-10-08 23:22:24,299 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=28.63 vs. limit=9.833
2024-10-08 23:22:27,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.62 vs. limit=3.4666
2024-10-08 23:22:30,328 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=26.35 vs. limit=9.833
2024-10-08 23:22:33,344 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.64 vs. limit=6.557
2024-10-08 23:22:33,694 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.97 vs. limit=8.66775
2024-10-08 23:22:40,469 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.06 vs. limit=5.7785
2024-10-08 23:22:45,690 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=3114.0, ans=0.35403125
2024-10-08 23:22:45,741 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=3114.0, ans=0.35403125
2024-10-08 23:22:50,311 INFO [train.py:1152] Epoch 2, batch 2500, loss[loss=0.3619, ctc_loss=0.3614, attn_decoder_loss=0.362, over 4741.00 frames. ], tot_loss[loss=0.3785, ctc_loss=0.3897, attn_decoder_loss=0.3757, over 966616.28 frames. ], batch size: 26, lr: 3.49e-02,
2024-10-08 23:23:03,830 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=6.07 vs. limit=5.246933333333334
2024-10-08 23:23:06,492 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=3120.6666666666665, ans=0.08297500000000001
2024-10-08 23:23:07,196 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.21 vs. limit=8.67025
2024-10-08 23:23:42,588 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.52 vs. limit=6.563666666666666
2024-10-08 23:23:48,501 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=24.95 vs. limit=9.8455
2024-10-08 23:23:49,522 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=3127.3333333333335, ans=0.35340625000000003
2024-10-08 23:23:55,987 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=3130.6666666666665, ans=0.35325
2024-10-08 23:24:07,219 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.136e+01 9.765e+01 1.106e+02 1.223e+02 4.190e+02, threshold=2.211e+02, percent-clipped=2.0
2024-10-08 23:24:10,502 INFO [train.py:1152] Epoch 2, batch 2550, loss[loss=0.3505, ctc_loss=0.3494, attn_decoder_loss=0.3508, over 4959.00 frames. ], tot_loss[loss=0.3784, ctc_loss=0.3891, attn_decoder_loss=0.3757, over 967028.66 frames. ], batch size: 19, lr: 3.48e-02,
2024-10-08 23:24:11,087 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten.whitening_limit, batch_count=3134.0, ans=9.8505
2024-10-08 23:24:33,115 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=3137.3333333333335, ans=0.7901933333333333
2024-10-08 23:24:40,125 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.89 vs. limit=9.853
2024-10-08 23:24:47,980 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.88 vs. limit=5.785166666666667
2024-10-08 23:24:51,580 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.49 vs. limit=6.570333333333333
2024-10-08 23:24:53,042 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.14 vs. limit=9.8555
2024-10-08 23:25:08,380 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=3144.0, ans=0.78996
2024-10-08 23:25:16,411 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.min_positive, batch_count=3147.3333333333335, ans=0.08032916666666667
2024-10-08 23:25:18,040 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3147.3333333333335, ans=0.26852666666666664
2024-10-08 23:25:20,053 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.09 vs. limit=8.680250000000001
2024-10-08 23:25:30,613 INFO [train.py:1152] Epoch 2, batch 2600, loss[loss=0.4016, ctc_loss=0.4011, attn_decoder_loss=0.4017, over 4852.00 frames. ], tot_loss[loss=0.3779, ctc_loss=0.3892, attn_decoder_loss=0.3751, over 966506.11 frames. ], batch size: 20, lr: 3.48e-02,
2024-10-08 23:25:41,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=3150.6666666666665, ans=0.35231250000000003
2024-10-08 23:26:41,117 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=3164.0, ans=0.7892600000000001
2024-10-08 23:26:47,185 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.674e+01 9.365e+01 1.052e+02 1.195e+02 2.636e+02, threshold=2.103e+02, percent-clipped=2.0
2024-10-08 23:26:50,497 INFO [train.py:1152] Epoch 2, batch 2650, loss[loss=0.3997, ctc_loss=0.4433, attn_decoder_loss=0.3888, over 4824.00 frames. ], tot_loss[loss=0.3767, ctc_loss=0.3872, attn_decoder_loss=0.3741, over 966162.46 frames. ], batch size: 38, lr: 3.47e-02,
2024-10-08 23:26:51,396 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=25.61 vs. limit=9.8755
2024-10-08 23:27:05,446 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=3170.6666666666665, ans=0.351375
2024-10-08 23:27:07,015 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=3170.6666666666665, ans=0.028660000000000005
2024-10-08 23:27:36,656 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.38 vs. limit=3.4761
2024-10-08 23:27:37,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=3177.3333333333335, ans=0.028509999999999994
2024-10-08 23:27:52,841 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.70 vs. limit=3.4766
2024-10-08 23:27:54,921 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=3180.6666666666665, ans=0.7886766666666667
2024-10-08 23:28:10,921 INFO [train.py:1152] Epoch 2, batch 2700, loss[loss=0.3436, ctc_loss=0.34, attn_decoder_loss=0.3445, over 4865.00 frames. ], tot_loss[loss=0.3765, ctc_loss=0.3864, attn_decoder_loss=0.374, over 966280.62 frames. ], batch size: 28, lr: 3.47e-02,
2024-10-08 23:28:19,854 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.09 vs. limit=8.693999999999999
2024-10-08 23:28:29,983 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.29 vs. limit=6.593666666666667
2024-10-08 23:28:47,092 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=13.16 vs. limit=8.6965
2024-10-08 23:28:54,451 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=3190.6666666666665, ans=0.3504375
2024-10-08 23:28:55,333 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=6.21 vs. limit=8.6965
2024-10-08 23:29:08,772 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=3194.0, ans=0.35028125
2024-10-08 23:29:11,314 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.74 vs. limit=3.4791
2024-10-08 23:29:17,490 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.17 vs. limit=9.898
2024-10-08 23:29:27,797 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.804e+01 9.321e+01 1.057e+02 1.219e+02 1.890e+02, threshold=2.114e+02, percent-clipped=0.0
2024-10-08 23:29:28,036 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=3197.3333333333335, ans=0.350125
2024-10-08 23:29:28,840 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.14 vs. limit=8.699
2024-10-08 23:29:31,006 INFO [train.py:1152] Epoch 2, batch 2750, loss[loss=0.341, ctc_loss=0.3205, attn_decoder_loss=0.3461, over 4800.00 frames. ], tot_loss[loss=0.3753, ctc_loss=0.3843, attn_decoder_loss=0.3731, over 966928.59 frames. ], batch size: 19, lr: 3.46e-02,
2024-10-08 23:29:52,175 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=3204.0, ans=0.079975
2024-10-08 23:30:00,725 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.45 vs. limit=5.2816
2024-10-08 23:30:21,095 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=3210.6666666666665, ans=0.7876266666666667
2024-10-08 23:30:22,000 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.75 vs. limit=9.908
2024-10-08 23:30:30,035 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.90 vs. limit=8.704
2024-10-08 23:30:40,764 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.45 vs. limit=9.910499999999999
2024-10-08 23:30:40,947 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.20 vs. limit=5.8035
2024-10-08 23:30:50,594 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.88 vs. limit=8.7065
2024-10-08 23:30:51,303 INFO [train.py:1152] Epoch 2, batch 2800, loss[loss=0.429, ctc_loss=0.4516, attn_decoder_loss=0.4234, over 4755.00 frames. ], tot_loss[loss=0.3747, ctc_loss=0.383, attn_decoder_loss=0.3727, over 967102.57 frames. ], batch size: 53, lr: 3.46e-02,
2024-10-08 23:30:55,436 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten.whitening_limit, batch_count=3217.3333333333335, ans=9.913
2024-10-08 23:30:56,271 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=3217.3333333333335, ans=0.3491875
2024-10-08 23:30:57,985 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=3217.3333333333335, ans=0.07934999999999999
2024-10-08 23:31:08,361 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=10.13 vs. limit=9.9155
2024-10-08 23:31:15,532 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=3220.6666666666665, ans=0.34903125
2024-10-08 23:31:21,994 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=3224.0, ans=0.348875
2024-10-08 23:31:23,023 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=4.67 vs. limit=4.6448
2024-10-08 23:31:48,223 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.31 vs. limit=9.9205
2024-10-08 23:32:07,539 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.583e+01 9.829e+01 1.127e+02 1.259e+02 3.335e+02, threshold=2.254e+02, percent-clipped=2.0
2024-10-08 23:32:10,743 INFO [train.py:1152] Epoch 2, batch 2850, loss[loss=0.3672, ctc_loss=0.3337, attn_decoder_loss=0.3756, over 4938.00 frames. ], tot_loss[loss=0.3757, ctc_loss=0.3838, attn_decoder_loss=0.3737, over 966836.47 frames. ], batch size: 20, lr: 3.45e-02,
2024-10-08 23:32:20,396 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=3234.0, ans=0.34840625000000003
2024-10-08 23:32:25,082 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=3237.3333333333335, ans=0.24856
2024-10-08 23:32:25,776 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.61 vs. limit=3.4856
2024-10-08 23:32:58,411 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=3244.0, ans=0.78646
2024-10-08 23:33:04,899 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=3244.0, ans=0.079725
2024-10-08 23:33:10,547 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.83 vs. limit=5.811
2024-10-08 23:33:30,224 INFO [train.py:1152] Epoch 2, batch 2900, loss[loss=0.3781, ctc_loss=0.3806, attn_decoder_loss=0.3775, over 4747.00 frames. ], tot_loss[loss=0.3751, ctc_loss=0.3837, attn_decoder_loss=0.373, over 965928.43 frames. ], batch size: 20, lr: 3.45e-02,
2024-10-08 23:33:31,025 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.71 vs. limit=5.3002666666666665
2024-10-08 23:33:34,251 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.24 vs. limit=5.812666666666667
2024-10-08 23:33:34,439 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.07 vs. limit=8.719
2024-10-08 23:33:36,320 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=5.49 vs. limit=4.650133333333334
2024-10-08 23:33:45,074 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=3254.0, ans=0.7861100000000001
2024-10-08 23:33:49,800 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=3254.0, ans=0.07797499999999999
2024-10-08 23:34:02,640 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 23:34:05,859 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=3257.3333333333335, ans=0.3473125
2024-10-08 23:34:19,900 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.85 vs. limit=6.630333333333333
2024-10-08 23:34:27,361 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.24 vs. limit=9.9455
2024-10-08 23:34:39,987 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.25 vs. limit=5.816
2024-10-08 23:34:40,375 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.45 vs. limit=9.948
2024-10-08 23:34:47,408 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.097e+01 9.502e+01 1.047e+02 1.175e+02 1.872e+02, threshold=2.093e+02, percent-clipped=0.0
2024-10-08 23:34:49,114 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=3267.3333333333335, ans=0.07
2024-10-08 23:34:49,659 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.01 vs. limit=9.9505
2024-10-08 23:34:50,570 INFO [train.py:1152] Epoch 2, batch 2950, loss[loss=0.3946, ctc_loss=0.3996, attn_decoder_loss=0.3934, over 4799.00 frames. ], tot_loss[loss=0.3743, ctc_loss=0.3817, attn_decoder_loss=0.3724, over 966518.31 frames. ], batch size: 19, lr: 3.44e-02,
2024-10-08 23:35:13,066 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=3270.6666666666665, ans=0.7855266666666667
2024-10-08 23:35:16,509 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=3270.6666666666665, ans=0.07735
2024-10-08 23:35:34,108 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=3274.0, ans=0.07722499999999999
2024-10-08 23:35:36,441 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.49 vs. limit=9.9555
2024-10-08 23:35:39,104 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=2.361e-01
2024-10-08 23:35:59,067 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=29.51 vs. limit=9.9605
2024-10-08 23:36:10,962 INFO [train.py:1152] Epoch 2, batch 3000, loss[loss=0.3406, ctc_loss=0.3317, attn_decoder_loss=0.3428, over 4834.00 frames. ], tot_loss[loss=0.3735, ctc_loss=0.3807, attn_decoder_loss=0.3717, over 967210.30 frames. ], batch size: 21, lr: 3.43e-02,
2024-10-08 23:36:10,963 INFO [train.py:1175] Computing validation loss
2024-10-08 23:36:20,185 INFO [train.py:1184] Epoch 2, validation: loss=0.2808, ctc_loss=0.188, attn_decoder_loss=0.3039, over 90464.00 frames.
2024-10-08 23:36:20,186 INFO [train.py:1185] Maximum memory allocated so far is 6613MB
2024-10-08 23:36:23,514 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=3284.0, ans=0.026109999999999994
2024-10-08 23:36:29,879 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=3284.0, ans=0.3460625
2024-10-08 23:36:32,437 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.66 vs. limit=6.6419999999999995
2024-10-08 23:36:39,136 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=3287.3333333333335, ans=0.7849433333333333
2024-10-08 23:36:41,184 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.65 vs. limit=3.4931
2024-10-08 23:36:46,910 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=3287.3333333333335, ans=0.34590624999999997
2024-10-08 23:36:52,355 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.27 vs. limit=8.734
2024-10-08 23:36:57,960 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=3290.6666666666665, ans=0.7848266666666667
2024-10-08 23:36:59,011 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.38 vs. limit=3.4936
2024-10-08 23:37:10,627 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=3294.0, ans=0.78471
2024-10-08 23:37:12,629 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=6.48 vs. limit=8.73525
2024-10-08 23:37:33,805 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=3297.3333333333335, ans=0.3454375
2024-10-08 23:37:35,097 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.761e+01 9.960e+01 1.106e+02 1.188e+02 2.816e+02, threshold=2.212e+02, percent-clipped=2.0
2024-10-08 23:37:38,324 INFO [train.py:1152] Epoch 2, batch 3050, loss[loss=0.3663, ctc_loss=0.3507, attn_decoder_loss=0.3702, over 4749.00 frames. ], tot_loss[loss=0.3726, ctc_loss=0.3795, attn_decoder_loss=0.3709, over 966712.08 frames. ], batch size: 19, lr: 3.43e-02,
2024-10-08 23:37:40,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=3300.6666666666665, ans=7.062916666666666
2024-10-08 23:37:44,716 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=3300.6666666666665, ans=0.24951
2024-10-08 23:37:57,116 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=3304.0, ans=0.039675
2024-10-08 23:37:57,762 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.51 vs. limit=9.978
2024-10-08 23:38:14,990 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.58 vs. limit=9.9805
2024-10-08 23:38:20,405 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=3307.3333333333335, ans=0.34496875
2024-10-08 23:38:25,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=3310.6666666666665, ans=0.7841266666666667
2024-10-08 23:38:38,053 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.90 vs. limit=8.7415
2024-10-08 23:38:47,326 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=17.40 vs. limit=9.9855
2024-10-08 23:38:50,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.08 vs. limit=9.9855
2024-10-08 23:38:54,361 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=3317.3333333333335, ans=0.34450000000000003
2024-10-08 23:38:55,663 INFO [train.py:1152] Epoch 2, batch 3100, loss[loss=0.3774, ctc_loss=0.4097, attn_decoder_loss=0.3694, over 4817.00 frames. ], tot_loss[loss=0.3728, ctc_loss=0.3792, attn_decoder_loss=0.3712, over 966447.38 frames. ], batch size: 38, lr: 3.42e-02,
2024-10-08 23:39:05,174 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=3317.3333333333335, ans=0.07559999999999999
2024-10-08 23:39:10,023 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=1.420e-01
2024-10-08 23:39:18,124 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.73 vs. limit=3.4981
2024-10-08 23:39:25,198 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.max_abs, batch_count=3324.0, ans=7.077500000000001
2024-10-08 23:39:26,123 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.55 vs. limit=6.662
2024-10-08 23:39:33,488 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=6.29 vs. limit=8.7465
2024-10-08 23:39:48,788 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.53 vs. limit=3.4991
2024-10-08 23:40:05,964 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=5.71 vs. limit=8.749
2024-10-08 23:40:09,574 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.50 vs. limit=3.4996
2024-10-08 23:40:09,854 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.444e+01 9.335e+01 1.054e+02 1.250e+02 1.841e+02, threshold=2.109e+02, percent-clipped=0.0
2024-10-08 23:40:11,462 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3334.0, ans=0.26666
2024-10-08 23:40:12,758 INFO [train.py:1152] Epoch 2, batch 3150, loss[loss=0.4087, ctc_loss=0.4448, attn_decoder_loss=0.3996, over 4796.00 frames. ], tot_loss[loss=0.3727, ctc_loss=0.3786, attn_decoder_loss=0.3713, over 966706.57 frames. ], batch size: 40, lr: 3.42e-02,
2024-10-08 23:40:33,621 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.23 vs. limit=10.003
2024-10-08 23:40:40,395 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=3337.3333333333335, ans=0.06227499999999997
2024-10-08 23:41:06,100 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=3344.0, ans=0.34325
2024-10-08 23:41:21,002 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=3347.3333333333335, ans=0.34309375
2024-10-08 23:41:23,171 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.78 vs. limit=3.5021
2024-10-08 23:41:28,340 INFO [train.py:1152] Epoch 2, batch 3200, loss[loss=0.4188, ctc_loss=0.4323, attn_decoder_loss=0.4154, over 4758.00 frames. ], tot_loss[loss=0.3725, ctc_loss=0.3781, attn_decoder_loss=0.3711, over 967163.60 frames. ], batch size: 20, lr: 3.41e-02,
2024-10-08 23:41:38,996 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=3350.6666666666665, ans=0.3429375
2024-10-08 23:41:42,832 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.03 vs. limit=10.0155
2024-10-08 23:41:53,238 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=54.07 vs. limit=10.0155
2024-10-08 23:41:54,095 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=3354.0, ans=0.06133750000000002
2024-10-08 23:42:06,130 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=3357.3333333333335, ans=0.03950833333333334
2024-10-08 23:42:07,758 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 23:42:11,561 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.71 vs. limit=6.6786666666666665
2024-10-08 23:42:39,415 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=3364.0, ans=0.78364
2024-10-08 23:42:39,480 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=3364.0, ans=0.3423125
2024-10-08 23:42:40,722 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.610e+01 9.388e+01 1.067e+02 1.236e+02 2.290e+02, threshold=2.133e+02, percent-clipped=3.0
2024-10-08 23:42:43,849 INFO [train.py:1152] Epoch 2, batch 3250, loss[loss=0.4449, ctc_loss=0.4621, attn_decoder_loss=0.4406, over 4860.00 frames. ], tot_loss[loss=0.372, ctc_loss=0.3768, attn_decoder_loss=0.3708, over 967266.74 frames. ], batch size: 24, lr: 3.41e-02,
2024-10-08 23:42:59,141 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3370.6666666666665, ans=0.2662933333333333
2024-10-08 23:43:13,186 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.93 vs. limit=5.8435
2024-10-08 23:43:20,761 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.56 vs. limit=10.0305
2024-10-08 23:43:28,571 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.06 vs. limit=5.844333333333333
2024-10-08 23:43:37,483 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module1.whiten, num_groups=1, num_channels=192, metric=3.52 vs. limit=8.7665
2024-10-08 23:43:41,494 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=3377.3333333333335, ans=10.033
2024-10-08 23:43:56,911 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.54 vs. limit=3.5071
2024-10-08 23:43:58,809 INFO [train.py:1152] Epoch 2, batch 3300, loss[loss=0.3903, ctc_loss=0.3896, attn_decoder_loss=0.3905, over 4840.00 frames. ], tot_loss[loss=0.3701, ctc_loss=0.3744, attn_decoder_loss=0.369, over 967749.25 frames. ], batch size: 43, lr: 3.40e-02,
2024-10-08 23:44:11,100 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=3384.0, ans=0.341375
2024-10-08 23:44:16,377 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.27 vs. limit=8.77025
2024-10-08 23:44:18,573 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=3387.3333333333335, ans=0.34121875
2024-10-08 23:44:18,624 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=3387.3333333333335, ans=0.07297499999999998
2024-10-08 23:44:19,264 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.66 vs. limit=10.0405
2024-10-08 23:44:33,450 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=3390.6666666666665, ans=0.3410625
2024-10-08 23:45:06,365 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=3397.3333333333335, ans=0.0726
2024-10-08 23:45:10,137 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=18.16 vs. limit=10.048
2024-10-08 23:45:10,730 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.343e+01 9.628e+01 1.049e+02 1.203e+02 2.227e+02, threshold=2.098e+02, percent-clipped=1.0
2024-10-08 23:45:13,620 INFO [train.py:1152] Epoch 2, batch 3350, loss[loss=0.3964, ctc_loss=0.4294, attn_decoder_loss=0.3882, over 4799.00 frames. ], tot_loss[loss=0.3708, ctc_loss=0.3756, attn_decoder_loss=0.3696, over 967044.17 frames. ], batch size: 40, lr: 3.40e-02,
2024-10-08 23:45:40,186 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=3404.0, ans=0.07235
2024-10-08 23:45:40,241 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=3404.0, ans=0.25106
2024-10-08 23:45:47,610 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=3407.3333333333335, ans=0.5
2024-10-08 23:45:49,598 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=16.38 vs. limit=10.0555
2024-10-08 23:45:53,874 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.51 vs. limit=8.77775
2024-10-08 23:45:56,987 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=25.23 vs. limit=10.058
2024-10-08 23:46:02,549 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.66 vs. limit=10.058
2024-10-08 23:46:15,955 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=7.89 vs. limit=8.78025
2024-10-08 23:46:27,289 INFO [train.py:1152] Epoch 2, batch 3400, loss[loss=0.3611, ctc_loss=0.3685, attn_decoder_loss=0.3592, over 4959.00 frames. ], tot_loss[loss=0.3696, ctc_loss=0.3746, attn_decoder_loss=0.3683, over 966834.69 frames. ], batch size: 19, lr: 3.39e-02,
2024-10-08 23:46:28,082 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=7.36 vs. limit=8.7815
2024-10-08 23:46:39,280 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=3417.3333333333335, ans=0.7803933333333334
2024-10-08 23:46:51,833 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=20.30 vs. limit=10.0655
2024-10-08 23:46:57,219 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=3424.0, ans=0.09899494936611666
2024-10-08 23:46:57,758 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.19 vs. limit=6.712
2024-10-08 23:46:57,802 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=20.17 vs. limit=8.784
2024-10-08 23:46:57,852 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.62 vs. limit=5.856
2024-10-08 23:47:00,160 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=3424.0, ans=0.0393
2024-10-08 23:47:06,778 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.65 vs. limit=6.712
2024-10-08 23:47:11,531 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=2.96 vs. limit=5.370933333333333
2024-10-08 23:47:33,901 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.32 vs. limit=8.7865
2024-10-08 23:47:39,089 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.704e+01 9.696e+01 1.080e+02 1.274e+02 2.041e+02, threshold=2.160e+02, percent-clipped=0.0
2024-10-08 23:47:39,282 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=3430.6666666666665, ans=0.7799266666666667
2024-10-08 23:47:42,265 INFO [train.py:1152] Epoch 2, batch 3450, loss[loss=0.3938, ctc_loss=0.4161, attn_decoder_loss=0.3882, over 4836.00 frames. ], tot_loss[loss=0.3696, ctc_loss=0.3746, attn_decoder_loss=0.3684, over 967029.08 frames. ], batch size: 43, lr: 3.39e-02,
2024-10-08 23:47:50,767 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=5.57 vs. limit=8.787749999999999
2024-10-08 23:47:52,119 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.99 vs. limit=10.0755
2024-10-08 23:48:19,027 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.51 vs. limit=10.0805
2024-10-08 23:48:55,185 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=9.43 vs. limit=8.79275
2024-10-08 23:48:57,114 INFO [train.py:1152] Epoch 2, batch 3500, loss[loss=0.3464, ctc_loss=0.3109, attn_decoder_loss=0.3553, over 4883.00 frames. ], tot_loss[loss=0.3679, ctc_loss=0.3724, attn_decoder_loss=0.3668, over 967387.59 frames. ], batch size: 19, lr: 3.38e-02,
2024-10-08 23:49:01,706 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=3450.6666666666665, ans=0.7792266666666667
2024-10-08 23:49:08,088 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.75 vs. limit=10.088000000000001
2024-10-08 23:49:11,010 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=3454.0, ans=0.33809374999999997
2024-10-08 23:49:27,128 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=3457.3333333333335, ans=0.3379375
2024-10-08 23:49:28,127 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=7.95 vs. limit=8.7965
2024-10-08 23:49:30,009 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=3457.3333333333335, ans=0.022209999999999994
2024-10-08 23:49:39,671 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=13.09 vs. limit=10.093
2024-10-08 23:49:47,527 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=3460.6666666666665, ans=0.022135000000000002
2024-10-08 23:49:59,727 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.33 vs. limit=10.097999999999999
2024-10-08 23:50:07,173 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=3464.0, ans=0.77876
2024-10-08 23:50:08,348 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.197e+01 9.437e+01 1.049e+02 1.211e+02 2.151e+02, threshold=2.098e+02, percent-clipped=0.0
2024-10-08 23:50:11,153 INFO [train.py:1152] Epoch 2, batch 3550, loss[loss=0.3366, ctc_loss=0.3379, attn_decoder_loss=0.3363, over 4795.00 frames. ], tot_loss[loss=0.3677, ctc_loss=0.3716, attn_decoder_loss=0.3667, over 967440.72 frames. ], batch size: 29, lr: 3.37e-02,
2024-10-08 23:50:29,109 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=3470.6666666666665, ans=0.0661666666666667
2024-10-08 23:50:52,352 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.62 vs. limit=6.737
2024-10-08 23:50:52,926 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=3474.0, ans=0.021834999999999993
2024-10-08 23:50:59,499 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=7.04 vs. limit=8.804
2024-10-08 23:51:20,330 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.13 vs. limit=5.870166666666667
2024-10-08 23:51:20,361 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.80 vs. limit=8.805250000000001
2024-10-08 23:51:25,485 INFO [train.py:1152] Epoch 2, batch 3600, loss[loss=0.3714, ctc_loss=0.338, attn_decoder_loss=0.3797, over 4925.00 frames. ], tot_loss[loss=0.3671, ctc_loss=0.3701, attn_decoder_loss=0.3663, over 967477.28 frames. ], batch size: 20, lr: 3.37e-02,
2024-10-08 23:51:27,761 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.79 vs. limit=3.5225999999999997
2024-10-08 23:51:30,751 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.73 vs. limit=10.113
2024-10-08 23:51:33,185 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=3484.0, ans=0.0391125
2024-10-08 23:51:42,093 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=3487.3333333333335, ans=0.021534999999999985
2024-10-08 23:51:52,444 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3487.3333333333335, ans=0.2651266666666667
2024-10-08 23:52:01,991 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.46 vs. limit=10.118
2024-10-08 23:52:03,199 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.04 vs. limit=10.118
2024-10-08 23:52:10,133 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer_ff2.min_abs, batch_count=3494.0, ans=0.08735000000000001
2024-10-08 23:52:23,576 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 23:52:25,529 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=12.66 vs. limit=8.8115
2024-10-08 23:52:33,945 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=3497.3333333333335, ans=0.7849733333333333
2024-10-08 23:52:36,655 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.576e+01 9.981e+01 1.086e+02 1.239e+02 1.896e+02, threshold=2.172e+02, percent-clipped=0.0
2024-10-08 23:52:39,633 INFO [train.py:1152] Epoch 2, batch 3650, loss[loss=0.4088, ctc_loss=0.4328, attn_decoder_loss=0.4027, over 4851.00 frames. ], tot_loss[loss=0.3671, ctc_loss=0.3692, attn_decoder_loss=0.3665, over 967979.47 frames. ], batch size: 31, lr: 3.36e-02,
2024-10-08 23:52:41,787 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.67 vs. limit=3.5251
2024-10-08 23:52:44,236 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=3500.6666666666665, ans=0.06872500000000001
2024-10-08 23:52:47,316 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=5.680e-01
2024-10-08 23:52:47,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.57 vs. limit=5.875166666666667
2024-10-08 23:52:56,640 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.20 vs. limit=6.752
2024-10-08 23:53:13,093 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.57 vs. limit=10.1305
2024-10-08 23:53:15,426 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=3507.3333333333335, ans=0.05271249999999997
2024-10-08 23:53:48,530 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.10 vs. limit=10.1355
2024-10-08 23:53:53,633 INFO [train.py:1152] Epoch 2, batch 3700, loss[loss=0.359, ctc_loss=0.3538, attn_decoder_loss=0.3603, over 4853.00 frames. ], tot_loss[loss=0.3668, ctc_loss=0.3681, attn_decoder_loss=0.3664, over 967367.50 frames. ], batch size: 24, lr: 3.36e-02,
2024-10-08 23:53:58,992 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.31 vs. limit=6.758666666666667
2024-10-08 23:54:00,616 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.18 vs. limit=8.818999999999999
2024-10-08 23:54:16,112 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=3520.6666666666665, ans=0.33496875000000004
2024-10-08 23:54:32,399 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=3524.0, ans=0.77666
2024-10-08 23:54:35,353 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=3524.0, ans=0.020710000000000006
2024-10-08 23:54:38,355 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=3527.3333333333335, ans=0.038977083333333336
2024-10-08 23:54:40,598 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.69 vs. limit=3.5291
2024-10-08 23:54:43,786 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.57 vs. limit=6.7636666666666665
2024-10-08 23:54:54,762 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=3530.6666666666665, ans=0.3345
2024-10-08 23:54:59,304 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3530.6666666666665, ans=0.26469333333333334
2024-10-08 23:55:01,448 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.94 vs. limit=8.824
2024-10-08 23:55:05,245 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.250e+01 9.790e+01 1.062e+02 1.219e+02 2.758e+02, threshold=2.124e+02, percent-clipped=1.0
2024-10-08 23:55:08,120 INFO [train.py:1152] Epoch 2, batch 3750, loss[loss=0.294, ctc_loss=0.2646, attn_decoder_loss=0.3014, over 4959.00 frames. ], tot_loss[loss=0.3663, ctc_loss=0.3663, attn_decoder_loss=0.3662, over 967811.19 frames. ], batch size: 19, lr: 3.35e-02,
2024-10-08 23:55:30,634 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=3537.3333333333335, ans=0.020409999999999984
2024-10-08 23:55:42,528 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3540.6666666666665, ans=0.26459333333333335
2024-10-08 23:55:56,074 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 23:56:03,563 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=3544.0, ans=0.33387500000000003
2024-10-08 23:56:13,992 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=3547.3333333333335, ans=0.038914583333333336
2024-10-08 23:56:15,559 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=3547.3333333333335, ans=0.07782916666666667
2024-10-08 23:56:20,047 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3547.3333333333335, ans=0.2645266666666667
2024-10-08 23:56:22,794 INFO [train.py:1152] Epoch 2, batch 3800, loss[loss=0.3384, ctc_loss=0.3352, attn_decoder_loss=0.3392, over 4774.00 frames. ], tot_loss[loss=0.3653, ctc_loss=0.3654, attn_decoder_loss=0.3653, over 967504.90 frames. ], batch size: 26, lr: 3.35e-02,
2024-10-08 23:56:23,729 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.45 vs. limit=10.163
2024-10-08 23:56:28,198 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.12 vs. limit=10.163
2024-10-08 23:56:38,674 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.11 vs. limit=8.83275
2024-10-08 23:57:10,920 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=3560.6666666666665, ans=0.07774583333333335
2024-10-08 23:57:23,945 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3564.0, ans=0.26436
2024-10-08 23:57:34,036 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.402e+01 9.333e+01 1.058e+02 1.239e+02 2.150e+02, threshold=2.116e+02, percent-clipped=1.0
2024-10-08 23:57:36,998 INFO [train.py:1152] Epoch 2, batch 3850, loss[loss=0.3369, ctc_loss=0.3322, attn_decoder_loss=0.3381, over 4821.00 frames. ], tot_loss[loss=0.3629, ctc_loss=0.362, attn_decoder_loss=0.3631, over 967557.35 frames. ], batch size: 38, lr: 3.34e-02,
2024-10-08 23:57:50,419 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=3570.6666666666665, ans=0.332625
2024-10-08 23:57:57,769 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3570.6666666666665, ans=0.2642933333333333
2024-10-08 23:58:00,762 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=3570.6666666666665, ans=0.332625
2024-10-08 23:58:08,160 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=3574.0, ans=0.33246875
2024-10-08 23:58:17,546 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=10.38 vs. limit=10.1805
2024-10-08 23:58:45,166 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=3580.6666666666665, ans=0.33215625
2024-10-08 23:58:46,807 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=3580.6666666666665, ans=0.33215625
2024-10-08 23:58:51,041 INFO [train.py:1152] Epoch 2, batch 3900, loss[loss=0.3732, ctc_loss=0.3812, attn_decoder_loss=0.3712, over 4751.00 frames. ], tot_loss[loss=0.3637, ctc_loss=0.3619, attn_decoder_loss=0.3642, over 967006.82 frames. ], batch size: 26, lr: 3.34e-02,
2024-10-08 23:58:57,217 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=3584.0, ans=0.33199999999999996
2024-10-08 23:58:57,843 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.34 vs. limit=10.188
2024-10-08 23:59:07,484 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=3587.3333333333335, ans=0.33184375
2024-10-08 23:59:23,027 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.96 vs. limit=8.8465
2024-10-08 23:59:25,990 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.87 vs. limit=8.8465
2024-10-08 23:59:45,009 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=3594.0, ans=0.33153125
2024-10-08 23:59:47,103 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.16 vs. limit=5.8985
2024-10-09 00:00:02,733 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.147e+01 9.354e+01 1.010e+02 1.172e+02 2.657e+02, threshold=2.019e+02, percent-clipped=2.0
2024-10-09 00:00:05,572 INFO [train.py:1152] Epoch 2, batch 3950, loss[loss=0.3926, ctc_loss=0.3945, attn_decoder_loss=0.3921, over 4837.00 frames. ], tot_loss[loss=0.3609, ctc_loss=0.3579, attn_decoder_loss=0.3617, over 967330.14 frames. ], batch size: 36, lr: 3.33e-02,
2024-10-09 00:00:17,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=3600.6666666666665, ans=0.07
2024-10-09 00:00:30,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=3604.0, ans=0.33106250000000004
2024-10-09 00:00:35,568 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=3607.3333333333335, ans=0.33090625
2024-10-09 00:00:43,566 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.36 vs. limit=5.901833333333333
2024-10-09 00:00:47,911 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.66 vs. limit=10.2055
2024-10-09 00:00:56,204 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=3610.6666666666665, ans=0.33075
2024-10-09 00:01:01,315 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.46 vs. limit=8.854
2024-10-09 00:01:12,812 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=3614.0, ans=0.33059375
2024-10-09 00:01:19,970 INFO [train.py:1152] Epoch 2, batch 4000, loss[loss=0.3267, ctc_loss=0.2925, attn_decoder_loss=0.3352, over 4815.00 frames. ], tot_loss[loss=0.3604, ctc_loss=0.3567, attn_decoder_loss=0.3613, over 967288.92 frames. ], batch size: 19, lr: 3.33e-02,
2024-10-09 00:01:45,434 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=3620.6666666666665, ans=0.33028124999999997
2024-10-09 00:02:01,904 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=3624.0, ans=0.330125
2024-10-09 00:02:05,815 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.66 vs. limit=10.2205
2024-10-09 00:02:13,806 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=3627.3333333333335, ans=0.018385
2024-10-09 00:02:23,336 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=24.40 vs. limit=10.222999999999999
2024-10-09 00:02:29,187 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=20.14 vs. limit=10.222999999999999
2024-10-09 00:02:31,410 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.871e+01 9.703e+01 1.094e+02 1.211e+02 2.173e+02, threshold=2.187e+02, percent-clipped=1.0
2024-10-09 00:02:31,690 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=3630.6666666666665, ans=0.018309999999999993
2024-10-09 00:02:34,383 INFO [train.py:1152] Epoch 2, batch 4050, loss[loss=0.4235, ctc_loss=0.4543, attn_decoder_loss=0.4158, over 4768.00 frames. ], tot_loss[loss=0.3602, ctc_loss=0.3552, attn_decoder_loss=0.3615, over 967673.41 frames. ], batch size: 53, lr: 3.32e-02,
2024-10-09 00:02:34,498 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=3634.0, ans=0.32965625
2024-10-09 00:02:40,474 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=3634.0, ans=0.018234999999999987
2024-10-09 00:02:41,995 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=3634.0, ans=0.32965625
2024-10-09 00:02:44,997 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=3634.0, ans=0.32965625
2024-10-09 00:02:49,486 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=3637.3333333333335, ans=0.06359999999999999
2024-10-09 00:03:07,627 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.25 vs. limit=10.2305
2024-10-09 00:03:48,458 INFO [train.py:1152] Epoch 2, batch 4100, loss[loss=0.3605, ctc_loss=0.362, attn_decoder_loss=0.3602, over 4868.00 frames. ], tot_loss[loss=0.3607, ctc_loss=0.3557, attn_decoder_loss=0.362, over 967077.11 frames. ], batch size: 31, lr: 3.32e-02,
2024-10-09 00:03:59,139 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=3650.6666666666665, ans=0.32887500000000003
2024-10-09 00:04:06,571 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=3654.0, ans=0.017784999999999995
2024-10-09 00:04:10,162 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=3.75 vs. limit=8.87025
2024-10-09 00:04:30,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=3657.3333333333335, ans=0.7865733333333333
2024-10-09 00:04:38,611 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2.whitening_limit, batch_count=3660.6666666666665, ans=6.830333333333333
2024-10-09 00:04:59,628 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.091e+01 9.265e+01 1.056e+02 1.176e+02 2.404e+02, threshold=2.112e+02, percent-clipped=1.0
2024-10-09 00:05:02,442 INFO [train.py:1152] Epoch 2, batch 4150, loss[loss=0.3678, ctc_loss=0.3562, attn_decoder_loss=0.3708, over 4749.00 frames. ], tot_loss[loss=0.3609, ctc_loss=0.3547, attn_decoder_loss=0.3625, over 967122.98 frames. ], batch size: 20, lr: 3.31e-02,
2024-10-09 00:05:09,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=3667.3333333333335, ans=0.32809374999999996
2024-10-09 00:05:12,818 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=3667.3333333333335, ans=0.32809374999999996
2024-10-09 00:05:14,443 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=3667.3333333333335, ans=0.32809374999999996
2024-10-09 00:05:25,246 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.31 vs. limit=10.253
2024-10-09 00:05:30,753 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=3674.0, ans=0.32778125
2024-10-09 00:06:16,146 INFO [train.py:1152] Epoch 2, batch 4200, loss[loss=0.3323, ctc_loss=0.2985, attn_decoder_loss=0.3407, over 4830.00 frames. ], tot_loss[loss=0.3602, ctc_loss=0.3536, attn_decoder_loss=0.3619, over 967292.55 frames. ], batch size: 31, lr: 3.31e-02,
2024-10-09 00:06:21,363 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.94 vs. limit=5.921
2024-10-09 00:07:10,305 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=3694.0, ans=0.016884999999999997
2024-10-09 00:07:12,251 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=9.67 vs. limit=8.88525
2024-10-09 00:07:16,713 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.06 vs. limit=10.273
2024-10-09 00:07:16,832 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.15 vs. limit=5.924333333333333
2024-10-09 00:07:22,128 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3697.3333333333335, ans=0.26302666666666663
2024-10-09 00:07:22,140 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=3697.3333333333335, ans=0.03783333333333333
2024-10-09 00:07:27,308 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=18.49 vs. limit=10.273
2024-10-09 00:07:27,943 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.226e+01 9.391e+01 1.022e+02 1.184e+02 2.577e+02, threshold=2.044e+02, percent-clipped=1.0
2024-10-09 00:07:31,003 INFO [train.py:1152] Epoch 2, batch 4250, loss[loss=0.3359, ctc_loss=0.305, attn_decoder_loss=0.3436, over 4745.00 frames. ], tot_loss[loss=0.357, ctc_loss=0.3487, attn_decoder_loss=0.3591, over 967191.80 frames. ], batch size: 19, lr: 3.30e-02,
2024-10-09 00:07:49,934 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.85 vs. limit=10.278
2024-10-09 00:08:11,306 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=3707.3333333333335, ans=0.32621875
2024-10-09 00:08:26,386 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=3710.6666666666665, ans=0.7701266666666667
2024-10-09 00:08:30,763 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=3714.0, ans=0.06072499999999997
2024-10-09 00:08:31,231 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.91 vs. limit=8.89275
2024-10-09 00:08:44,737 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.57 vs. limit=6.858666666666666
2024-10-09 00:08:45,428 INFO [train.py:1152] Epoch 2, batch 4300, loss[loss=0.4055, ctc_loss=0.4273, attn_decoder_loss=0.4, over 4839.00 frames. ], tot_loss[loss=0.3575, ctc_loss=0.3488, attn_decoder_loss=0.3596, over 967435.15 frames. ], batch size: 21, lr: 3.30e-02,
2024-10-09 00:09:00,906 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.58 vs. limit=10.2905
2024-10-09 00:09:17,029 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.54 vs. limit=8.8965
2024-10-09 00:09:36,983 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3727.3333333333335, ans=0.26272666666666666
2024-10-09 00:09:44,166 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=3730.6666666666665, ans=0.325125
2024-10-09 00:09:55,904 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.414e+01 9.427e+01 1.069e+02 1.251e+02 2.364e+02, threshold=2.138e+02, percent-clipped=2.0
2024-10-09 00:09:58,873 INFO [train.py:1152] Epoch 2, batch 4350, loss[loss=0.2993, ctc_loss=0.2614, attn_decoder_loss=0.3088, over 4840.00 frames. ], tot_loss[loss=0.3566, ctc_loss=0.3471, attn_decoder_loss=0.359, over 966282.73 frames. ], batch size: 21, lr: 3.29e-02,
2024-10-09 00:10:06,387 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=3734.0, ans=0.05997499999999997
2024-10-09 00:10:07,910 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=3734.0, ans=0.03996250000000001
2024-10-09 00:10:13,682 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=3737.3333333333335, ans=0.015909999999999994
2024-10-09 00:10:25,532 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=3737.3333333333335, ans=0.05984999999999999
2024-10-09 00:10:26,141 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.95 vs. limit=10.303
2024-10-09 00:10:27,120 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=3740.6666666666665, ans=0.7690766666666667
2024-10-09 00:10:36,975 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=13.02 vs. limit=10.3055
2024-10-09 00:10:37,909 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.02 vs. limit=10.3055
2024-10-09 00:10:46,311 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=3744.0, ans=0.0766
2024-10-09 00:11:03,126 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.64 vs. limit=8.90525
2024-10-09 00:11:04,103 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=3747.3333333333335, ans=0.01568499999999999
2024-10-09 00:11:07,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3747.3333333333335, ans=0.2625266666666667
2024-10-09 00:11:08,504 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=3747.3333333333335, ans=0.32434375
2024-10-09 00:11:12,917 INFO [train.py:1152] Epoch 2, batch 4400, loss[loss=0.3562, ctc_loss=0.3488, attn_decoder_loss=0.358, over 4748.00 frames. ], tot_loss[loss=0.3554, ctc_loss=0.3448, attn_decoder_loss=0.3581, over 965909.51 frames. ], batch size: 26, lr: 3.29e-02,
2024-10-09 00:11:16,045 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=3750.6666666666665, ans=0.7687266666666667
2024-10-09 00:11:17,546 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=3750.6666666666665, ans=0.015609999999999999
2024-10-09 00:11:26,954 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=3754.0, ans=0.32403125
2024-10-09 00:11:28,130 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=3754.0, ans=0.76861
2024-10-09 00:11:29,538 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=3754.0, ans=0.05922499999999997
2024-10-09 00:12:05,944 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=3760.6666666666665, ans=0.32371875
2024-10-09 00:12:19,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=3764.0, ans=0.058849999999999986
2024-10-09 00:12:23,368 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.833e+01 9.670e+01 1.123e+02 1.269e+02 2.475e+02, threshold=2.246e+02, percent-clipped=1.0
2024-10-09 00:12:26,402 INFO [train.py:1152] Epoch 2, batch 4450, loss[loss=0.3414, ctc_loss=0.2998, attn_decoder_loss=0.3518, over 4883.00 frames. ], tot_loss[loss=0.3563, ctc_loss=0.3452, attn_decoder_loss=0.3591, over 966227.33 frames. ], batch size: 19, lr: 3.28e-02,
2024-10-09 00:12:37,593 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.97 vs. limit=5.5069333333333335
2024-10-09 00:12:48,692 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=3770.6666666666665, ans=0.7680266666666667
2024-10-09 00:13:08,595 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.01 vs. limit=5.9435
2024-10-09 00:13:16,322 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.33 vs. limit=6.8886666666666665
2024-10-09 00:13:28,543 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.min_positive, batch_count=3780.6666666666665, ans=0.038185416666666666
2024-10-09 00:13:35,100 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.10 vs. limit=10.3355
2024-10-09 00:13:40,347 INFO [train.py:1152] Epoch 2, batch 4500, loss[loss=0.3856, ctc_loss=0.3964, attn_decoder_loss=0.3829, over 4845.00 frames. ], tot_loss[loss=0.3552, ctc_loss=0.3431, attn_decoder_loss=0.3582, over 966157.38 frames. ], batch size: 28, lr: 3.28e-02,
2024-10-09 00:13:41,959 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=3784.0, ans=0.76756
2024-10-09 00:13:45,030 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=3784.0, ans=0.76756
2024-10-09 00:13:49,901 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.61 vs. limit=10.338000000000001
2024-10-09 00:14:07,409 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.max_positive, batch_count=3787.3333333333335, ans=0.7878733333333333
2024-10-09 00:14:08,872 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=3790.6666666666665, ans=0.036775
2024-10-09 00:14:11,842 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=3790.6666666666665, ans=0.05785000000000001
2024-10-09 00:14:30,133 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=28.66 vs. limit=10.3455
2024-10-09 00:14:30,851 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=3794.0, ans=0.7672100000000001
2024-10-09 00:14:41,109 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=3797.3333333333335, ans=0.01455999999999999
2024-10-09 00:14:42,524 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=3797.3333333333335, ans=0.322
2024-10-09 00:14:51,431 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.139e+01 9.151e+01 1.028e+02 1.193e+02 1.759e+02, threshold=2.057e+02, percent-clipped=0.0
2024-10-09 00:14:54,359 INFO [train.py:1152] Epoch 2, batch 4550, loss[loss=0.2894, ctc_loss=0.2663, attn_decoder_loss=0.2951, over 4848.00 frames. ], tot_loss[loss=0.3557, ctc_loss=0.3432, attn_decoder_loss=0.3589, over 965944.90 frames. ], batch size: 20, lr: 3.27e-02,
2024-10-09 00:15:02,769 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.64 vs. limit=5.950166666666666
2024-10-09 00:15:14,026 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=3804.0, ans=0.036025
2024-10-09 00:15:15,417 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=3804.0, ans=0.3216875
2024-10-09 00:15:15,431 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=3804.0, ans=0.014410000000000006
2024-10-09 00:15:27,676 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.90 vs. limit=10.3555
2024-10-09 00:15:37,570 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=3810.6666666666665, ans=0.07618333333333334
2024-10-09 00:15:43,130 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=3810.6666666666665, ans=0.16475133333333336
2024-10-09 00:15:52,203 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 00:16:08,360 INFO [train.py:1152] Epoch 2, batch 4600, loss[loss=0.383, ctc_loss=0.3927, attn_decoder_loss=0.3805, over 4759.00 frames. ], tot_loss[loss=0.3548, ctc_loss=0.3416, attn_decoder_loss=0.3581, over 966274.07 frames. ], batch size: 45, lr: 3.27e-02,
2024-10-09 00:16:16,027 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=3817.3333333333335, ans=0.32106250000000003
2024-10-09 00:16:21,146 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.79 vs. limit=6.908666666666667
2024-10-09 00:16:22,548 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.65 vs. limit=10.3655
2024-10-09 00:16:24,149 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.61 vs. limit=3.5731
2024-10-09 00:16:29,232 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=3820.6666666666665, ans=0.7662766666666667
2024-10-09 00:16:29,980 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.46 vs. limit=10.3655
2024-10-09 00:16:31,494 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.30 vs. limit=8.93275
2024-10-09 00:16:53,106 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=3827.3333333333335, ans=0.013884999999999995
2024-10-09 00:17:02,060 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=3827.3333333333335, ans=0.32059375
2024-10-09 00:17:19,469 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.843e+01 9.448e+01 1.055e+02 1.177e+02 3.014e+02, threshold=2.110e+02, percent-clipped=1.0
2024-10-09 00:17:22,471 INFO [train.py:1152] Epoch 2, batch 4650, loss[loss=0.4029, ctc_loss=0.4308, attn_decoder_loss=0.396, over 4819.00 frames. ], tot_loss[loss=0.3544, ctc_loss=0.3407, attn_decoder_loss=0.3578, over 965720.76 frames. ], batch size: 36, lr: 3.26e-02,
2024-10-09 00:17:25,645 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=3834.0, ans=0.32028124999999996
2024-10-09 00:17:27,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=3834.0, ans=0.32028124999999996
2024-10-09 00:17:28,613 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=3834.0, ans=0.013734999999999997
2024-10-09 00:17:38,169 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.80 vs. limit=10.378
2024-10-09 00:18:02,375 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3840.6666666666665, ans=0.26159333333333334
2024-10-09 00:18:08,356 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=3844.0, ans=0.3198125
2024-10-09 00:18:11,758 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.34 vs. limit=6.922
2024-10-09 00:18:23,641 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.55 vs. limit=3.5770999999999997
2024-10-09 00:18:30,151 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=3847.3333333333335, ans=0.04949747468305833
2024-10-09 00:18:35,941 INFO [train.py:1152] Epoch 2, batch 4700, loss[loss=0.2957, ctc_loss=0.2603, attn_decoder_loss=0.3046, over 4940.00 frames. ], tot_loss[loss=0.3536, ctc_loss=0.3392, attn_decoder_loss=0.3572, over 965802.64 frames. ], batch size: 19, lr: 3.26e-02,
2024-10-09 00:18:52,835 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.05 vs. limit=10.3905
2024-10-09 00:19:23,894 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=22.29 vs. limit=10.3955
2024-10-09 00:19:25,257 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.60 vs. limit=10.3955
2024-10-09 00:19:46,897 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.390e+01 9.495e+01 1.057e+02 1.186e+02 3.270e+02, threshold=2.115e+02, percent-clipped=2.0
2024-10-09 00:19:49,694 INFO [train.py:1152] Epoch 2, batch 4750, loss[loss=0.3587, ctc_loss=0.3524, attn_decoder_loss=0.3603, over 4725.00 frames. ], tot_loss[loss=0.3547, ctc_loss=0.3412, attn_decoder_loss=0.3581, over 965609.32 frames. ], batch size: 45, lr: 3.25e-02,
2024-10-09 00:19:49,874 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=3867.3333333333335, ans=0.05497499999999997
2024-10-09 00:20:02,396 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten.whitening_limit, batch_count=3867.3333333333335, ans=8.95025
2024-10-09 00:20:02,809 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=18.08 vs. limit=10.400500000000001
2024-10-09 00:20:05,455 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.81 vs. limit=8.9515
2024-10-09 00:20:15,330 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=3870.6666666666665, ans=0.032275
2024-10-09 00:20:40,404 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.max_positive, batch_count=3877.3333333333335, ans=0.7887733333333333
2024-10-09 00:20:41,904 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=3877.3333333333335, ans=0.31825000000000003
2024-10-09 00:20:58,121 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=3880.6666666666665, ans=0.031712500000000005
2024-10-09 00:21:00,992 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=3880.6666666666665, ans=0.31809375
2024-10-09 00:21:03,935 INFO [train.py:1152] Epoch 2, batch 4800, loss[loss=0.3009, ctc_loss=0.2577, attn_decoder_loss=0.3117, over 4888.00 frames. ], tot_loss[loss=0.352, ctc_loss=0.3373, attn_decoder_loss=0.3557, over 966010.38 frames. ], batch size: 22, lr: 3.25e-02,
2024-10-09 00:21:04,163 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=3884.0, ans=0.012609999999999996
2024-10-09 00:21:10,711 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=18.48 vs. limit=8.9565
2024-10-09 00:21:12,999 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=3884.0, ans=0.7640600000000001
2024-10-09 00:21:25,707 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.32 vs. limit=8.95775
2024-10-09 00:21:42,677 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=3890.6666666666665, ans=0.04949747468305833
2024-10-09 00:21:56,114 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=3894.0, ans=0.21106
2024-10-09 00:22:15,391 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.698e+01 9.038e+01 1.007e+02 1.153e+02 3.779e+02, threshold=2.014e+02, percent-clipped=1.0
2024-10-09 00:22:18,470 INFO [train.py:1152] Epoch 2, batch 4850, loss[loss=0.3552, ctc_loss=0.3646, attn_decoder_loss=0.3528, over 4854.00 frames. ], tot_loss[loss=0.3519, ctc_loss=0.3365, attn_decoder_loss=0.3558, over 966638.87 frames. ], batch size: 28, lr: 3.24e-02,
2024-10-09 00:22:27,502 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=3900.6666666666665, ans=0.31715625000000003
2024-10-09 00:22:31,404 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=6.44 vs. limit=8.96275
2024-10-09 00:22:49,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=3907.3333333333335, ans=0.31684375
2024-10-09 00:23:01,889 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=3910.6666666666665, ans=0.05335000000000001
2024-10-09 00:23:09,362 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=3910.6666666666665, ans=0.05335000000000001
2024-10-09 00:23:29,409 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.58 vs. limit=3.5871
2024-10-09 00:23:32,250 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.62 vs. limit=6.958666666666667
2024-10-09 00:23:32,781 INFO [train.py:1152] Epoch 2, batch 4900, loss[loss=0.3004, ctc_loss=0.2622, attn_decoder_loss=0.31, over 4846.00 frames. ], tot_loss[loss=0.3527, ctc_loss=0.3371, attn_decoder_loss=0.3566, over 967160.19 frames. ], batch size: 21, lr: 3.24e-02,
2024-10-09 00:23:34,340 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=3917.3333333333335, ans=0.7628933333333334
2024-10-09 00:23:50,575 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=3920.6666666666665, ans=0.011785000000000004
2024-10-09 00:24:14,803 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.86 vs. limit=5.981
2024-10-09 00:24:23,231 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=3927.3333333333335, ans=0.07
2024-10-09 00:24:43,451 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.484e+01 9.627e+01 1.055e+02 1.234e+02 2.544e+02, threshold=2.109e+02, percent-clipped=4.0
2024-10-09 00:24:46,200 INFO [train.py:1152] Epoch 2, batch 4950, loss[loss=0.3923, ctc_loss=0.4048, attn_decoder_loss=0.3892, over 4772.00 frames. ], tot_loss[loss=0.3534, ctc_loss=0.3377, attn_decoder_loss=0.3573, over 966954.64 frames. ], batch size: 53, lr: 3.23e-02,
2024-10-09 00:24:59,648 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=3937.3333333333335, ans=0.7621933333333334
2024-10-09 00:25:04,724 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.91 vs. limit=5.984333333333334
2024-10-09 00:25:36,832 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=3944.0, ans=0.05209999999999998
2024-10-09 00:25:43,769 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.18 vs. limit=6.9719999999999995
2024-10-09 00:25:47,349 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=3947.3333333333335, ans=0.051974999999999966
2024-10-09 00:26:00,349 INFO [train.py:1152] Epoch 2, batch 5000, loss[loss=0.3658, ctc_loss=0.3696, attn_decoder_loss=0.3649, over 4801.00 frames. ], tot_loss[loss=0.3521, ctc_loss=0.3355, attn_decoder_loss=0.3563, over 967809.36 frames. ], batch size: 29, lr: 3.23e-02,
2024-10-09 00:26:05,747 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.24 vs. limit=10.463000000000001
2024-10-09 00:26:14,262 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=4.84 vs. limit=5.5816
2024-10-09 00:26:22,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=3954.0, ans=0.76161
2024-10-09 00:26:57,034 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=3960.6666666666665, ans=0.07
2024-10-09 00:27:07,462 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=3964.0, ans=0.05134999999999998
2024-10-09 00:27:09,610 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.03 vs. limit=5.5855999999999995
2024-10-09 00:27:11,753 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.798e+01 9.341e+01 1.058e+02 1.252e+02 2.434e+02, threshold=2.117e+02, percent-clipped=1.0
2024-10-09 00:27:11,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=3964.0, ans=0.3141875
2024-10-09 00:27:14,806 INFO [train.py:1152] Epoch 2, batch 5050, loss[loss=0.3755, ctc_loss=0.3636, attn_decoder_loss=0.3785, over 4851.00 frames. ], tot_loss[loss=0.3508, ctc_loss=0.3341, attn_decoder_loss=0.3549, over 968659.83 frames. ], batch size: 19, lr: 3.22e-02,
2024-10-09 00:27:21,611 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.40 vs. limit=5.5869333333333335
2024-10-09 00:27:57,072 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.61 vs. limit=10.4805
2024-10-09 00:28:05,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=3977.3333333333335, ans=0.010509999999999992
2024-10-09 00:28:07,024 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=3977.3333333333335, ans=0.31356249999999997
2024-10-09 00:28:07,755 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.56 vs. limit=5.9943333333333335
2024-10-09 00:28:09,964 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=3977.3333333333335, ans=0.31356249999999997
2024-10-09 00:28:18,915 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3980.6666666666665, ans=0.26019333333333333
2024-10-09 00:28:27,111 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.26 vs. limit=6.990333333333333
2024-10-09 00:28:29,095 INFO [train.py:1152] Epoch 2, batch 5100, loss[loss=0.3107, ctc_loss=0.282, attn_decoder_loss=0.3178, over 4817.00 frames. ], tot_loss[loss=0.3519, ctc_loss=0.3356, attn_decoder_loss=0.356, over 967844.55 frames. ], batch size: 19, lr: 3.22e-02,
2024-10-09 00:28:33,782 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=3984.0, ans=0.31325000000000003
2024-10-09 00:28:42,024 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.54 vs. limit=10.488
2024-10-09 00:28:55,468 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=13.48 vs. limit=10.4905
2024-10-09 00:29:25,062 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.24 vs. limit=6.997
2024-10-09 00:29:28,605 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=3997.3333333333335, ans=0.01006
2024-10-09 00:29:35,914 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.8.prob, batch_count=3997.3333333333335, ans=0.312625
2024-10-09 00:29:39,166 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_False/checkpoint-12000.pt
2024-10-09 00:29:40,723 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=3997.3333333333335, ans=0.05009999999999998
2024-10-09 00:29:41,659 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.538e+01 9.546e+01 1.041e+02 1.156e+02 2.407e+02, threshold=2.082e+02, percent-clipped=1.0
2024-10-09 00:29:41,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=3997.3333333333335, ans=0.025149999999999978
2024-10-09 00:29:44,157 INFO [train.py:1152] Epoch 2, batch 5150, loss[loss=0.3506, ctc_loss=0.3296, attn_decoder_loss=0.3559, over 4824.00 frames. ], tot_loss[loss=0.3508, ctc_loss=0.3337, attn_decoder_loss=0.3551, over 968029.23 frames. ], batch size: 36, lr: 3.21e-02,
2024-10-09 00:29:48,432 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=4000.6666666666665, ans=0.09899494936611666
2024-10-09 00:29:49,075 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.06 vs. limit=10.5005
2024-10-09 00:29:49,965 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=4000.6666666666665, ans=0.31246874999999996
2024-10-09 00:30:08,998 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.15 vs. limit=6.0009999999999994
2024-10-09 00:30:12,912 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.min_abs, batch_count=4007.3333333333335, ans=0.26011
2024-10-09 00:30:13,683 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.09 vs. limit=6.001833333333334
2024-10-09 00:30:17,027 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=4007.3333333333335, ans=0.31215625
2024-10-09 00:30:17,121 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=4007.3333333333335, ans=0.00999840579710145
2024-10-09 00:30:32,999 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=4010.6666666666665, ans=0.312
2024-10-09 00:30:37,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=4010.6666666666665, ans=0.312
2024-10-09 00:30:38,840 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=4010.6666666666665, ans=0.312
2024-10-09 00:30:43,819 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.85 vs. limit=6.0035
2024-10-09 00:30:56,145 INFO [train.py:1152] Epoch 2, batch 5200, loss[loss=0.3281, ctc_loss=0.3241, attn_decoder_loss=0.3291, over 4774.00 frames. ], tot_loss[loss=0.35, ctc_loss=0.3318, attn_decoder_loss=0.3546, over 967653.16 frames. ], batch size: 29, lr: 3.21e-02,
2024-10-09 00:31:14,164 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=4020.6666666666665, ans=0.31153125000000004
2024-10-09 00:31:19,176 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.14 vs. limit=10.5155
2024-10-09 00:31:23,249 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.34 vs. limit=9.00775
2024-10-09 00:31:43,590 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=4027.3333333333335, ans=0.09899494936611666
2024-10-09 00:31:59,821 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=4030.6666666666665, ans=0.0
2024-10-09 00:32:06,874 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.102e+01 9.672e+01 1.081e+02 1.253e+02 2.906e+02, threshold=2.161e+02, percent-clipped=2.0
2024-10-09 00:32:07,143 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=4030.6666666666665, ans=0.3110625
2024-10-09 00:32:09,961 INFO [train.py:1152] Epoch 2, batch 5250, loss[loss=0.33, ctc_loss=0.3004, attn_decoder_loss=0.3374, over 4868.00 frames. ], tot_loss[loss=0.3491, ctc_loss=0.3301, attn_decoder_loss=0.3538, over 967732.45 frames. ], batch size: 20, lr: 3.20e-02,
2024-10-09 00:32:14,523 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=4034.0, ans=0.31090625
2024-10-09 00:32:17,340 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=4034.0, ans=0.31090625
2024-10-09 00:32:27,623 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=4037.3333333333335, ans=0.7586933333333333
2024-10-09 00:32:44,020 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=4040.6666666666665, ans=0.31059375
2024-10-09 00:32:49,979 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=4040.6666666666665, ans=0.31059375
2024-10-09 00:32:51,970 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.53 vs. limit=3.6061
2024-10-09 00:32:53,658 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.18 vs. limit=10.533
2024-10-09 00:32:57,312 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=4044.0, ans=0.25956
2024-10-09 00:33:03,047 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=4044.0, ans=0.31043750000000003
2024-10-09 00:33:05,153 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.50 vs. limit=10.533
2024-10-09 00:33:09,534 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.25 vs. limit=10.535499999999999
2024-10-09 00:33:19,371 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=4047.3333333333335, ans=7.529583333333333
2024-10-09 00:33:23,217 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.15 vs. limit=10.538
2024-10-09 00:33:23,660 INFO [train.py:1152] Epoch 2, batch 5300, loss[loss=0.3615, ctc_loss=0.3488, attn_decoder_loss=0.3647, over 4823.00 frames. ], tot_loss[loss=0.3493, ctc_loss=0.3306, attn_decoder_loss=0.354, over 967938.02 frames. ], batch size: 38, lr: 3.20e-02,
2024-10-09 00:33:50,774 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=4054.0, ans=7.5337499999999995
2024-10-09 00:33:50,779 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=4054.0, ans=0.03733125
2024-10-09 00:33:59,805 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=4057.3333333333335, ans=0.3098125
2024-10-09 00:34:08,546 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=4060.6666666666665, ans=0.04974722222222223
2024-10-09 00:34:13,622 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.07 vs. limit=10.5455
2024-10-09 00:34:15,930 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4060.6666666666665, ans=0.2593933333333333
2024-10-09 00:34:17,427 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4060.6666666666665, ans=0.2593933333333333
2024-10-09 00:34:23,347 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=4064.0, ans=0.04973333333333334
2024-10-09 00:34:26,910 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.57 vs. limit=10.548
2024-10-09 00:34:35,138 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.134e+01 9.397e+01 1.023e+02 1.116e+02 3.221e+02, threshold=2.045e+02, percent-clipped=2.0
2024-10-09 00:34:38,045 INFO [train.py:1152] Epoch 2, batch 5350, loss[loss=0.3329, ctc_loss=0.2915, attn_decoder_loss=0.3433, over 4978.00 frames. ], tot_loss[loss=0.3489, ctc_loss=0.3287, attn_decoder_loss=0.354, over 967334.95 frames. ], batch size: 19, lr: 3.19e-02,
2024-10-09 00:35:14,909 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=4074.0, ans=0.30903125
2024-10-09 00:35:22,363 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=4077.3333333333335, ans=0.035
2024-10-09 00:35:31,924 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=4077.3333333333335, ans=0.0
2024-10-09 00:35:33,205 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=4077.3333333333335, ans=0.308875
2024-10-09 00:35:42,309 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.58 vs. limit=10.560500000000001
2024-10-09 00:35:51,521 INFO [train.py:1152] Epoch 2, batch 5400, loss[loss=0.3603, ctc_loss=0.3596, attn_decoder_loss=0.3605, over 4782.00 frames. ], tot_loss[loss=0.3496, ctc_loss=0.3301, attn_decoder_loss=0.3544, over 966509.97 frames. ], batch size: 49, lr: 3.19e-02,
2024-10-09 00:35:51,531 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=4084.0, ans=0.30856249999999996
2024-10-09 00:35:53,132 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=4084.0, ans=0.7570600000000001
2024-10-09 00:35:59,661 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.92 vs. limit=9.0315
2024-10-09 00:36:44,944 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=4094.0, ans=0.009979565217391304
2024-10-09 00:36:49,911 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.36 vs. limit=10.573
2024-10-09 00:36:53,104 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.80 vs. limit=3.6146000000000003
2024-10-09 00:36:55,571 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.50 vs. limit=3.6146000000000003
2024-10-09 00:36:59,505 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=4097.333333333333, ans=0.3079375
2024-10-09 00:37:01,052 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 00:37:02,274 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.731e+01 9.570e+01 1.139e+02 1.294e+02 2.334e+02, threshold=2.278e+02, percent-clipped=2.0
2024-10-09 00:37:05,312 INFO [train.py:1152] Epoch 2, batch 5450, loss[loss=0.3289, ctc_loss=0.296, attn_decoder_loss=0.3371, over 4940.00 frames. ], tot_loss[loss=0.3498, ctc_loss=0.3302, attn_decoder_loss=0.3546, over 967059.66 frames. ], batch size: 19, lr: 3.18e-02,
2024-10-09 00:37:13,629 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.87 vs. limit=10.5755
2024-10-09 00:37:16,245 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=5.31 vs. limit=5.640266666666667
2024-10-09 00:38:18,751 INFO [train.py:1152] Epoch 2, batch 5500, loss[loss=0.3851, ctc_loss=0.398, attn_decoder_loss=0.3819, over 4791.00 frames. ], tot_loss[loss=0.3486, ctc_loss=0.329, attn_decoder_loss=0.3534, over 967515.17 frames. ], batch size: 49, lr: 3.18e-02,
2024-10-09 00:38:31,220 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=17.37 vs. limit=9.044
2024-10-09 00:38:40,664 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=3.56 vs. limit=5.0
2024-10-09 00:38:44,556 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.04 vs. limit=10.5905
2024-10-09 00:38:56,346 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.94 vs. limit=10.593
2024-10-09 00:38:58,740 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=4124.0, ans=0.3066875
2024-10-09 00:39:04,836 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4127.333333333333, ans=0.25872666666666666
2024-10-09 00:39:11,261 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.72 vs. limit=10.5955
2024-10-09 00:39:29,866 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.867e+01 8.848e+01 1.012e+02 1.150e+02 2.789e+02, threshold=2.025e+02, percent-clipped=1.0
2024-10-09 00:39:32,733 INFO [train.py:1152] Epoch 2, batch 5550, loss[loss=0.3045, ctc_loss=0.2742, attn_decoder_loss=0.3121, over 4800.00 frames. ], tot_loss[loss=0.3468, ctc_loss=0.3271, attn_decoder_loss=0.3517, over 967285.15 frames. ], batch size: 19, lr: 3.17e-02,
2024-10-09 00:39:41,840 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=4134.0, ans=0.025
2024-10-09 00:39:54,502 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=4.33 vs. limit=9.0515
2024-10-09 00:40:11,462 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=4140.666666666667, ans=0.30590625
2024-10-09 00:40:12,862 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=4140.666666666667, ans=0.009969420289855072
2024-10-09 00:40:19,329 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=5.35 vs. limit=9.054
2024-10-09 00:40:34,005 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.12 vs. limit=7.073666666666666
2024-10-09 00:40:34,838 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=4147.333333333333, ans=0.037039583333333334
2024-10-09 00:40:37,877 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=4147.333333333333, ans=0.30559375
2024-10-09 00:40:46,667 INFO [train.py:1152] Epoch 2, batch 5600, loss[loss=0.3685, ctc_loss=0.3612, attn_decoder_loss=0.3703, over 4864.00 frames. ], tot_loss[loss=0.3461, ctc_loss=0.3263, attn_decoder_loss=0.351, over 967222.51 frames. ], batch size: 28, lr: 3.17e-02,
2024-10-09 00:40:51,952 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.26 vs. limit=10.613
2024-10-09 00:41:00,273 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=4154.0, ans=0.30528125
2024-10-09 00:41:04,659 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=4154.0, ans=0.26231000000000004
2024-10-09 00:41:08,224 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.48 vs. limit=9.05775
2024-10-09 00:41:09,755 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.54 vs. limit=10.6155
2024-10-09 00:41:20,149 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.02 vs. limit=7.078666666666667
2024-10-09 00:41:41,139 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.49 vs. limit=10.6205
2024-10-09 00:41:58,017 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.481e+01 9.237e+01 1.024e+02 1.238e+02 2.521e+02, threshold=2.049e+02, percent-clipped=5.0
2024-10-09 00:42:01,007 INFO [train.py:1152] Epoch 2, batch 5650, loss[loss=0.3926, ctc_loss=0.3957, attn_decoder_loss=0.3918, over 4747.00 frames. ], tot_loss[loss=0.3452, ctc_loss=0.3242, attn_decoder_loss=0.3504, over 967197.69 frames. ], batch size: 45, lr: 3.16e-02,
2024-10-09 00:42:26,582 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.73 vs. limit=10.628
2024-10-09 00:42:41,682 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.93 vs. limit=10.6305
2024-10-09 00:42:49,780 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=4177.333333333333, ans=0.7537933333333333
2024-10-09 00:42:51,296 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=4177.333333333333, ans=0.04926111111111112
2024-10-09 00:43:14,756 INFO [train.py:1152] Epoch 2, batch 5700, loss[loss=0.3762, ctc_loss=0.3622, attn_decoder_loss=0.3797, over 4891.00 frames. ], tot_loss[loss=0.3444, ctc_loss=0.3228, attn_decoder_loss=0.3498, over 966631.55 frames. ], batch size: 22, lr: 3.16e-02,
2024-10-09 00:43:17,009 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.81 vs. limit=3.6276
2024-10-09 00:43:17,021 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten.whitening_limit, batch_count=4184.0, ans=9.068999999999999
2024-10-09 00:43:44,795 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.78 vs. limit=6.047666666666666
2024-10-09 00:43:53,845 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.19 vs. limit=10.643
2024-10-09 00:43:56,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=4190.666666666667, ans=0.26286
2024-10-09 00:44:17,319 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 00:44:26,104 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.207e+01 9.574e+01 1.050e+02 1.160e+02 2.177e+02, threshold=2.100e+02, percent-clipped=1.0
2024-10-09 00:44:28,979 INFO [train.py:1152] Epoch 2, batch 5750, loss[loss=0.3806, ctc_loss=0.3782, attn_decoder_loss=0.3812, over 4836.00 frames. ], tot_loss[loss=0.3467, ctc_loss=0.3264, attn_decoder_loss=0.3517, over 966899.95 frames. ], batch size: 43, lr: 3.16e-02,
2024-10-09 00:44:29,761 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.58 vs. limit=9.07525
2024-10-09 00:44:38,016 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=4200.666666666667, ans=0.30309375
2024-10-09 00:44:54,985 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.11 vs. limit=10.653
2024-10-09 00:45:05,229 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.55 vs. limit=7.103666666666666
2024-10-09 00:45:06,626 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.75 vs. limit=10.6555
2024-10-09 00:45:09,614 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=5.49 vs. limit=9.07775
2024-10-09 00:45:16,143 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=4210.666666666667, ans=0.025
2024-10-09 00:45:40,993 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=4217.333333333333, ans=0.25782666666666665
2024-10-09 00:45:42,275 INFO [train.py:1152] Epoch 2, batch 5800, loss[loss=0.4285, ctc_loss=0.4468, attn_decoder_loss=0.4239, over 4834.00 frames. ], tot_loss[loss=0.3476, ctc_loss=0.3275, attn_decoder_loss=0.3526, over 966214.63 frames. ], batch size: 43, lr: 3.15e-02,
2024-10-09 00:45:49,940 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.max_positive, batch_count=4217.333333333333, ans=0.7921733333333333
2024-10-09 00:45:50,998 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=14.23 vs. limit=10.663
2024-10-09 00:46:38,921 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=6.63 vs. limit=9.08525
2024-10-09 00:46:48,904 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.min_positive, batch_count=4230.666666666667, ans=0.07355833333333334
2024-10-09 00:46:53,149 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.483e+01 9.401e+01 1.043e+02 1.182e+02 2.209e+02, threshold=2.087e+02, percent-clipped=1.0
2024-10-09 00:46:55,271 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=7.24 vs. limit=9.08775
2024-10-09 00:46:56,128 INFO [train.py:1152] Epoch 2, batch 5850, loss[loss=0.4027, ctc_loss=0.4031, attn_decoder_loss=0.4026, over 4736.00 frames. ], tot_loss[loss=0.3475, ctc_loss=0.327, attn_decoder_loss=0.3527, over 966622.28 frames. ], batch size: 45, lr: 3.15e-02,
2024-10-09 00:46:58,538 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.19 vs. limit=10.6755
2024-10-09 00:46:59,239 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=4234.0, ans=0.049025000000000006
2024-10-09 00:47:01,155 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=4234.0, ans=10.6755
2024-10-09 00:47:01,277 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.73 vs. limit=9.08775
2024-10-09 00:47:27,463 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=4240.666666666667, ans=0.7515766666666667
2024-10-09 00:47:46,038 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.28 vs. limit=10.683
2024-10-09 00:47:52,680 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=8.487e-02
2024-10-09 00:47:58,553 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4247.333333333333, ans=0.2575266666666667
2024-10-09 00:48:04,317 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=4247.333333333333, ans=0.04896944444444445
2024-10-09 00:48:06,526 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=20.24 vs. limit=10.6855
2024-10-09 00:48:10,159 INFO [train.py:1152] Epoch 2, batch 5900, loss[loss=0.3876, ctc_loss=0.3755, attn_decoder_loss=0.3906, over 4803.00 frames. ], tot_loss[loss=0.3481, ctc_loss=0.3275, attn_decoder_loss=0.3533, over 966764.37 frames. ], batch size: 34, lr: 3.14e-02,
2024-10-09 00:48:12,622 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=16.22 vs. limit=10.688
2024-10-09 00:48:14,648 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.prob, batch_count=4250.666666666667, ans=0.30074999999999996
2024-10-09 00:48:30,766 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4254.0, ans=0.25746
2024-10-09 00:48:32,336 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=4254.0, ans=0.79254
2024-10-09 00:48:38,208 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer2.prob, batch_count=4257.333333333333, ans=0.3004375
2024-10-09 00:48:43,191 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.23 vs. limit=9.096499999999999
2024-10-09 00:49:15,378 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.57 vs. limit=10.698
2024-10-09 00:49:19,396 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=4264.0, ans=0.300125
2024-10-09 00:49:20,624 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.397e+01 9.300e+01 1.087e+02 1.228e+02 2.362e+02, threshold=2.174e+02, percent-clipped=2.0
2024-10-09 00:49:22,551 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.59 vs. limit=10.7005
2024-10-09 00:49:23,390 INFO [train.py:1152] Epoch 2, batch 5950, loss[loss=0.3946, ctc_loss=0.4101, attn_decoder_loss=0.3908, over 4792.00 frames. ], tot_loss[loss=0.3472, ctc_loss=0.3258, attn_decoder_loss=0.3526, over 966255.73 frames. ], batch size: 34, lr: 3.14e-02,
2024-10-09 00:49:40,620 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=9.09 vs. limit=9.1015
2024-10-09 00:49:41,930 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.98 vs. limit=7.1353333333333335
2024-10-09 00:49:47,841 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.64 vs. limit=10.703
2024-10-09 00:50:05,393 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.81 vs. limit=3.6411
2024-10-09 00:50:13,040 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.58 vs. limit=3.6416
2024-10-09 00:50:17,146 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.53 vs. limit=9.104
2024-10-09 00:50:21,669 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.03 vs. limit=6.070166666666667
2024-10-09 00:50:23,884 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=4280.666666666667, ans=0.29934375
2024-10-09 00:50:30,377 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=10.99 vs. limit=10.7105
2024-10-09 00:50:37,069 INFO [train.py:1152] Epoch 2, batch 6000, loss[loss=0.3574, ctc_loss=0.3519, attn_decoder_loss=0.3588, over 4767.00 frames. ], tot_loss[loss=0.3469, ctc_loss=0.3266, attn_decoder_loss=0.352, over 966842.22 frames. ], batch size: 49, lr: 3.13e-02,
2024-10-09 00:50:37,070 INFO [train.py:1175] Computing validation loss
2024-10-09 00:50:43,393 INFO [zipformer.py:1858] name=encoder.encoders.1.encoder.layers.0.self_attn_weights, attn_weights_entropy = tensor([4.9307, 4.7780, 5.0187, 4.4752], device='cuda:0')
2024-10-09 00:50:45,864 INFO [train.py:1184] Epoch 2, validation: loss=0.2606, ctc_loss=0.151, attn_decoder_loss=0.288, over 90464.00 frames.
2024-10-09 00:50:45,865 INFO [train.py:1185] Maximum memory allocated so far is 6613MB
2024-10-09 00:50:53,977 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.46 vs. limit=7.1419999999999995
2024-10-09 00:51:25,161 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=4290.666666666667, ans=0.298875
2024-10-09 00:51:36,071 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.32 vs. limit=6.0735
2024-10-09 00:51:45,583 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 00:51:50,594 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.11 vs. limit=10.722999999999999
2024-10-09 00:51:55,684 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.506e+01 9.221e+01 1.070e+02 1.204e+02 2.088e+02, threshold=2.139e+02, percent-clipped=0.0
2024-10-09 00:51:58,665 INFO [train.py:1152] Epoch 2, batch 6050, loss[loss=0.3104, ctc_loss=0.255, attn_decoder_loss=0.3242, over 4817.00 frames. ], tot_loss[loss=0.3451, ctc_loss=0.3232, attn_decoder_loss=0.3505, over 966741.63 frames. ], batch size: 19, lr: 3.13e-02,
2024-10-09 00:51:58,768 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=4300.666666666667, ans=0.025
2024-10-09 00:52:29,994 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.43 vs. limit=9.11525
2024-10-09 00:52:38,169 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=4307.333333333333, ans=0.29809375
2024-10-09 00:52:42,122 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.73 vs. limit=7.155333333333333
2024-10-09 00:52:58,085 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.85 vs. limit=9.117750000000001
2024-10-09 00:53:10,642 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=4317.333333333333, ans=0.04867777777777778
2024-10-09 00:53:11,872 INFO [train.py:1152] Epoch 2, batch 6100, loss[loss=0.3892, ctc_loss=0.3977, attn_decoder_loss=0.3871, over 4829.00 frames. ], tot_loss[loss=0.3459, ctc_loss=0.3242, attn_decoder_loss=0.3514, over 966344.25 frames. ], batch size: 34, lr: 3.12e-02,
2024-10-09 00:53:19,074 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=4317.333333333333, ans=0.1600646666666667
2024-10-09 00:53:30,023 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.65 vs. limit=3.6481
2024-10-09 00:53:30,974 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=4320.666666666667, ans=0.009930289855072464
2024-10-09 00:53:40,695 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.59 vs. limit=10.743
2024-10-09 00:53:44,393 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff2_skip_rate, batch_count=4324.0, ans=0.009929565217391304
2024-10-09 00:53:48,790 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=4324.0, ans=0.2973125
2024-10-09 00:53:53,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.71 vs. limit=9.1215
2024-10-09 00:53:56,229 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=4327.333333333333, ans=0.025
2024-10-09 00:54:16,823 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=4330.666666666667, ans=0.048622222222222226
2024-10-09 00:54:20,181 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.45 vs. limit=9.124
2024-10-09 00:54:22,742 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.623e+01 9.441e+01 1.064e+02 1.203e+02 1.935e+02, threshold=2.128e+02, percent-clipped=0.0
2024-10-09 00:54:25,563 INFO [train.py:1152] Epoch 2, batch 6150, loss[loss=0.4023, ctc_loss=0.3981, attn_decoder_loss=0.4033, over 4819.00 frames. ], tot_loss[loss=0.3442, ctc_loss=0.322, attn_decoder_loss=0.3497, over 966514.96 frames. ], batch size: 43, lr: 3.12e-02,
2024-10-09 00:54:35,580 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=13.03 vs. limit=10.7505
2024-10-09 00:54:47,936 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=4337.333333333333, ans=0.2966875
2024-10-09 00:54:56,787 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=4340.666666666667, ans=0.7480766666666667
2024-10-09 00:55:04,813 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.93 vs. limit=10.755500000000001
2024-10-09 00:55:11,995 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.42 vs. limit=9.129
2024-10-09 00:55:18,901 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=4344.0, ans=0.036425
2024-10-09 00:55:21,822 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=4344.0, ans=0.04856666666666667
2024-10-09 00:55:39,351 INFO [train.py:1152] Epoch 2, batch 6200, loss[loss=0.3388, ctc_loss=0.3075, attn_decoder_loss=0.3466, over 4782.00 frames. ], tot_loss[loss=0.3445, ctc_loss=0.322, attn_decoder_loss=0.3501, over 966628.53 frames. ], batch size: 29, lr: 3.11e-02,
2024-10-09 00:55:52,713 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=4354.0, ans=0.74761
2024-10-09 00:56:00,038 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=4354.0, ans=0.29590625000000004
2024-10-09 00:56:04,402 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=4354.0, ans=0.29590625000000004
2024-10-09 00:56:09,627 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.53 vs. limit=7.1786666666666665
2024-10-09 00:56:14,016 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=55.70 vs. limit=9.134
2024-10-09 00:56:25,548 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.40 vs. limit=10.7705
2024-10-09 00:56:33,807 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=4360.666666666667, ans=0.048497222222222226
2024-10-09 00:56:35,318 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=4360.666666666667, ans=0.7473766666666667
2024-10-09 00:56:39,807 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=4364.0, ans=0.2954375
2024-10-09 00:56:41,206 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=4364.0, ans=0.26546000000000003
2024-10-09 00:56:49,910 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.580e+01 9.200e+01 1.027e+02 1.138e+02 1.637e+02, threshold=2.053e+02, percent-clipped=0.0
2024-10-09 00:56:52,878 INFO [train.py:1152] Epoch 2, batch 6250, loss[loss=0.3599, ctc_loss=0.3367, attn_decoder_loss=0.3657, over 4753.00 frames. ], tot_loss[loss=0.342, ctc_loss=0.3188, attn_decoder_loss=0.3479, over 966807.41 frames. ], batch size: 26, lr: 3.11e-02,
2024-10-09 00:56:53,005 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=4367.333333333333, ans=0.7471433333333334
2024-10-09 00:56:57,469 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=4367.333333333333, ans=0.29528125
2024-10-09 00:57:02,513 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.19 vs. limit=5.746933333333333
2024-10-09 00:57:07,122 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.95 vs. limit=9.139
2024-10-09 00:57:18,258 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=4370.666666666667, ans=0.03634166666666667
2024-10-09 00:57:26,479 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.53 vs. limit=10.7805
2024-10-09 00:57:27,137 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=4374.0, ans=0.29496875
2024-10-09 00:57:35,875 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=4377.333333333333, ans=0.009917971014492754
2024-10-09 00:57:37,311 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=4377.333333333333, ans=0.048427777777777785
2024-10-09 00:57:38,732 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=4377.333333333333, ans=0.048427777777777785
2024-10-09 00:58:04,498 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.69 vs. limit=3.6571
2024-10-09 00:58:05,274 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=4384.0, ans=0.2945
2024-10-09 00:58:06,599 INFO [train.py:1152] Epoch 2, batch 6300, loss[loss=0.2693, ctc_loss=0.2231, attn_decoder_loss=0.2808, over 4978.00 frames. ], tot_loss[loss=0.3413, ctc_loss=0.3186, attn_decoder_loss=0.347, over 966675.69 frames. ], batch size: 19, lr: 3.11e-02,
2024-10-09 00:58:06,801 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=4384.0, ans=0.2945
2024-10-09 00:58:08,785 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=23.12 vs. limit=10.788
2024-10-09 00:58:22,906 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=4387.333333333333, ans=0.025
2024-10-09 00:58:55,577 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=4394.0, ans=0.26591
2024-10-09 00:59:17,838 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.939e+01 9.227e+01 1.013e+02 1.151e+02 2.033e+02, threshold=2.027e+02, percent-clipped=0.0
2024-10-09 00:59:20,553 INFO [train.py:1152] Epoch 2, batch 6350, loss[loss=0.3382, ctc_loss=0.3141, attn_decoder_loss=0.3442, over 4816.00 frames. ], tot_loss[loss=0.3401, ctc_loss=0.3162, attn_decoder_loss=0.3461, over 966257.29 frames. ], batch size: 36, lr: 3.10e-02,
2024-10-09 00:59:40,409 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.96 vs. limit=10.803
2024-10-09 00:59:53,445 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.54 vs. limit=10.8055
2024-10-09 00:59:58,708 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=4407.333333333333, ans=0.025
2024-10-09 01:00:00,087 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.min_positive, batch_count=4407.333333333333, ans=0.07245416666666667
2024-10-09 01:00:33,768 INFO [train.py:1152] Epoch 2, batch 6400, loss[loss=0.3464, ctc_loss=0.3265, attn_decoder_loss=0.3514, over 4870.00 frames. ], tot_loss[loss=0.3385, ctc_loss=0.3142, attn_decoder_loss=0.3446, over 965976.39 frames. ], batch size: 23, lr: 3.10e-02,
2024-10-09 01:00:41,870 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.20 vs. limit=9.1565
2024-10-09 01:00:47,133 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4420.666666666667, ans=0.2557933333333333
2024-10-09 01:00:53,004 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=4420.666666666667, ans=0.025
2024-10-09 01:01:00,462 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=4420.666666666667, ans=0.7942066666666666
2024-10-09 01:01:01,042 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.38 vs. limit=9.15775
2024-10-09 01:01:17,147 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.53 vs. limit=10.8205
2024-10-09 01:01:31,266 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=4430.666666666667, ans=0.2923125
2024-10-09 01:01:37,044 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4430.666666666667, ans=0.25569333333333333
2024-10-09 01:01:39,940 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4430.666666666667, ans=0.25569333333333333
2024-10-09 01:01:44,120 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.406e+01 9.033e+01 1.041e+02 1.238e+02 2.300e+02, threshold=2.083e+02, percent-clipped=1.0
2024-10-09 01:01:45,010 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.48 vs. limit=6.107666666666667
2024-10-09 01:01:47,036 INFO [train.py:1152] Epoch 2, batch 6450, loss[loss=0.3367, ctc_loss=0.3047, attn_decoder_loss=0.3447, over 4752.00 frames. ], tot_loss[loss=0.3373, ctc_loss=0.312, attn_decoder_loss=0.3437, over 965310.76 frames. ], batch size: 26, lr: 3.09e-02,
2024-10-09 01:01:57,648 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4434.0, ans=0.25566
2024-10-09 01:01:58,193 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.99 vs. limit=10.8255
2024-10-09 01:02:00,780 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=4437.333333333333, ans=0.29200000000000004
2024-10-09 01:02:15,640 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=4440.666666666667, ans=0.7445766666666667
2024-10-09 01:02:15,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=4440.666666666667, ans=0.29184375
2024-10-09 01:02:22,303 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.98 vs. limit=10.8305
2024-10-09 01:02:23,032 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=4440.666666666667, ans=0.29184375
2024-10-09 01:02:35,280 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.85 vs. limit=6.111
2024-10-09 01:02:36,892 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.19 vs. limit=10.833
2024-10-09 01:02:37,345 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=4.88 vs. limit=4.8888
2024-10-09 01:02:40,667 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.min_positive, batch_count=4444.0, ans=0.07222500000000001
2024-10-09 01:02:48,682 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.34 vs. limit=10.8355
2024-10-09 01:03:01,403 INFO [train.py:1152] Epoch 2, batch 6500, loss[loss=0.3299, ctc_loss=0.3051, attn_decoder_loss=0.3361, over 4715.00 frames. ], tot_loss[loss=0.3369, ctc_loss=0.3105, attn_decoder_loss=0.3435, over 964908.23 frames. ], batch size: 26, lr: 3.09e-02,
2024-10-09 01:03:05,987 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=4450.666666666667, ans=0.03609166666666667
2024-10-09 01:03:11,843 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=4450.666666666667, ans=0.048122222222222226
2024-10-09 01:03:22,904 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.87 vs. limit=10.8405
2024-10-09 01:03:25,841 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.20 vs. limit=10.8405
2024-10-09 01:03:45,815 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.max_abs, batch_count=4460.666666666667, ans=7.787916666666667
2024-10-09 01:04:12,611 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.842e+01 9.553e+01 1.053e+02 1.192e+02 1.920e+02, threshold=2.106e+02, percent-clipped=0.0
2024-10-09 01:04:15,478 INFO [train.py:1152] Epoch 2, batch 6550, loss[loss=0.2821, ctc_loss=0.2454, attn_decoder_loss=0.2913, over 4978.00 frames. ], tot_loss[loss=0.3358, ctc_loss=0.3096, attn_decoder_loss=0.3424, over 964547.30 frames. ], batch size: 19, lr: 3.08e-02,
2024-10-09 01:04:20,639 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.40 vs. limit=10.8505
2024-10-09 01:04:36,389 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=4470.666666666667, ans=0.7435266666666667
2024-10-09 01:04:40,656 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=4470.666666666667, ans=0.2904375
2024-10-09 01:04:46,708 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=4474.0, ans=0.29028125
2024-10-09 01:04:46,779 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=4474.0, ans=0.048025000000000005
2024-10-09 01:04:55,313 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=14.51 vs. limit=10.8555
2024-10-09 01:05:00,875 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.54 vs. limit=3.6715999999999998
2024-10-09 01:05:07,453 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=4477.333333333333, ans=0.009896231884057971
2024-10-09 01:05:13,400 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=4480.666666666667, ans=0.28996875
2024-10-09 01:05:18,627 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.90 vs. limit=10.8605
2024-10-09 01:05:28,554 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=4484.0, ans=0.047983333333333336
2024-10-09 01:05:29,847 INFO [train.py:1152] Epoch 2, batch 6600, loss[loss=0.306, ctc_loss=0.254, attn_decoder_loss=0.319, over 4852.00 frames. ], tot_loss[loss=0.3349, ctc_loss=0.3076, attn_decoder_loss=0.3418, over 965213.75 frames. ], batch size: 23, lr: 3.08e-02,
2024-10-09 01:05:30,773 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=9.04 vs. limit=9.1815
2024-10-09 01:05:31,289 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=4484.0, ans=0.28981250000000003
2024-10-09 01:06:12,559 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.45 vs. limit=10.868
2024-10-09 01:06:21,791 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=13.28 vs. limit=9.18525
2024-10-09 01:06:41,982 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.125e+01 9.623e+01 1.027e+02 1.166e+02 2.174e+02, threshold=2.055e+02, percent-clipped=1.0
2024-10-09 01:06:44,926 INFO [train.py:1152] Epoch 2, batch 6650, loss[loss=0.3004, ctc_loss=0.2437, attn_decoder_loss=0.3146, over 4746.00 frames. ], tot_loss[loss=0.3331, ctc_loss=0.3047, attn_decoder_loss=0.3402, over 967021.79 frames. ], batch size: 20, lr: 3.07e-02,
2024-10-09 01:06:54,092 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff2_skip_rate, batch_count=4500.666666666667, ans=0.009891159420289855
2024-10-09 01:06:57,834 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.33 vs. limit=9.18775
2024-10-09 01:07:04,508 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=4504.0, ans=0.288875
2024-10-09 01:07:17,201 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.61 vs. limit=10.8805
2024-10-09 01:07:22,501 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=4507.333333333333, ans=0.28871875
2024-10-09 01:07:29,778 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=4510.666666666667, ans=7.819166666666667
2024-10-09 01:07:59,692 INFO [train.py:1152] Epoch 2, batch 6700, loss[loss=0.2949, ctc_loss=0.246, attn_decoder_loss=0.3071, over 4929.00 frames. ], tot_loss[loss=0.3304, ctc_loss=0.3009, attn_decoder_loss=0.3378, over 969157.59 frames. ], batch size: 20, lr: 3.07e-02,
2024-10-09 01:08:04,343 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=4517.333333333333, ans=0.28825
2024-10-09 01:08:17,880 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=4520.666666666667, ans=0.28809375000000004
2024-10-09 01:08:41,259 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.02 vs. limit=6.131
2024-10-09 01:08:43,819 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.24 vs. limit=9.19775
2024-10-09 01:08:51,008 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=4527.333333333333, ans=0.0
2024-10-09 01:08:54,004 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4527.333333333333, ans=0.25472666666666666
2024-10-09 01:09:01,393 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=4530.666666666667, ans=0.025
2024-10-09 01:09:12,029 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.499e+01 9.296e+01 1.038e+02 1.172e+02 3.103e+02, threshold=2.075e+02, percent-clipped=2.0
2024-10-09 01:09:15,061 INFO [train.py:1152] Epoch 2, batch 6750, loss[loss=0.3346, ctc_loss=0.3334, attn_decoder_loss=0.3349, over 4908.00 frames. ], tot_loss[loss=0.3278, ctc_loss=0.2985, attn_decoder_loss=0.3351, over 972224.50 frames. ], batch size: 19, lr: 3.07e-02,
2024-10-09 01:09:32,363 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.25 vs. limit=10.903
2024-10-09 01:09:34,265 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.45 vs. limit=7.268666666666666
2024-10-09 01:09:49,008 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.95 vs. limit=3.6811
2024-10-09 01:10:05,110 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=4544.0, ans=0.28700000000000003
2024-10-09 01:10:05,182 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=4544.0, ans=0.28700000000000003
2024-10-09 01:10:06,644 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=4544.0, ans=0.025
2024-10-09 01:10:10,181 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.55 vs. limit=10.908
2024-10-09 01:10:30,230 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.98 vs. limit=10.913
2024-10-09 01:10:30,568 INFO [train.py:1152] Epoch 2, batch 6800, loss[loss=0.3216, ctc_loss=0.2885, attn_decoder_loss=0.3299, over 4912.00 frames. ], tot_loss[loss=0.3241, ctc_loss=0.2938, attn_decoder_loss=0.3317, over 974552.59 frames. ], batch size: 19, lr: 3.06e-02,
2024-10-09 01:10:51,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=4554.0, ans=0.0
2024-10-09 01:10:55,100 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=4554.0, ans=0.04949747468305833
2024-10-09 01:10:57,539 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.27 vs. limit=7.277
2024-10-09 01:11:02,200 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=4.91 vs. limit=4.911466666666667
2024-10-09 01:11:10,203 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=4557.333333333333, ans=0.286375
2024-10-09 01:11:25,369 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=4560.666666666667, ans=0.0
2024-10-09 01:11:33,000 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=4564.0, ans=0.2860625
2024-10-09 01:11:38,309 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.47 vs. limit=9.211500000000001
2024-10-09 01:11:40,766 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=4564.0, ans=0.0
2024-10-09 01:11:43,590 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.453e+01 8.114e+01 9.479e+01 1.123e+02 1.653e+02, threshold=1.896e+02, percent-clipped=0.0
2024-10-09 01:11:46,676 INFO [train.py:1152] Epoch 2, batch 6850, loss[loss=0.3245, ctc_loss=0.2817, attn_decoder_loss=0.3352, over 4978.00 frames. ], tot_loss[loss=0.3202, ctc_loss=0.2896, attn_decoder_loss=0.3278, over 978900.94 frames. ], batch size: 19, lr: 3.06e-02,
2024-10-09 01:11:48,480 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_False/epoch-2.pt
2024-10-09 01:12:14,263 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=4568.0, ans=0.07
2024-10-09 01:12:15,144 INFO [train.py:1152] Epoch 3, batch 0, loss[loss=0.3359, ctc_loss=0.3021, attn_decoder_loss=0.3443, over 4853.00 frames. ], tot_loss[loss=0.3359, ctc_loss=0.3021, attn_decoder_loss=0.3443, over 4853.00 frames. ], batch size: 19, lr: 2.91e-02,
2024-10-09 01:12:15,145 INFO [train.py:1175] Computing validation loss
2024-10-09 01:12:22,187 INFO [train.py:1184] Epoch 3, validation: loss=0.2613, ctc_loss=0.1535, attn_decoder_loss=0.2882, over 90464.00 frames.
2024-10-09 01:12:22,187 INFO [train.py:1185] Maximum memory allocated so far is 6613MB
2024-10-09 01:12:33,356 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.45 vs. limit=9.213000000000001
2024-10-09 01:12:42,163 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=4571.333333333333, ans=0.28571875
2024-10-09 01:12:44,212 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.70 vs. limit=10.9285
2024-10-09 01:12:46,356 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=4571.333333333333, ans=0.28571875
2024-10-09 01:12:54,123 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.25 vs. limit=10.931000000000001
2024-10-09 01:12:56,423 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4574.666666666667, ans=0.25425333333333333
2024-10-09 01:13:08,067 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=4578.0, ans=0.28540625
2024-10-09 01:13:14,373 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.29 vs. limit=10.9335
2024-10-09 01:13:15,395 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=4578.0, ans=0.009874347826086956
2024-10-09 01:13:20,778 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=10.33 vs. limit=9.218
2024-10-09 01:13:32,850 INFO [train.py:1152] Epoch 3, batch 50, loss[loss=0.3244, ctc_loss=0.2749, attn_decoder_loss=0.3367, over 4910.00 frames. ], tot_loss[loss=0.3506, ctc_loss=0.335, attn_decoder_loss=0.3545, over 217751.89 frames. ], batch size: 19, lr: 2.90e-02,
2024-10-09 01:13:33,597 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.43 vs. limit=6.146166666666667
2024-10-09 01:13:46,276 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=4588.0, ans=0.2849375
2024-10-09 01:13:49,758 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.36 vs. limit=10.940999999999999
2024-10-09 01:13:50,621 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=4588.0, ans=0.04755
2024-10-09 01:13:52,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=4588.0, ans=0.025
2024-10-09 01:14:40,468 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.530e+01 9.250e+01 1.054e+02 1.194e+02 3.186e+02, threshold=2.108e+02, percent-clipped=4.0
2024-10-09 01:14:41,208 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.01 vs. limit=10.9485
2024-10-09 01:14:46,232 INFO [train.py:1152] Epoch 3, batch 100, loss[loss=0.3689, ctc_loss=0.3655, attn_decoder_loss=0.3697, over 4748.00 frames. ], tot_loss[loss=0.3457, ctc_loss=0.3287, attn_decoder_loss=0.35, over 383381.49 frames. ], batch size: 19, lr: 2.90e-02,
2024-10-09 01:14:53,050 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.10 vs. limit=10.951
2024-10-09 01:14:57,375 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=4601.333333333333, ans=0.7389533333333334
2024-10-09 01:15:32,683 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=4611.333333333333, ans=0.04949747468305833
2024-10-09 01:15:42,317 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.91 vs. limit=9.22925
2024-10-09 01:15:59,130 INFO [train.py:1152] Epoch 3, batch 150, loss[loss=0.3208, ctc_loss=0.2864, attn_decoder_loss=0.3294, over 4910.00 frames. ], tot_loss[loss=0.341, ctc_loss=0.32, attn_decoder_loss=0.3463, over 513301.12 frames. ], batch size: 19, lr: 2.89e-02,
2024-10-09 01:16:13,849 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=4621.333333333333, ans=0.04741111111111111
2024-10-09 01:16:17,501 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.01 vs. limit=7.310666666666666
2024-10-09 01:16:20,192 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.64 vs. limit=10.966000000000001
2024-10-09 01:16:34,374 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=4624.666666666667, ans=0.04739722222222222
2024-10-09 01:16:36,478 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.67 vs. limit=10.9685
2024-10-09 01:16:43,156 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=4628.0, ans=0.2830625
2024-10-09 01:16:54,960 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4628.0, ans=0.25372
2024-10-09 01:17:06,271 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.315e+01 9.252e+01 1.032e+02 1.174e+02 1.822e+02, threshold=2.065e+02, percent-clipped=0.0
2024-10-09 01:17:07,909 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=4631.333333333333, ans=0.28290625
2024-10-09 01:17:12,174 INFO [train.py:1152] Epoch 3, batch 200, loss[loss=0.3663, ctc_loss=0.363, attn_decoder_loss=0.3672, over 4732.00 frames. ], tot_loss[loss=0.338, ctc_loss=0.3146, attn_decoder_loss=0.3439, over 613779.36 frames. ], batch size: 45, lr: 2.89e-02,
2024-10-09 01:17:30,018 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=4638.0, ans=0.73767
2024-10-09 01:17:31,573 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=4638.0, ans=0.28259375
2024-10-09 01:17:41,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4641.333333333333, ans=0.2535866666666667
2024-10-09 01:17:41,804 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.prob, batch_count=4641.333333333333, ans=0.2824375
2024-10-09 01:17:52,591 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.78 vs. limit=10.981
2024-10-09 01:17:54,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=4644.666666666667, ans=0.28228125000000004
2024-10-09 01:18:03,770 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4644.666666666667, ans=0.2535533333333333
2024-10-09 01:18:08,553 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.98 vs. limit=9.24175
2024-10-09 01:18:11,610 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.82 vs. limit=10.986
2024-10-09 01:18:25,601 INFO [train.py:1152] Epoch 3, batch 250, loss[loss=0.3505, ctc_loss=0.314, attn_decoder_loss=0.3596, over 4819.00 frames. ], tot_loss[loss=0.3379, ctc_loss=0.3143, attn_decoder_loss=0.3438, over 692437.34 frames. ], batch size: 38, lr: 2.89e-02,
2024-10-09 01:18:50,606 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=4654.666666666667, ans=0.2818125
2024-10-09 01:18:59,461 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=4658.0, ans=0.28165625
2024-10-09 01:19:01,009 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=4658.0, ans=0.0
2024-10-09 01:19:01,769 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.70 vs. limit=3.6987
2024-10-09 01:19:07,645 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.71 vs. limit=10.993500000000001
2024-10-09 01:19:32,252 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=4664.666666666667, ans=0.28134375
2024-10-09 01:19:33,459 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.833e+01 8.984e+01 1.040e+02 1.148e+02 1.849e+02, threshold=2.080e+02, percent-clipped=0.0
2024-10-09 01:19:39,132 INFO [train.py:1152] Epoch 3, batch 300, loss[loss=0.3524, ctc_loss=0.3423, attn_decoder_loss=0.355, over 4751.00 frames. ], tot_loss[loss=0.3375, ctc_loss=0.3138, attn_decoder_loss=0.3434, over 752839.27 frames. ], batch size: 32, lr: 2.88e-02,
2024-10-09 01:19:42,312 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=4668.0, ans=0.04721666666666667
2024-10-09 01:19:52,584 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4671.333333333333, ans=0.25328666666666666
2024-10-09 01:20:27,920 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=4678.0, ans=0.28071875
2024-10-09 01:20:32,401 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=4678.0, ans=0.28071875
2024-10-09 01:20:43,631 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.21 vs. limit=11.011
2024-10-09 01:20:52,998 INFO [train.py:1152] Epoch 3, batch 350, loss[loss=0.2828, ctc_loss=0.2308, attn_decoder_loss=0.2958, over 4883.00 frames. ], tot_loss[loss=0.3359, ctc_loss=0.3118, attn_decoder_loss=0.342, over 800287.03 frames. ], batch size: 19, lr: 2.88e-02,
2024-10-09 01:21:16,266 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=13.30 vs. limit=11.016
2024-10-09 01:21:24,171 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=4691.333333333333, ans=0.04711944444444445
2024-10-09 01:21:32,233 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.94 vs. limit=11.0185
2024-10-09 01:21:44,181 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.74 vs. limit=11.021
2024-10-09 01:21:59,511 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=4698.0, ans=0.27978125
2024-10-09 01:22:00,781 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.482e+01 9.161e+01 1.026e+02 1.100e+02 2.324e+02, threshold=2.052e+02, percent-clipped=1.0
2024-10-09 01:22:05,941 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.86 vs. limit=7.350666666666666
2024-10-09 01:22:06,795 INFO [train.py:1152] Epoch 3, batch 400, loss[loss=0.333, ctc_loss=0.3036, attn_decoder_loss=0.3403, over 4891.00 frames. ], tot_loss[loss=0.3339, ctc_loss=0.308, attn_decoder_loss=0.3403, over 836908.40 frames. ], batch size: 22, lr: 2.87e-02,
2024-10-09 01:22:12,776 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=4701.333333333333, ans=0.04949747468305833
2024-10-09 01:22:45,716 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.30 vs. limit=11.031
2024-10-09 01:22:48,174 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.min_positive, batch_count=4708.0, ans=0.0352875
2024-10-09 01:22:56,146 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.68 vs. limit=9.26675
2024-10-09 01:23:11,616 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=4714.666666666667, ans=0.00984463768115942
2024-10-09 01:23:20,143 INFO [train.py:1152] Epoch 3, batch 450, loss[loss=0.3011, ctc_loss=0.2506, attn_decoder_loss=0.3138, over 4838.00 frames. ], tot_loss[loss=0.3327, ctc_loss=0.3061, attn_decoder_loss=0.3394, over 865371.94 frames. ], batch size: 23, lr: 2.87e-02,
2024-10-09 01:23:35,744 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.15 vs. limit=11.041
2024-10-09 01:23:41,619 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=13.02 vs. limit=9.2705
2024-10-09 01:23:47,199 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=4721.333333333333, ans=0.25278666666666666
2024-10-09 01:23:51,041 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=3.74 vs. limit=5.889866666666666
2024-10-09 01:23:53,636 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.88 vs. limit=11.0435
2024-10-09 01:24:00,959 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.71 vs. limit=11.0435
2024-10-09 01:24:05,431 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.51 vs. limit=11.046
2024-10-09 01:24:17,111 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4728.0, ans=0.25272
2024-10-09 01:24:28,918 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.382e+01 8.993e+01 1.010e+02 1.136e+02 1.922e+02, threshold=2.020e+02, percent-clipped=0.0
2024-10-09 01:24:34,759 INFO [train.py:1152] Epoch 3, batch 500, loss[loss=0.3398, ctc_loss=0.3188, attn_decoder_loss=0.345, over 4784.00 frames. ], tot_loss[loss=0.3323, ctc_loss=0.3048, attn_decoder_loss=0.3392, over 887930.17 frames. ], batch size: 34, lr: 2.87e-02,
2024-10-09 01:25:00,631 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.17 vs. limit=7.369
2024-10-09 01:25:08,820 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=4741.333333333333, ans=0.009838840579710146
2024-10-09 01:25:13,255 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=4741.333333333333, ans=0.27775
2024-10-09 01:25:35,931 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.63 vs. limit=3.7122
2024-10-09 01:25:40,697 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.96 vs. limit=7.3740000000000006
2024-10-09 01:25:48,523 INFO [train.py:1152] Epoch 3, batch 550, loss[loss=0.354, ctc_loss=0.3528, attn_decoder_loss=0.3543, over 4811.00 frames. ], tot_loss[loss=0.3334, ctc_loss=0.3067, attn_decoder_loss=0.34, over 905469.28 frames. ], batch size: 40, lr: 2.86e-02,
2024-10-09 01:25:49,244 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten.whitening_limit, batch_count=4751.333333333333, ans=9.28175
2024-10-09 01:25:54,482 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=4751.333333333333, ans=0.27728125000000003
2024-10-09 01:26:03,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=4754.666666666667, ans=0.09899494936611666
2024-10-09 01:26:10,145 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.99 vs. limit=9.283
2024-10-09 01:26:14,020 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=4754.666666666667, ans=0.009835942028985507
2024-10-09 01:26:54,132 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.97 vs. limit=6.191166666666667
2024-10-09 01:26:55,061 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=4764.666666666667, ans=0.0
2024-10-09 01:26:56,274 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.993e+01 9.237e+01 1.017e+02 1.162e+02 2.121e+02, threshold=2.033e+02, percent-clipped=1.0
2024-10-09 01:27:02,342 INFO [train.py:1152] Epoch 3, batch 600, loss[loss=0.3117, ctc_loss=0.3022, attn_decoder_loss=0.3141, over 4834.00 frames. ], tot_loss[loss=0.3329, ctc_loss=0.3057, attn_decoder_loss=0.3397, over 919397.50 frames. ], batch size: 38, lr: 2.86e-02,
2024-10-09 01:27:05,429 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=4768.0, ans=0.0468
2024-10-09 01:27:06,919 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=4768.0, ans=0.27649999999999997
2024-10-09 01:27:26,745 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=4771.333333333333, ans=0.27634375
2024-10-09 01:27:38,237 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=4774.666666666667, ans=0.009831594202898551
2024-10-09 01:27:39,644 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=4774.666666666667, ans=0.03507916666666667
2024-10-09 01:27:55,422 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=4778.0, ans=0.04675833333333333
2024-10-09 01:27:58,297 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=4778.0, ans=0.25222
2024-10-09 01:28:01,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.40 vs. limit=9.293
2024-10-09 01:28:15,685 INFO [train.py:1152] Epoch 3, batch 650, loss[loss=0.3661, ctc_loss=0.3507, attn_decoder_loss=0.3699, over 4830.00 frames. ], tot_loss[loss=0.331, ctc_loss=0.3028, attn_decoder_loss=0.338, over 930122.99 frames. ], batch size: 21, lr: 2.85e-02,
2024-10-09 01:28:21,092 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=13.66 vs. limit=11.0885
2024-10-09 01:28:34,859 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=4788.0, ans=0.2755625
2024-10-09 01:28:37,642 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.min_positive, batch_count=4788.0, ans=0.070075
2024-10-09 01:28:42,196 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=4788.0, ans=0.2755625
2024-10-09 01:28:59,199 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.69 vs. limit=3.7192
2024-10-09 01:29:05,608 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4794.666666666667, ans=0.25205333333333335
2024-10-09 01:29:10,549 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.23 vs. limit=11.096
2024-10-09 01:29:23,428 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.254e+01 8.848e+01 1.000e+02 1.143e+02 2.046e+02, threshold=2.001e+02, percent-clipped=1.0
2024-10-09 01:29:24,997 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=4798.0, ans=0.27509375
2024-10-09 01:29:29,213 INFO [train.py:1152] Epoch 3, batch 700, loss[loss=0.3123, ctc_loss=0.281, attn_decoder_loss=0.3201, over 4744.00 frames. ], tot_loss[loss=0.3319, ctc_loss=0.304, attn_decoder_loss=0.3388, over 937973.14 frames. ], batch size: 19, lr: 2.85e-02,
2024-10-09 01:29:33,746 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4801.333333333333, ans=0.2519866666666667
2024-10-09 01:29:38,187 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=4801.333333333333, ans=0.2749375
2024-10-09 01:29:42,599 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=4804.666666666667, ans=0.27478125
2024-10-09 01:30:00,062 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=4808.0, ans=0.274625
2024-10-09 01:30:10,942 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.36 vs. limit=11.106
2024-10-09 01:30:13,998 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=5.52 vs. limit=9.30425
2024-10-09 01:30:18,507 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=13.53 vs. limit=9.30425
2024-10-09 01:30:36,590 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.89 vs. limit=3.7222
2024-10-09 01:30:43,153 INFO [train.py:1152] Epoch 3, batch 750, loss[loss=0.3107, ctc_loss=0.261, attn_decoder_loss=0.3231, over 4881.00 frames. ], tot_loss[loss=0.33, ctc_loss=0.3019, attn_decoder_loss=0.337, over 944846.41 frames. ], batch size: 22, lr: 2.85e-02,
2024-10-09 01:30:54,926 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=4818.0, ans=0.27415625
2024-10-09 01:31:27,878 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=5.37 vs. limit=5.9312000000000005
2024-10-09 01:31:29,629 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.62 vs. limit=11.121
2024-10-09 01:31:50,726 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.311e+01 9.056e+01 1.010e+02 1.101e+02 1.630e+02, threshold=2.020e+02, percent-clipped=0.0
2024-10-09 01:31:56,713 INFO [train.py:1152] Epoch 3, batch 800, loss[loss=0.322, ctc_loss=0.2962, attn_decoder_loss=0.3285, over 4854.00 frames. ], tot_loss[loss=0.3292, ctc_loss=0.3014, attn_decoder_loss=0.3361, over 949610.04 frames. ], batch size: 19, lr: 2.84e-02,
2024-10-09 01:32:14,865 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=4838.0, ans=0.04650833333333333
2024-10-09 01:32:21,323 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.30 vs. limit=6.2095
2024-10-09 01:32:40,087 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=4844.666666666667, ans=0.03486041666666667
2024-10-09 01:33:10,961 INFO [train.py:1152] Epoch 3, batch 850, loss[loss=0.3291, ctc_loss=0.3031, attn_decoder_loss=0.3356, over 4784.00 frames. ], tot_loss[loss=0.3279, ctc_loss=0.2992, attn_decoder_loss=0.3351, over 953994.92 frames. ], batch size: 29, lr: 2.84e-02,
2024-10-09 01:33:11,115 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4851.333333333333, ans=0.25148666666666664
2024-10-09 01:33:35,084 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.17 vs. limit=11.141
2024-10-09 01:34:18,329 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.432e+01 8.321e+01 9.572e+01 1.106e+02 1.548e+02, threshold=1.914e+02, percent-clipped=0.0
2024-10-09 01:34:23,701 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.65 vs. limit=11.151
2024-10-09 01:34:24,276 INFO [train.py:1152] Epoch 3, batch 900, loss[loss=0.3105, ctc_loss=0.2658, attn_decoder_loss=0.3217, over 4853.00 frames. ], tot_loss[loss=0.3276, ctc_loss=0.299, attn_decoder_loss=0.3348, over 956834.94 frames. ], batch size: 19, lr: 2.83e-02,
2024-10-09 01:34:51,317 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.27 vs. limit=5.948533333333334
2024-10-09 01:34:52,750 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.72 vs. limit=11.156
2024-10-09 01:35:03,215 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten.whitening_limit, batch_count=4874.666666666667, ans=9.328
2024-10-09 01:35:33,420 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 01:35:37,662 INFO [train.py:1152] Epoch 3, batch 950, loss[loss=0.2727, ctc_loss=0.237, attn_decoder_loss=0.2816, over 4814.00 frames. ], tot_loss[loss=0.329, ctc_loss=0.301, attn_decoder_loss=0.3359, over 958831.21 frames. ], batch size: 19, lr: 2.83e-02,
2024-10-09 01:35:44,605 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.01 vs. limit=7.442333333333334
2024-10-09 01:35:48,124 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=4884.666666666667, ans=0.27103125
2024-10-09 01:35:57,315 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.95 vs. limit=9.333
2024-10-09 01:36:02,572 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4888.0, ans=0.25112
2024-10-09 01:36:06,146 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.47 vs. limit=3.7337
2024-10-09 01:36:44,844 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.556e+01 9.409e+01 1.058e+02 1.193e+02 2.041e+02, threshold=2.116e+02, percent-clipped=1.0
2024-10-09 01:36:47,122 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.33 vs. limit=11.1735
2024-10-09 01:36:50,792 INFO [train.py:1152] Epoch 3, batch 1000, loss[loss=0.3202, ctc_loss=0.2636, attn_decoder_loss=0.3343, over 4933.00 frames. ], tot_loss[loss=0.3309, ctc_loss=0.304, attn_decoder_loss=0.3376, over 960663.23 frames. ], batch size: 20, lr: 2.83e-02,
2024-10-09 01:37:13,493 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=4904.666666666667, ans=0.7283366666666666
2024-10-09 01:37:15,772 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.05 vs. limit=11.1785
2024-10-09 01:37:16,975 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.48 vs. limit=11.1785
2024-10-09 01:37:29,875 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=4908.0, ans=0.27362
2024-10-09 01:37:31,240 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=4908.0, ans=0.7282200000000001
2024-10-09 01:37:32,017 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.79 vs. limit=11.181000000000001
2024-10-09 01:37:50,481 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=4914.666666666667, ans=0.009801159420289855
2024-10-09 01:38:05,089 INFO [train.py:1152] Epoch 3, batch 1050, loss[loss=0.3347, ctc_loss=0.2931, attn_decoder_loss=0.3451, over 4808.00 frames. ], tot_loss[loss=0.3301, ctc_loss=0.3025, attn_decoder_loss=0.337, over 962743.06 frames. ], batch size: 25, lr: 2.82e-02,
2024-10-09 01:38:11,669 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.72 vs. limit=9.34425
2024-10-09 01:38:12,575 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=4918.0, ans=0.26946875000000003
2024-10-09 01:38:24,527 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.max_positive, batch_count=4921.333333333333, ans=0.7992133333333333
2024-10-09 01:38:25,089 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=88.84 vs. limit=9.3455
2024-10-09 01:38:47,284 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.94 vs. limit=11.1935
2024-10-09 01:38:48,970 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.82 vs. limit=9.348
2024-10-09 01:38:59,528 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.81 vs. limit=11.196
2024-10-09 01:39:00,283 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=4928.0, ans=0.7275200000000001
2024-10-09 01:39:00,745 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.64 vs. limit=5.9712
2024-10-09 01:39:13,536 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.828e+01 8.746e+01 9.882e+01 1.135e+02 2.810e+02, threshold=1.976e+02, percent-clipped=1.0
2024-10-09 01:39:20,256 INFO [train.py:1152] Epoch 3, batch 1100, loss[loss=0.3132, ctc_loss=0.2866, attn_decoder_loss=0.3199, over 4853.00 frames. ], tot_loss[loss=0.3298, ctc_loss=0.3016, attn_decoder_loss=0.3369, over 964075.80 frames. ], batch size: 20, lr: 2.82e-02,
2024-10-09 01:39:23,874 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=16.77 vs. limit=11.201
2024-10-09 01:39:29,622 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.30 vs. limit=6.233666666666666
2024-10-09 01:39:46,539 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.01 vs. limit=11.2035
2024-10-09 01:39:56,561 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.22 vs. limit=11.206
2024-10-09 01:39:56,648 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.65 vs. limit=3.7412
2024-10-09 01:39:56,670 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.00 vs. limit=9.353
2024-10-09 01:39:58,146 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.75 vs. limit=6.235333333333333
2024-10-09 01:40:03,475 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=4944.666666666667, ans=0.06909583333333333
2024-10-09 01:40:06,401 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=4944.666666666667, ans=0.26821875
2024-10-09 01:40:15,047 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4944.666666666667, ans=0.2505533333333333
2024-10-09 01:40:32,322 INFO [train.py:1152] Epoch 3, batch 1150, loss[loss=0.3461, ctc_loss=0.2966, attn_decoder_loss=0.3585, over 4849.00 frames. ], tot_loss[loss=0.3284, ctc_loss=0.2994, attn_decoder_loss=0.3356, over 964446.24 frames. ], batch size: 20, lr: 2.81e-02,
2024-10-09 01:40:45,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4954.666666666667, ans=0.2504533333333333
2024-10-09 01:40:53,710 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.53 vs. limit=11.216000000000001
2024-10-09 01:40:56,741 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.74 vs. limit=7.477333333333334
2024-10-09 01:41:18,819 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=4.94 vs. limit=4.992266666666667
2024-10-09 01:41:29,913 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.22 vs. limit=7.482333333333333
2024-10-09 01:41:37,710 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module1.whiten, num_groups=1, num_channels=192, metric=3.27 vs. limit=9.36175
2024-10-09 01:41:39,526 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.925e+01 8.755e+01 9.855e+01 1.113e+02 1.948e+02, threshold=1.971e+02, percent-clipped=0.0
2024-10-09 01:41:45,465 INFO [train.py:1152] Epoch 3, batch 1200, loss[loss=0.3074, ctc_loss=0.2596, attn_decoder_loss=0.3194, over 4816.00 frames. ], tot_loss[loss=0.3284, ctc_loss=0.2988, attn_decoder_loss=0.3359, over 964407.43 frames. ], batch size: 25, lr: 2.81e-02,
2024-10-09 01:41:59,481 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.66 vs. limit=7.485666666666667
2024-10-09 01:42:10,735 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=4971.333333333333, ans=0.7260033333333333
2024-10-09 01:42:12,106 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.min_positive, batch_count=4971.333333333333, ans=0.06892916666666668
2024-10-09 01:42:28,135 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=4978.0, ans=0.045925
2024-10-09 01:42:31,149 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=4978.0, ans=0.27466999999999997
2024-10-09 01:42:35,525 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=4978.0, ans=0.72577
2024-10-09 01:42:47,827 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.84 vs. limit=5.992533333333333
2024-10-09 01:42:54,659 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=4981.333333333333, ans=0.27472
2024-10-09 01:42:58,757 INFO [train.py:1152] Epoch 3, batch 1250, loss[loss=0.3629, ctc_loss=0.3529, attn_decoder_loss=0.3654, over 4796.00 frames. ], tot_loss[loss=0.3284, ctc_loss=0.2982, attn_decoder_loss=0.336, over 964358.53 frames. ], batch size: 32, lr: 2.81e-02,
2024-10-09 01:43:04,849 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=4984.666666666667, ans=0.7255366666666667
2024-10-09 01:43:12,390 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=4988.0, ans=0.04588333333333334
2024-10-09 01:43:18,281 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.min_positive, batch_count=4988.0, ans=0.0344125
2024-10-09 01:43:19,289 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.whiten.whitening_limit, batch_count=4988.0, ans=9.3705
2024-10-09 01:43:25,009 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.58 vs. limit=9.3705
2024-10-09 01:43:25,612 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=4988.0, ans=0.2661875
2024-10-09 01:43:32,139 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.18 vs. limit=9.37175
2024-10-09 01:43:37,555 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=4991.333333333333, ans=0.009784492753623189
2024-10-09 01:43:38,845 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=4991.333333333333, ans=0.26603125
2024-10-09 01:43:48,428 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.83 vs. limit=9.373
2024-10-09 01:43:51,494 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.59 vs. limit=11.246
2024-10-09 01:43:54,518 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.52 vs. limit=11.246
2024-10-09 01:44:07,100 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.558e+01 9.029e+01 9.560e+01 1.097e+02 1.858e+02, threshold=1.912e+02, percent-clipped=0.0
2024-10-09 01:44:12,815 INFO [train.py:1152] Epoch 3, batch 1300, loss[loss=0.349, ctc_loss=0.3355, attn_decoder_loss=0.3524, over 4821.00 frames. ], tot_loss[loss=0.3277, ctc_loss=0.298, attn_decoder_loss=0.3351, over 965497.62 frames. ], batch size: 43, lr: 2.80e-02,
2024-10-09 01:44:17,395 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=5001.333333333333, ans=0.26556250000000003
2024-10-09 01:44:51,344 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=5008.0, ans=0.009780869565217391
2024-10-09 01:44:52,101 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.24 vs. limit=7.504
2024-10-09 01:44:55,844 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=5011.333333333333, ans=8.132083333333334
2024-10-09 01:45:01,342 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.50 vs. limit=7.5056666666666665
2024-10-09 01:45:06,382 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=5011.333333333333, ans=0.26509375
2024-10-09 01:45:08,774 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.87 vs. limit=7.5056666666666665
2024-10-09 01:45:19,244 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=5.46 vs. limit=9.3805
2024-10-09 01:45:19,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5014.666666666667, ans=0.24985333333333332
2024-10-09 01:45:26,980 INFO [train.py:1152] Epoch 3, batch 1350, loss[loss=0.3592, ctc_loss=0.3427, attn_decoder_loss=0.3633, over 4859.00 frames. ], tot_loss[loss=0.3265, ctc_loss=0.2959, attn_decoder_loss=0.3342, over 966383.31 frames. ], batch size: 21, lr: 2.80e-02,
2024-10-09 01:45:27,087 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=5018.0, ans=0.009778695652173913
2024-10-09 01:45:31,569 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5018.0, ans=0.24982
2024-10-09 01:45:36,017 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=5018.0, ans=0.26478124999999997
2024-10-09 01:45:42,533 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.92 vs. limit=11.266
2024-10-09 01:45:55,811 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.70 vs. limit=11.2685
2024-10-09 01:46:00,179 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.18 vs. limit=11.2685
2024-10-09 01:46:01,947 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.99 vs. limit=11.2685
2024-10-09 01:46:16,582 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten.whitening_limit, batch_count=5028.0, ans=9.3855
2024-10-09 01:46:35,045 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.698e+01 9.104e+01 9.904e+01 1.153e+02 1.789e+02, threshold=1.981e+02, percent-clipped=0.0
2024-10-09 01:46:37,468 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.71 vs. limit=11.2735
2024-10-09 01:46:41,115 INFO [train.py:1152] Epoch 3, batch 1400, loss[loss=0.2925, ctc_loss=0.2428, attn_decoder_loss=0.3049, over 4940.00 frames. ], tot_loss[loss=0.3269, ctc_loss=0.2966, attn_decoder_loss=0.3345, over 966619.67 frames. ], batch size: 19, lr: 2.80e-02,
2024-10-09 01:46:41,298 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=5034.666666666667, ans=0.264
2024-10-09 01:46:47,788 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.12 vs. limit=11.276
2024-10-09 01:46:55,433 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.33 vs. limit=9.38925
2024-10-09 01:47:04,767 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=5038.0, ans=0.26384375
2024-10-09 01:47:04,845 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=5038.0, ans=0.72367
2024-10-09 01:47:23,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=5044.666666666667, ans=0.0
2024-10-09 01:47:32,426 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=5044.666666666667, ans=0.06847083333333334
2024-10-09 01:47:36,815 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=5044.666666666667, ans=0.7234366666666667
2024-10-09 01:47:54,236 INFO [train.py:1152] Epoch 3, batch 1450, loss[loss=0.3392, ctc_loss=0.3233, attn_decoder_loss=0.3431, over 4790.00 frames. ], tot_loss[loss=0.3268, ctc_loss=0.2963, attn_decoder_loss=0.3344, over 966514.54 frames. ], batch size: 34, lr: 2.79e-02,
2024-10-09 01:48:10,584 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=5054.666666666667, ans=0.0
2024-10-09 01:48:11,217 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=25.99 vs. limit=9.3955
2024-10-09 01:48:45,939 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=5061.333333333333, ans=0.06836666666666667
2024-10-09 01:48:50,199 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=5061.333333333333, ans=0.7228533333333333
2024-10-09 01:48:56,769 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=13.44 vs. limit=9.39925
2024-10-09 01:48:58,217 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.35 vs. limit=6.266166666666667
2024-10-09 01:49:02,019 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.222e+01 9.093e+01 9.797e+01 1.151e+02 1.594e+02, threshold=1.959e+02, percent-clipped=0.0
2024-10-09 01:49:02,589 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.82 vs. limit=11.2985
2024-10-09 01:49:07,764 INFO [train.py:1152] Epoch 3, batch 1500, loss[loss=0.3286, ctc_loss=0.2952, attn_decoder_loss=0.3369, over 4725.00 frames. ], tot_loss[loss=0.3275, ctc_loss=0.2973, attn_decoder_loss=0.335, over 966200.56 frames. ], batch size: 26, lr: 2.79e-02,
2024-10-09 01:49:16,667 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 01:49:16,749 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=5068.0, ans=0.2624375
2024-10-09 01:49:16,759 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=5068.0, ans=0.2624375
2024-10-09 01:49:32,151 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.77 vs. limit=9.40175
2024-10-09 01:49:35,013 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.65 vs. limit=3.7607
2024-10-09 01:49:42,447 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.24 vs. limit=6.268666666666666
2024-10-09 01:49:44,497 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=5074.666666666667, ans=0.262125
2024-10-09 01:50:01,925 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.max_abs, batch_count=5078.0, ans=8.17375
2024-10-09 01:50:01,974 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=5078.0, ans=0.0682625
2024-10-09 01:50:19,503 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=5084.666666666667, ans=0.009764202898550725
2024-10-09 01:50:20,897 INFO [train.py:1152] Epoch 3, batch 1550, loss[loss=0.3312, ctc_loss=0.2872, attn_decoder_loss=0.3422, over 4843.00 frames. ], tot_loss[loss=0.3289, ctc_loss=0.3004, attn_decoder_loss=0.336, over 966131.15 frames. ], batch size: 31, lr: 2.79e-02,
2024-10-09 01:50:28,280 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=5084.666666666667, ans=0.26165625000000003
2024-10-09 01:50:40,418 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=5088.0, ans=0.2615
2024-10-09 01:50:44,631 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=5088.0, ans=0.2615
2024-10-09 01:50:49,519 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.04 vs. limit=11.3185
2024-10-09 01:50:51,187 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.83 vs. limit=11.3185
2024-10-09 01:50:54,643 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=5091.333333333333, ans=0.26134375
2024-10-09 01:50:55,680 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=13.76 vs. limit=11.3185
2024-10-09 01:50:58,961 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=5091.333333333333, ans=0.7218033333333334
2024-10-09 01:51:15,288 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.79 vs. limit=11.321
2024-10-09 01:51:19,229 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=5098.0, ans=0.26103125
2024-10-09 01:51:19,869 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=7.04 vs. limit=9.41175
2024-10-09 01:51:27,699 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.243e+01 9.187e+01 1.016e+02 1.253e+02 2.843e+02, threshold=2.031e+02, percent-clipped=3.0
2024-10-09 01:51:33,726 INFO [train.py:1152] Epoch 3, batch 1600, loss[loss=0.3166, ctc_loss=0.2856, attn_decoder_loss=0.3243, over 4813.00 frames. ], tot_loss[loss=0.3287, ctc_loss=0.2997, attn_decoder_loss=0.3359, over 966293.26 frames. ], batch size: 25, lr: 2.78e-02,
2024-10-09 01:51:40,233 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.43 vs. limit=6.275333333333333
2024-10-09 01:51:49,067 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.38 vs. limit=11.3285
2024-10-09 01:51:52,821 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=5104.666666666667, ans=0.26071875
2024-10-09 01:51:58,629 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=5104.666666666667, ans=0.009759855072463769
2024-10-09 01:52:11,689 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=5108.0, ans=0.04538333333333334
2024-10-09 01:52:14,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=5108.0, ans=0.26056250000000003
2024-10-09 01:52:17,304 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=14.41 vs. limit=11.3335
2024-10-09 01:52:18,308 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.93 vs. limit=11.3335
2024-10-09 01:52:25,421 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.26 vs. limit=11.3335
2024-10-09 01:52:44,318 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=5114.666666666667, ans=0.025
2024-10-09 01:52:47,047 INFO [train.py:1152] Epoch 3, batch 1650, loss[loss=0.2921, ctc_loss=0.2382, attn_decoder_loss=0.3055, over 4769.00 frames. ], tot_loss[loss=0.3283, ctc_loss=0.2996, attn_decoder_loss=0.3354, over 966869.12 frames. ], batch size: 29, lr: 2.78e-02,
2024-10-09 01:52:54,560 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=5118.0, ans=0.0
2024-10-09 01:53:01,939 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=5121.333333333333, ans=0.24878666666666666
2024-10-09 01:53:09,224 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=5121.333333333333, ans=0.04532777777777778
2024-10-09 01:53:09,404 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-09 01:53:12,796 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.64 vs. limit=11.341000000000001
2024-10-09 01:53:12,800 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.35 vs. limit=6.280333333333333
2024-10-09 01:53:21,618 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.44 vs. limit=6.281166666666667
2024-10-09 01:53:28,219 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=5124.666666666667, ans=0.0
2024-10-09 01:53:30,370 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.80 vs. limit=11.346
2024-10-09 01:53:44,439 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=5131.333333333333, ans=0.25946875
2024-10-09 01:53:50,242 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=5131.333333333333, ans=0.035
2024-10-09 01:53:54,666 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.037e+01 8.754e+01 9.553e+01 1.094e+02 1.966e+02, threshold=1.911e+02, percent-clipped=1.0
2024-10-09 01:54:00,640 INFO [train.py:1152] Epoch 3, batch 1700, loss[loss=0.3435, ctc_loss=0.3119, attn_decoder_loss=0.3514, over 4940.00 frames. ], tot_loss[loss=0.326, ctc_loss=0.2965, attn_decoder_loss=0.3334, over 967050.32 frames. ], batch size: 19, lr: 2.77e-02,
2024-10-09 01:54:02,790 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten.whitening_limit, batch_count=5134.666666666667, ans=9.4255
2024-10-09 01:54:38,306 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=11.32 vs. limit=9.428
2024-10-09 01:54:52,009 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=5144.666666666667, ans=0.25884375000000004
2024-10-09 01:54:53,571 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=5144.666666666667, ans=0.7199366666666667
2024-10-09 01:55:10,324 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=6.95 vs. limit=9.4305
2024-10-09 01:55:13,861 INFO [train.py:1152] Epoch 3, batch 1750, loss[loss=0.2844, ctc_loss=0.2256, attn_decoder_loss=0.299, over 4959.00 frames. ], tot_loss[loss=0.3238, ctc_loss=0.2933, attn_decoder_loss=0.3314, over 967082.60 frames. ], batch size: 19, lr: 2.77e-02,
2024-10-09 01:55:24,383 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=2.531e-01
2024-10-09 01:55:32,365 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.54 vs. limit=11.366
2024-10-09 01:55:33,326 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=5154.666666666667, ans=0.7195866666666667
2024-10-09 01:55:33,931 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=26.25 vs. limit=9.433
2024-10-09 01:55:34,789 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=5154.666666666667, ans=0.009748985507246377
2024-10-09 01:55:43,056 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.41 vs. limit=11.368500000000001
2024-10-09 01:55:50,339 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.72 vs. limit=7.579000000000001
2024-10-09 01:55:53,819 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=5158.0, ans=0.009748260869565218
2024-10-09 01:56:01,774 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=2.63 vs. limit=9.4355
2024-10-09 01:56:07,333 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.18 vs. limit=11.371
2024-10-09 01:56:09,289 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.93 vs. limit=11.371
2024-10-09 01:56:21,311 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.494e+01 9.003e+01 1.039e+02 1.165e+02 2.287e+02, threshold=2.077e+02, percent-clipped=2.0
2024-10-09 01:56:21,513 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=5164.666666666667, ans=0.25790625
2024-10-09 01:56:27,306 INFO [train.py:1152] Epoch 3, batch 1800, loss[loss=0.3308, ctc_loss=0.2913, attn_decoder_loss=0.3406, over 4882.00 frames. ], tot_loss[loss=0.3253, ctc_loss=0.2953, attn_decoder_loss=0.3327, over 967932.67 frames. ], batch size: 23, lr: 2.77e-02,
2024-10-09 01:56:27,437 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5168.0, ans=0.24831999999999999
2024-10-09 01:56:31,849 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=5168.0, ans=0.71912
2024-10-09 01:56:36,269 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=2.505e-03
2024-10-09 01:56:40,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=5171.333333333333, ans=0.04511944444444445
2024-10-09 01:57:00,480 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.88 vs. limit=6.069866666666667
2024-10-09 01:57:19,629 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.60 vs. limit=6.0712
2024-10-09 01:57:27,040 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.84 vs. limit=9.443
2024-10-09 01:57:40,656 INFO [train.py:1152] Epoch 3, batch 1850, loss[loss=0.3387, ctc_loss=0.3146, attn_decoder_loss=0.3447, over 4733.00 frames. ], tot_loss[loss=0.3249, ctc_loss=0.2945, attn_decoder_loss=0.3325, over 968082.61 frames. ], batch size: 26, lr: 2.76e-02,
2024-10-09 01:57:40,775 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=5184.666666666667, ans=0.25696874999999997
2024-10-09 01:57:43,178 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.82 vs. limit=3.7777
2024-10-09 01:57:46,702 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=5184.666666666667, ans=0.25696874999999997
2024-10-09 01:58:37,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten.whitening_limit, batch_count=5194.666666666667, ans=9.448
2024-10-09 01:58:38,094 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=5198.0, ans=0.009739565217391305
2024-10-09 01:58:46,097 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.61 vs. limit=7.599
2024-10-09 01:58:48,170 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.635e+01 8.712e+01 9.556e+01 1.066e+02 1.648e+02, threshold=1.911e+02, percent-clipped=0.0
2024-10-09 01:58:51,190 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=5198.0, ans=0.25634375
2024-10-09 01:58:53,931 INFO [train.py:1152] Epoch 3, batch 1900, loss[loss=0.3693, ctc_loss=0.3631, attn_decoder_loss=0.3709, over 4781.00 frames. ], tot_loss[loss=0.3266, ctc_loss=0.296, attn_decoder_loss=0.3342, over 967979.38 frames. ], batch size: 29, lr: 2.76e-02,
2024-10-09 01:58:57,683 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.31 vs. limit=11.401
2024-10-09 01:59:08,630 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=5204.666666666667, ans=0.7178366666666667
2024-10-09 01:59:22,013 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=5208.0, ans=0.71772
2024-10-09 01:59:29,280 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=5208.0, ans=0.25587499999999996
2024-10-09 01:59:29,341 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=5208.0, ans=0.25587499999999996
2024-10-09 01:59:30,019 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.73 vs. limit=3.7812
2024-10-09 01:59:37,571 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.71 vs. limit=9.45425
2024-10-09 01:59:42,951 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.27 vs. limit=7.605666666666666
2024-10-09 02:00:07,234 INFO [train.py:1152] Epoch 3, batch 1950, loss[loss=0.3409, ctc_loss=0.2959, attn_decoder_loss=0.3522, over 4860.00 frames. ], tot_loss[loss=0.3258, ctc_loss=0.295, attn_decoder_loss=0.3335, over 966832.86 frames. ], batch size: 20, lr: 2.76e-02,
2024-10-09 02:00:09,057 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=5218.0, ans=0.025
2024-10-09 02:00:09,643 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.98 vs. limit=11.413499999999999
2024-10-09 02:00:18,138 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.66 vs. limit=6.0872
2024-10-09 02:00:24,935 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=5221.333333333333, ans=0.25525
2024-10-09 02:00:45,320 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5224.666666666667, ans=0.24775333333333333
2024-10-09 02:00:56,952 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5228.0, ans=0.24772
2024-10-09 02:01:11,111 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.60 vs. limit=11.4235
2024-10-09 02:01:13,555 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=5231.333333333333, ans=0.25478124999999996
2024-10-09 02:01:14,692 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.092e+01 9.434e+01 1.043e+02 1.223e+02 2.094e+02, threshold=2.085e+02, percent-clipped=1.0
2024-10-09 02:01:15,723 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.39 vs. limit=9.46175
2024-10-09 02:01:20,660 INFO [train.py:1152] Epoch 3, batch 2000, loss[loss=0.2906, ctc_loss=0.2332, attn_decoder_loss=0.305, over 4959.00 frames. ], tot_loss[loss=0.328, ctc_loss=0.2981, attn_decoder_loss=0.3354, over 966651.70 frames. ], batch size: 19, lr: 2.75e-02,
2024-10-09 02:01:25,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=5234.666666666667, ans=0.7167866666666667
2024-10-09 02:01:31,185 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.40 vs. limit=9.463000000000001
2024-10-09 02:01:39,485 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=5238.0, ans=0.25446875
2024-10-09 02:01:59,241 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.25 vs. limit=6.310333333333333
2024-10-09 02:02:09,495 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.36 vs. limit=11.4335
2024-10-09 02:02:11,608 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=2.666e-02
2024-10-09 02:02:33,051 INFO [train.py:1152] Epoch 3, batch 2050, loss[loss=0.3117, ctc_loss=0.2921, attn_decoder_loss=0.3165, over 4911.00 frames. ], tot_loss[loss=0.3273, ctc_loss=0.2967, attn_decoder_loss=0.335, over 967015.70 frames. ], batch size: 19, lr: 2.75e-02,
2024-10-09 02:02:36,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=5251.333333333333, ans=0.24748666666666666
2024-10-09 02:02:57,600 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=5.14 vs. limit=5.050933333333333
2024-10-09 02:03:06,072 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.36 vs. limit=11.4435
2024-10-09 02:03:08,369 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=5258.0, ans=0.025
2024-10-09 02:03:14,153 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=5258.0, ans=0.25353125
2024-10-09 02:03:15,637 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=5261.333333333333, ans=0.04474444444444445
2024-10-09 02:03:15,641 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=5261.333333333333, ans=0.24738666666666667
2024-10-09 02:03:21,388 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=5261.333333333333, ans=0.24738666666666667
2024-10-09 02:03:25,249 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=13.34 vs. limit=11.446
2024-10-09 02:03:26,435 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.49 vs. limit=11.446
2024-10-09 02:03:39,334 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=5264.666666666667, ans=0.25321875
2024-10-09 02:03:40,543 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.515e+01 8.501e+01 9.745e+01 1.128e+02 1.964e+02, threshold=1.949e+02, percent-clipped=0.0
2024-10-09 02:03:42,161 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=5264.666666666667, ans=0.06709583333333333
2024-10-09 02:03:46,473 INFO [train.py:1152] Epoch 3, batch 2100, loss[loss=0.2681, ctc_loss=0.2151, attn_decoder_loss=0.2814, over 4868.00 frames. ], tot_loss[loss=0.3249, ctc_loss=0.293, attn_decoder_loss=0.3329, over 967194.34 frames. ], batch size: 21, lr: 2.75e-02,
2024-10-09 02:03:53,010 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=15.33 vs. limit=9.4755
2024-10-09 02:04:02,527 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=5271.333333333333, ans=0.7155033333333334
2024-10-09 02:04:06,198 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.68 vs. limit=11.4535
2024-10-09 02:04:08,310 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=5271.333333333333, ans=0.25290625
2024-10-09 02:04:09,751 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=5271.333333333333, ans=0.7155033333333334
2024-10-09 02:04:20,801 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.12 vs. limit=9.478
2024-10-09 02:04:23,690 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.85 vs. limit=3.7912
2024-10-09 02:04:45,416 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=5281.333333333333, ans=0.7151533333333333
2024-10-09 02:04:54,181 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=5281.333333333333, ans=0.07
2024-10-09 02:04:58,661 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=5284.666666666667, ans=0.19715333333333335
2024-10-09 02:04:59,405 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.62 vs. limit=3.7927
2024-10-09 02:04:59,906 INFO [train.py:1152] Epoch 3, batch 2150, loss[loss=0.3323, ctc_loss=0.3077, attn_decoder_loss=0.3385, over 4858.00 frames. ], tot_loss[loss=0.3234, ctc_loss=0.2911, attn_decoder_loss=0.3315, over 967926.42 frames. ], batch size: 20, lr: 2.74e-02,
2024-10-09 02:05:05,944 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=5284.666666666667, ans=0.25228125
2024-10-09 02:05:06,607 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.66 vs. limit=11.4635
2024-10-09 02:05:08,018 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.99 vs. limit=9.48175
2024-10-09 02:05:08,865 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.max_abs, batch_count=5284.666666666667, ans=8.302916666666667
2024-10-09 02:05:24,133 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.66 vs. limit=11.466000000000001
2024-10-09 02:05:28,491 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.24 vs. limit=9.48425
2024-10-09 02:05:35,166 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=5291.333333333333, ans=0.025
2024-10-09 02:05:44,226 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=5.52 vs. limit=6.117866666666667
2024-10-09 02:05:48,094 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=5294.666666666667, ans=0.009718550724637681
2024-10-09 02:05:50,585 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=10.17 vs. limit=9.4855
2024-10-09 02:05:54,043 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=5294.666666666667, ans=0.7146866666666667
2024-10-09 02:06:06,873 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.913e+01 8.878e+01 9.960e+01 1.121e+02 1.620e+02, threshold=1.992e+02, percent-clipped=0.0
2024-10-09 02:06:08,993 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.50 vs. limit=11.4735
2024-10-09 02:06:12,702 INFO [train.py:1152] Epoch 3, batch 2200, loss[loss=0.31, ctc_loss=0.2956, attn_decoder_loss=0.3136, over 4734.00 frames. ], tot_loss[loss=0.3239, ctc_loss=0.2923, attn_decoder_loss=0.3318, over 967621.86 frames. ], batch size: 26, lr: 2.74e-02,
2024-10-09 02:06:47,515 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.47 vs. limit=7.654
2024-10-09 02:06:56,315 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.75 vs. limit=11.4835
2024-10-09 02:07:02,390 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.86 vs. limit=7.655666666666667
2024-10-09 02:07:26,130 INFO [train.py:1152] Epoch 3, batch 2250, loss[loss=0.3045, ctc_loss=0.2585, attn_decoder_loss=0.316, over 4871.00 frames. ], tot_loss[loss=0.3245, ctc_loss=0.2933, attn_decoder_loss=0.3323, over 967584.85 frames. ], batch size: 22, lr: 2.73e-02,
2024-10-09 02:07:31,237 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.93 vs. limit=11.4885
2024-10-09 02:08:02,891 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=13.97 vs. limit=11.493500000000001
2024-10-09 02:08:32,818 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_False/checkpoint-16000.pt
2024-10-09 02:08:35,245 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.220e+01 8.935e+01 1.004e+02 1.160e+02 1.727e+02, threshold=2.008e+02, percent-clipped=0.0
2024-10-09 02:08:40,403 INFO [train.py:1152] Epoch 3, batch 2300, loss[loss=0.2988, ctc_loss=0.2586, attn_decoder_loss=0.3088, over 4883.00 frames. ], tot_loss[loss=0.3237, ctc_loss=0.2918, attn_decoder_loss=0.3317, over 968262.63 frames. ], batch size: 19, lr: 2.73e-02,
2024-10-09 02:08:49,255 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=5334.666666666667, ans=0.24993749999999998
2024-10-09 02:09:01,342 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.05 vs. limit=6.3345
2024-10-09 02:09:14,119 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.38 vs. limit=7.6706666666666665
2024-10-09 02:09:14,236 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.78 vs. limit=11.506
2024-10-09 02:09:40,984 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=5348.0, ans=0.2493125
2024-10-09 02:09:43,923 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=5348.0, ans=0.28022
2024-10-09 02:09:52,478 INFO [train.py:1152] Epoch 3, batch 2350, loss[loss=0.294, ctc_loss=0.2444, attn_decoder_loss=0.3063, over 4888.00 frames. ], tot_loss[loss=0.3233, ctc_loss=0.2912, attn_decoder_loss=0.3314, over 968426.88 frames. ], batch size: 23, lr: 2.73e-02,
2024-10-09 02:09:58,485 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=5351.333333333333, ans=0.24915625000000002
2024-10-09 02:09:59,252 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=5351.333333333333, ans=11.5135
2024-10-09 02:10:33,854 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.10 vs. limit=9.50925
2024-10-09 02:10:36,868 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.87 vs. limit=11.521
2024-10-09 02:10:39,191 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=5361.333333333333, ans=0.2486875
2024-10-09 02:10:46,830 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.17 vs. limit=9.5105
2024-10-09 02:10:48,624 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=5361.333333333333, ans=0.009704057971014493
2024-10-09 02:10:58,502 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=5364.666666666667, ans=0.24853124999999998
2024-10-09 02:10:58,515 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5364.666666666667, ans=0.2463533333333333
2024-10-09 02:10:59,717 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.664e+01 8.414e+01 9.392e+01 1.100e+02 1.810e+02, threshold=1.878e+02, percent-clipped=0.0
2024-10-09 02:11:03,302 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.66 vs. limit=11.5235
2024-10-09 02:11:05,406 INFO [train.py:1152] Epoch 3, batch 2400, loss[loss=0.3139, ctc_loss=0.2871, attn_decoder_loss=0.3206, over 4755.00 frames. ], tot_loss[loss=0.3228, ctc_loss=0.2912, attn_decoder_loss=0.3307, over 967756.35 frames. ], batch size: 19, lr: 2.72e-02,
2024-10-09 02:11:05,575 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=5368.0, ans=0.248375
2024-10-09 02:11:08,329 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=5368.0, ans=0.248375
2024-10-09 02:11:14,720 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.20 vs. limit=11.526
2024-10-09 02:11:19,364 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=5.21 vs. limit=5.0742666666666665
2024-10-09 02:11:20,310 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.85 vs. limit=11.528500000000001
2024-10-09 02:11:23,119 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.33 vs. limit=7.685666666666666
2024-10-09 02:11:28,446 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=5371.333333333333, ans=0.025
2024-10-09 02:11:52,910 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=5378.0, ans=0.24790625
2024-10-09 02:12:17,573 INFO [train.py:1152] Epoch 3, batch 2450, loss[loss=0.3315, ctc_loss=0.3009, attn_decoder_loss=0.3391, over 4884.00 frames. ], tot_loss[loss=0.3241, ctc_loss=0.2925, attn_decoder_loss=0.3321, over 966894.84 frames. ], batch size: 22, lr: 2.72e-02,
2024-10-09 02:12:40,100 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.06 vs. limit=9.5205
2024-10-09 02:13:03,299 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=12.45 vs. limit=9.523
2024-10-09 02:13:05,611 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=5394.666666666667, ans=0.24712499999999998
2024-10-09 02:13:24,514 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.074e+01 9.336e+01 1.042e+02 1.167e+02 2.221e+02, threshold=2.085e+02, percent-clipped=2.0
2024-10-09 02:13:27,493 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=5398.0, ans=0.24696875000000001
2024-10-09 02:13:30,249 INFO [train.py:1152] Epoch 3, batch 2500, loss[loss=0.3335, ctc_loss=0.3058, attn_decoder_loss=0.3404, over 4725.00 frames. ], tot_loss[loss=0.3237, ctc_loss=0.2924, attn_decoder_loss=0.3316, over 966508.97 frames. ], batch size: 26, lr: 2.72e-02,
2024-10-09 02:13:39,846 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.77 vs. limit=9.525500000000001
2024-10-09 02:14:07,122 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=5408.0, ans=0.07
2024-10-09 02:14:25,422 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.07 vs. limit=11.5585
2024-10-09 02:14:38,394 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.22 vs. limit=11.561
2024-10-09 02:14:43,643 INFO [train.py:1152] Epoch 3, batch 2550, loss[loss=0.278, ctc_loss=0.2179, attn_decoder_loss=0.293, over 4959.00 frames. ], tot_loss[loss=0.3239, ctc_loss=0.2929, attn_decoder_loss=0.3317, over 967015.16 frames. ], batch size: 19, lr: 2.71e-02,
2024-10-09 02:15:01,307 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=5421.333333333333, ans=0.7102533333333334
2024-10-09 02:15:01,360 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=5421.333333333333, ans=0.009691014492753624
2024-10-09 02:15:02,754 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=5421.333333333333, ans=0.245875
2024-10-09 02:15:04,952 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.45 vs. limit=11.565999999999999
2024-10-09 02:15:14,515 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=5424.666666666667, ans=0.24571874999999999
2024-10-09 02:15:39,021 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.min_positive, batch_count=5428.0, ans=0.066075
2024-10-09 02:15:43,507 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=5431.333333333333, ans=0.044036111111111116
2024-10-09 02:15:45,627 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.92 vs. limit=11.5735
2024-10-09 02:15:50,605 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.358e+01 8.659e+01 9.597e+01 1.066e+02 2.029e+02, threshold=1.919e+02, percent-clipped=0.0
2024-10-09 02:15:56,425 INFO [train.py:1152] Epoch 3, batch 2600, loss[loss=0.3071, ctc_loss=0.2713, attn_decoder_loss=0.316, over 4861.00 frames. ], tot_loss[loss=0.3231, ctc_loss=0.2919, attn_decoder_loss=0.331, over 966572.97 frames. ], batch size: 20, lr: 2.71e-02,
2024-10-09 02:15:56,603 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=5434.666666666667, ans=0.24525000000000002
2024-10-09 02:16:08,246 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=5434.666666666667, ans=0.009688115942028985
2024-10-09 02:16:08,941 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.85 vs. limit=3.8152
2024-10-09 02:17:00,795 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=5448.0, ans=0.24462499999999998
2024-10-09 02:17:03,669 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=5448.0, ans=0.04949747468305833
2024-10-09 02:17:08,306 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 02:17:09,474 INFO [train.py:1152] Epoch 3, batch 2650, loss[loss=0.3647, ctc_loss=0.3716, attn_decoder_loss=0.363, over 4824.00 frames. ], tot_loss[loss=0.3244, ctc_loss=0.2941, attn_decoder_loss=0.332, over 966183.60 frames. ], batch size: 38, lr: 2.71e-02,
2024-10-09 02:17:19,244 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.48 vs. limit=6.180533333333333
2024-10-09 02:17:26,291 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=3.04 vs. limit=9.5455
2024-10-09 02:17:41,005 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.35 vs. limit=11.593499999999999
2024-10-09 02:17:44,507 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=5458.0, ans=0.24415625000000002
2024-10-09 02:17:56,710 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.80 vs. limit=9.548
2024-10-09 02:18:04,867 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=5461.333333333333, ans=0.244
2024-10-09 02:18:16,486 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.649e+01 9.252e+01 1.024e+02 1.181e+02 2.561e+02, threshold=2.048e+02, percent-clipped=1.0
2024-10-09 02:18:18,091 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5464.666666666667, ans=0.2453533333333333
2024-10-09 02:18:19,499 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=5464.666666666667, ans=0.24384375000000003
2024-10-09 02:18:22,236 INFO [train.py:1152] Epoch 3, batch 2700, loss[loss=0.3598, ctc_loss=0.3304, attn_decoder_loss=0.3671, over 4859.00 frames. ], tot_loss[loss=0.325, ctc_loss=0.295, attn_decoder_loss=0.3325, over 966413.52 frames. ], batch size: 28, lr: 2.70e-02,
2024-10-09 02:18:34,094 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=5468.0, ans=0.04388333333333334
2024-10-09 02:18:47,385 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=5471.333333333333, ans=0.24353124999999998
2024-10-09 02:19:00,559 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=5474.666666666667, ans=0.032891666666666666
2024-10-09 02:19:10,856 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=5478.0, ans=0.24321874999999998
2024-10-09 02:19:35,444 INFO [train.py:1152] Epoch 3, batch 2750, loss[loss=0.3089, ctc_loss=0.265, attn_decoder_loss=0.3199, over 4802.00 frames. ], tot_loss[loss=0.3233, ctc_loss=0.2924, attn_decoder_loss=0.331, over 967104.01 frames. ], batch size: 19, lr: 2.70e-02,
2024-10-09 02:19:57,696 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=5488.0, ans=0.009676521739130434
2024-10-09 02:20:13,895 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=5491.333333333333, ans=0.043786111111111116
2024-10-09 02:20:15,688 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=16.13 vs. limit=9.55925
2024-10-09 02:20:17,225 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=5491.333333333333, ans=11.618500000000001
2024-10-09 02:20:25,302 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer_na.min_abs, batch_count=5494.666666666667, ans=0.02
2024-10-09 02:20:26,704 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=5494.666666666667, ans=0.009675072463768116
2024-10-09 02:20:37,662 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.07 vs. limit=6.3745
2024-10-09 02:20:40,201 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=15.40 vs. limit=9.56175
2024-10-09 02:20:42,204 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.156e+01 8.638e+01 9.603e+01 1.079e+02 1.649e+02, threshold=1.921e+02, percent-clipped=0.0
2024-10-09 02:20:46,773 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=5501.333333333333, ans=0.009673623188405797
2024-10-09 02:20:47,954 INFO [train.py:1152] Epoch 3, batch 2800, loss[loss=0.3341, ctc_loss=0.3105, attn_decoder_loss=0.3401, over 4771.00 frames. ], tot_loss[loss=0.3224, ctc_loss=0.2914, attn_decoder_loss=0.3302, over 967090.38 frames. ], batch size: 53, lr: 2.70e-02,
2024-10-09 02:21:05,416 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=5504.666666666667, ans=0.8050466666666667
2024-10-09 02:21:06,965 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 02:21:14,030 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=5504.666666666667, ans=0.24196875
2024-10-09 02:21:23,251 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=24.98 vs. limit=7.754
2024-10-09 02:21:26,890 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=5508.0, ans=0.065575
2024-10-09 02:21:44,402 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=5514.666666666667, ans=0.2415
2024-10-09 02:21:46,198 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=3.68 vs. limit=9.568
2024-10-09 02:21:58,315 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.84 vs. limit=3.8272
2024-10-09 02:22:00,277 INFO [train.py:1152] Epoch 3, batch 2850, loss[loss=0.2969, ctc_loss=0.2514, attn_decoder_loss=0.3082, over 4934.00 frames. ], tot_loss[loss=0.3234, ctc_loss=0.2931, attn_decoder_loss=0.331, over 967037.05 frames. ], batch size: 20, lr: 2.69e-02,
2024-10-09 02:22:06,872 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.86 vs. limit=6.2072
2024-10-09 02:22:09,083 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.min_abs, batch_count=5518.0, ans=0.28277
2024-10-09 02:22:09,801 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.72 vs. limit=11.6385
2024-10-09 02:22:38,304 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=5524.666666666667, ans=0.24103124999999997
2024-10-09 02:22:52,858 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=5528.0, ans=0.240875
2024-10-09 02:23:07,185 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.756e+01 8.951e+01 9.887e+01 1.087e+02 2.185e+02, threshold=1.977e+02, percent-clipped=1.0
2024-10-09 02:23:12,951 INFO [train.py:1152] Epoch 3, batch 2900, loss[loss=0.2816, ctc_loss=0.2349, attn_decoder_loss=0.2933, over 4752.00 frames. ], tot_loss[loss=0.3227, ctc_loss=0.2915, attn_decoder_loss=0.3306, over 966250.42 frames. ], batch size: 20, lr: 2.69e-02,
2024-10-09 02:23:23,911 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.36 vs. limit=6.383666666666667
2024-10-09 02:23:28,220 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.44 vs. limit=11.653500000000001
2024-10-09 02:23:43,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5541.333333333333, ans=0.24458666666666667
2024-10-09 02:23:45,799 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.75 vs. limit=11.655999999999999
2024-10-09 02:23:50,908 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=5541.333333333333, ans=0.24025000000000002
2024-10-09 02:23:50,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=5541.333333333333, ans=0.24025000000000002
2024-10-09 02:24:17,437 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.27 vs. limit=11.661
2024-10-09 02:24:25,221 INFO [train.py:1152] Epoch 3, batch 2950, loss[loss=0.3097, ctc_loss=0.2607, attn_decoder_loss=0.3219, over 4799.00 frames. ], tot_loss[loss=0.3227, ctc_loss=0.2913, attn_decoder_loss=0.3305, over 966736.13 frames. ], batch size: 19, lr: 2.69e-02,
2024-10-09 02:24:28,279 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=5551.333333333333, ans=0.23978125
2024-10-09 02:24:29,697 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5551.333333333333, ans=0.24448666666666666
2024-10-09 02:24:45,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=5554.666666666667, ans=0.23962499999999998
2024-10-09 02:24:50,272 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=5554.666666666667, ans=0.025
2024-10-09 02:24:58,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.70 vs. limit=3.8337
2024-10-09 02:25:00,538 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=5558.0, ans=0.23946875
2024-10-09 02:25:02,553 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.06 vs. limit=11.6685
2024-10-09 02:25:06,392 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5558.0, ans=0.24442
2024-10-09 02:25:07,913 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=5561.333333333333, ans=0.23931249999999998
2024-10-09 02:25:32,647 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.417e+01 8.416e+01 9.556e+01 1.098e+02 2.061e+02, threshold=1.911e+02, percent-clipped=1.0
2024-10-09 02:25:38,606 INFO [train.py:1152] Epoch 3, batch 3000, loss[loss=0.3192, ctc_loss=0.2881, attn_decoder_loss=0.327, over 4852.00 frames. ], tot_loss[loss=0.3208, ctc_loss=0.2884, attn_decoder_loss=0.3288, over 967399.23 frames. ], batch size: 21, lr: 2.68e-02,
2024-10-09 02:25:38,606 INFO [train.py:1175] Computing validation loss
2024-10-09 02:25:45,456 INFO [zipformer.py:1858] name=encoder.encoders.2.encoder.layers.2.self_attn_weights, attn_weights_entropy = tensor([3.4121, 3.5397, 3.2607, 3.2469], device='cuda:0')
2024-10-09 02:25:47,359 INFO [train.py:1184] Epoch 3, validation: loss=0.2448, ctc_loss=0.1273, attn_decoder_loss=0.2742, over 90464.00 frames.
2024-10-09 02:25:47,359 INFO [train.py:1185] Maximum memory allocated so far is 6613MB
2024-10-09 02:25:56,076 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5568.0, ans=0.24431999999999998
2024-10-09 02:26:14,218 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.50 vs. limit=11.6785
2024-10-09 02:26:20,673 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=5574.666666666667, ans=0.0
2024-10-09 02:26:23,596 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5574.666666666667, ans=0.24425333333333332
2024-10-09 02:26:35,286 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=5578.0, ans=0.24422
2024-10-09 02:26:53,224 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.69 vs. limit=6.395333333333333
2024-10-09 02:26:58,256 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=5584.666666666667, ans=0.23821874999999998
2024-10-09 02:26:59,580 INFO [train.py:1152] Epoch 3, batch 3050, loss[loss=0.2876, ctc_loss=0.2278, attn_decoder_loss=0.3026, over 4751.00 frames. ], tot_loss[loss=0.3209, ctc_loss=0.2881, attn_decoder_loss=0.3291, over 966865.61 frames. ], batch size: 19, lr: 2.68e-02,
2024-10-09 02:27:01,178 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.min_abs, batch_count=5584.666666666667, ans=0.28377
2024-10-09 02:27:11,429 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5584.666666666667, ans=0.24415333333333333
2024-10-09 02:27:13,317 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.98 vs. limit=11.690999999999999
2024-10-09 02:27:39,744 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.22 vs. limit=11.6935
2024-10-09 02:27:49,363 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=5594.666666666667, ans=0.04335555555555556
2024-10-09 02:27:58,745 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=19.11 vs. limit=9.59925
2024-10-09 02:28:02,428 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=5598.0, ans=0.04334166666666667
2024-10-09 02:28:03,914 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=5598.0, ans=0.04334166666666667
2024-10-09 02:28:06,922 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.630e+01 9.039e+01 9.933e+01 1.100e+02 2.128e+02, threshold=1.987e+02, percent-clipped=2.0
2024-10-09 02:28:07,141 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=5598.0, ans=0.70407
2024-10-09 02:28:12,648 INFO [train.py:1152] Epoch 3, batch 3100, loss[loss=0.3077, ctc_loss=0.2862, attn_decoder_loss=0.313, over 4835.00 frames. ], tot_loss[loss=0.3201, ctc_loss=0.2874, attn_decoder_loss=0.3283, over 966586.56 frames. ], batch size: 38, lr: 2.68e-02,
2024-10-09 02:28:18,812 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=5601.333333333333, ans=0.23743750000000002
2024-10-09 02:28:33,991 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2.whitening_limit, batch_count=5604.666666666667, ans=7.802333333333333
2024-10-09 02:28:42,045 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=5608.0, ans=0.009650434782608696
2024-10-09 02:29:12,683 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=5614.666666666667, ans=0.23681249999999998
2024-10-09 02:29:17,673 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.88 vs. limit=9.6055
2024-10-09 02:29:24,200 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=5618.0, ans=0.23665625
2024-10-09 02:29:25,367 INFO [train.py:1152] Epoch 3, batch 3150, loss[loss=0.4057, ctc_loss=0.4269, attn_decoder_loss=0.4004, over 4808.00 frames. ], tot_loss[loss=0.3195, ctc_loss=0.287, attn_decoder_loss=0.3276, over 966855.46 frames. ], batch size: 40, lr: 2.67e-02,
2024-10-09 02:29:40,235 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=5621.333333333333, ans=0.043244444444444446
2024-10-09 02:29:48,057 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=21.86 vs. limit=9.608
2024-10-09 02:29:53,842 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=5.55 vs. limit=7.812333333333333
2024-10-09 02:30:00,411 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=5624.666666666667, ans=0.23634375000000002
2024-10-09 02:30:26,640 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=5631.333333333333, ans=0.7029033333333334
2024-10-09 02:30:32,232 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.284e+01 8.538e+01 9.697e+01 1.116e+02 1.711e+02, threshold=1.939e+02, percent-clipped=0.0
2024-10-09 02:30:38,133 INFO [train.py:1152] Epoch 3, batch 3200, loss[loss=0.3477, ctc_loss=0.3179, attn_decoder_loss=0.3552, over 4752.00 frames. ], tot_loss[loss=0.3194, ctc_loss=0.2866, attn_decoder_loss=0.3276, over 967342.17 frames. ], batch size: 20, lr: 2.67e-02,
2024-10-09 02:30:41,798 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=11.54 vs. limit=9.613
2024-10-09 02:30:51,270 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=5638.0, ans=0.23571874999999998
2024-10-09 02:30:52,354 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=5.09 vs. limit=5.1276
2024-10-09 02:31:00,123 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=5638.0, ans=0.23571874999999998
2024-10-09 02:31:24,770 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=5644.666666666667, ans=0.7024366666666667
2024-10-09 02:31:32,509 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=9.40 vs. limit=9.61675
2024-10-09 02:31:39,338 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=5648.0, ans=0.24352
2024-10-09 02:31:50,889 INFO [train.py:1152] Epoch 3, batch 3250, loss[loss=0.3242, ctc_loss=0.3047, attn_decoder_loss=0.3291, over 4864.00 frames. ], tot_loss[loss=0.3195, ctc_loss=0.2866, attn_decoder_loss=0.3277, over 967433.32 frames. ], batch size: 24, lr: 2.67e-02,
2024-10-09 02:32:01,983 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=13.48 vs. limit=9.619250000000001
2024-10-09 02:32:15,314 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.85 vs. limit=7.827333333333334
2024-10-09 02:32:21,837 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=5658.0, ans=0.23478125
2024-10-09 02:32:39,118 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=5661.333333333333, ans=0.043077777777777784
2024-10-09 02:32:49,544 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=5664.666666666667, ans=0.04306388888888889
2024-10-09 02:32:50,937 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5664.666666666667, ans=0.2433533333333333
2024-10-09 02:32:55,421 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=5664.666666666667, ans=0.009638115942028985
2024-10-09 02:32:56,299 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=16.24 vs. limit=11.7485
2024-10-09 02:32:58,409 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.697e+01 8.879e+01 9.924e+01 1.121e+02 1.327e+02, threshold=1.985e+02, percent-clipped=0.0
2024-10-09 02:33:04,324 INFO [train.py:1152] Epoch 3, batch 3300, loss[loss=0.3526, ctc_loss=0.339, attn_decoder_loss=0.356, over 4828.00 frames. ], tot_loss[loss=0.3177, ctc_loss=0.284, attn_decoder_loss=0.3262, over 967978.69 frames. ], batch size: 43, lr: 2.66e-02,
2024-10-09 02:33:06,435 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.62 vs. limit=11.751000000000001
2024-10-09 02:33:14,516 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=5668.0, ans=0.23431249999999998
2024-10-09 02:33:25,487 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.86 vs. limit=9.62675
2024-10-09 02:33:27,772 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=5671.333333333333, ans=0.043036111111111115
2024-10-09 02:33:41,430 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2.whitening_limit, batch_count=5674.666666666667, ans=7.8373333333333335
2024-10-09 02:33:56,799 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5678.0, ans=0.24322
2024-10-09 02:34:14,113 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=5681.333333333333, ans=0.7011533333333334
2024-10-09 02:34:16,801 INFO [train.py:1152] Epoch 3, batch 3350, loss[loss=0.3259, ctc_loss=0.3122, attn_decoder_loss=0.3294, over 4792.00 frames. ], tot_loss[loss=0.3191, ctc_loss=0.2858, attn_decoder_loss=0.3275, over 967075.10 frames. ], batch size: 40, lr: 2.66e-02,
2024-10-09 02:34:23,101 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=4.10 vs. limit=9.63175
2024-10-09 02:34:29,974 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=5688.0, ans=0.70092
2024-10-09 02:34:36,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.61 vs. limit=9.633
2024-10-09 02:34:45,862 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=5691.333333333333, ans=0.23321874999999997
2024-10-09 02:34:53,171 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=5691.333333333333, ans=0.06442916666666668
2024-10-09 02:35:23,734 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.934e+01 8.833e+01 9.909e+01 1.087e+02 1.666e+02, threshold=1.982e+02, percent-clipped=0.0
2024-10-09 02:35:26,792 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=5698.0, ans=0.042925000000000005
2024-10-09 02:35:28,296 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=5701.333333333333, ans=0.042911111111111115
2024-10-09 02:35:29,681 INFO [train.py:1152] Epoch 3, batch 3400, loss[loss=0.2757, ctc_loss=0.2552, attn_decoder_loss=0.2808, over 4959.00 frames. ], tot_loss[loss=0.3192, ctc_loss=0.2867, attn_decoder_loss=0.3274, over 966800.52 frames. ], batch size: 19, lr: 2.66e-02,
2024-10-09 02:35:31,290 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=5701.333333333333, ans=0.23275
2024-10-09 02:35:40,042 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=5701.333333333333, ans=0.23275
2024-10-09 02:35:48,780 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=5704.666666666667, ans=0.03217291666666667
2024-10-09 02:35:50,255 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=5704.666666666667, ans=0.23259375
2024-10-09 02:35:56,235 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=5704.666666666667, ans=0.025
2024-10-09 02:35:58,382 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.08 vs. limit=3.8562
2024-10-09 02:36:05,423 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.04 vs. limit=3.8562
2024-10-09 02:36:07,861 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=5708.0, ans=0.23243750000000002
2024-10-09 02:36:20,219 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.97 vs. limit=6.284533333333333
2024-10-09 02:36:34,883 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.02 vs. limit=11.786
2024-10-09 02:36:42,901 INFO [train.py:1152] Epoch 3, batch 3450, loss[loss=0.3374, ctc_loss=0.3248, attn_decoder_loss=0.3406, over 4841.00 frames. ], tot_loss[loss=0.3202, ctc_loss=0.2876, attn_decoder_loss=0.3284, over 966952.54 frames. ], batch size: 43, lr: 2.65e-02,
2024-10-09 02:37:01,959 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=5721.333333333333, ans=0.025
2024-10-09 02:37:12,159 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=5724.666666666667, ans=0.23165625
2024-10-09 02:37:16,587 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.70 vs. limit=3.8587
2024-10-09 02:37:16,937 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=5724.666666666667, ans=0.03211041666666667
2024-10-09 02:37:20,172 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=12.07 vs. limit=7.862333333333334
2024-10-09 02:37:25,431 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=5728.0, ans=0.69952
2024-10-09 02:37:30,117 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.87 vs. limit=11.796
2024-10-09 02:37:35,388 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=5728.0, ans=0.0
2024-10-09 02:37:42,503 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=5731.333333333333, ans=0.009623623188405797
2024-10-09 02:37:46,866 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=5731.333333333333, ans=0.6994033333333334
2024-10-09 02:37:49,797 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.623e+01 8.889e+01 9.853e+01 1.128e+02 2.152e+02, threshold=1.971e+02, percent-clipped=1.0
2024-10-09 02:37:52,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=5731.333333333333, ans=0.23134375000000001
2024-10-09 02:37:55,527 INFO [train.py:1152] Epoch 3, batch 3500, loss[loss=0.2998, ctc_loss=0.2326, attn_decoder_loss=0.3166, over 4883.00 frames. ], tot_loss[loss=0.321, ctc_loss=0.2895, attn_decoder_loss=0.3289, over 967293.16 frames. ], batch size: 19, lr: 2.65e-02,
2024-10-09 02:37:57,160 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=5734.666666666667, ans=0.2311875
2024-10-09 02:37:57,661 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=18.04 vs. limit=11.801
2024-10-09 02:38:21,936 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=5738.0, ans=0.042758333333333336
2024-10-09 02:38:42,693 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.69 vs. limit=11.8085
2024-10-09 02:39:03,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.22 vs. limit=9.6555
2024-10-09 02:39:04,920 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.63 vs. limit=6.437
2024-10-09 02:39:08,262 INFO [train.py:1152] Epoch 3, batch 3550, loss[loss=0.3258, ctc_loss=0.3144, attn_decoder_loss=0.3287, over 4778.00 frames. ], tot_loss[loss=0.3197, ctc_loss=0.2874, attn_decoder_loss=0.3277, over 967149.41 frames. ], batch size: 29, lr: 2.65e-02,
2024-10-09 02:39:09,291 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.32 vs. limit=9.65675
2024-10-09 02:39:20,039 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=5751.333333333333, ans=0.009619275362318842
2024-10-09 02:39:37,539 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=5758.0, ans=0.23009374999999999
2024-10-09 02:39:39,562 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.16 vs. limit=11.8185
2024-10-09 02:39:44,621 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=5758.0, ans=0.23009374999999999
2024-10-09 02:39:49,099 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=5758.0, ans=0.23009374999999999
2024-10-09 02:39:51,995 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=5761.333333333333, ans=0.009617101449275363
2024-10-09 02:39:52,813 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=3.94 vs. limit=6.3045333333333335
2024-10-09 02:40:13,791 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=5764.666666666667, ans=0.22978125
2024-10-09 02:40:15,052 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.660e+01 8.630e+01 9.816e+01 1.106e+02 1.921e+02, threshold=1.963e+02, percent-clipped=0.0
2024-10-09 02:40:20,978 INFO [train.py:1152] Epoch 3, batch 3600, loss[loss=0.3144, ctc_loss=0.2595, attn_decoder_loss=0.3281, over 4929.00 frames. ], tot_loss[loss=0.3188, ctc_loss=0.2853, attn_decoder_loss=0.3271, over 967229.23 frames. ], batch size: 20, lr: 2.64e-02,
2024-10-09 02:40:26,901 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.8.prob, batch_count=5768.0, ans=0.22962500000000002
2024-10-09 02:40:28,499 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=5768.0, ans=0.22962500000000002
2024-10-09 02:40:41,758 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=5771.333333333333, ans=0.025
2024-10-09 02:41:02,258 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=5774.666666666667, ans=0.042605555555555556
2024-10-09 02:41:10,079 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.80 vs. limit=6.4445
2024-10-09 02:41:10,868 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5778.0, ans=0.24222
2024-10-09 02:41:27,331 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.26 vs. limit=11.836
2024-10-09 02:41:33,887 INFO [train.py:1152] Epoch 3, batch 3650, loss[loss=0.3209, ctc_loss=0.2877, attn_decoder_loss=0.3291, over 4854.00 frames. ], tot_loss[loss=0.3186, ctc_loss=0.285, attn_decoder_loss=0.3269, over 967729.50 frames. ], batch size: 31, lr: 2.64e-02,
2024-10-09 02:41:34,628 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.46 vs. limit=9.66925
2024-10-09 02:41:41,248 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=5784.666666666667, ans=0.22884375
2024-10-09 02:41:44,243 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=2.159e-01
2024-10-09 02:41:44,263 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=5784.666666666667, ans=0.22884375
2024-10-09 02:41:55,807 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=5788.0, ans=0.009611304347826087
2024-10-09 02:42:11,852 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=5791.333333333333, ans=0.09899494936611666
2024-10-09 02:42:25,470 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.06 vs. limit=9.673
2024-10-09 02:42:40,905 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.285e+01 8.720e+01 9.653e+01 1.086e+02 2.471e+02, threshold=1.931e+02, percent-clipped=1.0
2024-10-09 02:42:46,640 INFO [train.py:1152] Epoch 3, batch 3700, loss[loss=0.3177, ctc_loss=0.288, attn_decoder_loss=0.3251, over 4862.00 frames. ], tot_loss[loss=0.3175, ctc_loss=0.283, attn_decoder_loss=0.3262, over 967099.11 frames. ], batch size: 24, lr: 2.64e-02,
2024-10-09 02:43:04,330 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=5804.666666666667, ans=0.22790624999999998
2024-10-09 02:43:08,194 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.53 vs. limit=11.8535
2024-10-09 02:43:08,816 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=5804.666666666667, ans=0.025
2024-10-09 02:43:11,836 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=5804.666666666667, ans=0.042480555555555556
2024-10-09 02:43:14,162 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.54 vs. limit=11.8535
2024-10-09 02:43:14,721 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=5808.0, ans=0.22775
2024-10-09 02:43:19,136 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=5808.0, ans=0.009606956521739131
2024-10-09 02:43:23,607 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-09 02:43:36,702 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=5811.333333333333, ans=0.0
2024-10-09 02:43:41,154 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5811.333333333333, ans=0.24188666666666667
2024-10-09 02:43:59,986 INFO [train.py:1152] Epoch 3, batch 3750, loss[loss=0.2768, ctc_loss=0.2201, attn_decoder_loss=0.291, over 4959.00 frames. ], tot_loss[loss=0.3181, ctc_loss=0.2843, attn_decoder_loss=0.3265, over 967529.72 frames. ], batch size: 19, lr: 2.63e-02,
2024-10-09 02:44:09,788 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.64 vs. limit=9.681750000000001
2024-10-09 02:44:14,970 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=5821.333333333333, ans=0.24178666666666668
2024-10-09 02:44:17,879 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=5821.333333333333, ans=0.22712500000000002
2024-10-09 02:44:25,127 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=5821.333333333333, ans=0.22712500000000002
2024-10-09 02:44:41,621 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=4.95 vs. limit=9.68425
2024-10-09 02:44:54,240 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=5828.0, ans=0.0
2024-10-09 02:44:57,854 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.12 vs. limit=7.915666666666667
2024-10-09 02:45:00,736 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=11.87 vs. limit=9.68675
2024-10-09 02:45:02,979 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5831.333333333333, ans=0.24168666666666666
2024-10-09 02:45:05,036 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.94 vs. limit=11.8735
2024-10-09 02:45:07,129 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.010e+01 8.532e+01 9.750e+01 1.120e+02 2.534e+02, threshold=1.950e+02, percent-clipped=2.0
2024-10-09 02:45:13,019 INFO [train.py:1152] Epoch 3, batch 3800, loss[loss=0.3093, ctc_loss=0.2905, attn_decoder_loss=0.314, over 4773.00 frames. ], tot_loss[loss=0.3166, ctc_loss=0.2819, attn_decoder_loss=0.3252, over 967356.79 frames. ], batch size: 26, lr: 2.63e-02,
2024-10-09 02:45:20,561 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=5834.666666666667, ans=0.042355555555555556
2024-10-09 02:45:58,875 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.90 vs. limit=11.8835
2024-10-09 02:46:12,756 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=5848.0, ans=0.0
2024-10-09 02:46:13,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.36 vs. limit=11.886
2024-10-09 02:46:25,911 INFO [train.py:1152] Epoch 3, batch 3850, loss[loss=0.3379, ctc_loss=0.3071, attn_decoder_loss=0.3455, over 4825.00 frames. ], tot_loss[loss=0.3144, ctc_loss=0.2784, attn_decoder_loss=0.3234, over 967355.20 frames. ], batch size: 38, lr: 2.63e-02,
2024-10-09 02:46:28,962 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=5851.333333333333, ans=0.009597536231884057
2024-10-09 02:46:43,625 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=5854.666666666667, ans=0.2255625
2024-10-09 02:46:47,215 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.84 vs. limit=11.891
2024-10-09 02:46:50,936 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=5854.666666666667, ans=0.042272222222222225
2024-10-09 02:47:19,886 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=5861.333333333333, ans=0.22525
2024-10-09 02:47:31,827 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=5864.666666666667, ans=0.22509374999999998
2024-10-09 02:47:32,993 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.510e+01 8.608e+01 9.501e+01 1.055e+02 1.567e+02, threshold=1.900e+02, percent-clipped=0.0
2024-10-09 02:47:38,768 INFO [train.py:1152] Epoch 3, batch 3900, loss[loss=0.3352, ctc_loss=0.3082, attn_decoder_loss=0.3419, over 4700.00 frames. ], tot_loss[loss=0.3148, ctc_loss=0.2793, attn_decoder_loss=0.3236, over 966869.22 frames. ], batch size: 26, lr: 2.63e-02,
2024-10-09 02:47:41,815 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.prob, batch_count=5868.0, ans=0.2249375
2024-10-09 02:47:43,200 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=5868.0, ans=0.2249375
2024-10-09 02:47:48,950 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=5868.0, ans=0.025
2024-10-09 02:47:49,110 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 02:47:56,771 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.11 vs. limit=11.903500000000001
2024-10-09 02:48:13,428 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=5874.666666666667, ans=0.22462500000000002
2024-10-09 02:48:13,915 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.96 vs. limit=11.905999999999999
2024-10-09 02:48:15,686 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.59 vs. limit=6.468666666666667
2024-10-09 02:48:51,766 INFO [train.py:1152] Epoch 3, batch 3950, loss[loss=0.3543, ctc_loss=0.3415, attn_decoder_loss=0.3575, over 4831.00 frames. ], tot_loss[loss=0.3139, ctc_loss=0.2784, attn_decoder_loss=0.3228, over 967039.88 frames. ], batch size: 36, lr: 2.62e-02,
2024-10-09 02:49:06,681 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=5888.0, ans=0.22399999999999998
2024-10-09 02:49:09,971 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.16 vs. limit=11.916
2024-10-09 02:49:12,460 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=5888.0, ans=0.22399999999999998
2024-10-09 02:49:43,198 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=5894.666666666667, ans=0.09899494936611666
2024-10-09 02:49:54,819 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=5898.0, ans=0.22353125000000001
2024-10-09 02:49:58,904 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.562e+01 8.721e+01 9.694e+01 1.134e+02 2.029e+02, threshold=1.939e+02, percent-clipped=1.0
2024-10-09 02:50:01,925 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=5898.0, ans=0.22353125000000001
2024-10-09 02:50:04,752 INFO [train.py:1152] Epoch 3, batch 4000, loss[loss=0.3139, ctc_loss=0.2574, attn_decoder_loss=0.328, over 4818.00 frames. ], tot_loss[loss=0.3132, ctc_loss=0.2774, attn_decoder_loss=0.3222, over 967180.30 frames. ], batch size: 19, lr: 2.62e-02,
2024-10-09 02:50:14,567 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.96 vs. limit=3.8852
2024-10-09 02:50:34,157 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=5908.0, ans=0.009585217391304348
2024-10-09 02:50:42,174 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.15 vs. limit=9.7155
2024-10-09 02:51:10,859 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=5914.666666666667, ans=0.0
2024-10-09 02:51:18,058 INFO [train.py:1152] Epoch 3, batch 4050, loss[loss=0.3464, ctc_loss=0.3217, attn_decoder_loss=0.3525, over 4778.00 frames. ], tot_loss[loss=0.3151, ctc_loss=0.2797, attn_decoder_loss=0.3239, over 967493.81 frames. ], batch size: 53, lr: 2.62e-02,
2024-10-09 02:51:18,178 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=5918.0, ans=0.22259374999999998
2024-10-09 02:51:27,609 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=18.76 vs. limit=11.938500000000001
2024-10-09 02:51:29,957 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=5918.0, ans=0.22259374999999998
2024-10-09 02:51:46,356 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.78 vs. limit=9.72175
2024-10-09 02:51:48,864 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=5924.666666666667, ans=0.009581594202898551
2024-10-09 02:52:25,465 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.328e+01 8.524e+01 9.889e+01 1.116e+02 2.008e+02, threshold=1.978e+02, percent-clipped=2.0
2024-10-09 02:52:29,051 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.46 vs. limit=11.9485
2024-10-09 02:52:29,933 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=5934.666666666667, ans=0.22181250000000002
2024-10-09 02:52:31,199 INFO [train.py:1152] Epoch 3, batch 4100, loss[loss=0.3265, ctc_loss=0.2928, attn_decoder_loss=0.3349, over 4855.00 frames. ], tot_loss[loss=0.3168, ctc_loss=0.2821, attn_decoder_loss=0.3254, over 966874.33 frames. ], batch size: 31, lr: 2.61e-02,
2024-10-09 02:52:37,104 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=5934.666666666667, ans=0.04193888888888889
2024-10-09 02:52:37,125 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=5934.666666666667, ans=0.22181250000000002
2024-10-09 02:53:00,403 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=5941.333333333333, ans=0.06286666666666668
2024-10-09 02:53:16,010 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5944.666666666667, ans=0.2405533333333333
2024-10-09 02:53:21,842 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=5944.666666666667, ans=0.22134375
2024-10-09 02:53:43,364 INFO [train.py:1152] Epoch 3, batch 4150, loss[loss=0.3173, ctc_loss=0.257, attn_decoder_loss=0.3323, over 4758.00 frames. ], tot_loss[loss=0.317, ctc_loss=0.2821, attn_decoder_loss=0.3258, over 967037.12 frames. ], batch size: 20, lr: 2.61e-02,
2024-10-09 02:53:52,771 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=10.06 vs. limit=9.73175
2024-10-09 02:54:21,826 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.72 vs. limit=9.73425
2024-10-09 02:54:38,385 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.70 vs. limit=11.971
2024-10-09 02:54:50,305 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.638e+01 8.395e+01 9.636e+01 1.081e+02 2.821e+02, threshold=1.927e+02, percent-clipped=1.0
2024-10-09 02:54:56,169 INFO [train.py:1152] Epoch 3, batch 4200, loss[loss=0.3072, ctc_loss=0.2928, attn_decoder_loss=0.3108, over 4840.00 frames. ], tot_loss[loss=0.3161, ctc_loss=0.2808, attn_decoder_loss=0.325, over 967257.29 frames. ], batch size: 31, lr: 2.61e-02,
2024-10-09 02:54:57,715 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=5968.0, ans=0.22025
2024-10-09 02:55:01,368 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.25 vs. limit=9.738
2024-10-09 02:55:06,570 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=5968.0, ans=0.22025
2024-10-09 02:55:16,050 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.28 vs. limit=11.9785
2024-10-09 02:55:17,201 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.22 vs. limit=11.9785
2024-10-09 02:55:40,190 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=5978.0, ans=0.21978124999999998
2024-10-09 02:55:58,148 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.15 vs. limit=6.392533333333333
2024-10-09 02:56:06,424 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=5981.333333333333, ans=0.21962500000000001
2024-10-09 02:56:09,087 INFO [train.py:1152] Epoch 3, batch 4250, loss[loss=0.298, ctc_loss=0.2436, attn_decoder_loss=0.3116, over 4751.00 frames. ], tot_loss[loss=0.3136, ctc_loss=0.2776, attn_decoder_loss=0.3226, over 967186.42 frames. ], batch size: 19, lr: 2.60e-02,
2024-10-09 02:56:20,824 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=5984.666666666667, ans=0.041730555555555555
2024-10-09 02:56:26,188 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.94 vs. limit=3.8982
2024-10-09 02:56:37,040 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=5991.333333333333, ans=0.6903033333333334
2024-10-09 02:56:44,070 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=5991.333333333333, ans=0.04170277777777778
2024-10-09 02:56:58,119 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=3.09 vs. limit=9.748000000000001
2024-10-09 02:57:13,292 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=5998.0, ans=0.21884375
2024-10-09 02:57:16,277 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.028e+01 8.492e+01 9.374e+01 1.076e+02 1.880e+02, threshold=1.875e+02, percent-clipped=0.0
2024-10-09 02:57:22,189 INFO [train.py:1152] Epoch 3, batch 4300, loss[loss=0.3083, ctc_loss=0.288, attn_decoder_loss=0.3134, over 4830.00 frames. ], tot_loss[loss=0.3149, ctc_loss=0.2792, attn_decoder_loss=0.3239, over 967467.95 frames. ], batch size: 21, lr: 2.60e-02,
2024-10-09 02:57:26,735 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer_ff2.min_abs, batch_count=6001.333333333333, ans=0.1
2024-10-09 02:57:29,506 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=6001.333333333333, ans=0.025
2024-10-09 02:58:01,836 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.22 vs. limit=12.006
2024-10-09 02:58:04,152 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6011.333333333333, ans=0.23988666666666666
2024-10-09 02:58:15,107 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.76 vs. limit=8.005666666666666
2024-10-09 02:58:19,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=6014.666666666667, ans=0.2180625
2024-10-09 02:58:21,316 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6014.666666666667, ans=0.2398533333333333
2024-10-09 02:58:31,505 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=6014.666666666667, ans=0.2180625
2024-10-09 02:58:34,284 INFO [train.py:1152] Epoch 3, batch 4350, loss[loss=0.3034, ctc_loss=0.2447, attn_decoder_loss=0.3181, over 4832.00 frames. ], tot_loss[loss=0.3157, ctc_loss=0.2797, attn_decoder_loss=0.3247, over 966343.99 frames. ], batch size: 21, lr: 2.60e-02,
2024-10-09 02:58:40,272 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=6018.0, ans=0.21790625000000002
2024-10-09 02:58:50,326 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=6021.333333333333, ans=0.21775
2024-10-09 02:58:52,602 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.03 vs. limit=3.9032
2024-10-09 02:59:00,452 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=6021.333333333333, ans=0.009560579710144929
2024-10-09 02:59:16,549 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=6028.0, ans=0.2174375
2024-10-09 02:59:26,290 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=3.89 vs. limit=5.0
2024-10-09 02:59:35,442 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.min_positive, batch_count=6031.333333333333, ans=0.062304166666666674
2024-10-09 02:59:41,006 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.116e+01 8.690e+01 9.608e+01 1.065e+02 2.163e+02, threshold=1.922e+02, percent-clipped=1.0
2024-10-09 02:59:44,200 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=6031.333333333333, ans=0.21728124999999998
2024-10-09 02:59:46,955 INFO [train.py:1152] Epoch 3, batch 4400, loss[loss=0.2988, ctc_loss=0.2505, attn_decoder_loss=0.3109, over 4733.00 frames. ], tot_loss[loss=0.3154, ctc_loss=0.2792, attn_decoder_loss=0.3244, over 965986.74 frames. ], batch size: 26, lr: 2.59e-02,
2024-10-09 03:00:12,993 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=6038.0, ans=0.21696875
2024-10-09 03:00:54,990 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=6048.0, ans=0.025
2024-10-09 03:00:59,081 INFO [train.py:1152] Epoch 3, batch 4450, loss[loss=0.2816, ctc_loss=0.2332, attn_decoder_loss=0.2937, over 4883.00 frames. ], tot_loss[loss=0.3161, ctc_loss=0.2804, attn_decoder_loss=0.325, over 966268.77 frames. ], batch size: 19, lr: 2.59e-02,
2024-10-09 03:01:10,873 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=6051.333333333333, ans=0.21634375
2024-10-09 03:01:18,747 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.45 vs. limit=12.041
2024-10-09 03:01:37,482 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.04 vs. limit=3.9087
2024-10-09 03:01:39,185 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.88 vs. limit=12.0435
2024-10-09 03:01:45,812 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=6061.333333333333, ans=0.0
2024-10-09 03:01:48,054 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.32 vs. limit=12.046
2024-10-09 03:01:51,650 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=6061.333333333333, ans=0.21587499999999998
2024-10-09 03:02:06,223 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.523e+01 8.561e+01 9.874e+01 1.161e+02 2.089e+02, threshold=1.975e+02, percent-clipped=1.0
2024-10-09 03:02:07,026 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.90 vs. limit=12.0485
2024-10-09 03:02:12,032 INFO [train.py:1152] Epoch 3, batch 4500, loss[loss=0.3387, ctc_loss=0.3091, attn_decoder_loss=0.346, over 4836.00 frames. ], tot_loss[loss=0.3164, ctc_loss=0.2806, attn_decoder_loss=0.3254, over 966294.40 frames. ], batch size: 28, lr: 2.59e-02,
2024-10-09 03:02:12,204 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6068.0, ans=0.23931999999999998
2024-10-09 03:02:28,324 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=6071.333333333333, ans=0.21540625000000002
2024-10-09 03:03:18,997 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6081.333333333333, ans=0.23918666666666666
2024-10-09 03:03:19,621 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.82 vs. limit=8.040666666666667
2024-10-09 03:03:24,855 INFO [train.py:1152] Epoch 3, batch 4550, loss[loss=0.2798, ctc_loss=0.2393, attn_decoder_loss=0.2899, over 4856.00 frames. ], tot_loss[loss=0.3161, ctc_loss=0.2807, attn_decoder_loss=0.325, over 966131.79 frames. ], batch size: 20, lr: 2.58e-02,
2024-10-09 03:03:29,351 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=6084.666666666667, ans=0.21478124999999998
2024-10-09 03:03:31,569 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.40 vs. limit=9.78175
2024-10-09 03:03:34,321 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.92 vs. limit=12.063500000000001
2024-10-09 03:03:44,199 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer2.prob, batch_count=6088.0, ans=0.214625
2024-10-09 03:03:49,999 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=6088.0, ans=0.214625
2024-10-09 03:03:50,016 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=6088.0, ans=0.0
2024-10-09 03:04:31,848 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.674e+01 8.494e+01 9.285e+01 1.089e+02 1.399e+02, threshold=1.857e+02, percent-clipped=0.0
2024-10-09 03:04:34,054 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.56 vs. limit=12.0735
2024-10-09 03:04:37,784 INFO [train.py:1152] Epoch 3, batch 4600, loss[loss=0.3412, ctc_loss=0.333, attn_decoder_loss=0.3432, over 4744.00 frames. ], tot_loss[loss=0.3157, ctc_loss=0.28, attn_decoder_loss=0.3246, over 966414.35 frames. ], batch size: 45, lr: 2.58e-02,
2024-10-09 03:05:01,189 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=6104.666666666667, ans=0.6863366666666667
2024-10-09 03:05:01,238 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=6104.666666666667, ans=0.21384375
2024-10-09 03:05:20,976 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.67 vs. limit=9.79175
2024-10-09 03:05:30,074 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=6111.333333333333, ans=0.21353125
2024-10-09 03:05:38,253 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.80 vs. limit=12.086
2024-10-09 03:05:47,834 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6114.666666666667, ans=0.2388533333333333
2024-10-09 03:05:50,472 INFO [train.py:1152] Epoch 3, batch 4650, loss[loss=0.3311, ctc_loss=0.304, attn_decoder_loss=0.3379, over 4833.00 frames. ], tot_loss[loss=0.3155, ctc_loss=0.2798, attn_decoder_loss=0.3244, over 965625.55 frames. ], batch size: 36, lr: 2.58e-02,
2024-10-09 03:05:53,552 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=6118.0, ans=0.21321875
2024-10-09 03:06:26,959 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=6124.666666666667, ans=0.21290625000000002
2024-10-09 03:06:26,980 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=6124.666666666667, ans=0.8112466666666667
2024-10-09 03:06:36,254 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=6128.0, ans=0.035
2024-10-09 03:06:57,799 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.142e+01 8.694e+01 9.815e+01 1.078e+02 1.539e+02, threshold=1.963e+02, percent-clipped=0.0
2024-10-09 03:07:03,609 INFO [train.py:1152] Epoch 3, batch 4700, loss[loss=0.3014, ctc_loss=0.2564, attn_decoder_loss=0.3127, over 4940.00 frames. ], tot_loss[loss=0.3145, ctc_loss=0.2782, attn_decoder_loss=0.3236, over 965504.98 frames. ], batch size: 19, lr: 2.58e-02,
2024-10-09 03:07:03,765 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=6134.666666666667, ans=0.2124375
2024-10-09 03:07:04,499 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.60 vs. limit=12.100999999999999
2024-10-09 03:07:09,569 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=6134.666666666667, ans=0.041105555555555555
2024-10-09 03:07:21,284 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=6138.0, ans=0.21228124999999998
2024-10-09 03:07:29,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=6138.0, ans=0.21228124999999998
2024-10-09 03:07:32,791 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=6141.333333333333, ans=0.09899494936611666
2024-10-09 03:07:37,753 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.48 vs. limit=6.456533333333333
2024-10-09 03:07:56,616 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.66 vs. limit=12.1085
2024-10-09 03:08:03,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6148.0, ans=0.23852
2024-10-09 03:08:05,488 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.62 vs. limit=12.111
2024-10-09 03:08:06,158 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=6148.0, ans=0.09899494936611666
2024-10-09 03:08:15,971 INFO [train.py:1152] Epoch 3, batch 4750, loss[loss=0.3129, ctc_loss=0.2607, attn_decoder_loss=0.3259, over 4734.00 frames. ], tot_loss[loss=0.3162, ctc_loss=0.2816, attn_decoder_loss=0.3248, over 965342.80 frames. ], batch size: 45, lr: 2.57e-02,
2024-10-09 03:08:31,338 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=13.89 vs. limit=9.808
2024-10-09 03:08:32,117 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=6154.666666666667, ans=0.21150000000000002
2024-10-09 03:08:34,084 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.81 vs. limit=12.116
2024-10-09 03:08:37,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=6154.666666666667, ans=0.23845333333333332
2024-10-09 03:08:49,394 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.max_abs, batch_count=6158.0, ans=8.848749999999999
2024-10-09 03:08:56,685 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=6158.0, ans=0.041008333333333334
2024-10-09 03:08:58,266 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=6161.333333333333, ans=0.025
2024-10-09 03:09:22,726 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.275e+01 9.004e+01 9.904e+01 1.116e+02 1.737e+02, threshold=1.981e+02, percent-clipped=0.0
2024-10-09 03:09:28,733 INFO [train.py:1152] Epoch 3, batch 4800, loss[loss=0.2681, ctc_loss=0.1951, attn_decoder_loss=0.2863, over 4903.00 frames. ], tot_loss[loss=0.3142, ctc_loss=0.2782, attn_decoder_loss=0.3232, over 965755.66 frames. ], batch size: 22, lr: 2.57e-02,
2024-10-09 03:09:34,699 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=6168.0, ans=0.009528695652173914
2024-10-09 03:10:06,986 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=6174.666666666667, ans=0.04093888888888889
2024-10-09 03:10:10,753 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.71 vs. limit=8.087333333333333
2024-10-09 03:10:17,053 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.min_positive, batch_count=6178.0, ans=0.030693750000000002
2024-10-09 03:10:33,104 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=6181.333333333333, ans=0.23818666666666666
2024-10-09 03:10:41,817 INFO [train.py:1152] Epoch 3, batch 4850, loss[loss=0.3215, ctc_loss=0.2959, attn_decoder_loss=0.3279, over 4847.00 frames. ], tot_loss[loss=0.3138, ctc_loss=0.2774, attn_decoder_loss=0.3229, over 966401.69 frames. ], batch size: 28, lr: 2.57e-02,
2024-10-09 03:10:43,971 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.69 vs. limit=12.1385
2024-10-09 03:10:50,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=6184.666666666667, ans=0.6835366666666667
2024-10-09 03:10:57,239 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=10.24 vs. limit=9.8205
2024-10-09 03:11:17,832 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.99 vs. limit=3.9287
2024-10-09 03:11:19,848 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=6191.333333333333, ans=0.009523623188405798
2024-10-09 03:11:21,819 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.27 vs. limit=3.9287
2024-10-09 03:11:22,829 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=6191.333333333333, ans=0.009523623188405798
2024-10-09 03:11:27,853 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.92 vs. limit=12.146
2024-10-09 03:11:37,436 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=6194.666666666667, ans=0.009522898550724638
2024-10-09 03:11:37,438 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=6194.666666666667, ans=0.0
2024-10-09 03:11:46,951 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.38 vs. limit=12.1485
2024-10-09 03:11:49,002 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.137e+01 8.304e+01 9.552e+01 1.048e+02 1.473e+02, threshold=1.910e+02, percent-clipped=0.0
2024-10-09 03:11:54,909 INFO [train.py:1152] Epoch 3, batch 4900, loss[loss=0.3121, ctc_loss=0.2659, attn_decoder_loss=0.3236, over 4848.00 frames. ], tot_loss[loss=0.314, ctc_loss=0.2778, attn_decoder_loss=0.323, over 967082.33 frames. ], batch size: 21, lr: 2.56e-02,
2024-10-09 03:11:55,859 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.95 vs. limit=8.100666666666665
2024-10-09 03:12:01,382 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.08 vs. limit=12.151
2024-10-09 03:12:37,495 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=6211.333333333333, ans=0.6826033333333335
2024-10-09 03:13:07,961 INFO [train.py:1152] Epoch 3, batch 4950, loss[loss=0.3758, ctc_loss=0.3767, attn_decoder_loss=0.3756, over 4783.00 frames. ], tot_loss[loss=0.3149, ctc_loss=0.2796, attn_decoder_loss=0.3237, over 966807.76 frames. ], batch size: 53, lr: 2.56e-02,
2024-10-09 03:13:22,763 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=6221.333333333333, ans=0.20837499999999998
2024-10-09 03:13:23,337 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.06 vs. limit=12.166
2024-10-09 03:13:46,187 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=6224.666666666667, ans=0.061095833333333335
2024-10-09 03:13:47,633 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=6224.666666666667, ans=0.20821875
2024-10-09 03:13:55,000 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=6228.0, ans=0.009515652173913043
2024-10-09 03:13:55,136 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=6228.0, ans=0.040716666666666665
2024-10-09 03:13:58,632 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.05 vs. limit=12.171
2024-10-09 03:13:59,997 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.37 vs. limit=9.8355
2024-10-09 03:14:08,261 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=6231.333333333333, ans=0.07
2024-10-09 03:14:11,760 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.11 vs. limit=8.115666666666666
2024-10-09 03:14:14,057 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=6231.333333333333, ans=0.04070277777777778
2024-10-09 03:14:15,253 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.384e+01 8.420e+01 9.442e+01 1.090e+02 2.569e+02, threshold=1.888e+02, percent-clipped=3.0
2024-10-09 03:14:20,927 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=4.91 vs. limit=5.246933333333334
2024-10-09 03:14:21,189 INFO [train.py:1152] Epoch 3, batch 5000, loss[loss=0.2867, ctc_loss=0.2318, attn_decoder_loss=0.3004, over 4751.00 frames. ], tot_loss[loss=0.3136, ctc_loss=0.2776, attn_decoder_loss=0.3226, over 967640.14 frames. ], batch size: 29, lr: 2.56e-02,
2024-10-09 03:14:21,309 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6234.666666666667, ans=0.23765333333333333
2024-10-09 03:15:02,798 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.whiten.whitening_limit, batch_count=6241.333333333333, ans=6.496533333333334
2024-10-09 03:15:06,543 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=6244.666666666667, ans=0.20728124999999997
2024-10-09 03:15:06,650 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=6244.666666666667, ans=0.20728124999999997
2024-10-09 03:15:18,222 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=6248.0, ans=0.207125
2024-10-09 03:15:34,115 INFO [train.py:1152] Epoch 3, batch 5050, loss[loss=0.3144, ctc_loss=0.2667, attn_decoder_loss=0.3263, over 4852.00 frames. ], tot_loss[loss=0.3119, ctc_loss=0.2749, attn_decoder_loss=0.3212, over 968572.55 frames. ], batch size: 19, lr: 2.56e-02,
2024-10-09 03:15:41,581 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6251.333333333333, ans=0.23748666666666668
2024-10-09 03:15:45,935 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=6251.333333333333, ans=0.20696874999999998
2024-10-09 03:15:47,388 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=6254.666666666667, ans=0.07
2024-10-09 03:15:48,786 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=6254.666666666667, ans=0.2068125
2024-10-09 03:15:56,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=6254.666666666667, ans=0.29382
2024-10-09 03:16:00,533 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=6254.666666666667, ans=0.04060555555555556
2024-10-09 03:16:16,975 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=17.67 vs. limit=9.847999999999999
2024-10-09 03:16:19,450 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=6261.333333333333, ans=0.20650000000000002
2024-10-09 03:16:25,243 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=6261.333333333333, ans=0.6808533333333334
2024-10-09 03:16:30,139 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.35 vs. limit=12.196
2024-10-09 03:16:38,811 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.14 vs. limit=6.505866666666667
2024-10-09 03:16:41,091 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.923e+01 8.427e+01 9.256e+01 1.035e+02 1.759e+02, threshold=1.851e+02, percent-clipped=0.0
2024-10-09 03:16:46,866 INFO [train.py:1152] Epoch 3, batch 5100, loss[loss=0.3254, ctc_loss=0.2716, attn_decoder_loss=0.3389, over 4814.00 frames. ], tot_loss[loss=0.3139, ctc_loss=0.2772, attn_decoder_loss=0.3231, over 967940.39 frames. ], batch size: 19, lr: 2.55e-02,
2024-10-09 03:16:54,160 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=6268.0, ans=0.20618750000000002
2024-10-09 03:17:05,251 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.56 vs. limit=8.135666666666665
2024-10-09 03:17:17,148 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.13 vs. limit=12.206
2024-10-09 03:17:19,902 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.61 vs. limit=6.568666666666667
2024-10-09 03:17:26,491 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=6274.666666666667, ans=0.009505507246376811
2024-10-09 03:17:41,179 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=6278.0, ans=0.040508333333333334
2024-10-09 03:17:44,695 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.59 vs. limit=9.8555
2024-10-09 03:17:55,378 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=6281.333333333333, ans=0.6801533333333334
2024-10-09 03:17:55,384 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=6281.333333333333, ans=0.06074166666666667
2024-10-09 03:17:59,694 INFO [train.py:1152] Epoch 3, batch 5150, loss[loss=0.2923, ctc_loss=0.2383, attn_decoder_loss=0.3058, over 4833.00 frames. ], tot_loss[loss=0.3133, ctc_loss=0.2763, attn_decoder_loss=0.3225, over 967998.85 frames. ], batch size: 36, lr: 2.55e-02,
2024-10-09 03:18:20,830 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.67 vs. limit=12.216000000000001
2024-10-09 03:18:54,137 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=6294.666666666667, ans=0.04043888888888889
2024-10-09 03:19:06,902 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.382e+01 8.560e+01 9.096e+01 1.063e+02 1.692e+02, threshold=1.819e+02, percent-clipped=0.0
2024-10-09 03:19:12,697 INFO [train.py:1152] Epoch 3, batch 5200, loss[loss=0.3044, ctc_loss=0.273, attn_decoder_loss=0.3123, over 4770.00 frames. ], tot_loss[loss=0.3132, ctc_loss=0.2763, attn_decoder_loss=0.3224, over 967581.87 frames. ], batch size: 29, lr: 2.55e-02,
2024-10-09 03:19:22,337 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.70 vs. limit=12.225999999999999
2024-10-09 03:19:46,572 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=6308.0, ans=0.060575000000000004
2024-10-09 03:19:52,446 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=6308.0, ans=0.040383333333333334
2024-10-09 03:20:00,281 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=4.81 vs. limit=9.86675
2024-10-09 03:20:03,943 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=6311.333333333333, ans=0.20415624999999998
2024-10-09 03:20:12,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=6314.666666666667, ans=0.030266666666666667
2024-10-09 03:20:14,144 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer1.prob, batch_count=6314.666666666667, ans=0.20400000000000001
2024-10-09 03:20:24,514 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=6318.0, ans=0.20384375
2024-10-09 03:20:25,801 INFO [train.py:1152] Epoch 3, batch 5250, loss[loss=0.2877, ctc_loss=0.2289, attn_decoder_loss=0.3023, over 4860.00 frames. ], tot_loss[loss=0.3126, ctc_loss=0.2754, attn_decoder_loss=0.3219, over 967743.81 frames. ], batch size: 20, lr: 2.54e-02,
2024-10-09 03:20:30,255 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=6318.0, ans=0.20384375
2024-10-09 03:20:33,281 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=6318.0, ans=0.0
2024-10-09 03:20:37,580 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=6318.0, ans=0.04034166666666667
2024-10-09 03:20:47,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.max_abs, batch_count=6321.333333333333, ans=8.950833333333334
2024-10-09 03:20:50,600 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=6321.333333333333, ans=0.00949536231884058
2024-10-09 03:21:21,749 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.91 vs. limit=12.246
2024-10-09 03:21:22,532 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=6331.333333333333, ans=0.0
2024-10-09 03:21:30,335 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.83 vs. limit=9.87425
2024-10-09 03:21:32,784 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.389e+01 8.182e+01 9.340e+01 1.059e+02 3.155e+02, threshold=1.868e+02, percent-clipped=2.0
2024-10-09 03:21:37,324 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=6334.666666666667, ans=0.20306249999999998
2024-10-09 03:21:38,615 INFO [train.py:1152] Epoch 3, batch 5300, loss[loss=0.3437, ctc_loss=0.3266, attn_decoder_loss=0.348, over 4846.00 frames. ], tot_loss[loss=0.3121, ctc_loss=0.2746, attn_decoder_loss=0.3215, over 967904.97 frames. ], batch size: 38, lr: 2.54e-02,
2024-10-09 03:22:17,019 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=6341.333333333333, ans=0.20274999999999999
2024-10-09 03:22:19,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=6341.333333333333, ans=0.009491014492753623
2024-10-09 03:22:26,010 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten.whitening_limit, batch_count=6344.666666666667, ans=12.2585
2024-10-09 03:22:46,151 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=6348.0, ans=0.2024375
2024-10-09 03:22:51,747 INFO [train.py:1152] Epoch 3, batch 5350, loss[loss=0.2565, ctc_loss=0.1874, attn_decoder_loss=0.2738, over 4978.00 frames. ], tot_loss[loss=0.3118, ctc_loss=0.2735, attn_decoder_loss=0.3213, over 967291.11 frames. ], batch size: 19, lr: 2.54e-02,
2024-10-09 03:23:00,191 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=18.26 vs. limit=12.2635
2024-10-09 03:23:03,654 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=6351.333333333333, ans=0.009488840579710145
2024-10-09 03:23:29,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=6358.0, ans=0.040175
2024-10-09 03:23:41,259 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=6361.333333333333, ans=0.2018125
2024-10-09 03:23:50,020 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=6364.666666666667, ans=0.04014722222222222
2024-10-09 03:23:58,474 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.139e+01 8.701e+01 9.797e+01 1.106e+02 2.423e+02, threshold=1.959e+02, percent-clipped=2.0
2024-10-09 03:24:04,353 INFO [train.py:1152] Epoch 3, batch 5400, loss[loss=0.3492, ctc_loss=0.3565, attn_decoder_loss=0.3474, over 4796.00 frames. ], tot_loss[loss=0.3135, ctc_loss=0.2771, attn_decoder_loss=0.3227, over 966660.44 frames. ], batch size: 49, lr: 2.53e-02,
2024-10-09 03:24:11,770 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=6368.0, ans=0.2015
2024-10-09 03:24:16,170 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=6368.0, ans=0.67712
2024-10-09 03:24:16,191 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=6368.0, ans=0.04013333333333333
2024-10-09 03:24:18,496 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.97 vs. limit=12.278500000000001
2024-10-09 03:24:23,901 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.65 vs. limit=9.88925
2024-10-09 03:24:30,850 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=6371.333333333333, ans=8.982083333333334
2024-10-09 03:24:34,099 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=6374.666666666667, ans=0.20118750000000002
2024-10-09 03:24:37,572 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.76 vs. limit=9.8905
2024-10-09 03:24:38,689 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.38 vs. limit=12.280999999999999
2024-10-09 03:24:45,495 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=6374.666666666667, ans=0.20118750000000002
2024-10-09 03:25:01,543 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=6381.333333333333, ans=0.025
2024-10-09 03:25:05,100 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.22 vs. limit=8.190666666666667
2024-10-09 03:25:12,159 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=16.34 vs. limit=12.286
2024-10-09 03:25:17,514 INFO [train.py:1152] Epoch 3, batch 5450, loss[loss=0.3275, ctc_loss=0.2857, attn_decoder_loss=0.338, over 4940.00 frames. ], tot_loss[loss=0.3125, ctc_loss=0.2751, attn_decoder_loss=0.3218, over 967274.00 frames. ], batch size: 19, lr: 2.53e-02,
2024-10-09 03:25:51,677 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6391.333333333333, ans=0.23608666666666667
2024-10-09 03:25:55,620 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.55 vs. limit=8.195666666666666
2024-10-09 03:26:00,405 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=6394.666666666667, ans=0.20024999999999998
2024-10-09 03:26:07,872 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=6394.666666666667, ans=0.20024999999999998
2024-10-09 03:26:13,449 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=6394.666666666667, ans=0.6761866666666667
2024-10-09 03:26:14,947 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=6398.0, ans=0.6760700000000001
2024-10-09 03:26:25,156 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.699e+01 8.299e+01 9.337e+01 1.050e+02 1.654e+02, threshold=1.867e+02, percent-clipped=0.0
2024-10-09 03:26:31,042 INFO [train.py:1152] Epoch 3, batch 5500, loss[loss=0.308, ctc_loss=0.29, attn_decoder_loss=0.3125, over 4782.00 frames. ], tot_loss[loss=0.3115, ctc_loss=0.2736, attn_decoder_loss=0.3209, over 967661.73 frames. ], batch size: 49, lr: 2.53e-02,
2024-10-09 03:26:32,592 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=6401.333333333333, ans=0.039994444444444444
2024-10-09 03:26:34,647 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.36 vs. limit=12.301
2024-10-09 03:26:43,248 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.93 vs. limit=6.600333333333333
2024-10-09 03:26:50,069 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6404.666666666667, ans=0.23595333333333332
2024-10-09 03:26:52,085 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.21 vs. limit=3.9607
2024-10-09 03:27:11,234 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten.whitening_limit, batch_count=6408.0, ans=9.903
2024-10-09 03:27:13,259 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=6411.333333333333, ans=0.09899494936611666
2024-10-09 03:27:16,410 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=6.583e-02
2024-10-09 03:27:22,009 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=6411.333333333333, ans=0.19946874999999997
2024-10-09 03:27:37,853 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=6414.666666666667, ans=0.059908333333333334
2024-10-09 03:27:42,705 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.73 vs. limit=12.313500000000001
2024-10-09 03:27:43,517 INFO [train.py:1152] Epoch 3, batch 5550, loss[loss=0.2903, ctc_loss=0.2437, attn_decoder_loss=0.302, over 4799.00 frames. ], tot_loss[loss=0.3104, ctc_loss=0.2724, attn_decoder_loss=0.3198, over 967338.35 frames. ], batch size: 19, lr: 2.53e-02,
2024-10-09 03:27:47,940 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=6418.0, ans=0.039925
2024-10-09 03:27:53,031 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.whiten.whitening_limit, batch_count=6418.0, ans=9.90675
2024-10-09 03:27:55,360 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=6418.0, ans=0.19915624999999998
2024-10-09 03:28:10,072 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=6421.333333333333, ans=0.025
2024-10-09 03:28:23,224 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=6424.666666666667, ans=0.04949747468305833
2024-10-09 03:28:24,719 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=6424.666666666667, ans=0.19884374999999999
2024-10-09 03:28:50,644 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.459e+01 8.006e+01 9.088e+01 1.055e+02 1.690e+02, threshold=1.818e+02, percent-clipped=0.0
2024-10-09 03:28:53,725 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=6431.333333333333, ans=0.23568666666666666
2024-10-09 03:28:56,515 INFO [train.py:1152] Epoch 3, batch 5600, loss[loss=0.282, ctc_loss=0.2333, attn_decoder_loss=0.2942, over 4851.00 frames. ], tot_loss[loss=0.3104, ctc_loss=0.2716, attn_decoder_loss=0.3201, over 967370.76 frames. ], batch size: 28, lr: 2.52e-02,
2024-10-09 03:29:00,045 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=10.06 vs. limit=9.913
2024-10-09 03:29:01,043 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=6434.666666666667, ans=0.23565333333333333
2024-10-09 03:29:10,635 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.06 vs. limit=12.3285
2024-10-09 03:29:15,276 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.09 vs. limit=6.575200000000001
2024-10-09 03:29:28,486 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.66 vs. limit=12.331
2024-10-09 03:29:32,166 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=6441.333333333333, ans=0.0
2024-10-09 03:29:56,437 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.12 vs. limit=12.336
2024-10-09 03:30:10,308 INFO [train.py:1152] Epoch 3, batch 5650, loss[loss=0.3454, ctc_loss=0.3237, attn_decoder_loss=0.3508, over 4764.00 frames. ], tot_loss[loss=0.3101, ctc_loss=0.2711, attn_decoder_loss=0.3199, over 967382.17 frames. ], batch size: 45, lr: 2.52e-02,
2024-10-09 03:30:33,513 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=6454.666666666667, ans=0.1974375
2024-10-09 03:30:36,445 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=6454.666666666667, ans=0.1974375
2024-10-09 03:30:40,809 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=6458.0, ans=0.19728125000000002
2024-10-09 03:31:17,272 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.767e+01 8.659e+01 9.798e+01 1.114e+02 1.687e+02, threshold=1.960e+02, percent-clipped=0.0
2024-10-09 03:31:22,946 INFO [train.py:1152] Epoch 3, batch 5700, loss[loss=0.2991, ctc_loss=0.2554, attn_decoder_loss=0.31, over 4894.00 frames. ], tot_loss[loss=0.3107, ctc_loss=0.2721, attn_decoder_loss=0.3203, over 966743.36 frames. ], batch size: 22, lr: 2.52e-02,
2024-10-09 03:31:33,265 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.min_abs, batch_count=6468.0, ans=0.29702
2024-10-09 03:31:52,749 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten.whitening_limit, batch_count=6474.666666666667, ans=9.928
2024-10-09 03:32:02,380 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=6474.666666666667, ans=0.23525333333333331
2024-10-09 03:32:14,826 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=6.92 vs. limit=6.591200000000001
2024-10-09 03:32:35,999 INFO [train.py:1152] Epoch 3, batch 5750, loss[loss=0.356, ctc_loss=0.3363, attn_decoder_loss=0.3609, over 4813.00 frames. ], tot_loss[loss=0.3108, ctc_loss=0.272, attn_decoder_loss=0.3204, over 967027.33 frames. ], batch size: 43, lr: 2.51e-02,
2024-10-09 03:32:43,311 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=6484.666666666667, ans=0.19603125
2024-10-09 03:33:02,383 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=6488.0, ans=0.19587500000000002
2024-10-09 03:33:10,228 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.73 vs. limit=12.368500000000001
2024-10-09 03:33:23,985 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=6494.666666666667, ans=0.6726866666666667
2024-10-09 03:33:42,612 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.540e+01 8.283e+01 9.295e+01 1.036e+02 1.670e+02, threshold=1.859e+02, percent-clipped=0.0
2024-10-09 03:33:48,490 INFO [train.py:1152] Epoch 3, batch 5800, loss[loss=0.3397, ctc_loss=0.3281, attn_decoder_loss=0.3426, over 4831.00 frames. ], tot_loss[loss=0.311, ctc_loss=0.2731, attn_decoder_loss=0.3205, over 966374.15 frames. ], batch size: 43, lr: 2.51e-02,
2024-10-09 03:33:51,607 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=6501.333333333333, ans=9.063333333333333
2024-10-09 03:34:20,929 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.07 vs. limit=12.381
2024-10-09 03:34:22,061 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6508.0, ans=0.23492
2024-10-09 03:34:23,410 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=6508.0, ans=0.67222
2024-10-09 03:34:30,711 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=6511.333333333333, ans=0.0
2024-10-09 03:34:34,270 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.86 vs. limit=12.3835
2024-10-09 03:35:01,267 INFO [train.py:1152] Epoch 3, batch 5850, loss[loss=0.3534, ctc_loss=0.3547, attn_decoder_loss=0.3531, over 4768.00 frames. ], tot_loss[loss=0.3106, ctc_loss=0.2723, attn_decoder_loss=0.3202, over 966613.31 frames. ], batch size: 45, lr: 2.51e-02,
2024-10-09 03:35:04,398 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=6518.0, ans=0.03950833333333334
2024-10-09 03:35:08,833 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6518.0, ans=0.23481999999999997
2024-10-09 03:35:12,521 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.41 vs. limit=9.94425
2024-10-09 03:35:22,820 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.15 vs. limit=12.391
2024-10-09 03:35:24,833 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=6521.333333333333, ans=0.029620833333333336
2024-10-09 03:35:51,337 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=6528.0, ans=9.08
2024-10-09 03:36:04,414 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6531.333333333333, ans=0.23468666666666665
2024-10-09 03:36:06,408 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.34 vs. limit=9.94925
2024-10-09 03:36:08,738 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.529e+01 8.402e+01 9.442e+01 1.077e+02 1.557e+02, threshold=1.888e+02, percent-clipped=0.0
2024-10-09 03:36:14,649 INFO [train.py:1152] Epoch 3, batch 5900, loss[loss=0.2945, ctc_loss=0.2639, attn_decoder_loss=0.3022, over 4794.00 frames. ], tot_loss[loss=0.3087, ctc_loss=0.2689, attn_decoder_loss=0.3186, over 966628.14 frames. ], batch size: 34, lr: 2.51e-02,
2024-10-09 03:36:16,176 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=6534.666666666667, ans=0.18465333333333334
2024-10-09 03:36:17,727 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=6534.666666666667, ans=0.1936875
2024-10-09 03:36:19,698 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.82 vs. limit=12.401
2024-10-09 03:36:49,930 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=6541.333333333333, ans=0.03941111111111111
2024-10-09 03:36:57,173 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6544.666666666667, ans=0.23455333333333334
2024-10-09 03:37:00,135 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=6544.666666666667, ans=0.19321875
2024-10-09 03:37:01,425 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=6544.666666666667, ans=0.19321875
2024-10-09 03:37:06,552 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=24.82 vs. limit=8.272333333333334
2024-10-09 03:37:17,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6548.0, ans=0.23452
2024-10-09 03:37:23,854 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.52 vs. limit=9.9555
2024-10-09 03:37:27,405 INFO [train.py:1152] Epoch 3, batch 5950, loss[loss=0.3313, ctc_loss=0.3107, attn_decoder_loss=0.3364, over 4803.00 frames. ], tot_loss[loss=0.3094, ctc_loss=0.2696, attn_decoder_loss=0.3194, over 965992.88 frames. ], batch size: 34, lr: 2.50e-02,
2024-10-09 03:37:48,173 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=6554.666666666667, ans=0.0
2024-10-09 03:38:08,571 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=6558.0, ans=9.098749999999999
2024-10-09 03:38:11,548 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=6561.333333333333, ans=0.009443188405797102
2024-10-09 03:38:12,980 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=6561.333333333333, ans=0.03932777777777778
2024-10-09 03:38:32,073 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=6564.666666666667, ans=0.025
2024-10-09 03:38:34,777 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.684e+01 8.758e+01 9.893e+01 1.100e+02 1.734e+02, threshold=1.979e+02, percent-clipped=0.0
2024-10-09 03:38:40,605 INFO [train.py:1152] Epoch 3, batch 6000, loss[loss=0.3553, ctc_loss=0.3405, attn_decoder_loss=0.359, over 4774.00 frames. ], tot_loss[loss=0.3104, ctc_loss=0.2707, attn_decoder_loss=0.3203, over 966493.57 frames. ], batch size: 49, lr: 2.50e-02,
2024-10-09 03:38:40,606 INFO [train.py:1175] Computing validation loss
2024-10-09 03:38:49,414 INFO [train.py:1184] Epoch 3, validation: loss=0.2385, ctc_loss=0.1139, attn_decoder_loss=0.2696, over 90464.00 frames.
2024-10-09 03:38:49,415 INFO [train.py:1185] Maximum memory allocated so far is 6613MB
2024-10-09 03:38:58,087 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=6568.0, ans=0.192125
2024-10-09 03:39:11,288 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=6571.333333333333, ans=0.03928611111111111
2024-10-09 03:39:27,018 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=6574.666666666667, ans=0.1918125
2024-10-09 03:39:32,699 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=6578.0, ans=0.19165624999999997
2024-10-09 03:39:32,810 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=6578.0, ans=0.29867
2024-10-09 03:39:46,101 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 03:39:54,935 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=6581.333333333333, ans=0.6696533333333334
2024-10-09 03:40:01,878 INFO [train.py:1152] Epoch 3, batch 6050, loss[loss=0.2743, ctc_loss=0.2167, attn_decoder_loss=0.2887, over 4815.00 frames. ], tot_loss[loss=0.3091, ctc_loss=0.2691, attn_decoder_loss=0.3191, over 966472.46 frames. ], batch size: 19, lr: 2.50e-02,
2024-10-09 03:40:12,809 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.86 vs. limit=12.438500000000001
2024-10-09 03:40:22,355 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=6588.0, ans=0.1911875
2024-10-09 03:40:36,945 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=6591.333333333333, ans=0.009436666666666666
2024-10-09 03:40:38,391 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=6591.333333333333, ans=0.19103124999999999
2024-10-09 03:40:39,757 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6591.333333333333, ans=0.23408666666666667
2024-10-09 03:40:46,065 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.74 vs. limit=9.972999999999999
2024-10-09 03:40:57,928 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.52 vs. limit=8.297333333333334
2024-10-09 03:41:01,507 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=6598.0, ans=0.19071875
2024-10-09 03:41:08,784 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.674e+01 8.309e+01 9.137e+01 1.028e+02 1.308e+02, threshold=1.827e+02, percent-clipped=0.0
2024-10-09 03:41:08,992 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=6598.0, ans=0.039175
2024-10-09 03:41:14,558 INFO [train.py:1152] Epoch 3, batch 6100, loss[loss=0.3526, ctc_loss=0.3226, attn_decoder_loss=0.3601, over 4799.00 frames. ], tot_loss[loss=0.3084, ctc_loss=0.2674, attn_decoder_loss=0.3186, over 966082.57 frames. ], batch size: 34, lr: 2.50e-02,
2024-10-09 03:41:32,158 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=6604.666666666667, ans=0.6688366666666667
2024-10-09 03:41:39,429 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 03:41:40,944 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=6604.666666666667, ans=0.009433768115942029
2024-10-09 03:41:48,424 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6608.0, ans=0.23392
2024-10-09 03:42:05,919 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6611.333333333333, ans=0.2338866666666667
2024-10-09 03:42:27,639 INFO [train.py:1152] Epoch 3, batch 6150, loss[loss=0.3551, ctc_loss=0.3414, attn_decoder_loss=0.3586, over 4845.00 frames. ], tot_loss[loss=0.3073, ctc_loss=0.266, attn_decoder_loss=0.3177, over 966240.37 frames. ], batch size: 43, lr: 2.49e-02,
2024-10-09 03:42:28,289 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.53 vs. limit=8.309000000000001
2024-10-09 03:42:43,973 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.max_positive, batch_count=6621.333333333333, ans=0.8162133333333333
2024-10-09 03:42:57,070 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=6624.666666666667, ans=0.6681366666666667
2024-10-09 03:43:15,807 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=6628.0, ans=0.1893125
2024-10-09 03:43:17,271 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=6628.0, ans=0.6680200000000001
2024-10-09 03:43:25,035 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=3.95 vs. limit=6.652533333333333
2024-10-09 03:43:26,030 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=6631.333333333333, ans=0.18915625000000003
2024-10-09 03:43:30,837 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.62 vs. limit=12.4735
2024-10-09 03:43:34,605 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.798e+01 8.487e+01 9.349e+01 1.041e+02 1.757e+02, threshold=1.870e+02, percent-clipped=0.0
2024-10-09 03:43:40,534 INFO [train.py:1152] Epoch 3, batch 6200, loss[loss=0.3418, ctc_loss=0.3047, attn_decoder_loss=0.3511, over 4780.00 frames. ], tot_loss[loss=0.3075, ctc_loss=0.2663, attn_decoder_loss=0.3178, over 966522.05 frames. ], batch size: 29, lr: 2.49e-02,
2024-10-09 03:43:50,930 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=6634.666666666667, ans=0.6677866666666668
2024-10-09 03:43:53,044 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.15 vs. limit=9.988
2024-10-09 03:43:58,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=6638.0, ans=0.18884374999999998
2024-10-09 03:44:05,227 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=6638.0, ans=0.04949747468305833
2024-10-09 03:44:08,266 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=6641.333333333333, ans=0.23358666666666666
2024-10-09 03:44:14,170 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=6641.333333333333, ans=0.03899444444444445
2024-10-09 03:44:21,916 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.93 vs. limit=9.9905
2024-10-09 03:44:27,168 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=6644.666666666667, ans=0.18853124999999998
2024-10-09 03:44:33,587 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.98 vs. limit=12.4835
2024-10-09 03:44:35,935 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=6644.666666666667, ans=0.009425072463768116
2024-10-09 03:44:40,343 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=6648.0, ans=0.09899494936611666
2024-10-09 03:44:53,237 INFO [train.py:1152] Epoch 3, batch 6250, loss[loss=0.2854, ctc_loss=0.2334, attn_decoder_loss=0.2985, over 4724.00 frames. ], tot_loss[loss=0.3059, ctc_loss=0.2632, attn_decoder_loss=0.3166, over 966807.08 frames. ], batch size: 26, lr: 2.49e-02,
2024-10-09 03:44:59,235 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=6651.333333333333, ans=0.18821875
2024-10-09 03:45:05,528 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.63 vs. limit=6.660533333333333
2024-10-09 03:45:11,442 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.83 vs. limit=9.9955
2024-10-09 03:45:16,621 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6654.666666666667, ans=0.23345333333333332
2024-10-09 03:45:17,632 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.83 vs. limit=8.327333333333334
2024-10-09 03:45:27,551 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=8.96 vs. limit=9.99675
2024-10-09 03:45:29,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=6658.0, ans=0.6669700000000001
2024-10-09 03:45:31,299 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=6658.0, ans=0.18790625
2024-10-09 03:45:42,853 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=6661.333333333333, ans=0.18338666666666667
2024-10-09 03:45:43,728 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.01 vs. limit=3.9992
2024-10-09 03:45:51,667 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=6664.666666666667, ans=0.6667366666666668
2024-10-09 03:45:53,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=6664.666666666667, ans=0.18759375
2024-10-09 03:45:59,183 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_False/checkpoint-20000.pt
2024-10-09 03:46:00,678 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=6664.666666666667, ans=0.0
2024-10-09 03:46:01,544 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.052e+01 8.222e+01 9.072e+01 9.909e+01 1.392e+02, threshold=1.814e+02, percent-clipped=0.0
2024-10-09 03:46:05,575 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=6668.0, ans=0.18743749999999998
2024-10-09 03:46:06,768 INFO [train.py:1152] Epoch 3, batch 6300, loss[loss=0.3034, ctc_loss=0.2489, attn_decoder_loss=0.3171, over 4978.00 frames. ], tot_loss[loss=0.3063, ctc_loss=0.2638, attn_decoder_loss=0.317, over 966443.54 frames. ], batch size: 19, lr: 2.48e-02,
2024-10-09 03:46:13,704 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.26 vs. limit=8.334
2024-10-09 03:46:57,899 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.27 vs. limit=10.004249999999999
2024-10-09 03:47:18,727 INFO [train.py:1152] Epoch 3, batch 6350, loss[loss=0.3548, ctc_loss=0.3366, attn_decoder_loss=0.3594, over 4817.00 frames. ], tot_loss[loss=0.3039, ctc_loss=0.2613, attn_decoder_loss=0.3145, over 966125.21 frames. ], batch size: 36, lr: 2.48e-02,
2024-10-09 03:47:42,142 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=6688.0, ans=0.0388
2024-10-09 03:47:49,335 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=6691.333333333333, ans=0.18634374999999997
2024-10-09 03:47:56,641 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=6691.333333333333, ans=0.025
2024-10-09 03:48:16,882 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=6698.0, ans=0.18603124999999998
2024-10-09 03:48:25,316 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.940e+01 8.386e+01 9.262e+01 1.044e+02 1.741e+02, threshold=1.852e+02, percent-clipped=0.0
2024-10-09 03:48:31,198 INFO [train.py:1152] Epoch 3, batch 6400, loss[loss=0.3115, ctc_loss=0.2637, attn_decoder_loss=0.3234, over 4861.00 frames. ], tot_loss[loss=0.3043, ctc_loss=0.2618, attn_decoder_loss=0.3149, over 965953.90 frames. ], batch size: 23, lr: 2.48e-02,
2024-10-09 03:48:34,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=6701.333333333333, ans=0.0
2024-10-09 03:48:36,614 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.55 vs. limit=10.013
2024-10-09 03:49:00,509 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 03:49:10,667 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=6708.0, ans=0.07
2024-10-09 03:49:12,633 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.09 vs. limit=12.530999999999999
2024-10-09 03:49:16,450 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=6711.333333333333, ans=0.18540625
2024-10-09 03:49:20,883 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=7.298e-02
2024-10-09 03:49:29,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=6714.666666666667, ans=0.18525000000000003
2024-10-09 03:49:41,615 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.55 vs. limit=10.018
2024-10-09 03:49:42,481 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=6718.0, ans=0.00940913043478261
2024-10-09 03:49:43,729 INFO [train.py:1152] Epoch 3, batch 6450, loss[loss=0.3117, ctc_loss=0.2646, attn_decoder_loss=0.3235, over 4716.00 frames. ], tot_loss[loss=0.3035, ctc_loss=0.2612, attn_decoder_loss=0.3141, over 965338.34 frames. ], batch size: 26, lr: 2.48e-02,
2024-10-09 03:50:05,898 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 03:50:29,786 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.99 vs. limit=12.546
2024-10-09 03:50:31,821 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=6728.0, ans=0.025
2024-10-09 03:50:34,142 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.33 vs. limit=6.682
2024-10-09 03:50:39,087 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=6728.0, ans=0.18462499999999998
2024-10-09 03:50:43,458 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=6731.333333333333, ans=0.18446875000000001
2024-10-09 03:50:50,759 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.770e+01 8.476e+01 9.396e+01 1.056e+02 1.893e+02, threshold=1.879e+02, percent-clipped=1.0
2024-10-09 03:50:50,957 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=6731.333333333333, ans=0.23268666666666665
2024-10-09 03:50:56,464 INFO [train.py:1152] Epoch 3, batch 6500, loss[loss=0.3159, ctc_loss=0.3031, attn_decoder_loss=0.3191, over 4747.00 frames. ], tot_loss[loss=0.3024, ctc_loss=0.2595, attn_decoder_loss=0.3132, over 964952.77 frames. ], batch size: 26, lr: 2.47e-02,
2024-10-09 03:51:05,305 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=6734.666666666667, ans=0.03860555555555556
2024-10-09 03:51:14,030 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=6738.0, ans=0.18415625000000002
2024-10-09 03:51:17,348 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.77 vs. limit=12.5535
2024-10-09 03:51:20,666 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=14.12 vs. limit=12.5535
2024-10-09 03:51:24,924 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.07 vs. limit=12.556000000000001
2024-10-09 03:51:30,111 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=6741.333333333333, ans=9.213333333333333
2024-10-09 03:51:33,683 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.74 vs. limit=12.556000000000001
2024-10-09 03:51:34,604 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=6741.333333333333, ans=0.03857777777777778
2024-10-09 03:51:51,927 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=6744.666666666667, ans=0.03856388888888889
2024-10-09 03:52:02,115 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=6748.0, ans=0.1836875
2024-10-09 03:52:09,302 INFO [train.py:1152] Epoch 3, batch 6550, loss[loss=0.2436, ctc_loss=0.175, attn_decoder_loss=0.2607, over 4978.00 frames. ], tot_loss[loss=0.3006, ctc_loss=0.2569, attn_decoder_loss=0.3116, over 964727.79 frames. ], batch size: 19, lr: 2.47e-02,
2024-10-09 03:52:27,029 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=6754.666666666667, ans=0.183375
2024-10-09 03:52:44,638 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=6758.0, ans=0.18321874999999999
2024-10-09 03:52:50,757 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 03:53:02,445 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=6761.333333333333, ans=0.18306250000000002
2024-10-09 03:53:16,985 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.621e+01 8.693e+01 9.379e+01 1.039e+02 1.558e+02, threshold=1.876e+02, percent-clipped=0.0
2024-10-09 03:53:22,908 INFO [train.py:1152] Epoch 3, batch 6600, loss[loss=0.2711, ctc_loss=0.2269, attn_decoder_loss=0.2822, over 4850.00 frames. ], tot_loss[loss=0.2997, ctc_loss=0.2547, attn_decoder_loss=0.311, over 965312.97 frames. ], batch size: 23, lr: 2.47e-02,
2024-10-09 03:53:33,494 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=6768.0, ans=0.18275000000000002
2024-10-09 03:53:35,000 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=6768.0, ans=0.009398260869565217
2024-10-09 03:53:36,436 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=6771.333333333333, ans=0.18259375
2024-10-09 03:53:42,365 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=6771.333333333333, ans=0.025
2024-10-09 03:53:58,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=6774.666666666667, ans=0.0
2024-10-09 03:54:02,075 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.38 vs. limit=6.709866666666667
2024-10-09 03:54:20,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=6781.333333333333, ans=0.025
2024-10-09 03:54:30,475 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.39 vs. limit=12.586
2024-10-09 03:54:36,912 INFO [train.py:1152] Epoch 3, batch 6650, loss[loss=0.2797, ctc_loss=0.2354, attn_decoder_loss=0.2908, over 4744.00 frames. ], tot_loss[loss=0.2976, ctc_loss=0.2521, attn_decoder_loss=0.309, over 966984.39 frames. ], batch size: 20, lr: 2.47e-02,
2024-10-09 03:54:58,138 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.12 vs. limit=4.0182
2024-10-09 03:55:00,604 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=6788.0, ans=0.1818125
2024-10-09 03:55:09,537 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=6791.333333333333, ans=0.009393188405797101
2024-10-09 03:55:12,435 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=6791.333333333333, ans=0.03836944444444445
2024-10-09 03:55:45,201 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.865e+01 8.318e+01 9.228e+01 1.057e+02 1.514e+02, threshold=1.846e+02, percent-clipped=0.0
2024-10-09 03:55:51,207 INFO [train.py:1152] Epoch 3, batch 6700, loss[loss=0.2764, ctc_loss=0.2003, attn_decoder_loss=0.2955, over 4935.00 frames. ], tot_loss[loss=0.2954, ctc_loss=0.249, attn_decoder_loss=0.307, over 969139.28 frames. ], batch size: 20, lr: 2.46e-02,
2024-10-09 03:55:53,390 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.18 vs. limit=8.400666666666666
2024-10-09 03:55:57,908 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.08 vs. limit=10.0505
2024-10-09 03:57:01,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6814.666666666667, ans=0.2318533333333333
2024-10-09 03:57:05,484 INFO [train.py:1152] Epoch 3, batch 6750, loss[loss=0.2656, ctc_loss=0.2357, attn_decoder_loss=0.2731, over 4907.00 frames. ], tot_loss[loss=0.2919, ctc_loss=0.245, attn_decoder_loss=0.3036, over 972216.00 frames. ], batch size: 19, lr: 2.46e-02,
2024-10-09 03:57:07,110 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.min_positive, batch_count=6818.0, ans=0.18181999999999998
2024-10-09 03:57:18,884 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6821.333333333333, ans=0.23178666666666667
2024-10-09 03:57:24,846 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=6821.333333333333, ans=0.03824444444444445
2024-10-09 03:57:35,685 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.13 vs. limit=4.0237
2024-10-09 03:57:49,567 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=3.82 vs. limit=6.731199999999999
2024-10-09 03:58:13,673 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.140e+01 8.124e+01 8.829e+01 9.881e+01 1.844e+02, threshold=1.766e+02, percent-clipped=0.0
2024-10-09 03:58:19,658 INFO [train.py:1152] Epoch 3, batch 6800, loss[loss=0.2857, ctc_loss=0.2416, attn_decoder_loss=0.2968, over 4911.00 frames. ], tot_loss[loss=0.2899, ctc_loss=0.2423, attn_decoder_loss=0.3019, over 974537.90 frames. ], batch size: 19, lr: 2.46e-02,
2024-10-09 03:58:39,563 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=9.323e-02
2024-10-09 03:58:59,042 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=6841.333333333333, ans=0.17931249999999999
2024-10-09 03:59:12,773 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=6844.666666666667, ans=0.17915625000000002
2024-10-09 03:59:16,994 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=6844.666666666667, ans=0.17915625000000002
2024-10-09 03:59:19,395 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.53 vs. limit=6.7392
2024-10-09 03:59:26,774 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.86 vs. limit=10.068
2024-10-09 03:59:35,115 INFO [train.py:1152] Epoch 3, batch 6850, loss[loss=0.2676, ctc_loss=0.2336, attn_decoder_loss=0.2762, over 4978.00 frames. ], tot_loss[loss=0.2886, ctc_loss=0.2403, attn_decoder_loss=0.3006, over 978887.83 frames. ], batch size: 19, lr: 2.45e-02,
2024-10-09 03:59:36,760 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_False/epoch-3.pt
2024-10-09 04:00:03,780 INFO [train.py:1152] Epoch 4, batch 0, loss[loss=0.2767, ctc_loss=0.2241, attn_decoder_loss=0.2898, over 4852.00 frames. ], tot_loss[loss=0.2767, ctc_loss=0.2241, attn_decoder_loss=0.2898, over 4852.00 frames. ], batch size: 19, lr: 2.30e-02,
2024-10-09 04:00:03,781 INFO [train.py:1175] Computing validation loss
2024-10-09 04:00:10,915 INFO [train.py:1184] Epoch 4, validation: loss=0.2392, ctc_loss=0.1123, attn_decoder_loss=0.271, over 90464.00 frames.
2024-10-09 04:00:10,916 INFO [train.py:1185] Maximum memory allocated so far is 6613MB
2024-10-09 04:00:20,702 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.96 vs. limit=6.713
2024-10-09 04:00:28,462 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.75 vs. limit=12.6415
2024-10-09 04:00:30,676 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 04:00:40,329 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=6858.666666666667, ans=0.1785
2024-10-09 04:00:41,768 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=6858.666666666667, ans=0.1785
2024-10-09 04:00:53,218 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6862.0, ans=0.23138
2024-10-09 04:00:57,579 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=6862.0, ans=0.17834375000000002
2024-10-09 04:00:59,687 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.03 vs. limit=12.6465
2024-10-09 04:01:06,342 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=6865.333333333333, ans=0.038061111111111115
2024-10-09 04:01:12,247 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.485e+01 7.853e+01 9.194e+01 1.038e+02 1.705e+02, threshold=1.839e+02, percent-clipped=0.0
2024-10-09 04:01:20,653 INFO [train.py:1152] Epoch 4, batch 50, loss[loss=0.2647, ctc_loss=0.1957, attn_decoder_loss=0.282, over 4908.00 frames. ], tot_loss[loss=0.3113, ctc_loss=0.2717, attn_decoder_loss=0.3212, over 217764.35 frames. ], batch size: 19, lr: 2.29e-02,
2024-10-09 04:01:33,943 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=6872.0, ans=0.038033333333333336
2024-10-09 04:01:39,038 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.27 vs. limit=6.7488
2024-10-09 04:02:00,294 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=4.11 vs. limit=10.07825
2024-10-09 04:02:01,333 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=6875.333333333333, ans=0.038019444444444446
2024-10-09 04:02:04,625 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.79 vs. limit=12.658999999999999
2024-10-09 04:02:09,890 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=6878.666666666667, ans=0.1775625
2024-10-09 04:02:14,927 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.74 vs. limit=10.0795
2024-10-09 04:02:28,845 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=6882.0, ans=0.025
2024-10-09 04:02:33,054 INFO [train.py:1152] Epoch 4, batch 100, loss[loss=0.301, ctc_loss=0.2547, attn_decoder_loss=0.3126, over 4752.00 frames. ], tot_loss[loss=0.3084, ctc_loss=0.2686, attn_decoder_loss=0.3183, over 383461.72 frames. ], batch size: 19, lr: 2.29e-02,
2024-10-09 04:02:40,321 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=6885.333333333333, ans=0.6590133333333335
2024-10-09 04:02:50,773 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=6888.666666666667, ans=0.025
2024-10-09 04:03:02,758 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.83 vs. limit=12.669
2024-10-09 04:03:37,138 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.689e+01 8.140e+01 9.226e+01 9.961e+01 1.374e+02, threshold=1.845e+02, percent-clipped=0.0
2024-10-09 04:03:37,378 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=6898.666666666667, ans=0.6585466666666667
2024-10-09 04:03:45,905 INFO [train.py:1152] Epoch 4, batch 150, loss[loss=0.2934, ctc_loss=0.2399, attn_decoder_loss=0.3068, over 4910.00 frames. ], tot_loss[loss=0.3044, ctc_loss=0.2632, attn_decoder_loss=0.3147, over 513379.36 frames. ], batch size: 19, lr: 2.29e-02,
2024-10-09 04:03:49,634 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.77 vs. limit=12.6765
2024-10-09 04:03:58,346 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=19.11 vs. limit=10.08825
2024-10-09 04:04:13,695 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=6908.666666666667, ans=0.17615625000000001
2024-10-09 04:04:32,435 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=6912.0, ans=0.176
2024-10-09 04:04:32,446 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=6912.0, ans=0.23088
2024-10-09 04:04:49,757 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=6915.333333333333, ans=0.6579633333333335
2024-10-09 04:04:58,204 INFO [train.py:1152] Epoch 4, batch 200, loss[loss=0.3207, ctc_loss=0.2816, attn_decoder_loss=0.3305, over 4743.00 frames. ], tot_loss[loss=0.304, ctc_loss=0.2622, attn_decoder_loss=0.3145, over 613758.04 frames. ], batch size: 45, lr: 2.29e-02,
2024-10-09 04:05:15,637 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=6922.0, ans=0.17553125000000003
2024-10-09 04:05:28,903 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6925.333333333333, ans=0.23074666666666666
2024-10-09 04:05:39,107 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=6925.333333333333, ans=0.0
2024-10-09 04:06:02,399 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.547e+01 7.799e+01 8.712e+01 1.049e+02 1.684e+02, threshold=1.742e+02, percent-clipped=0.0
2024-10-09 04:06:10,955 INFO [train.py:1152] Epoch 4, batch 250, loss[loss=0.3653, ctc_loss=0.3295, attn_decoder_loss=0.3742, over 4833.00 frames. ], tot_loss[loss=0.3034, ctc_loss=0.2606, attn_decoder_loss=0.3141, over 692511.38 frames. ], batch size: 38, lr: 2.28e-02,
2024-10-09 04:06:11,113 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=6935.333333333333, ans=0.17490624999999999
2024-10-09 04:06:12,561 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6935.333333333333, ans=0.23064666666666667
2024-10-09 04:06:17,714 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.82 vs. limit=10.10075
2024-10-09 04:06:18,863 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.99 vs. limit=6.733833333333333
2024-10-09 04:06:19,790 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.attention_skip_rate, batch_count=6935.333333333333, ans=0.03776944444444445
2024-10-09 04:06:31,416 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=6938.666666666667, ans=0.037755555555555556
2024-10-09 04:06:46,703 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.99 vs. limit=12.7065
2024-10-09 04:07:00,247 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=6945.333333333333, ans=0.17443750000000002
2024-10-09 04:07:23,690 INFO [train.py:1152] Epoch 4, batch 300, loss[loss=0.3118, ctc_loss=0.2942, attn_decoder_loss=0.3162, over 4762.00 frames. ], tot_loss[loss=0.3021, ctc_loss=0.2591, attn_decoder_loss=0.3129, over 752887.33 frames. ], batch size: 32, lr: 2.28e-02,
2024-10-09 04:07:32,872 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.64 vs. limit=6.7379999999999995
2024-10-09 04:07:33,883 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=6952.0, ans=0.17412499999999997
2024-10-09 04:07:39,537 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=6955.333333333333, ans=0.17396875
2024-10-09 04:07:51,008 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=6958.666666666667, ans=0.17381249999999998
2024-10-09 04:07:56,027 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.03 vs. limit=12.719000000000001
2024-10-09 04:08:08,405 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6962.0, ans=0.23038
2024-10-09 04:08:19,043 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.whiten.whitening_limit, batch_count=6962.0, ans=6.784800000000001
2024-10-09 04:08:27,163 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.878e+01 8.095e+01 9.215e+01 1.035e+02 1.897e+02, threshold=1.843e+02, percent-clipped=1.0
2024-10-09 04:08:31,756 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=6965.333333333333, ans=0.1735
2024-10-09 04:08:35,750 INFO [train.py:1152] Epoch 4, batch 350, loss[loss=0.2941, ctc_loss=0.241, attn_decoder_loss=0.3074, over 4883.00 frames. ], tot_loss[loss=0.3028, ctc_loss=0.26, attn_decoder_loss=0.3135, over 800322.46 frames. ], batch size: 19, lr: 2.28e-02,
2024-10-09 04:08:35,929 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=6968.666666666667, ans=0.037630555555555556
2024-10-09 04:08:48,190 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.01 vs. limit=8.484333333333334
2024-10-09 04:09:44,017 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=6982.0, ans=0.17271874999999998
2024-10-09 04:09:48,151 INFO [train.py:1152] Epoch 4, batch 400, loss[loss=0.2994, ctc_loss=0.2556, attn_decoder_loss=0.3103, over 4862.00 frames. ], tot_loss[loss=0.3015, ctc_loss=0.2579, attn_decoder_loss=0.3124, over 837047.37 frames. ], batch size: 22, lr: 2.28e-02,
2024-10-09 04:10:01,854 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.40 vs. limit=10.120750000000001
2024-10-09 04:10:46,265 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=6998.666666666667, ans=0.17193750000000002
2024-10-09 04:10:50,724 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer1.prob, batch_count=6998.666666666667, ans=0.17193750000000002
2024-10-09 04:10:51,893 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.690e+01 7.957e+01 8.888e+01 9.643e+01 1.375e+02, threshold=1.778e+02, percent-clipped=0.0
2024-10-09 04:10:52,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=6998.666666666667, ans=0.009348115942028985
2024-10-09 04:10:54,821 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=6998.666666666667, ans=0.17193750000000002
2024-10-09 04:11:00,426 INFO [train.py:1152] Epoch 4, batch 450, loss[loss=0.2829, ctc_loss=0.2279, attn_decoder_loss=0.2966, over 4866.00 frames. ], tot_loss[loss=0.3006, ctc_loss=0.2562, attn_decoder_loss=0.3117, over 865684.31 frames. ], batch size: 23, lr: 2.27e-02,
2024-10-09 04:11:18,054 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=7005.333333333333, ans=0.009346666666666666
2024-10-09 04:11:19,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=7005.333333333333, ans=0.6548133333333334
2024-10-09 04:11:35,619 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=7008.666666666667, ans=0.03746388888888889
2024-10-09 04:11:36,286 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.18 vs. limit=12.756499999999999
2024-10-09 04:11:58,518 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=7.30 vs. limit=6.753833333333333
2024-10-09 04:12:13,213 INFO [train.py:1152] Epoch 4, batch 500, loss[loss=0.2907, ctc_loss=0.2455, attn_decoder_loss=0.3021, over 4799.00 frames. ], tot_loss[loss=0.2993, ctc_loss=0.2541, attn_decoder_loss=0.3107, over 888223.35 frames. ], batch size: 34, lr: 2.27e-02,
2024-10-09 04:12:33,990 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 04:12:44,623 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.75 vs. limit=10.1345
2024-10-09 04:12:46,942 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=7025.333333333333, ans=0.028045833333333336
2024-10-09 04:12:56,399 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=6.61 vs. limit=10.13575
2024-10-09 04:13:04,529 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=7028.666666666667, ans=0.17053125000000002
2024-10-09 04:13:05,898 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=7028.666666666667, ans=0.025
2024-10-09 04:13:17,295 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.997e+01 8.042e+01 9.531e+01 1.095e+02 1.930e+02, threshold=1.906e+02, percent-clipped=1.0
2024-10-09 04:13:26,028 INFO [train.py:1152] Epoch 4, batch 550, loss[loss=0.3222, ctc_loss=0.2826, attn_decoder_loss=0.3321, over 4803.00 frames. ], tot_loss[loss=0.3011, ctc_loss=0.2571, attn_decoder_loss=0.3121, over 905650.63 frames. ], batch size: 40, lr: 2.27e-02,
2024-10-09 04:13:32,014 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 04:13:35,347 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=10.46 vs. limit=10.13825
2024-10-09 04:13:41,728 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten.whitening_limit, batch_count=7038.666666666667, ans=10.1395
2024-10-09 04:13:45,054 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=7038.666666666667, ans=0.1700625
2024-10-09 04:14:07,441 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.96 vs. limit=6.7605
2024-10-09 04:14:09,115 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.42 vs. limit=10.142
2024-10-09 04:14:11,117 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=7045.333333333333, ans=0.16975
2024-10-09 04:14:19,670 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=7045.333333333333, ans=0.16975
2024-10-09 04:14:22,667 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=7048.666666666667, ans=0.16959375
2024-10-09 04:14:38,724 INFO [train.py:1152] Epoch 4, batch 600, loss[loss=0.2885, ctc_loss=0.2357, attn_decoder_loss=0.3018, over 4826.00 frames. ], tot_loss[loss=0.2992, ctc_loss=0.2538, attn_decoder_loss=0.3105, over 919433.57 frames. ], batch size: 38, lr: 2.27e-02,
2024-10-09 04:14:59,171 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=7055.333333333333, ans=0.09899494936611666
2024-10-09 04:15:02,672 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.28 vs. limit=10.14575
2024-10-09 04:15:05,654 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten.whitening_limit, batch_count=7055.333333333333, ans=10.14575
2024-10-09 04:15:21,078 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=7062.0, ans=0.30593000000000004
2024-10-09 04:15:38,374 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=7065.333333333333, ans=0.16881249999999998
2024-10-09 04:15:42,701 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.114e+01 7.662e+01 8.665e+01 9.940e+01 1.864e+02, threshold=1.733e+02, percent-clipped=0.0
2024-10-09 04:15:51,360 INFO [train.py:1152] Epoch 4, batch 650, loss[loss=0.3009, ctc_loss=0.2542, attn_decoder_loss=0.3126, over 4834.00 frames. ], tot_loss[loss=0.2971, ctc_loss=0.251, attn_decoder_loss=0.3087, over 930187.25 frames. ], batch size: 21, lr: 2.26e-02,
2024-10-09 04:15:55,793 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.min_positive, batch_count=7068.666666666667, ans=0.05582083333333333
2024-10-09 04:16:08,799 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=7072.0, ans=0.16849999999999998
2024-10-09 04:16:11,755 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=7072.0, ans=0.009332173913043479
2024-10-09 04:16:22,583 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.07 vs. limit=12.8065
2024-10-09 04:16:24,844 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=7075.333333333333, ans=0.16834375000000001
2024-10-09 04:16:40,924 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=7078.666666666667, ans=0.22921333333333332
2024-10-09 04:16:42,471 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=7078.666666666667, ans=0.1681875
2024-10-09 04:17:03,412 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.74 vs. limit=8.542666666666666
2024-10-09 04:17:03,816 INFO [train.py:1152] Epoch 4, batch 700, loss[loss=0.292, ctc_loss=0.2438, attn_decoder_loss=0.304, over 4751.00 frames. ], tot_loss[loss=0.2966, ctc_loss=0.2507, attn_decoder_loss=0.3081, over 938143.18 frames. ], batch size: 19, lr: 2.26e-02,
2024-10-09 04:17:08,314 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7085.333333333333, ans=0.22914666666666667
2024-10-09 04:17:24,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=7088.666666666667, ans=0.07
2024-10-09 04:17:30,119 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=7088.666666666667, ans=0.30633
2024-10-09 04:17:31,441 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7092.0, ans=0.22908
2024-10-09 04:17:39,067 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.33 vs. limit=12.818999999999999
2024-10-09 04:17:55,568 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.40 vs. limit=10.16075
2024-10-09 04:18:07,640 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.147e+01 8.067e+01 9.110e+01 1.020e+02 1.637e+02, threshold=1.822e+02, percent-clipped=0.0
2024-10-09 04:18:16,547 INFO [train.py:1152] Epoch 4, batch 750, loss[loss=0.2933, ctc_loss=0.2415, attn_decoder_loss=0.3063, over 4884.00 frames. ], tot_loss[loss=0.2969, ctc_loss=0.2509, attn_decoder_loss=0.3083, over 945003.39 frames. ], batch size: 22, lr: 2.26e-02,
2024-10-09 04:18:22,422 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7102.0, ans=0.22898
2024-10-09 04:18:23,940 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=7102.0, ans=0.16709374999999999
2024-10-09 04:18:29,681 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.min_positive, batch_count=7105.333333333333, ans=0.05559166666666667
2024-10-09 04:18:36,094 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.73 vs. limit=12.829
2024-10-09 04:19:13,032 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=7115.333333333333, ans=0.16646875
2024-10-09 04:19:16,491 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.23 vs. limit=4.0672999999999995
2024-10-09 04:19:18,817 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7115.333333333333, ans=0.22884666666666667
2024-10-09 04:19:20,244 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=7115.333333333333, ans=0.0
2024-10-09 04:19:24,664 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=7115.333333333333, ans=0.16646875
2024-10-09 04:19:30,157 INFO [train.py:1152] Epoch 4, batch 800, loss[loss=0.2715, ctc_loss=0.2307, attn_decoder_loss=0.2817, over 4855.00 frames. ], tot_loss[loss=0.2963, ctc_loss=0.25, attn_decoder_loss=0.3079, over 949810.43 frames. ], batch size: 19, lr: 2.26e-02,
2024-10-09 04:19:30,611 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=4.96 vs. limit=6.7796666666666665
2024-10-09 04:19:41,138 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=7118.666666666667, ans=0.009322028985507247
2024-10-09 04:19:56,586 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=7125.333333333333, ans=0.16599999999999998
2024-10-09 04:20:18,203 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=7128.666666666667, ans=0.036963888888888886
2024-10-09 04:20:32,576 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.890e+01 7.718e+01 8.577e+01 9.126e+01 1.684e+02, threshold=1.715e+02, percent-clipped=0.0
2024-10-09 04:20:32,800 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=7132.0, ans=0.22868
2024-10-09 04:20:41,204 INFO [train.py:1152] Epoch 4, batch 850, loss[loss=0.3023, ctc_loss=0.2633, attn_decoder_loss=0.312, over 4773.00 frames. ], tot_loss[loss=0.2961, ctc_loss=0.2493, attn_decoder_loss=0.3078, over 954002.51 frames. ], batch size: 29, lr: 2.25e-02,
2024-10-09 04:20:42,868 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=7135.333333333333, ans=0.16553125000000002
2024-10-09 04:20:44,195 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=7135.333333333333, ans=0.16553125000000002
2024-10-09 04:21:00,084 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7138.666666666667, ans=0.22861333333333334
2024-10-09 04:21:03,657 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.25 vs. limit=10.177
2024-10-09 04:21:12,448 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=14.94 vs. limit=12.8565
2024-10-09 04:21:28,868 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7145.333333333333, ans=0.22854666666666668
2024-10-09 04:21:30,279 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=7145.333333333333, ans=0.07
2024-10-09 04:21:31,652 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=7145.333333333333, ans=0.1650625
2024-10-09 04:21:42,682 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.83 vs. limit=12.8615
2024-10-09 04:21:43,248 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.prob, batch_count=7148.666666666667, ans=0.16490624999999998
2024-10-09 04:21:53,282 INFO [train.py:1152] Epoch 4, batch 900, loss[loss=0.2595, ctc_loss=0.216, attn_decoder_loss=0.2704, over 4852.00 frames. ], tot_loss[loss=0.2963, ctc_loss=0.2498, attn_decoder_loss=0.3079, over 956970.46 frames. ], batch size: 19, lr: 2.25e-02,
2024-10-09 04:21:55,718 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.52 vs. limit=10.182
2024-10-09 04:21:59,742 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.18 vs. limit=12.864
2024-10-09 04:22:07,448 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.59 vs. limit=12.8665
2024-10-09 04:22:14,245 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.97 vs. limit=4.0733
2024-10-09 04:22:43,351 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=20.22 vs. limit=8.581
2024-10-09 04:22:57,059 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.840e+01 8.089e+01 8.990e+01 1.028e+02 2.434e+02, threshold=1.798e+02, percent-clipped=3.0
2024-10-09 04:23:00,235 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=7165.333333333333, ans=0.16412500000000002
2024-10-09 04:23:04,532 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=7168.666666666667, ans=0.0
2024-10-09 04:23:05,880 INFO [train.py:1152] Epoch 4, batch 950, loss[loss=0.2943, ctc_loss=0.2289, attn_decoder_loss=0.3107, over 4819.00 frames. ], tot_loss[loss=0.2963, ctc_loss=0.2496, attn_decoder_loss=0.308, over 958840.31 frames. ], batch size: 19, lr: 2.25e-02,
2024-10-09 04:23:09,649 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.66 vs. limit=6.867466666666667
2024-10-09 04:23:11,217 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.17 vs. limit=8.584333333333333
2024-10-09 04:23:11,731 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=7168.666666666667, ans=0.16396875
2024-10-09 04:23:34,841 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=7175.333333333333, ans=0.03676944444444445
2024-10-09 04:23:36,864 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.35 vs. limit=10.19075
2024-10-09 04:23:54,967 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7178.666666666667, ans=0.22821333333333332
2024-10-09 04:24:17,539 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.93 vs. limit=12.889
2024-10-09 04:24:18,192 INFO [train.py:1152] Epoch 4, batch 1000, loss[loss=0.2328, ctc_loss=0.1758, attn_decoder_loss=0.2471, over 4931.00 frames. ], tot_loss[loss=0.2957, ctc_loss=0.2494, attn_decoder_loss=0.3073, over 960696.18 frames. ], batch size: 20, lr: 2.25e-02,
2024-10-09 04:24:25,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff2_skip_rate, batch_count=7185.333333333333, ans=0.009307536231884059
2024-10-09 04:24:37,169 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 04:24:55,124 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.85 vs. limit=12.894
2024-10-09 04:25:00,501 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=7195.333333333333, ans=0.025
2024-10-09 04:25:01,219 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.71 vs. limit=10.19825
2024-10-09 04:25:12,209 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=7195.333333333333, ans=0.6481633333333334
2024-10-09 04:25:22,187 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.103e+01 7.679e+01 8.627e+01 9.471e+01 1.227e+02, threshold=1.725e+02, percent-clipped=0.0
2024-10-09 04:25:30,738 INFO [train.py:1152] Epoch 4, batch 1050, loss[loss=0.2664, ctc_loss=0.2074, attn_decoder_loss=0.2812, over 4796.00 frames. ], tot_loss[loss=0.294, ctc_loss=0.2464, attn_decoder_loss=0.3058, over 962859.70 frames. ], batch size: 25, lr: 2.25e-02,
2024-10-09 04:25:34,719 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=13.43 vs. limit=12.9015
2024-10-09 04:25:48,977 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.90 vs. limit=6.801333333333333
2024-10-09 04:25:55,555 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=7205.333333333333, ans=0.025
2024-10-09 04:26:04,297 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=7208.666666666667, ans=0.009302463768115943
2024-10-09 04:26:07,795 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.53 vs. limit=10.20325
2024-10-09 04:26:23,378 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=7212.0, ans=0.0
2024-10-09 04:26:32,094 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=7215.333333333333, ans=0.009301014492753624
2024-10-09 04:26:42,356 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=7218.666666666667, ans=0.2278133333333333
2024-10-09 04:26:43,617 INFO [train.py:1152] Epoch 4, batch 1100, loss[loss=0.2879, ctc_loss=0.2234, attn_decoder_loss=0.3041, over 4860.00 frames. ], tot_loss[loss=0.2943, ctc_loss=0.2469, attn_decoder_loss=0.3062, over 964083.16 frames. ], batch size: 20, lr: 2.24e-02,
2024-10-09 04:26:52,511 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=7218.666666666667, ans=0.16162500000000002
2024-10-09 04:27:06,986 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=7222.0, ans=0.009299565217391304
2024-10-09 04:27:09,126 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=5.13 vs. limit=8.611
2024-10-09 04:27:12,810 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=7225.333333333333, ans=0.22774666666666665
2024-10-09 04:27:13,439 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.47 vs. limit=10.2095
2024-10-09 04:27:21,475 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=7225.333333333333, ans=0.09899494936611666
2024-10-09 04:27:21,844 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.20 vs. limit=10.2095
2024-10-09 04:27:46,992 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.008e+01 7.678e+01 8.773e+01 9.884e+01 1.791e+02, threshold=1.755e+02, percent-clipped=1.0
2024-10-09 04:27:55,683 INFO [train.py:1152] Epoch 4, batch 1150, loss[loss=0.2884, ctc_loss=0.2284, attn_decoder_loss=0.3034, over 4863.00 frames. ], tot_loss[loss=0.2947, ctc_loss=0.247, attn_decoder_loss=0.3066, over 964369.01 frames. ], batch size: 20, lr: 2.24e-02,
2024-10-09 04:28:21,717 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=7238.666666666667, ans=0.036505555555555555
2024-10-09 04:28:30,145 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7242.0, ans=0.22758
2024-10-09 04:28:35,914 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 04:28:44,412 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=7245.333333333333, ans=0.160375
2024-10-09 04:29:06,159 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=7252.0, ans=0.09899494936611666
2024-10-09 04:29:07,453 INFO [train.py:1152] Epoch 4, batch 1200, loss[loss=0.3142, ctc_loss=0.2613, attn_decoder_loss=0.3274, over 4793.00 frames. ], tot_loss[loss=0.2949, ctc_loss=0.247, attn_decoder_loss=0.3068, over 964319.14 frames. ], batch size: 25, lr: 2.24e-02,
2024-10-09 04:29:07,570 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=7252.0, ans=0.1600625
2024-10-09 04:29:12,671 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.06 vs. limit=10.2195
2024-10-09 04:29:43,795 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7258.666666666667, ans=0.2274133333333333
2024-10-09 04:29:53,964 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=7262.0, ans=0.15959374999999998
2024-10-09 04:29:56,581 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=7262.0, ans=0.13282650000000001
2024-10-09 04:30:01,118 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7262.0, ans=0.22738
2024-10-09 04:30:04,126 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=7265.333333333333, ans=0.6457133333333334
2024-10-09 04:30:11,215 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.138e+01 8.439e+01 9.300e+01 1.020e+02 2.302e+02, threshold=1.860e+02, percent-clipped=1.0
2024-10-09 04:30:19,881 INFO [train.py:1152] Epoch 4, batch 1250, loss[loss=0.3376, ctc_loss=0.3126, attn_decoder_loss=0.3439, over 4758.00 frames. ], tot_loss[loss=0.295, ctc_loss=0.2471, attn_decoder_loss=0.3069, over 964346.90 frames. ], batch size: 32, lr: 2.24e-02,
2024-10-09 04:30:28,839 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=7268.666666666667, ans=0.025
2024-10-09 04:30:31,707 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=7268.666666666667, ans=0.009289420289855072
2024-10-09 04:30:33,241 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=7272.0, ans=0.15912500000000002
2024-10-09 04:30:34,156 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module2.whiten, num_groups=1, num_channels=192, metric=2.63 vs. limit=10.227
2024-10-09 04:30:45,295 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.97 vs. limit=10.227
2024-10-09 04:30:48,215 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.28 vs. limit=10.22825
2024-10-09 04:31:03,612 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=7278.666666666667, ans=0.009287246376811595
2024-10-09 04:31:31,772 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=6.15 vs. limit=10.232
2024-10-09 04:31:32,302 INFO [train.py:1152] Epoch 4, batch 1300, loss[loss=0.3137, ctc_loss=0.2696, attn_decoder_loss=0.3247, over 4823.00 frames. ], tot_loss[loss=0.294, ctc_loss=0.246, attn_decoder_loss=0.306, over 965666.90 frames. ], batch size: 43, lr: 2.23e-02,
2024-10-09 04:31:40,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=7285.333333333333, ans=0.15849999999999997
2024-10-09 04:31:40,916 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=7285.333333333333, ans=0.03631111111111111
2024-10-09 04:31:42,429 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=7285.333333333333, ans=0.6450133333333334
2024-10-09 04:31:43,070 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.03 vs. limit=12.964
2024-10-09 04:31:58,108 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=7288.666666666667, ans=0.027222916666666666
2024-10-09 04:32:11,527 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.89 vs. limit=10.2345
2024-10-09 04:32:12,524 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=7292.0, ans=0.0272125
2024-10-09 04:32:14,646 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.92 vs. limit=12.971499999999999
2024-10-09 04:32:15,143 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=7295.333333333333, ans=0.15803125
2024-10-09 04:32:26,973 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=7295.333333333333, ans=0.009283623188405797
2024-10-09 04:32:35,561 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.564e+01 7.758e+01 8.891e+01 1.006e+02 2.400e+02, threshold=1.778e+02, percent-clipped=1.0
2024-10-09 04:32:42,231 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.94 vs. limit=12.974
2024-10-09 04:32:44,314 INFO [train.py:1152] Epoch 4, batch 1350, loss[loss=0.2588, ctc_loss=0.1885, attn_decoder_loss=0.2764, over 4855.00 frames. ], tot_loss[loss=0.2933, ctc_loss=0.2449, attn_decoder_loss=0.3054, over 966349.53 frames. ], batch size: 21, lr: 2.23e-02,
2024-10-09 04:33:12,039 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=7308.666666666667, ans=0.15740625000000003
2024-10-09 04:33:21,359 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.18 vs. limit=4.0963
2024-10-09 04:33:36,787 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=7312.0, ans=0.00928
2024-10-09 04:33:56,775 INFO [train.py:1152] Epoch 4, batch 1400, loss[loss=0.2737, ctc_loss=0.2293, attn_decoder_loss=0.2848, over 4940.00 frames. ], tot_loss[loss=0.294, ctc_loss=0.2458, attn_decoder_loss=0.306, over 966568.61 frames. ], batch size: 19, lr: 2.23e-02,
2024-10-09 04:34:22,753 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=7322.0, ans=0.15678124999999998
2024-10-09 04:34:43,730 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.07 vs. limit=4.0992999999999995
2024-10-09 04:34:54,999 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.95 vs. limit=12.998999999999999
2024-10-09 04:34:55,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=7332.0, ans=0.15631250000000002
2024-10-09 04:35:00,178 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.492e+01 7.920e+01 8.780e+01 9.953e+01 1.365e+02, threshold=1.756e+02, percent-clipped=0.0
2024-10-09 04:35:08,936 INFO [train.py:1152] Epoch 4, batch 1450, loss[loss=0.3287, ctc_loss=0.293, attn_decoder_loss=0.3376, over 4795.00 frames. ], tot_loss[loss=0.2953, ctc_loss=0.2474, attn_decoder_loss=0.3072, over 966449.11 frames. ], batch size: 34, lr: 2.23e-02,
2024-10-09 04:35:24,719 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=7338.666666666667, ans=0.025
2024-10-09 04:35:43,566 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=7342.0, ans=0.15584375
2024-10-09 04:35:49,160 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=7342.0, ans=0.15584375
2024-10-09 04:35:50,061 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.76 vs. limit=10.25325
2024-10-09 04:36:20,919 INFO [train.py:1152] Epoch 4, batch 1500, loss[loss=0.2777, ctc_loss=0.2091, attn_decoder_loss=0.2949, over 4725.00 frames. ], tot_loss[loss=0.2956, ctc_loss=0.2483, attn_decoder_loss=0.3074, over 966192.21 frames. ], batch size: 26, lr: 2.22e-02,
2024-10-09 04:36:49,027 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=4.28 vs. limit=10.2595
2024-10-09 04:37:21,878 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=7365.333333333333, ans=0.15475
2024-10-09 04:37:23,342 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=7365.333333333333, ans=0.22634666666666667
2024-10-09 04:37:24,495 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.368e+01 8.180e+01 8.993e+01 9.984e+01 1.571e+02, threshold=1.799e+02, percent-clipped=0.0
2024-10-09 04:37:31,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=7368.666666666667, ans=0.15459374999999997
2024-10-09 04:37:31,967 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=7368.666666666667, ans=0.15459374999999997
2024-10-09 04:37:33,192 INFO [train.py:1152] Epoch 4, batch 1550, loss[loss=0.236, ctc_loss=0.1604, attn_decoder_loss=0.2549, over 4872.00 frames. ], tot_loss[loss=0.2967, ctc_loss=0.2498, attn_decoder_loss=0.3085, over 966190.01 frames. ], batch size: 31, lr: 2.22e-02,
2024-10-09 04:37:46,261 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=7372.0, ans=0.03595
2024-10-09 04:38:21,152 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 04:38:44,043 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=7385.333333333333, ans=0.15381250000000002
2024-10-09 04:38:45,340 INFO [train.py:1152] Epoch 4, batch 1600, loss[loss=0.2923, ctc_loss=0.2389, attn_decoder_loss=0.3057, over 4825.00 frames. ], tot_loss[loss=0.2959, ctc_loss=0.2485, attn_decoder_loss=0.3078, over 966281.81 frames. ], batch size: 25, lr: 2.22e-02,
2024-10-09 04:39:03,566 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.99 vs. limit=4.1083
2024-10-09 04:39:09,450 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.93 vs. limit=10.27075
2024-10-09 04:39:30,498 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=7395.333333333333, ans=0.6411633333333333
2024-10-09 04:39:49,314 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.522e+01 7.801e+01 8.743e+01 1.012e+02 1.361e+02, threshold=1.749e+02, percent-clipped=0.0
2024-10-09 04:39:58,013 INFO [train.py:1152] Epoch 4, batch 1650, loss[loss=0.2563, ctc_loss=0.1716, attn_decoder_loss=0.2775, over 4774.00 frames. ], tot_loss[loss=0.2957, ctc_loss=0.249, attn_decoder_loss=0.3074, over 966607.59 frames. ], batch size: 29, lr: 2.22e-02,
2024-10-09 04:40:19,780 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=7405.333333333333, ans=0.15287499999999998
2024-10-09 04:40:44,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=7412.0, ans=0.035783333333333334
2024-10-09 04:40:49,068 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=7412.0, ans=0.035783333333333334
2024-10-09 04:40:55,609 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.52 vs. limit=13.061499999999999
2024-10-09 04:40:56,415 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=7415.333333333333, ans=0.15240625000000002
2024-10-09 04:41:10,632 INFO [train.py:1152] Epoch 4, batch 1700, loss[loss=0.2903, ctc_loss=0.2451, attn_decoder_loss=0.3016, over 4940.00 frames. ], tot_loss[loss=0.2945, ctc_loss=0.2465, attn_decoder_loss=0.3065, over 966818.46 frames. ], batch size: 19, lr: 2.22e-02,
2024-10-09 04:41:35,874 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=7.26 vs. limit=10.283249999999999
2024-10-09 04:42:11,907 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=7432.0, ans=0.151625
2024-10-09 04:42:14,570 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.537e+01 7.483e+01 8.503e+01 9.636e+01 2.036e+02, threshold=1.701e+02, percent-clipped=1.0
2024-10-09 04:42:23,257 INFO [train.py:1152] Epoch 4, batch 1750, loss[loss=0.2611, ctc_loss=0.1986, attn_decoder_loss=0.2767, over 4959.00 frames. ], tot_loss[loss=0.2938, ctc_loss=0.2446, attn_decoder_loss=0.3061, over 966900.45 frames. ], batch size: 19, lr: 2.21e-02,
2024-10-09 04:42:23,389 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=7435.333333333333, ans=0.15146874999999999
2024-10-09 04:42:49,331 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=7438.666666666667, ans=0.009252463768115941
2024-10-09 04:43:10,900 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=7445.333333333333, ans=0.15100000000000002
2024-10-09 04:43:13,839 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=7445.333333333333, ans=0.053466666666666676
2024-10-09 04:43:25,990 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=15.49 vs. limit=10.29325
2024-10-09 04:43:33,431 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.26 vs. limit=10.29325
2024-10-09 04:43:35,304 INFO [train.py:1152] Epoch 4, batch 1800, loss[loss=0.2897, ctc_loss=0.2356, attn_decoder_loss=0.3032, over 4873.00 frames. ], tot_loss[loss=0.2954, ctc_loss=0.2469, attn_decoder_loss=0.3075, over 967780.13 frames. ], batch size: 23, lr: 2.21e-02,
2024-10-09 04:43:39,937 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=7452.0, ans=0.15068749999999997
2024-10-09 04:43:48,532 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=7455.333333333333, ans=0.15053125
2024-10-09 04:44:25,204 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=7462.0, ans=0.035575
2024-10-09 04:44:26,661 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=7462.0, ans=0.15021875
2024-10-09 04:44:29,563 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=7462.0, ans=0.15021875
2024-10-09 04:44:39,640 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.300e+01 7.917e+01 8.825e+01 9.512e+01 1.919e+02, threshold=1.765e+02, percent-clipped=1.0
2024-10-09 04:44:44,011 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer1.prob, batch_count=7465.333333333333, ans=0.1500625
2024-10-09 04:44:48,036 INFO [train.py:1152] Epoch 4, batch 1850, loss[loss=0.2821, ctc_loss=0.2295, attn_decoder_loss=0.2952, over 4736.00 frames. ], tot_loss[loss=0.2944, ctc_loss=0.2458, attn_decoder_loss=0.3066, over 967924.76 frames. ], batch size: 26, lr: 2.21e-02,
2024-10-09 04:45:02,645 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=7472.0, ans=0.63848
2024-10-09 04:45:11,214 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=7472.0, ans=0.009245217391304348
2024-10-09 04:45:33,488 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.59 vs. limit=13.109
2024-10-09 04:45:48,296 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=5.30 vs. limit=5.4963999999999995
2024-10-09 04:45:54,384 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=7482.0, ans=0.14928124999999998
2024-10-09 04:45:55,057 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.18 vs. limit=13.1115
2024-10-09 04:46:00,090 INFO [train.py:1152] Epoch 4, batch 1900, loss[loss=0.3233, ctc_loss=0.2847, attn_decoder_loss=0.3329, over 4784.00 frames. ], tot_loss[loss=0.2945, ctc_loss=0.2459, attn_decoder_loss=0.3066, over 967832.65 frames. ], batch size: 29, lr: 2.21e-02,
2024-10-09 04:46:10,220 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=7485.333333333333, ans=0.1307606666666667
2024-10-09 04:46:13,252 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7488.666666666667, ans=0.22511333333333333
2024-10-09 04:46:24,122 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=7.20 vs. limit=6.995466666666667
2024-10-09 04:46:35,819 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=9.44 vs. limit=10.3095
2024-10-09 04:46:43,543 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=7495.333333333333, ans=0.14865625
2024-10-09 04:47:03,568 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.102e+01 7.954e+01 8.852e+01 9.789e+01 1.314e+02, threshold=1.770e+02, percent-clipped=0.0
2024-10-09 04:47:11,146 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=7502.0, ans=0.63743
2024-10-09 04:47:12,344 INFO [train.py:1152] Epoch 4, batch 1950, loss[loss=0.2798, ctc_loss=0.2322, attn_decoder_loss=0.2917, over 4862.00 frames. ], tot_loss[loss=0.2951, ctc_loss=0.2471, attn_decoder_loss=0.3071, over 966738.15 frames. ], batch size: 20, lr: 2.20e-02,
2024-10-09 04:47:16,799 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=7502.0, ans=0.63743
2024-10-09 04:47:24,014 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=7502.0, ans=0.14834375
2024-10-09 04:47:26,082 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.65 vs. limit=6.876333333333333
2024-10-09 04:47:46,521 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.max_abs, batch_count=7508.666666666667, ans=9.692916666666667
2024-10-09 04:47:49,503 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=7508.666666666667, ans=0.14803125
2024-10-09 04:47:58,122 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=7512.0, ans=0.03536666666666667
2024-10-09 04:48:06,371 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=14.03 vs. limit=13.134
2024-10-09 04:48:23,905 INFO [train.py:1152] Epoch 4, batch 2000, loss[loss=0.2952, ctc_loss=0.2257, attn_decoder_loss=0.3125, over 4959.00 frames. ], tot_loss[loss=0.2954, ctc_loss=0.2478, attn_decoder_loss=0.3073, over 966583.32 frames. ], batch size: 19, lr: 2.20e-02,
2024-10-09 04:48:24,063 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=7518.666666666667, ans=0.09899494936611666
2024-10-09 04:48:43,906 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.54 vs. limit=8.761
2024-10-09 04:48:47,852 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=7.40 vs. limit=10.32075
2024-10-09 04:48:51,597 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=7525.333333333333, ans=0.025
2024-10-09 04:48:52,950 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=7525.333333333333, ans=0.22474666666666665
2024-10-09 04:49:00,270 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=7525.333333333333, ans=0.14725
2024-10-09 04:49:04,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=7525.333333333333, ans=0.07
2024-10-09 04:49:27,612 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.511e+01 7.883e+01 8.914e+01 1.000e+02 1.419e+02, threshold=1.783e+02, percent-clipped=0.0
2024-10-09 04:49:29,186 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=7532.0, ans=0.03528333333333333
2024-10-09 04:49:31,988 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=7532.0, ans=0.1469375
2024-10-09 04:49:36,209 INFO [train.py:1152] Epoch 4, batch 2050, loss[loss=0.2985, ctc_loss=0.2408, attn_decoder_loss=0.313, over 4910.00 frames. ], tot_loss[loss=0.2948, ctc_loss=0.2471, attn_decoder_loss=0.3068, over 966862.03 frames. ], batch size: 19, lr: 2.20e-02,
2024-10-09 04:49:37,763 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=7535.333333333333, ans=0.03526944444444445
2024-10-09 04:49:38,353 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.29 vs. limit=13.1515
2024-10-09 04:49:45,048 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 04:50:04,839 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.94 vs. limit=8.771
2024-10-09 04:50:06,901 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=7542.0, ans=0.14646874999999998
2024-10-09 04:50:15,338 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=7542.0, ans=0.00923
2024-10-09 04:50:18,188 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=7545.333333333333, ans=0.025
2024-10-09 04:50:22,476 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=7545.333333333333, ans=0.6359133333333333
2024-10-09 04:50:48,556 INFO [train.py:1152] Epoch 4, batch 2100, loss[loss=0.2486, ctc_loss=0.1886, attn_decoder_loss=0.2636, over 4842.00 frames. ], tot_loss[loss=0.2931, ctc_loss=0.2451, attn_decoder_loss=0.3051, over 967040.29 frames. ], batch size: 21, lr: 2.20e-02,
2024-10-09 04:51:03,254 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=7555.333333333333, ans=0.0
2024-10-09 04:51:07,593 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=7555.333333333333, ans=0.14584375
2024-10-09 04:51:31,343 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.05 vs. limit=10.33575
2024-10-09 04:51:32,225 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=7562.0, ans=0.14553125
2024-10-09 04:51:38,000 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=7562.0, ans=0.14553125
2024-10-09 04:51:52,245 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.386e+01 7.819e+01 8.594e+01 1.012e+02 1.593e+02, threshold=1.719e+02, percent-clipped=0.0
2024-10-09 04:51:52,504 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=7565.333333333333, ans=0.6352133333333334
2024-10-09 04:51:55,773 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.71 vs. limit=10.337
2024-10-09 04:52:01,019 INFO [train.py:1152] Epoch 4, batch 2150, loss[loss=0.336, ctc_loss=0.3119, attn_decoder_loss=0.342, over 4863.00 frames. ], tot_loss[loss=0.2913, ctc_loss=0.2431, attn_decoder_loss=0.3034, over 967876.40 frames. ], batch size: 20, lr: 2.20e-02,
2024-10-09 04:52:24,192 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=7572.0, ans=0.0
2024-10-09 04:53:02,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7582.0, ans=0.22418
2024-10-09 04:53:12,833 INFO [train.py:1152] Epoch 4, batch 2200, loss[loss=0.2646, ctc_loss=0.2109, attn_decoder_loss=0.278, over 4746.00 frames. ], tot_loss[loss=0.2913, ctc_loss=0.2418, attn_decoder_loss=0.3037, over 967551.32 frames. ], batch size: 26, lr: 2.19e-02,
2024-10-09 04:53:27,265 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=7588.666666666667, ans=0.14428125000000003
2024-10-09 04:53:34,346 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=7588.666666666667, ans=0.03504722222222222
2024-10-09 04:53:44,353 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7592.0, ans=0.22408
2024-10-09 04:53:45,890 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=7592.0, ans=0.03503333333333333
2024-10-09 04:53:57,488 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=7595.333333333333, ans=0.0
2024-10-09 04:54:03,232 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=7595.333333333333, ans=0.14396874999999998
2024-10-09 04:54:03,246 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer1.prob, batch_count=7595.333333333333, ans=0.14396874999999998
2024-10-09 04:54:06,109 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7595.333333333333, ans=0.22404666666666667
2024-10-09 04:54:16,207 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.222e+01 7.453e+01 8.291e+01 9.843e+01 1.523e+02, threshold=1.658e+02, percent-clipped=0.0
2024-10-09 04:54:24,899 INFO [train.py:1152] Epoch 4, batch 2250, loss[loss=0.2622, ctc_loss=0.1957, attn_decoder_loss=0.2788, over 4869.00 frames. ], tot_loss[loss=0.2918, ctc_loss=0.2425, attn_decoder_loss=0.3041, over 967613.06 frames. ], batch size: 22, lr: 2.19e-02,
2024-10-09 04:54:30,761 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=7602.0, ans=0.03499166666666667
2024-10-09 04:54:48,338 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=1.018e-02
2024-10-09 04:54:56,913 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=7608.666666666667, ans=0.03496388888888889
2024-10-09 04:55:02,806 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=7608.666666666667, ans=0.009215507246376813
2024-10-09 04:55:17,315 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=7612.0, ans=0.14318750000000002
2024-10-09 04:55:18,765 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=7612.0, ans=0.07
2024-10-09 04:55:21,503 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=7615.333333333333, ans=0.14303125
2024-10-09 04:55:37,159 INFO [train.py:1152] Epoch 4, batch 2300, loss[loss=0.2866, ctc_loss=0.2256, attn_decoder_loss=0.3018, over 4883.00 frames. ], tot_loss[loss=0.291, ctc_loss=0.2416, attn_decoder_loss=0.3034, over 968207.48 frames. ], batch size: 19, lr: 2.19e-02,
2024-10-09 04:55:44,459 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=7618.666666666667, ans=0.035
2024-10-09 04:55:47,448 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=7618.666666666667, ans=0.14287499999999997
2024-10-09 04:55:57,367 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7622.0, ans=0.22377999999999998
2024-10-09 04:56:00,367 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=7622.0, ans=0.009212608695652174
2024-10-09 04:56:04,674 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=7625.333333333333, ans=0.14256249999999998
2024-10-09 04:56:15,686 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.19 vs. limit=13.219000000000001
2024-10-09 04:56:18,059 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.90 vs. limit=13.219000000000001
2024-10-09 04:56:22,657 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.68 vs. limit=6.907166666666667
2024-10-09 04:56:24,661 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=7628.666666666667, ans=0.14240625
2024-10-09 04:56:29,012 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=7628.666666666667, ans=0.14240625
2024-10-09 04:56:40,170 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.253e+01 7.830e+01 8.843e+01 1.002e+02 1.402e+02, threshold=1.769e+02, percent-clipped=0.0
2024-10-09 04:56:49,025 INFO [train.py:1152] Epoch 4, batch 2350, loss[loss=0.2922, ctc_loss=0.2467, attn_decoder_loss=0.3035, over 4859.00 frames. ], tot_loss[loss=0.2914, ctc_loss=0.2409, attn_decoder_loss=0.304, over 968307.41 frames. ], batch size: 23, lr: 2.19e-02,
2024-10-09 04:56:51,617 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.49 vs. limit=6.908833333333333
2024-10-09 04:57:14,854 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.prob, batch_count=7638.666666666667, ans=0.1419375
2024-10-09 04:57:17,756 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7642.0, ans=0.22358
2024-10-09 04:57:23,541 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=7642.0, ans=0.14178125000000003
2024-10-09 04:57:30,719 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=7645.333333333333, ans=0.141625
2024-10-09 04:57:30,731 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=7645.333333333333, ans=0.141625
2024-10-09 04:57:36,313 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=7645.333333333333, ans=0.141625
2024-10-09 04:57:45,171 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=7648.666666666667, ans=0.14146874999999998
2024-10-09 04:57:55,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=7648.666666666667, ans=0.14146874999999998
2024-10-09 04:58:00,817 INFO [train.py:1152] Epoch 4, batch 2400, loss[loss=0.2786, ctc_loss=0.2245, attn_decoder_loss=0.2921, over 4750.00 frames. ], tot_loss[loss=0.2926, ctc_loss=0.2426, attn_decoder_loss=0.3051, over 967582.97 frames. ], batch size: 19, lr: 2.19e-02,
2024-10-09 04:58:21,087 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=7655.333333333333, ans=0.03476944444444445
2024-10-09 04:58:25,299 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=7655.333333333333, ans=0.14115624999999998
2024-10-09 04:58:38,952 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=5.31 vs. limit=8.829333333333334
2024-10-09 04:58:41,232 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=7658.666666666667, ans=0.03475555555555555
2024-10-09 04:59:04,274 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.230e+01 8.274e+01 8.994e+01 9.788e+01 1.332e+02, threshold=1.799e+02, percent-clipped=0.0
2024-10-09 04:59:04,542 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7665.333333333333, ans=0.22334666666666667
2024-10-09 04:59:10,140 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=7665.333333333333, ans=0.14068750000000002
2024-10-09 04:59:12,902 INFO [train.py:1152] Epoch 4, batch 2450, loss[loss=0.2823, ctc_loss=0.2179, attn_decoder_loss=0.2984, over 4877.00 frames. ], tot_loss[loss=0.2949, ctc_loss=0.2458, attn_decoder_loss=0.3071, over 966796.70 frames. ], batch size: 22, lr: 2.18e-02,
2024-10-09 04:59:35,253 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.54 vs. limit=13.254
2024-10-09 04:59:41,873 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=7675.333333333333, ans=0.009201014492753623
2024-10-09 04:59:51,966 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=7675.333333333333, ans=0.03468611111111111
2024-10-09 05:00:22,657 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.13 vs. limit=13.2615
2024-10-09 05:00:24,739 INFO [train.py:1152] Epoch 4, batch 2500, loss[loss=0.2849, ctc_loss=0.2371, attn_decoder_loss=0.2968, over 4728.00 frames. ], tot_loss[loss=0.294, ctc_loss=0.245, attn_decoder_loss=0.3063, over 966471.94 frames. ], batch size: 26, lr: 2.18e-02,
2024-10-09 05:00:24,936 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=7685.333333333333, ans=0.03464444444444445
2024-10-09 05:00:27,767 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=7685.333333333333, ans=0.31528
2024-10-09 05:00:33,577 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=7685.333333333333, ans=0.03464444444444445
2024-10-09 05:00:37,079 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.80 vs. limit=13.264
2024-10-09 05:01:06,460 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=7695.333333333333, ans=0.13928125000000002
2024-10-09 05:01:09,398 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=7695.333333333333, ans=0.13928125000000002
2024-10-09 05:01:27,305 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.36 vs. limit=8.849333333333334
2024-10-09 05:01:28,081 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.545e+01 8.046e+01 9.275e+01 1.013e+02 1.904e+02, threshold=1.855e+02, percent-clipped=1.0
2024-10-09 05:01:33,973 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=7698.666666666667, ans=0.139125
2024-10-09 05:01:35,517 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=7702.0, ans=0.13896874999999997
2024-10-09 05:01:36,870 INFO [train.py:1152] Epoch 4, batch 2550, loss[loss=0.2678, ctc_loss=0.205, attn_decoder_loss=0.2835, over 4959.00 frames. ], tot_loss[loss=0.2941, ctc_loss=0.2452, attn_decoder_loss=0.3063, over 966913.72 frames. ], batch size: 19, lr: 2.18e-02,
2024-10-09 05:01:49,205 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.42 vs. limit=13.2765
2024-10-09 05:02:00,067 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=7705.333333333333, ans=0.1388125
2024-10-09 05:02:33,351 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=7715.333333333333, ans=0.00919231884057971
2024-10-09 05:02:37,781 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=7715.333333333333, ans=0.03451944444444445
2024-10-09 05:02:48,755 INFO [train.py:1152] Epoch 4, batch 2600, loss[loss=0.2698, ctc_loss=0.1915, attn_decoder_loss=0.2894, over 4863.00 frames. ], tot_loss[loss=0.2936, ctc_loss=0.2448, attn_decoder_loss=0.3058, over 966399.94 frames. ], batch size: 20, lr: 2.18e-02,
2024-10-09 05:02:55,978 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=7718.666666666667, ans=0.13818750000000002
2024-10-09 05:03:26,295 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.68 vs. limit=10.397
2024-10-09 05:03:51,655 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.112e+01 7.806e+01 8.737e+01 1.023e+02 2.032e+02, threshold=1.747e+02, percent-clipped=1.0
2024-10-09 05:04:00,124 INFO [train.py:1152] Epoch 4, batch 2650, loss[loss=0.329, ctc_loss=0.3032, attn_decoder_loss=0.3355, over 4828.00 frames. ], tot_loss[loss=0.2942, ctc_loss=0.2459, attn_decoder_loss=0.3063, over 966153.63 frames. ], batch size: 38, lr: 2.17e-02,
2024-10-09 05:04:26,299 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=7738.666666666667, ans=0.6291466666666667
2024-10-09 05:04:32,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=7742.0, ans=0.03440833333333333
2024-10-09 05:05:05,677 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.30 vs. limit=7.099466666666666
2024-10-09 05:05:12,233 INFO [train.py:1152] Epoch 4, batch 2700, loss[loss=0.3109, ctc_loss=0.2869, attn_decoder_loss=0.3168, over 4828.00 frames. ], tot_loss[loss=0.2943, ctc_loss=0.2459, attn_decoder_loss=0.3064, over 966241.39 frames. ], batch size: 28, lr: 2.17e-02,
2024-10-09 05:05:15,371 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=7752.0, ans=0.136625
2024-10-09 05:05:16,313 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.49 vs. limit=8.876
2024-10-09 05:05:21,140 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=7752.0, ans=0.136625
2024-10-09 05:05:27,700 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.89 vs. limit=13.3165
2024-10-09 05:05:40,043 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=7758.666666666667, ans=0.03433888888888889
2024-10-09 05:05:47,800 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.38 vs. limit=10.4095
2024-10-09 05:06:13,220 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=7765.333333333333, ans=0.6282133333333334
2024-10-09 05:06:15,880 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.700e+01 7.502e+01 8.589e+01 9.549e+01 1.512e+02, threshold=1.718e+02, percent-clipped=0.0
2024-10-09 05:06:24,727 INFO [train.py:1152] Epoch 4, batch 2750, loss[loss=0.2552, ctc_loss=0.1671, attn_decoder_loss=0.2772, over 4799.00 frames. ], tot_loss[loss=0.2915, ctc_loss=0.2422, attn_decoder_loss=0.3038, over 966976.71 frames. ], batch size: 19, lr: 2.17e-02,
2024-10-09 05:06:45,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=7772.0, ans=0.62798
2024-10-09 05:07:05,306 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=7775.333333333333, ans=0.13553125
2024-10-09 05:07:36,822 INFO [train.py:1152] Epoch 4, batch 2800, loss[loss=0.3434, ctc_loss=0.3001, attn_decoder_loss=0.3542, over 4791.00 frames. ], tot_loss[loss=0.2915, ctc_loss=0.2422, attn_decoder_loss=0.3038, over 967125.62 frames. ], batch size: 53, lr: 2.17e-02,
2024-10-09 05:07:56,212 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=13.71 vs. limit=10.42075
2024-10-09 05:08:04,887 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.41 vs. limit=13.344000000000001
2024-10-09 05:08:11,601 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=7792.0, ans=0.13474999999999998
2024-10-09 05:08:15,844 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 05:08:17,356 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=7792.0, ans=0.6272800000000001
2024-10-09 05:08:27,379 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=7795.333333333333, ans=0.13459375
2024-10-09 05:08:39,905 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.09 vs. limit=13.349
2024-10-09 05:08:40,410 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.988e+01 7.811e+01 8.979e+01 9.835e+01 1.766e+02, threshold=1.796e+02, percent-clipped=1.0
2024-10-09 05:08:49,009 INFO [train.py:1152] Epoch 4, batch 2850, loss[loss=0.2619, ctc_loss=0.1977, attn_decoder_loss=0.278, over 4941.00 frames. ], tot_loss[loss=0.2927, ctc_loss=0.2446, attn_decoder_loss=0.3047, over 966916.17 frames. ], batch size: 20, lr: 2.17e-02,
2024-10-09 05:08:53,276 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=7802.0, ans=0.03415833333333333
2024-10-09 05:09:00,484 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=7802.0, ans=0.0
2024-10-09 05:09:07,026 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.88 vs. limit=4.1708
2024-10-09 05:09:16,236 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=7808.666666666667, ans=0.13396875000000003
2024-10-09 05:09:23,545 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=7808.666666666667, ans=0.13396875000000003
2024-10-09 05:10:00,860 INFO [train.py:1152] Epoch 4, batch 2900, loss[loss=0.2756, ctc_loss=0.1979, attn_decoder_loss=0.295, over 4737.00 frames. ], tot_loss[loss=0.2939, ctc_loss=0.2452, attn_decoder_loss=0.3061, over 966035.40 frames. ], batch size: 20, lr: 2.16e-02,
2024-10-09 05:10:24,148 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=7822.0, ans=0.13334374999999998
2024-10-09 05:10:31,214 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7825.333333333333, ans=0.22174666666666668
2024-10-09 05:10:50,497 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=8.62 vs. limit=10.43575
2024-10-09 05:11:04,076 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.717e+01 7.598e+01 8.676e+01 9.812e+01 1.478e+02, threshold=1.735e+02, percent-clipped=0.0
2024-10-09 05:11:08,597 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=7832.0, ans=0.62588
2024-10-09 05:11:12,638 INFO [train.py:1152] Epoch 4, batch 2950, loss[loss=0.2829, ctc_loss=0.2359, attn_decoder_loss=0.2947, over 4797.00 frames. ], tot_loss[loss=0.2927, ctc_loss=0.2443, attn_decoder_loss=0.3048, over 966627.27 frames. ], batch size: 19, lr: 2.16e-02,
2024-10-09 05:11:23,672 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.94 vs. limit=10.43825
2024-10-09 05:11:39,456 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=7838.666666666667, ans=0.03400555555555555
2024-10-09 05:11:47,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=7842.0, ans=0.13240625
2024-10-09 05:11:49,199 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=7842.0, ans=0.13240625
2024-10-09 05:12:02,711 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.22 vs. limit=10.442
2024-10-09 05:12:13,447 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=7848.666666666667, ans=0.009163333333333334
2024-10-09 05:12:19,170 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=7848.666666666667, ans=0.13209375
2024-10-09 05:12:24,717 INFO [train.py:1152] Epoch 4, batch 3000, loss[loss=0.3074, ctc_loss=0.2644, attn_decoder_loss=0.3182, over 4841.00 frames. ], tot_loss[loss=0.293, ctc_loss=0.2442, attn_decoder_loss=0.3053, over 967160.60 frames. ], batch size: 21, lr: 2.16e-02,
2024-10-09 05:12:24,717 INFO [train.py:1175] Computing validation loss
2024-10-09 05:12:26,973 INFO [zipformer.py:1858] name=encoder.encoders.0.layers.0.self_attn_weights, attn_weights_entropy = tensor([5.3527, 5.2468, 5.1974, 4.9976], device='cuda:0')
2024-10-09 05:12:32,165 INFO [zipformer.py:1858] name=encoder.encoders.0.layers.1.self_attn_weights, attn_weights_entropy = tensor([5.5628, 5.0633, 5.3072, 5.6392], device='cuda:0')
2024-10-09 05:12:33,630 INFO [train.py:1184] Epoch 4, validation: loss=0.2269, ctc_loss=0.09728, attn_decoder_loss=0.2593, over 90464.00 frames.
2024-10-09 05:12:33,631 INFO [train.py:1185] Maximum memory allocated so far is 6613MB
2024-10-09 05:13:03,782 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7858.666666666667, ans=0.22141333333333332
2024-10-09 05:13:12,598 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=7858.666666666667, ans=0.131625
2024-10-09 05:13:21,824 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.09 vs. limit=10.44825
2024-10-09 05:13:37,193 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.626e+01 8.102e+01 9.198e+01 9.927e+01 1.849e+02, threshold=1.840e+02, percent-clipped=1.0
2024-10-09 05:13:38,831 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=7865.333333333333, ans=0.025
2024-10-09 05:13:45,752 INFO [train.py:1152] Epoch 4, batch 3050, loss[loss=0.2585, ctc_loss=0.1956, attn_decoder_loss=0.2742, over 4749.00 frames. ], tot_loss[loss=0.2933, ctc_loss=0.2444, attn_decoder_loss=0.3055, over 966580.40 frames. ], batch size: 19, lr: 2.16e-02,
2024-10-09 05:13:47,386 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=7868.666666666667, ans=0.13115624999999997
2024-10-09 05:13:51,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=7868.666666666667, ans=0.13115624999999997
2024-10-09 05:14:10,931 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.07 vs. limit=8.936
2024-10-09 05:14:14,177 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=17.98 vs. limit=13.406500000000001
2024-10-09 05:14:17,617 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=7875.333333333333, ans=0.13084374999999998
2024-10-09 05:14:19,994 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=8.87 vs. limit=10.45325
2024-10-09 05:14:34,907 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=7878.666666666667, ans=0.1306875
2024-10-09 05:14:52,891 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.53 vs. limit=10.45575
2024-10-09 05:14:55,692 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=17.51 vs. limit=13.4115
2024-10-09 05:14:57,748 INFO [train.py:1152] Epoch 4, batch 3100, loss[loss=0.2869, ctc_loss=0.254, attn_decoder_loss=0.2951, over 4816.00 frames. ], tot_loss[loss=0.2923, ctc_loss=0.2431, attn_decoder_loss=0.3046, over 966382.63 frames. ], batch size: 38, lr: 2.16e-02,
2024-10-09 05:15:11,486 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=6.16 vs. limit=10.45825
2024-10-09 05:15:29,680 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7892.0, ans=0.22108
2024-10-09 05:15:30,085 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.21 vs. limit=10.4595
2024-10-09 05:15:32,033 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=9.09 vs. limit=10.4595
2024-10-09 05:15:51,142 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=7895.333333333333, ans=0.12990625
2024-10-09 05:16:01,033 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.814e+01 7.575e+01 8.661e+01 9.840e+01 1.945e+02, threshold=1.732e+02, percent-clipped=1.0
2024-10-09 05:16:04,255 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=7898.666666666667, ans=0.12974999999999998
2024-10-09 05:16:09,712 INFO [train.py:1152] Epoch 4, batch 3150, loss[loss=0.3548, ctc_loss=0.3421, attn_decoder_loss=0.358, over 4777.00 frames. ], tot_loss[loss=0.2922, ctc_loss=0.2431, attn_decoder_loss=0.3045, over 966720.36 frames. ], batch size: 40, lr: 2.15e-02,
2024-10-09 05:16:13,127 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.67 vs. limit=13.4265
2024-10-09 05:16:14,989 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.83 vs. limit=6.9755
2024-10-09 05:16:18,447 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=7902.0, ans=0.03374166666666667
2024-10-09 05:16:30,209 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=7905.333333333333, ans=0.6233133333333334
2024-10-09 05:16:33,914 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=15.30 vs. limit=13.429
2024-10-09 05:16:41,528 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=7908.666666666667, ans=0.6231966666666666
2024-10-09 05:16:43,052 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=7908.666666666667, ans=0.03371388888888889
2024-10-09 05:16:48,903 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7908.666666666667, ans=0.22091333333333332
2024-10-09 05:17:06,095 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=7915.333333333333, ans=0.12896875000000002
2024-10-09 05:17:06,655 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.20 vs. limit=10.46825
2024-10-09 05:17:14,789 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=7915.333333333333, ans=0.6229633333333333
2024-10-09 05:17:18,182 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=13.14 vs. limit=10.46825
2024-10-09 05:17:21,826 INFO [train.py:1152] Epoch 4, batch 3200, loss[loss=0.2857, ctc_loss=0.229, attn_decoder_loss=0.2999, over 4743.00 frames. ], tot_loss[loss=0.2927, ctc_loss=0.2438, attn_decoder_loss=0.3049, over 967160.04 frames. ], batch size: 20, lr: 2.15e-02,
2024-10-09 05:18:24,768 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.02 vs. limit=13.449
2024-10-09 05:18:25,208 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.070e+01 7.738e+01 8.771e+01 9.403e+01 1.332e+02, threshold=1.754e+02, percent-clipped=0.0
2024-10-09 05:18:33,685 INFO [train.py:1152] Epoch 4, batch 3250, loss[loss=0.2773, ctc_loss=0.2119, attn_decoder_loss=0.2937, over 4858.00 frames. ], tot_loss[loss=0.2931, ctc_loss=0.2429, attn_decoder_loss=0.3057, over 967218.86 frames. ], batch size: 24, lr: 2.15e-02,
2024-10-09 05:18:43,981 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=7935.333333333333, ans=0.025
2024-10-09 05:19:03,923 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=7942.0, ans=0.12771875
2024-10-09 05:19:11,292 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7942.0, ans=0.22058
2024-10-09 05:19:12,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=7942.0, ans=0.00914304347826087
2024-10-09 05:19:29,720 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=7948.666666666667, ans=0.009141594202898552
2024-10-09 05:19:45,453 INFO [train.py:1152] Epoch 4, batch 3300, loss[loss=0.3252, ctc_loss=0.285, attn_decoder_loss=0.3353, over 4862.00 frames. ], tot_loss[loss=0.2929, ctc_loss=0.2426, attn_decoder_loss=0.3055, over 967776.76 frames. ], batch size: 43, lr: 2.15e-02,
2024-10-09 05:19:48,447 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7952.0, ans=0.22048
2024-10-09 05:19:55,505 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7952.0, ans=0.22048
2024-10-09 05:20:09,739 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7955.333333333333, ans=0.22044666666666668
2024-10-09 05:20:26,557 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.38 vs. limit=7.183466666666667
2024-10-09 05:20:27,764 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=10.16 vs. limit=10.48575
2024-10-09 05:20:32,878 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=7962.0, ans=0.62133
2024-10-09 05:20:42,588 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=7965.333333333333, ans=0.03347777777777779
2024-10-09 05:20:47,967 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.472e+01 7.982e+01 8.750e+01 9.614e+01 1.577e+02, threshold=1.750e+02, percent-clipped=0.0
2024-10-09 05:20:54,734 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=11.95 vs. limit=8.982666666666667
2024-10-09 05:20:55,981 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.60 vs. limit=13.4765
2024-10-09 05:20:56,567 INFO [train.py:1152] Epoch 4, batch 3350, loss[loss=0.2992, ctc_loss=0.2429, attn_decoder_loss=0.3133, over 4796.00 frames. ], tot_loss[loss=0.2938, ctc_loss=0.2438, attn_decoder_loss=0.3063, over 966947.08 frames. ], batch size: 40, lr: 2.15e-02,
2024-10-09 05:21:00,938 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=7968.666666666667, ans=0.12646875000000002
2024-10-09 05:21:09,465 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=7972.0, ans=0.03345
2024-10-09 05:21:26,397 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=7975.333333333333, ans=0.1262281666666667
2024-10-09 05:21:57,568 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.61 vs. limit=7.1928
2024-10-09 05:21:59,584 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=7982.0, ans=0.0
2024-10-09 05:22:00,630 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=3.64 vs. limit=5.0
2024-10-09 05:22:08,098 INFO [train.py:1152] Epoch 4, batch 3400, loss[loss=0.254, ctc_loss=0.1939, attn_decoder_loss=0.269, over 4959.00 frames. ], tot_loss[loss=0.2933, ctc_loss=0.2434, attn_decoder_loss=0.3058, over 966712.10 frames. ], batch size: 19, lr: 2.14e-02,
2024-10-09 05:22:13,336 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.28 vs. limit=13.489
2024-10-09 05:22:44,296 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=7992.0, ans=0.009132173913043478
2024-10-09 05:22:54,767 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.41 vs. limit=8.997666666666667
2024-10-09 05:23:10,297 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_False/checkpoint-24000.pt
2024-10-09 05:23:12,752 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.185e+01 7.537e+01 8.512e+01 9.415e+01 1.476e+02, threshold=1.702e+02, percent-clipped=0.0
2024-10-09 05:23:20,677 INFO [train.py:1152] Epoch 4, batch 3450, loss[loss=0.3276, ctc_loss=0.3074, attn_decoder_loss=0.3326, over 4855.00 frames. ], tot_loss[loss=0.2938, ctc_loss=0.2446, attn_decoder_loss=0.306, over 967019.24 frames. ], batch size: 43, lr: 2.14e-02,
2024-10-09 05:23:23,619 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=8002.0, ans=0.125
2024-10-09 05:23:41,610 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=8005.333333333333, ans=0.6198133333333333
2024-10-09 05:23:50,153 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=8008.666666666667, ans=0.21991333333333332
2024-10-09 05:24:02,404 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.95 vs. limit=7.2048000000000005
2024-10-09 05:24:17,415 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=8015.333333333333, ans=0.03326944444444445
2024-10-09 05:24:18,795 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=8015.333333333333, ans=0.009127101449275362
2024-10-09 05:24:25,342 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.77 vs. limit=13.5115
2024-10-09 05:24:26,032 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=8015.333333333333, ans=0.2198466666666667
2024-10-09 05:24:26,679 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.04 vs. limit=10.505749999999999
2024-10-09 05:24:31,668 INFO [train.py:1152] Epoch 4, batch 3500, loss[loss=0.2723, ctc_loss=0.2253, attn_decoder_loss=0.284, over 4883.00 frames. ], tot_loss[loss=0.2927, ctc_loss=0.243, attn_decoder_loss=0.3051, over 967279.20 frames. ], batch size: 19, lr: 2.14e-02,
2024-10-09 05:24:38,346 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.52 vs. limit=10.507
2024-10-09 05:24:50,394 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=8022.0, ans=0.21977999999999998
2024-10-09 05:25:09,085 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=8025.333333333333, ans=0.125
2024-10-09 05:25:10,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8025.333333333333, ans=0.21974666666666667
2024-10-09 05:25:15,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=8028.666666666667, ans=0.125
2024-10-09 05:25:30,855 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=8032.0, ans=0.009123478260869565
2024-10-09 05:25:35,032 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.703e+01 7.438e+01 8.564e+01 9.598e+01 2.198e+02, threshold=1.713e+02, percent-clipped=1.0
2024-10-09 05:25:43,675 INFO [train.py:1152] Epoch 4, batch 3550, loss[loss=0.3232, ctc_loss=0.2698, attn_decoder_loss=0.3365, over 4778.00 frames. ], tot_loss[loss=0.2923, ctc_loss=0.2426, attn_decoder_loss=0.3048, over 967251.70 frames. ], batch size: 29, lr: 2.14e-02,
2024-10-09 05:25:57,553 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.75 vs. limit=13.529
2024-10-09 05:26:46,928 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=8048.666666666667, ans=0.12554983333333336
2024-10-09 05:26:50,049 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=8048.666666666667, ans=0.125
2024-10-09 05:26:55,541 INFO [train.py:1152] Epoch 4, batch 3600, loss[loss=0.2691, ctc_loss=0.2011, attn_decoder_loss=0.2861, over 4927.00 frames. ], tot_loss[loss=0.2924, ctc_loss=0.2428, attn_decoder_loss=0.3048, over 967421.45 frames. ], batch size: 20, lr: 2.14e-02,
2024-10-09 05:27:14,663 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=8055.333333333333, ans=0.0
2024-10-09 05:27:34,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=8058.666666666667, ans=0.03308888888888889
2024-10-09 05:27:36,161 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=8058.666666666667, ans=0.03308888888888889
2024-10-09 05:27:38,038 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=6.93 vs. limit=10.52325
2024-10-09 05:27:42,488 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.09 vs. limit=13.5465
2024-10-09 05:27:49,829 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.98 vs. limit=10.52325
2024-10-09 05:27:54,074 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten.whitening_limit, batch_count=8065.333333333333, ans=13.549
2024-10-09 05:27:59,348 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.141e+01 7.494e+01 8.510e+01 9.552e+01 1.120e+03, threshold=1.702e+02, percent-clipped=2.0
2024-10-09 05:28:03,834 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=8065.333333333333, ans=0.03306111111111111
2024-10-09 05:28:07,874 INFO [train.py:1152] Epoch 4, batch 3650, loss[loss=0.3318, ctc_loss=0.3082, attn_decoder_loss=0.3377, over 4844.00 frames. ], tot_loss[loss=0.2913, ctc_loss=0.2412, attn_decoder_loss=0.3038, over 967860.66 frames. ], batch size: 31, lr: 2.13e-02,
2024-10-09 05:28:24,012 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=8072.0, ans=0.125
2024-10-09 05:28:29,694 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=8072.0, ans=0.03303333333333333
2024-10-09 05:28:49,563 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=8078.666666666667, ans=0.125
2024-10-09 05:28:55,675 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.84 vs. limit=7.019666666666667
2024-10-09 05:29:10,126 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.55 vs. limit=9.041
2024-10-09 05:29:19,026 INFO [train.py:1152] Epoch 4, batch 3700, loss[loss=0.2945, ctc_loss=0.2516, attn_decoder_loss=0.3052, over 4855.00 frames. ], tot_loss[loss=0.2897, ctc_loss=0.2387, attn_decoder_loss=0.3024, over 967172.94 frames. ], batch size: 24, lr: 2.13e-02,
2024-10-09 05:29:27,686 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=8085.333333333333, ans=0.0
2024-10-09 05:29:30,012 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.05 vs. limit=4.2128
2024-10-09 05:29:42,885 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.27 vs. limit=7.235466666666667
2024-10-09 05:29:43,635 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=8088.666666666667, ans=0.6168966666666666
2024-10-09 05:29:43,636 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=8088.666666666667, ans=0.025
2024-10-09 05:29:46,725 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 05:29:50,755 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=8092.0, ans=0.009110434782608695
2024-10-09 05:29:56,542 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=8092.0, ans=0.6167800000000001
2024-10-09 05:30:16,633 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=8098.666666666667, ans=0.21901333333333334
2024-10-09 05:30:16,738 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=8098.666666666667, ans=0.04949747468305833
2024-10-09 05:30:22,172 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.320e+01 7.712e+01 8.505e+01 9.600e+01 1.296e+02, threshold=1.701e+02, percent-clipped=0.0
2024-10-09 05:30:28,850 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.55 vs. limit=7.239466666666667
2024-10-09 05:30:30,815 INFO [train.py:1152] Epoch 4, batch 3750, loss[loss=0.2442, ctc_loss=0.1827, attn_decoder_loss=0.2596, over 4959.00 frames. ], tot_loss[loss=0.2889, ctc_loss=0.2373, attn_decoder_loss=0.3018, over 967701.72 frames. ], batch size: 19, lr: 2.13e-02,
2024-10-09 05:30:30,958 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=8102.0, ans=0.07
2024-10-09 05:31:04,978 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=10.77 vs. limit=9.054333333333334
2024-10-09 05:31:06,885 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=8108.666666666667, ans=0.025
2024-10-09 05:31:20,996 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=10.24 vs. limit=10.542
2024-10-09 05:31:35,884 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=8115.333333333333, ans=0.6159633333333334
2024-10-09 05:31:35,886 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=8115.333333333333, ans=0.2188466666666667
2024-10-09 05:31:37,296 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=8115.333333333333, ans=10.0
2024-10-09 05:31:42,859 INFO [train.py:1152] Epoch 4, batch 3800, loss[loss=0.2952, ctc_loss=0.244, attn_decoder_loss=0.308, over 4728.00 frames. ], tot_loss[loss=0.2881, ctc_loss=0.2367, attn_decoder_loss=0.3009, over 967518.54 frames. ], batch size: 26, lr: 2.13e-02,
2024-10-09 05:32:01,678 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=8122.0, ans=0.032825
2024-10-09 05:32:13,445 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=8125.333333333333, ans=0.125
2024-10-09 05:32:27,491 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.82 vs. limit=4.2193000000000005
2024-10-09 05:32:29,928 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.43 vs. limit=13.596499999999999
2024-10-09 05:32:41,319 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.72 vs. limit=10.5495
2024-10-09 05:32:43,644 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=8132.0, ans=0.125
2024-10-09 05:32:46,585 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.313e+01 7.438e+01 8.249e+01 9.175e+01 1.262e+02, threshold=1.650e+02, percent-clipped=0.0
2024-10-09 05:32:55,109 INFO [train.py:1152] Epoch 4, batch 3850, loss[loss=0.3539, ctc_loss=0.3324, attn_decoder_loss=0.3593, over 4806.00 frames. ], tot_loss[loss=0.2876, ctc_loss=0.236, attn_decoder_loss=0.3005, over 967434.96 frames. ], batch size: 38, lr: 2.13e-02,
2024-10-09 05:33:40,512 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.28 vs. limit=4.2218
2024-10-09 05:34:00,522 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.52 vs. limit=4.2223
2024-10-09 05:34:06,886 INFO [train.py:1152] Epoch 4, batch 3900, loss[loss=0.2658, ctc_loss=0.2015, attn_decoder_loss=0.2819, over 4728.00 frames. ], tot_loss[loss=0.2884, ctc_loss=0.2378, attn_decoder_loss=0.3011, over 966924.61 frames. ], batch size: 26, lr: 2.12e-02,
2024-10-09 05:34:15,569 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=8152.0, ans=0.6146800000000001
2024-10-09 05:34:53,781 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.93 vs. limit=7.2648
2024-10-09 05:35:07,213 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.86 vs. limit=13.623999999999999
2024-10-09 05:35:07,997 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=8165.333333333333, ans=0.125
2024-10-09 05:35:10,636 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.540e+01 7.728e+01 8.635e+01 9.499e+01 1.247e+02, threshold=1.727e+02, percent-clipped=0.0
2024-10-09 05:35:19,321 INFO [train.py:1152] Epoch 4, batch 3950, loss[loss=0.2963, ctc_loss=0.244, attn_decoder_loss=0.3094, over 4827.00 frames. ], tot_loss[loss=0.2882, ctc_loss=0.2368, attn_decoder_loss=0.3011, over 967008.77 frames. ], batch size: 36, lr: 2.12e-02,
2024-10-09 05:35:25,828 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.78 vs. limit=10.56325
2024-10-09 05:35:32,398 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=8172.0, ans=0.03261666666666667
2024-10-09 05:35:36,920 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8172.0, ans=0.21828
2024-10-09 05:35:58,377 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=8175.333333333333, ans=0.6138633333333334
2024-10-09 05:36:04,706 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.81 vs. limit=13.634
2024-10-09 05:36:17,722 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=8182.0, ans=0.6136300000000001
2024-10-09 05:36:22,886 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=8182.0, ans=0.6136300000000001
2024-10-09 05:36:31,162 INFO [train.py:1152] Epoch 4, batch 4000, loss[loss=0.2612, ctc_loss=0.1943, attn_decoder_loss=0.2779, over 4815.00 frames. ], tot_loss[loss=0.2887, ctc_loss=0.2375, attn_decoder_loss=0.3016, over 966962.57 frames. ], batch size: 19, lr: 2.12e-02,
2024-10-09 05:36:31,328 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=8185.333333333333, ans=0.009090144927536232
2024-10-09 05:36:33,906 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=8185.333333333333, ans=0.125
2024-10-09 05:36:51,151 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=8188.666666666667, ans=0.009089420289855073
2024-10-09 05:37:06,886 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=8192.0, ans=0.125
2024-10-09 05:37:28,478 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8198.666666666666, ans=0.21801333333333334
2024-10-09 05:37:31,263 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=8198.666666666666, ans=0.025
2024-10-09 05:37:34,201 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.317e+01 7.903e+01 8.704e+01 9.424e+01 1.397e+02, threshold=1.741e+02, percent-clipped=0.0
2024-10-09 05:37:42,782 INFO [train.py:1152] Epoch 4, batch 4050, loss[loss=0.3142, ctc_loss=0.2729, attn_decoder_loss=0.3246, over 4762.00 frames. ], tot_loss[loss=0.2891, ctc_loss=0.2383, attn_decoder_loss=0.3018, over 967359.95 frames. ], batch size: 53, lr: 2.12e-02,
2024-10-09 05:37:43,341 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.45 vs. limit=13.6515
2024-10-09 05:37:57,159 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=8205.333333333334, ans=0.03247777777777777
2024-10-09 05:38:12,980 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=8208.666666666666, ans=0.03246388888888889
2024-10-09 05:38:16,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.75 vs. limit=7.2834666666666665
2024-10-09 05:38:52,081 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=18.10 vs. limit=10.58075
2024-10-09 05:38:54,346 INFO [train.py:1152] Epoch 4, batch 4100, loss[loss=0.2936, ctc_loss=0.2352, attn_decoder_loss=0.3083, over 4863.00 frames. ], tot_loss[loss=0.2884, ctc_loss=0.2372, attn_decoder_loss=0.3012, over 966842.87 frames. ], batch size: 31, lr: 2.12e-02,
2024-10-09 05:39:00,293 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=8218.666666666666, ans=0.125
2024-10-09 05:39:03,515 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2.whitening_limit, batch_count=8218.666666666666, ans=9.109333333333332
2024-10-09 05:39:16,145 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=8222.0, ans=0.07
2024-10-09 05:39:17,601 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=8222.0, ans=0.03240833333333333
2024-10-09 05:39:21,947 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=2.620e-02
2024-10-09 05:39:36,532 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 05:39:53,590 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=8232.0, ans=0.03236666666666667
2024-10-09 05:39:57,684 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.479e+01 7.654e+01 8.714e+01 9.829e+01 1.570e+02, threshold=1.743e+02, percent-clipped=0.0
2024-10-09 05:39:57,948 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=8232.0, ans=0.125
2024-10-09 05:39:59,281 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer1.prob, batch_count=8232.0, ans=0.125
2024-10-09 05:40:03,594 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=8232.0, ans=0.125
2024-10-09 05:40:06,392 INFO [train.py:1152] Epoch 4, batch 4150, loss[loss=0.3114, ctc_loss=0.2569, attn_decoder_loss=0.3251, over 4737.00 frames. ], tot_loss[loss=0.2887, ctc_loss=0.2382, attn_decoder_loss=0.3013, over 967007.94 frames. ], batch size: 20, lr: 2.11e-02,
2024-10-09 05:40:18,100 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=8235.333333333334, ans=0.125
2024-10-09 05:40:25,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=8238.666666666666, ans=0.03233888888888889
2024-10-09 05:40:28,875 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.54 vs. limit=7.295466666666666
2024-10-09 05:40:37,571 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.95 vs. limit=7.0605
2024-10-09 05:41:18,074 INFO [train.py:1152] Epoch 4, batch 4200, loss[loss=0.3083, ctc_loss=0.2535, attn_decoder_loss=0.322, over 4845.00 frames. ], tot_loss[loss=0.2886, ctc_loss=0.237, attn_decoder_loss=0.3015, over 967112.54 frames. ], batch size: 31, lr: 2.11e-02,
2024-10-09 05:41:29,720 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=8252.0, ans=0.8325199999999999
2024-10-09 05:41:42,607 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=8255.333333333334, ans=0.0
2024-10-09 05:41:44,035 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8255.333333333334, ans=0.21744666666666668
2024-10-09 05:41:47,778 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.96 vs. limit=4.2387999999999995
2024-10-09 05:42:21,518 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.262e+01 7.404e+01 8.265e+01 9.380e+01 1.355e+02, threshold=1.653e+02, percent-clipped=0.0
2024-10-09 05:42:30,110 INFO [train.py:1152] Epoch 4, batch 4250, loss[loss=0.2645, ctc_loss=0.1923, attn_decoder_loss=0.2825, over 4751.00 frames. ], tot_loss[loss=0.2869, ctc_loss=0.2343, attn_decoder_loss=0.3001, over 966969.17 frames. ], batch size: 19, lr: 2.11e-02,
2024-10-09 05:42:35,117 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten.whitening_limit, batch_count=8268.666666666666, ans=13.7015
2024-10-09 05:42:40,416 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=8268.666666666666, ans=0.125
2024-10-09 05:43:09,198 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=8275.333333333334, ans=0.03218611111111111
2024-10-09 05:43:20,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=8278.666666666666, ans=0.125
2024-10-09 05:43:22,526 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.58 vs. limit=10.6045
2024-10-09 05:43:41,898 INFO [train.py:1152] Epoch 4, batch 4300, loss[loss=0.3157, ctc_loss=0.2907, attn_decoder_loss=0.322, over 4841.00 frames. ], tot_loss[loss=0.2879, ctc_loss=0.2365, attn_decoder_loss=0.3008, over 967331.43 frames. ], batch size: 21, lr: 2.11e-02,
2024-10-09 05:44:10,401 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.51 vs. limit=10.6095
2024-10-09 05:44:19,830 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=8292.0, ans=0.125
2024-10-09 05:44:35,965 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.16 vs. limit=7.318133333333334
2024-10-09 05:44:38,286 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=5.087e-03
2024-10-09 05:44:39,631 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=8298.666666666666, ans=0.125
2024-10-09 05:44:41,127 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=8298.666666666666, ans=0.21701333333333334
2024-10-09 05:44:43,696 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.315e+01 7.734e+01 8.781e+01 1.005e+02 1.587e+02, threshold=1.756e+02, percent-clipped=0.0
2024-10-09 05:44:52,414 INFO [train.py:1152] Epoch 4, batch 4350, loss[loss=0.2708, ctc_loss=0.1965, attn_decoder_loss=0.2894, over 4841.00 frames. ], tot_loss[loss=0.2885, ctc_loss=0.2367, attn_decoder_loss=0.3014, over 966255.84 frames. ], batch size: 21, lr: 2.11e-02,
2024-10-09 05:45:05,487 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=8305.333333333334, ans=0.6093133333333334
2024-10-09 05:45:09,627 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=8305.333333333334, ans=0.125
2024-10-09 05:45:18,199 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=8305.333333333334, ans=0.009064057971014493
2024-10-09 05:45:37,047 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=8312.0, ans=0.125
2024-10-09 05:45:44,269 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=8312.0, ans=0.009062608695652175
2024-10-09 05:46:01,700 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=8315.333333333334, ans=0.125
2024-10-09 05:46:04,392 INFO [train.py:1152] Epoch 4, batch 4400, loss[loss=0.294, ctc_loss=0.2684, attn_decoder_loss=0.3004, over 4755.00 frames. ], tot_loss[loss=0.2901, ctc_loss=0.238, attn_decoder_loss=0.3031, over 965912.00 frames. ], batch size: 26, lr: 2.10e-02,
2024-10-09 05:46:12,988 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=8318.666666666666, ans=0.032005555555555565
2024-10-09 05:46:27,489 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=8322.0, ans=0.60873
2024-10-09 05:47:06,619 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=8332.0, ans=0.125
2024-10-09 05:47:07,753 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.563e+01 7.956e+01 9.005e+01 1.008e+02 3.452e+02, threshold=1.801e+02, percent-clipped=2.0
2024-10-09 05:47:16,338 INFO [train.py:1152] Epoch 4, batch 4450, loss[loss=0.2617, ctc_loss=0.1972, attn_decoder_loss=0.2778, over 4883.00 frames. ], tot_loss[loss=0.2907, ctc_loss=0.2392, attn_decoder_loss=0.3035, over 966257.97 frames. ], batch size: 19, lr: 2.10e-02,
2024-10-09 05:47:49,179 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=8342.0, ans=0.03190833333333333
2024-10-09 05:48:27,854 INFO [train.py:1152] Epoch 4, batch 4500, loss[loss=0.2913, ctc_loss=0.2478, attn_decoder_loss=0.3022, over 4864.00 frames. ], tot_loss[loss=0.29, ctc_loss=0.2381, attn_decoder_loss=0.303, over 966250.51 frames. ], batch size: 28, lr: 2.10e-02,
2024-10-09 05:48:28,055 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=8352.0, ans=0.125
2024-10-09 05:48:35,806 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.45 vs. limit=7.3408
2024-10-09 05:48:39,177 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=14.00 vs. limit=13.764
2024-10-09 05:48:42,513 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8355.333333333334, ans=0.21644666666666668
2024-10-09 05:48:51,740 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.64 vs. limit=10.63325
2024-10-09 05:48:58,600 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=8358.666666666666, ans=0.6074466666666667
2024-10-09 05:49:04,876 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.25 vs. limit=9.179333333333332
2024-10-09 05:49:07,108 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=8358.666666666666, ans=0.125
2024-10-09 05:49:11,393 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=8362.0, ans=0.125
2024-10-09 05:49:31,360 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.293e+01 7.955e+01 8.968e+01 1.054e+02 2.343e+02, threshold=1.794e+02, percent-clipped=1.0
2024-10-09 05:49:33,606 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.94 vs. limit=9.182666666666666
2024-10-09 05:49:39,974 INFO [train.py:1152] Epoch 4, batch 4550, loss[loss=0.2984, ctc_loss=0.2556, attn_decoder_loss=0.3091, over 4866.00 frames. ], tot_loss[loss=0.2898, ctc_loss=0.2378, attn_decoder_loss=0.3028, over 966090.00 frames. ], batch size: 20, lr: 2.10e-02,
2024-10-09 05:49:52,320 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=11.18 vs. limit=10.63825
2024-10-09 05:49:54,106 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.86 vs. limit=10.6395
2024-10-09 05:50:09,790 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1.whitening_limit, batch_count=8375.333333333334, ans=7.093833333333333
2024-10-09 05:50:17,705 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=8375.333333333334, ans=0.125
2024-10-09 05:50:19,779 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.57 vs. limit=7.350133333333334
2024-10-09 05:50:32,958 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.56 vs. limit=13.783999999999999
2024-10-09 05:50:37,934 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=8382.0, ans=0.03174166666666667
2024-10-09 05:50:45,072 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=8382.0, ans=0.125
2024-10-09 05:50:49,236 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer_ff3.min_abs, batch_count=8382.0, ans=0.2
2024-10-09 05:50:51,874 INFO [train.py:1152] Epoch 4, batch 4600, loss[loss=0.3388, ctc_loss=0.3076, attn_decoder_loss=0.3466, over 4764.00 frames. ], tot_loss[loss=0.2889, ctc_loss=0.2361, attn_decoder_loss=0.3021, over 966420.07 frames. ], batch size: 45, lr: 2.10e-02,
2024-10-09 05:51:17,783 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=8388.666666666666, ans=0.03171388888888889
2024-10-09 05:51:19,288 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=8392.0, ans=0.125
2024-10-09 05:51:25,536 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=8.32 vs. limit=7.3568
2024-10-09 05:51:27,650 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=8392.0, ans=0.025
2024-10-09 05:51:36,237 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=8395.333333333334, ans=0.125
2024-10-09 05:51:49,160 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=8398.666666666666, ans=0.125
2024-10-09 05:51:54,974 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.247e+01 7.642e+01 8.638e+01 9.754e+01 1.653e+02, threshold=1.728e+02, percent-clipped=0.0
2024-10-09 05:51:57,882 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=8398.666666666666, ans=0.6060466666666667
2024-10-09 05:52:02,255 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=8402.0, ans=0.125
2024-10-09 05:52:03,559 INFO [train.py:1152] Epoch 4, batch 4650, loss[loss=0.3106, ctc_loss=0.2691, attn_decoder_loss=0.3209, over 4837.00 frames. ], tot_loss[loss=0.2897, ctc_loss=0.2374, attn_decoder_loss=0.3027, over 965661.48 frames. ], batch size: 36, lr: 2.09e-02,
2024-10-09 05:52:13,878 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=8402.0, ans=0.009043043478260869
2024-10-09 05:52:21,584 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=6.72 vs. limit=7.362133333333333
2024-10-09 05:52:26,684 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=8405.333333333334, ans=0.125
2024-10-09 05:52:52,497 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=8412.0, ans=0.03161666666666667
2024-10-09 05:53:08,415 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=8415.333333333334, ans=0.125
2024-10-09 05:53:08,899 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.38 vs. limit=7.366133333333334
2024-10-09 05:53:15,328 INFO [train.py:1152] Epoch 4, batch 4700, loss[loss=0.2682, ctc_loss=0.2045, attn_decoder_loss=0.2841, over 4940.00 frames. ], tot_loss[loss=0.289, ctc_loss=0.2369, attn_decoder_loss=0.302, over 965599.57 frames. ], batch size: 19, lr: 2.09e-02,
2024-10-09 05:54:08,271 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.75 vs. limit=13.8215
2024-10-09 05:54:17,483 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=8432.0, ans=0.125
2024-10-09 05:54:18,664 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.749e+01 7.811e+01 8.788e+01 9.650e+01 2.259e+02, threshold=1.758e+02, percent-clipped=1.0
2024-10-09 05:54:20,306 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=8432.0, ans=0.025
2024-10-09 05:54:24,622 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=8432.0, ans=0.125
2024-10-09 05:54:27,209 INFO [train.py:1152] Epoch 4, batch 4750, loss[loss=0.2982, ctc_loss=0.2474, attn_decoder_loss=0.3109, over 4734.00 frames. ], tot_loss[loss=0.2891, ctc_loss=0.237, attn_decoder_loss=0.3021, over 965540.25 frames. ], batch size: 45, lr: 2.09e-02,
2024-10-09 05:54:27,347 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=8435.333333333334, ans=0.125
2024-10-09 05:55:01,031 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.33 vs. limit=13.8315
2024-10-09 05:55:04,705 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=8442.0, ans=0.125
2024-10-09 05:55:36,057 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 05:55:38,798 INFO [train.py:1152] Epoch 4, batch 4800, loss[loss=0.2814, ctc_loss=0.202, attn_decoder_loss=0.3012, over 4876.00 frames. ], tot_loss[loss=0.2879, ctc_loss=0.2347, attn_decoder_loss=0.3012, over 965952.03 frames. ], batch size: 22, lr: 2.09e-02,
2024-10-09 05:55:48,520 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=12.93 vs. limit=9.225999999999999
2024-10-09 05:55:54,891 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=8455.333333333334, ans=0.125
2024-10-09 05:56:11,818 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=6.40 vs. limit=10.672
2024-10-09 05:56:17,496 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.77 vs. limit=13.844
2024-10-09 05:56:23,925 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=8462.0, ans=0.6038300000000001
2024-10-09 05:56:23,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=8462.0, ans=0.00903
2024-10-09 05:56:33,957 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=8462.0, ans=0.07
2024-10-09 05:56:33,984 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=8462.0, ans=0.6038300000000001
2024-10-09 05:56:41,502 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=8465.333333333334, ans=0.125
2024-10-09 05:56:42,701 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.355e+01 7.348e+01 8.320e+01 9.354e+01 1.515e+02, threshold=1.664e+02, percent-clipped=0.0
2024-10-09 05:56:44,160 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=8465.333333333334, ans=0.21534666666666666
2024-10-09 05:56:51,226 INFO [train.py:1152] Epoch 4, batch 4850, loss[loss=0.306, ctc_loss=0.2412, attn_decoder_loss=0.3222, over 4861.00 frames. ], tot_loss[loss=0.2878, ctc_loss=0.2348, attn_decoder_loss=0.3011, over 966767.74 frames. ], batch size: 28, lr: 2.09e-02,
2024-10-09 05:56:54,264 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=8468.666666666666, ans=0.125
2024-10-09 05:57:07,195 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=8472.0, ans=10.0
2024-10-09 05:57:11,497 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=8472.0, ans=0.60348
2024-10-09 05:57:12,435 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.39 vs. limit=4.2707999999999995
2024-10-09 05:57:30,872 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module1.whiten, num_groups=1, num_channels=192, metric=4.71 vs. limit=10.67825
2024-10-09 05:57:36,330 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.93 vs. limit=13.858999999999998
2024-10-09 05:57:51,329 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=8482.0, ans=0.60313
2024-10-09 05:58:02,770 INFO [train.py:1152] Epoch 4, batch 4900, loss[loss=0.2801, ctc_loss=0.2381, attn_decoder_loss=0.2906, over 4836.00 frames. ], tot_loss[loss=0.286, ctc_loss=0.2319, attn_decoder_loss=0.2995, over 967276.54 frames. ], batch size: 21, lr: 2.08e-02,
2024-10-09 05:58:15,625 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=8488.666666666666, ans=0.031297222222222226
2024-10-09 05:58:32,224 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=31.95 vs. limit=10.6845
2024-10-09 05:58:39,398 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.66 vs. limit=10.6845
2024-10-09 05:58:42,316 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=14.57 vs. limit=10.6845
2024-10-09 05:58:46,729 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=10.31 vs. limit=10.68575
2024-10-09 05:59:05,963 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.666e+01 7.828e+01 8.467e+01 9.393e+01 1.418e+02, threshold=1.693e+02, percent-clipped=0.0
2024-10-09 05:59:14,562 INFO [train.py:1152] Epoch 4, batch 4950, loss[loss=0.3292, ctc_loss=0.3147, attn_decoder_loss=0.3328, over 4770.00 frames. ], tot_loss[loss=0.288, ctc_loss=0.2349, attn_decoder_loss=0.3012, over 966984.76 frames. ], batch size: 53, lr: 2.08e-02,
2024-10-09 05:59:29,282 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=8505.333333333334, ans=0.21494666666666667
2024-10-09 05:59:36,422 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=8505.333333333334, ans=0.031227777777777778
2024-10-09 06:00:00,967 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=8512.0, ans=0.031200000000000002
2024-10-09 06:00:05,264 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=8512.0, ans=0.0
2024-10-09 06:00:08,094 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8512.0, ans=0.21488000000000002
2024-10-09 06:00:26,810 INFO [train.py:1152] Epoch 4, batch 5000, loss[loss=0.29, ctc_loss=0.2262, attn_decoder_loss=0.306, over 4748.00 frames. ], tot_loss[loss=0.2858, ctc_loss=0.232, attn_decoder_loss=0.2993, over 967819.16 frames. ], batch size: 29, lr: 2.08e-02,
2024-10-09 06:00:52,007 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.83 vs. limit=10.69575
2024-10-09 06:00:53,447 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.04 vs. limit=13.8915
2024-10-09 06:00:57,253 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=8525.333333333334, ans=0.00901623188405797
2024-10-09 06:01:12,051 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.65 vs. limit=10.69825
2024-10-09 06:01:30,465 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.828e+01 7.261e+01 8.291e+01 9.208e+01 1.608e+02, threshold=1.658e+02, percent-clipped=0.0
2024-10-09 06:01:39,054 INFO [train.py:1152] Epoch 4, batch 5050, loss[loss=0.2957, ctc_loss=0.25, attn_decoder_loss=0.3072, over 4852.00 frames. ], tot_loss[loss=0.284, ctc_loss=0.23, attn_decoder_loss=0.2975, over 968708.17 frames. ], batch size: 19, lr: 2.08e-02,
2024-10-09 06:02:00,662 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8538.666666666666, ans=0.21461333333333332
2024-10-09 06:02:12,182 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=8542.0, ans=0.031075000000000002
2024-10-09 06:02:19,404 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=8542.0, ans=0.031075000000000002
2024-10-09 06:02:25,981 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.53 vs. limit=9.272666666666666
2024-10-09 06:02:28,827 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=10.91 vs. limit=10.7045
2024-10-09 06:02:50,875 INFO [train.py:1152] Epoch 4, batch 5100, loss[loss=0.265, ctc_loss=0.1829, attn_decoder_loss=0.2855, over 4816.00 frames. ], tot_loss[loss=0.2867, ctc_loss=0.234, attn_decoder_loss=0.2999, over 967981.24 frames. ], batch size: 19, lr: 2.08e-02,
2024-10-09 06:03:06,912 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=8555.333333333334, ans=0.125
2024-10-09 06:03:15,696 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=8555.333333333334, ans=0.0
2024-10-09 06:03:18,554 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=8558.666666666666, ans=0.125
2024-10-09 06:03:24,304 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8558.666666666666, ans=0.21441333333333334
2024-10-09 06:03:36,193 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.41 vs. limit=10.71075
2024-10-09 06:03:37,037 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=8562.0, ans=0.009008260869565217
2024-10-09 06:03:50,227 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.30 vs. limit=13.924
2024-10-09 06:03:51,615 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.09 vs. limit=13.924
2024-10-09 06:03:53,790 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.555e+01 8.011e+01 8.855e+01 9.758e+01 1.765e+02, threshold=1.771e+02, percent-clipped=1.0
2024-10-09 06:03:58,371 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=8565.333333333334, ans=0.125
2024-10-09 06:04:02,485 INFO [train.py:1152] Epoch 4, batch 5150, loss[loss=0.2576, ctc_loss=0.2049, attn_decoder_loss=0.2708, over 4813.00 frames. ], tot_loss[loss=0.2879, ctc_loss=0.2358, attn_decoder_loss=0.3009, over 968103.64 frames. ], batch size: 36, lr: 2.08e-02,
2024-10-09 06:04:17,027 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=8572.0, ans=0.025
2024-10-09 06:04:21,237 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=8572.0, ans=0.025
2024-10-09 06:04:24,082 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8572.0, ans=0.21428
2024-10-09 06:04:25,422 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=8572.0, ans=0.125
2024-10-09 06:04:39,781 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 06:05:14,242 INFO [train.py:1152] Epoch 4, batch 5200, loss[loss=0.2691, ctc_loss=0.2009, attn_decoder_loss=0.2861, over 4795.00 frames. ], tot_loss[loss=0.2872, ctc_loss=0.2348, attn_decoder_loss=0.3003, over 967692.32 frames. ], batch size: 29, lr: 2.07e-02,
2024-10-09 06:05:15,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=8585.333333333334, ans=0.0
2024-10-09 06:05:31,736 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=8588.666666666666, ans=0.125
2024-10-09 06:05:33,179 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=8588.666666666666, ans=0.07
2024-10-09 06:05:47,570 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer_na.min_abs, batch_count=8592.0, ans=0.02
2024-10-09 06:05:51,968 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=8592.0, ans=0.125
2024-10-09 06:06:06,282 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=8595.333333333334, ans=0.125
2024-10-09 06:06:17,757 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.288e+01 7.139e+01 8.624e+01 9.483e+01 2.360e+02, threshold=1.725e+02, percent-clipped=1.0
2024-10-09 06:06:26,324 INFO [train.py:1152] Epoch 4, batch 5250, loss[loss=0.2687, ctc_loss=0.1939, attn_decoder_loss=0.2874, over 4859.00 frames. ], tot_loss[loss=0.2859, ctc_loss=0.2334, attn_decoder_loss=0.299, over 967721.45 frames. ], batch size: 20, lr: 2.07e-02,
2024-10-09 06:06:36,014 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.17 vs. limit=9.301
2024-10-09 06:06:39,430 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=8605.333333333334, ans=0.21394666666666667
2024-10-09 06:06:41,704 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=5.85 vs. limit=10.727
2024-10-09 06:07:05,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=8608.666666666666, ans=0.5986966666666667
2024-10-09 06:07:15,648 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=12.31 vs. limit=9.306000000000001
2024-10-09 06:07:25,830 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=16.41 vs. limit=13.961500000000001
2024-10-09 06:07:35,674 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=12.88 vs. limit=10.73075
2024-10-09 06:07:36,584 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=8618.666666666666, ans=0.008995942028985508
2024-10-09 06:07:37,811 INFO [train.py:1152] Epoch 4, batch 5300, loss[loss=0.3298, ctc_loss=0.2851, attn_decoder_loss=0.341, over 4838.00 frames. ], tot_loss[loss=0.2854, ctc_loss=0.2328, attn_decoder_loss=0.2985, over 967801.02 frames. ], batch size: 38, lr: 2.07e-02,
2024-10-09 06:07:55,015 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=8622.0, ans=0.030741666666666667
2024-10-09 06:07:57,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=8622.0, ans=0.025
2024-10-09 06:08:16,519 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=8625.333333333334, ans=0.125
2024-10-09 06:08:29,975 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.32 vs. limit=13.971499999999999
2024-10-09 06:08:40,574 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.099e+01 7.349e+01 8.281e+01 8.913e+01 1.440e+02, threshold=1.656e+02, percent-clipped=0.0
2024-10-09 06:08:47,366 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.43 vs. limit=10.737
2024-10-09 06:08:49,314 INFO [train.py:1152] Epoch 4, batch 5350, loss[loss=0.2913, ctc_loss=0.2305, attn_decoder_loss=0.3065, over 4978.00 frames. ], tot_loss[loss=0.2848, ctc_loss=0.2312, attn_decoder_loss=0.2982, over 967204.21 frames. ], batch size: 19, lr: 2.07e-02,
2024-10-09 06:09:04,054 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8638.666666666666, ans=0.21361333333333332
2024-10-09 06:09:30,255 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.09 vs. limit=7.1605
2024-10-09 06:09:38,314 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=8645.333333333334, ans=0.025
2024-10-09 06:09:39,754 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=8645.333333333334, ans=0.125
2024-10-09 06:09:42,772 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=8645.333333333334, ans=10.0
2024-10-09 06:09:51,978 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.24 vs. limit=13.9865
2024-10-09 06:10:01,301 INFO [train.py:1152] Epoch 4, batch 5400, loss[loss=0.3433, ctc_loss=0.3087, attn_decoder_loss=0.3519, over 4784.00 frames. ], tot_loss[loss=0.2865, ctc_loss=0.2328, attn_decoder_loss=0.2999, over 966471.34 frames. ], batch size: 49, lr: 2.07e-02,
2024-10-09 06:10:01,416 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=8652.0, ans=0.125
2024-10-09 06:10:15,739 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=8655.333333333334, ans=0.125
2024-10-09 06:10:35,206 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=3.34 vs. limit=10.747
2024-10-09 06:10:41,604 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=8658.666666666666, ans=0.0
2024-10-09 06:10:43,716 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.88 vs. limit=13.996500000000001
2024-10-09 06:11:04,672 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.122e+01 7.676e+01 8.512e+01 9.251e+01 1.474e+02, threshold=1.702e+02, percent-clipped=0.0
2024-10-09 06:11:10,528 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=8665.333333333334, ans=0.008985797101449276
2024-10-09 06:11:13,251 INFO [train.py:1152] Epoch 4, batch 5450, loss[loss=0.2657, ctc_loss=0.2029, attn_decoder_loss=0.2814, over 4940.00 frames. ], tot_loss[loss=0.2866, ctc_loss=0.2331, attn_decoder_loss=0.3, over 967068.74 frames. ], batch size: 19, lr: 2.06e-02,
2024-10-09 06:11:21,061 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.83 vs. limit=14.0015
2024-10-09 06:11:30,375 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=8672.0, ans=0.125
2024-10-09 06:11:45,464 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.72 vs. limit=7.168833333333334
2024-10-09 06:11:46,716 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.07 vs. limit=10.753250000000001
2024-10-09 06:11:49,671 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.08 vs. limit=14.0065
2024-10-09 06:11:53,541 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=8675.333333333334, ans=0.125
2024-10-09 06:11:55,630 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.33 vs. limit=14.009
2024-10-09 06:12:23,005 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=8.99 vs. limit=10.755749999999999
2024-10-09 06:12:24,835 INFO [train.py:1152] Epoch 4, batch 5500, loss[loss=0.2935, ctc_loss=0.2622, attn_decoder_loss=0.3013, over 4803.00 frames. ], tot_loss[loss=0.2861, ctc_loss=0.2323, attn_decoder_loss=0.2996, over 967428.19 frames. ], batch size: 49, lr: 2.06e-02,
2024-10-09 06:12:27,687 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=8685.333333333334, ans=0.21314666666666665
2024-10-09 06:12:30,690 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=8685.333333333334, ans=0.21314666666666665
2024-10-09 06:12:48,837 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.07 vs. limit=4.3033
2024-10-09 06:13:03,465 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.00 vs. limit=10.7595
2024-10-09 06:13:19,978 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=8695.333333333334, ans=0.125
2024-10-09 06:13:28,329 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.828e+01 7.360e+01 8.263e+01 9.200e+01 1.758e+02, threshold=1.653e+02, percent-clipped=1.0
2024-10-09 06:13:29,928 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=8698.666666666666, ans=0.21301333333333333
2024-10-09 06:13:36,170 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.80 vs. limit=14.0265
2024-10-09 06:13:36,935 INFO [train.py:1152] Epoch 4, batch 5550, loss[loss=0.2459, ctc_loss=0.187, attn_decoder_loss=0.2606, over 4803.00 frames. ], tot_loss[loss=0.2862, ctc_loss=0.2325, attn_decoder_loss=0.2996, over 967271.40 frames. ], batch size: 19, lr: 2.06e-02,
2024-10-09 06:13:42,097 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.64 vs. limit=14.0265
2024-10-09 06:13:47,609 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.51 vs. limit=9.350999999999999
2024-10-09 06:13:54,458 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=8705.333333333334, ans=0.030394444444444443
2024-10-09 06:13:56,494 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.64 vs. limit=14.029
2024-10-09 06:14:03,621 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.53 vs. limit=7.176333333333334
2024-10-09 06:14:13,063 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=8708.666666666666, ans=0.5951966666666668
2024-10-09 06:14:40,442 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=8715.333333333334, ans=0.5949633333333333
2024-10-09 06:14:49,012 INFO [train.py:1152] Epoch 4, batch 5600, loss[loss=0.2705, ctc_loss=0.2052, attn_decoder_loss=0.2869, over 4848.00 frames. ], tot_loss[loss=0.2847, ctc_loss=0.23, attn_decoder_loss=0.2983, over 967235.20 frames. ], batch size: 28, lr: 2.06e-02,
2024-10-09 06:14:49,138 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=8718.666666666666, ans=0.125
2024-10-09 06:14:59,224 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=8718.666666666666, ans=0.21281333333333333
2024-10-09 06:15:19,430 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=8725.333333333334, ans=0.125
2024-10-09 06:15:52,481 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.518e+01 7.288e+01 8.315e+01 9.541e+01 1.420e+02, threshold=1.663e+02, percent-clipped=0.0
2024-10-09 06:15:56,896 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=8732.0, ans=0.05
2024-10-09 06:16:00,946 INFO [train.py:1152] Epoch 4, batch 5650, loss[loss=0.3044, ctc_loss=0.2673, attn_decoder_loss=0.3137, over 4753.00 frames. ], tot_loss[loss=0.2829, ctc_loss=0.2272, attn_decoder_loss=0.2969, over 967211.39 frames. ], batch size: 45, lr: 2.06e-02,
2024-10-09 06:16:02,580 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer_na.min_abs, batch_count=8735.333333333334, ans=0.02
2024-10-09 06:16:18,152 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=5.932e-02
2024-10-09 06:16:21,210 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.15 vs. limit=14.054
2024-10-09 06:16:42,347 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=8745.333333333334, ans=0.025
2024-10-09 06:17:04,591 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.79 vs. limit=10.78075
2024-10-09 06:17:12,570 INFO [train.py:1152] Epoch 4, batch 5700, loss[loss=0.3065, ctc_loss=0.2567, attn_decoder_loss=0.3189, over 4888.00 frames. ], tot_loss[loss=0.2838, ctc_loss=0.2278, attn_decoder_loss=0.2978, over 966561.90 frames. ], batch size: 22, lr: 2.05e-02,
2024-10-09 06:17:16,924 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=8752.0, ans=0.59368
2024-10-09 06:17:24,671 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.46 vs. limit=10.782
2024-10-09 06:17:25,590 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=8755.333333333334, ans=0.125
2024-10-09 06:17:37,251 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=8755.333333333334, ans=0.5935633333333333
2024-10-09 06:17:43,076 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=9.456e-02
2024-10-09 06:17:45,900 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=8758.666666666666, ans=0.125
2024-10-09 06:18:01,831 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=8762.0, ans=0.125
2024-10-09 06:18:11,894 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=8765.333333333334, ans=0.030144444444444442
2024-10-09 06:18:16,048 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.446e+01 7.781e+01 8.532e+01 9.478e+01 1.377e+02, threshold=1.706e+02, percent-clipped=0.0
2024-10-09 06:18:24,773 INFO [train.py:1152] Epoch 4, batch 5750, loss[loss=0.3046, ctc_loss=0.2626, attn_decoder_loss=0.3151, over 4826.00 frames. ], tot_loss[loss=0.2855, ctc_loss=0.2307, attn_decoder_loss=0.2992, over 966951.82 frames. ], batch size: 43, lr: 2.05e-02,
2024-10-09 06:18:30,539 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=8768.666666666666, ans=0.04949747468305833
2024-10-09 06:18:57,906 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=8775.333333333334, ans=0.125
2024-10-09 06:18:57,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=8775.333333333334, ans=0.125
2024-10-09 06:19:18,245 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=8778.666666666666, ans=0.030088888888888894
2024-10-09 06:19:36,377 INFO [train.py:1152] Epoch 4, batch 5800, loss[loss=0.3039, ctc_loss=0.2705, attn_decoder_loss=0.3122, over 4834.00 frames. ], tot_loss[loss=0.2854, ctc_loss=0.231, attn_decoder_loss=0.299, over 966260.21 frames. ], batch size: 43, lr: 2.05e-02,
2024-10-09 06:19:46,061 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=7.81 vs. limit=10.7945
2024-10-09 06:19:49,408 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=8788.666666666666, ans=0.008958985507246378
2024-10-09 06:19:50,765 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=8788.666666666666, ans=0.125
2024-10-09 06:19:57,023 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=5.67 vs. limit=9.394333333333332
2024-10-09 06:19:57,748 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=8788.666666666666, ans=0.125
2024-10-09 06:20:06,327 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=8792.0, ans=0.125
2024-10-09 06:20:36,484 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=8798.666666666666, ans=0.125
2024-10-09 06:20:39,409 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.913e+01 7.286e+01 7.799e+01 8.808e+01 1.463e+02, threshold=1.560e+02, percent-clipped=0.0
2024-10-09 06:20:48,007 INFO [train.py:1152] Epoch 4, batch 5850, loss[loss=0.2906, ctc_loss=0.2606, attn_decoder_loss=0.2981, over 4765.00 frames. ], tot_loss[loss=0.2858, ctc_loss=0.2311, attn_decoder_loss=0.2995, over 966586.09 frames. ], batch size: 45, lr: 2.05e-02,
2024-10-09 06:21:02,517 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=8805.333333333334, ans=0.029977777777777777
2024-10-09 06:21:11,256 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8805.333333333334, ans=0.21194666666666667
2024-10-09 06:21:12,839 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=8805.333333333334, ans=0.125
2024-10-09 06:21:15,669 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=8808.666666666666, ans=0.125
2024-10-09 06:21:31,928 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.75 vs. limit=10.8045
2024-10-09 06:21:38,743 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=8812.0, ans=0.025
2024-10-09 06:22:00,300 INFO [train.py:1152] Epoch 4, batch 5900, loss[loss=0.266, ctc_loss=0.2181, attn_decoder_loss=0.278, over 4831.00 frames. ], tot_loss[loss=0.2845, ctc_loss=0.2291, attn_decoder_loss=0.2984, over 966724.47 frames. ], batch size: 34, lr: 2.05e-02,
2024-10-09 06:22:05,404 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=6.82 vs. limit=7.527466666666666
2024-10-09 06:22:10,479 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=8818.666666666666, ans=0.02992222222222223
2024-10-09 06:22:16,958 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.66 vs. limit=10.808250000000001
2024-10-09 06:22:34,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8825.333333333334, ans=0.21174666666666664
2024-10-09 06:22:37,703 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=8825.333333333334, ans=0.21174666666666664
2024-10-09 06:22:40,685 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=8825.333333333334, ans=0.125
2024-10-09 06:22:40,733 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=8825.333333333334, ans=0.125
2024-10-09 06:22:49,298 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=8828.666666666666, ans=0.008950289855072464
2024-10-09 06:23:03,551 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.285e+01 7.429e+01 8.342e+01 9.262e+01 1.510e+02, threshold=1.668e+02, percent-clipped=0.0
2024-10-09 06:23:06,596 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=8832.0, ans=0.125
2024-10-09 06:23:12,181 INFO [train.py:1152] Epoch 4, batch 5950, loss[loss=0.2894, ctc_loss=0.2492, attn_decoder_loss=0.2994, over 4807.00 frames. ], tot_loss[loss=0.2841, ctc_loss=0.2289, attn_decoder_loss=0.2979, over 966152.48 frames. ], batch size: 34, lr: 2.05e-02,
2024-10-09 06:23:20,127 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.51 vs. limit=10.81325
2024-10-09 06:23:23,823 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=8835.333333333334, ans=0.125
2024-10-09 06:23:27,234 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.51 vs. limit=10.814499999999999
2024-10-09 06:24:11,433 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=8848.666666666666, ans=0.008945942028985507
2024-10-09 06:24:11,977 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=9.18 vs. limit=10.818249999999999
2024-10-09 06:24:17,848 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.11 vs. limit=14.1365
2024-10-09 06:24:24,320 INFO [train.py:1152] Epoch 4, batch 6000, loss[loss=0.3285, ctc_loss=0.3069, attn_decoder_loss=0.3339, over 4808.00 frames. ], tot_loss[loss=0.2841, ctc_loss=0.229, attn_decoder_loss=0.2979, over 966757.28 frames. ], batch size: 49, lr: 2.04e-02,
2024-10-09 06:24:24,320 INFO [train.py:1175] Computing validation loss
2024-10-09 06:24:33,114 INFO [train.py:1184] Epoch 4, validation: loss=0.2209, ctc_loss=0.0895, attn_decoder_loss=0.2538, over 90464.00 frames.
2024-10-09 06:24:33,115 INFO [train.py:1185] Maximum memory allocated so far is 6613MB
2024-10-09 06:24:48,488 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=8855.333333333334, ans=0.008944492753623188
2024-10-09 06:25:18,707 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=8862.0, ans=0.0
2024-10-09 06:25:27,489 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=8862.0, ans=0.125
2024-10-09 06:25:36,070 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.367e+01 7.563e+01 8.795e+01 9.818e+01 1.694e+02, threshold=1.759e+02, percent-clipped=1.0
2024-10-09 06:25:44,524 INFO [train.py:1152] Epoch 4, batch 6050, loss[loss=0.2686, ctc_loss=0.1998, attn_decoder_loss=0.2858, over 4814.00 frames. ], tot_loss[loss=0.2835, ctc_loss=0.2278, attn_decoder_loss=0.2974, over 966741.66 frames. ], batch size: 19, lr: 2.04e-02,
2024-10-09 06:25:52,616 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.80 vs. limit=14.1515
2024-10-09 06:26:00,868 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.71 vs. limit=14.154
2024-10-09 06:26:01,631 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=8872.0, ans=0.125
2024-10-09 06:26:03,785 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.90 vs. limit=14.154
2024-10-09 06:26:33,635 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.48 vs. limit=4.3318
2024-10-09 06:26:34,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=8878.666666666666, ans=0.025
2024-10-09 06:26:41,723 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=8882.0, ans=0.125
2024-10-09 06:26:43,176 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=8882.0, ans=0.0
2024-10-09 06:26:55,994 INFO [train.py:1152] Epoch 4, batch 6100, loss[loss=0.2946, ctc_loss=0.231, attn_decoder_loss=0.3105, over 4806.00 frames. ], tot_loss[loss=0.2824, ctc_loss=0.2263, attn_decoder_loss=0.2965, over 966206.70 frames. ], batch size: 34, lr: 2.04e-02,
2024-10-09 06:27:07,587 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=8885.333333333334, ans=0.125
2024-10-09 06:27:11,714 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=8888.666666666666, ans=0.125
2024-10-09 06:27:23,385 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=8892.0, ans=0.125
2024-10-09 06:27:27,732 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=8892.0, ans=0.125
2024-10-09 06:27:49,280 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.min_positive, batch_count=8895.333333333334, ans=0.05
2024-10-09 06:27:50,064 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.77 vs. limit=9.447666666666667
2024-10-09 06:27:59,178 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.879e+01 7.520e+01 8.561e+01 9.453e+01 1.811e+02, threshold=1.712e+02, percent-clipped=1.0
2024-10-09 06:27:59,452 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=8898.666666666666, ans=0.125
2024-10-09 06:28:04,521 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.34 vs. limit=7.224666666666666
2024-10-09 06:28:05,021 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=8898.666666666666, ans=0.125
2024-10-09 06:28:07,610 INFO [train.py:1152] Epoch 4, batch 6150, loss[loss=0.3619, ctc_loss=0.3387, attn_decoder_loss=0.3677, over 4838.00 frames. ], tot_loss[loss=0.2835, ctc_loss=0.2275, attn_decoder_loss=0.2975, over 966429.92 frames. ], batch size: 43, lr: 2.04e-02,
2024-10-09 06:28:14,058 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.66 vs. limit=14.1765
2024-10-09 06:28:15,079 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=8902.0, ans=0.029575
2024-10-09 06:28:28,579 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2.whitening_limit, batch_count=8905.333333333334, ans=9.452666666666667
2024-10-09 06:28:41,250 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.55 vs. limit=7.227166666666666
2024-10-09 06:28:52,256 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=8912.0, ans=0.33368
2024-10-09 06:28:55,072 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=8912.0, ans=0.125
2024-10-09 06:29:16,718 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=8915.333333333334, ans=0.025
2024-10-09 06:29:19,416 INFO [train.py:1152] Epoch 4, batch 6200, loss[loss=0.2745, ctc_loss=0.2056, attn_decoder_loss=0.2918, over 4786.00 frames. ], tot_loss[loss=0.2839, ctc_loss=0.2275, attn_decoder_loss=0.298, over 966677.19 frames. ], batch size: 29, lr: 2.04e-02,
2024-10-09 06:29:54,145 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=8925.333333333334, ans=0.125
2024-10-09 06:30:09,294 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.74 vs. limit=14.1965
2024-10-09 06:30:17,888 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.20 vs. limit=7.2330000000000005
2024-10-09 06:30:18,168 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn1.whiten.whitening_limit, batch_count=8932.0, ans=14.199
2024-10-09 06:30:22,993 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.669e+01 7.101e+01 7.883e+01 9.090e+01 1.629e+02, threshold=1.577e+02, percent-clipped=0.0
2024-10-09 06:30:31,676 INFO [train.py:1152] Epoch 4, batch 6250, loss[loss=0.2991, ctc_loss=0.2383, attn_decoder_loss=0.3143, over 4735.00 frames. ], tot_loss[loss=0.2825, ctc_loss=0.2254, attn_decoder_loss=0.2968, over 966906.63 frames. ], batch size: 26, lr: 2.04e-02,
2024-10-09 06:30:33,345 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=8935.333333333334, ans=0.0
2024-10-09 06:30:33,853 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.40 vs. limit=10.85075
2024-10-09 06:30:51,211 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.43 vs. limit=10.852
2024-10-09 06:30:51,478 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=13.38 vs. limit=10.852
2024-10-09 06:30:59,310 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=8942.0, ans=0.125
2024-10-09 06:31:26,386 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=8945.333333333334, ans=0.07
2024-10-09 06:31:43,509 INFO [train.py:1152] Epoch 4, batch 6300, loss[loss=0.2706, ctc_loss=0.2068, attn_decoder_loss=0.2865, over 4978.00 frames. ], tot_loss[loss=0.2825, ctc_loss=0.2249, attn_decoder_loss=0.2969, over 966568.99 frames. ], batch size: 19, lr: 2.03e-02,
2024-10-09 06:31:48,032 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=8952.0, ans=0.21048
2024-10-09 06:31:55,083 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=8952.0, ans=0.0
2024-10-09 06:32:02,332 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.max_positive, batch_count=8955.333333333334, ans=0.8395533333333334
2024-10-09 06:32:02,879 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.57 vs. limit=14.2165
2024-10-09 06:32:06,671 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=8955.333333333334, ans=0.029352777777777776
2024-10-09 06:32:19,057 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.63 vs. limit=14.219000000000001
2024-10-09 06:32:36,420 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=2.93 vs. limit=10.86075
2024-10-09 06:32:42,682 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=8965.333333333334, ans=0.5862133333333334
2024-10-09 06:32:44,790 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.38 vs. limit=10.862
2024-10-09 06:32:46,770 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.736e+01 7.287e+01 8.223e+01 9.177e+01 1.256e+02, threshold=1.645e+02, percent-clipped=0.0
2024-10-09 06:32:54,172 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=8968.666666666666, ans=0.029297222222222228
2024-10-09 06:32:55,440 INFO [train.py:1152] Epoch 4, batch 6350, loss[loss=0.3239, ctc_loss=0.3034, attn_decoder_loss=0.329, over 4794.00 frames. ], tot_loss[loss=0.281, ctc_loss=0.223, attn_decoder_loss=0.2955, over 966129.58 frames. ], batch size: 36, lr: 2.03e-02,
2024-10-09 06:33:01,814 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.70 vs. limit=9.484333333333332
2024-10-09 06:33:40,054 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=8978.666666666666, ans=0.125
2024-10-09 06:33:40,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=8978.666666666666, ans=0.125
2024-10-09 06:34:07,324 INFO [train.py:1152] Epoch 4, batch 6400, loss[loss=0.3024, ctc_loss=0.2369, attn_decoder_loss=0.3188, over 4873.00 frames. ], tot_loss[loss=0.2801, ctc_loss=0.2216, attn_decoder_loss=0.2948, over 965952.16 frames. ], batch size: 23, lr: 2.03e-02,
2024-10-09 06:34:10,789 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.07 vs. limit=14.239
2024-10-09 06:34:23,224 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=8988.666666666666, ans=0.008915507246376811
2024-10-09 06:34:35,598 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.86 vs. limit=14.244
2024-10-09 06:34:43,435 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=8992.0, ans=0.21008
2024-10-09 06:34:43,444 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=8992.0, ans=0.05
2024-10-09 06:35:05,746 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.62 vs. limit=10.8745
2024-10-09 06:35:10,881 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.757e+01 7.540e+01 8.563e+01 9.375e+01 1.341e+02, threshold=1.713e+02, percent-clipped=0.0
2024-10-09 06:35:11,473 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.58 vs. limit=10.8745
2024-10-09 06:35:19,470 INFO [train.py:1152] Epoch 4, batch 6450, loss[loss=0.2733, ctc_loss=0.2057, attn_decoder_loss=0.2902, over 4737.00 frames. ], tot_loss[loss=0.2805, ctc_loss=0.2218, attn_decoder_loss=0.2951, over 965341.23 frames. ], batch size: 26, lr: 2.03e-02,
2024-10-09 06:35:30,790 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=9.33 vs. limit=10.87575
2024-10-09 06:35:37,087 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=9005.333333333334, ans=0.125
2024-10-09 06:35:37,688 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.62 vs. limit=14.254
2024-10-09 06:35:45,716 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=9005.333333333334, ans=0.029144444444444445
2024-10-09 06:35:55,856 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=9008.666666666666, ans=0.125
2024-10-09 06:36:17,472 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=9015.333333333334, ans=0.008909710144927536
2024-10-09 06:36:30,514 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=9018.666666666666, ans=0.5843466666666668
2024-10-09 06:36:31,750 INFO [train.py:1152] Epoch 4, batch 6500, loss[loss=0.2793, ctc_loss=0.2109, attn_decoder_loss=0.2965, over 4730.00 frames. ], tot_loss[loss=0.2795, ctc_loss=0.2204, attn_decoder_loss=0.2942, over 965084.87 frames. ], batch size: 26, lr: 2.03e-02,
2024-10-09 06:37:03,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=9025.333333333334, ans=0.125
2024-10-09 06:37:22,470 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=9028.666666666666, ans=0.125
2024-10-09 06:37:25,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=9028.666666666666, ans=0.0089068115942029
2024-10-09 06:37:28,731 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.49 vs. limit=14.274000000000001
2024-10-09 06:37:33,429 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.74 vs. limit=9.516
2024-10-09 06:37:35,251 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.745e+01 7.656e+01 8.455e+01 9.443e+01 1.858e+02, threshold=1.691e+02, percent-clipped=1.0
2024-10-09 06:37:43,995 INFO [train.py:1152] Epoch 4, batch 6550, loss[loss=0.2357, ctc_loss=0.1503, attn_decoder_loss=0.2571, over 4978.00 frames. ], tot_loss[loss=0.278, ctc_loss=0.2184, attn_decoder_loss=0.2929, over 964638.33 frames. ], batch size: 19, lr: 2.02e-02,
2024-10-09 06:37:48,296 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=9035.333333333334, ans=0.5837633333333334
2024-10-09 06:38:16,853 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.25 vs. limit=4.3563
2024-10-09 06:38:17,297 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=9042.0, ans=0.025
2024-10-09 06:38:28,882 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9045.333333333334, ans=0.20954666666666666
2024-10-09 06:38:32,498 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.19 vs. limit=4.3568
2024-10-09 06:38:33,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=9045.333333333334, ans=0.125
2024-10-09 06:38:56,516 INFO [train.py:1152] Epoch 4, batch 6600, loss[loss=0.2395, ctc_loss=0.1591, attn_decoder_loss=0.2596, over 4875.00 frames. ], tot_loss[loss=0.2771, ctc_loss=0.2171, attn_decoder_loss=0.2921, over 965391.54 frames. ], batch size: 23, lr: 2.02e-02,
2024-10-09 06:39:04,070 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=9052.0, ans=0.20948
2024-10-09 06:39:08,483 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=9052.0, ans=0.125
2024-10-09 06:39:11,853 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.10 vs. limit=7.263833333333333
2024-10-09 06:39:37,443 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.94 vs. limit=14.294
2024-10-09 06:39:43,924 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=9062.0, ans=0.125
2024-10-09 06:40:01,847 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.145e+01 7.403e+01 7.888e+01 9.087e+01 1.290e+02, threshold=1.578e+02, percent-clipped=0.0
2024-10-09 06:40:10,396 INFO [train.py:1152] Epoch 4, batch 6650, loss[loss=0.2312, ctc_loss=0.1533, attn_decoder_loss=0.2507, over 4745.00 frames. ], tot_loss[loss=0.2755, ctc_loss=0.2136, attn_decoder_loss=0.2909, over 967281.90 frames. ], batch size: 20, lr: 2.02e-02,
2024-10-09 06:40:39,913 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=9075.333333333334, ans=0.5823633333333333
2024-10-09 06:41:22,622 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=9085.333333333334, ans=0.125
2024-10-09 06:41:23,819 INFO [train.py:1152] Epoch 4, batch 6700, loss[loss=0.2439, ctc_loss=0.1715, attn_decoder_loss=0.262, over 4937.00 frames. ], tot_loss[loss=0.2735, ctc_loss=0.2114, attn_decoder_loss=0.289, over 969364.73 frames. ], batch size: 20, lr: 2.02e-02,
2024-10-09 06:41:25,444 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=9085.333333333334, ans=0.125
2024-10-09 06:41:26,025 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.64 vs. limit=10.907
2024-10-09 06:41:28,327 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9085.333333333334, ans=0.20914666666666665
2024-10-09 06:41:54,960 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=9092.0, ans=0.04949747468305833
2024-10-09 06:42:04,324 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=11.22 vs. limit=10.9095
2024-10-09 06:42:25,772 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=9098.666666666666, ans=0.028755555555555562
2024-10-09 06:42:28,524 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.322e+01 7.164e+01 8.104e+01 9.507e+01 1.355e+02, threshold=1.621e+02, percent-clipped=0.0
2024-10-09 06:42:30,243 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=9098.666666666666, ans=0.028755555555555562
2024-10-09 06:42:33,254 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=9098.666666666666, ans=0.0
2024-10-09 06:42:33,947 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.56 vs. limit=9.549333333333333
2024-10-09 06:42:37,413 INFO [train.py:1152] Epoch 4, batch 6750, loss[loss=0.281, ctc_loss=0.2319, attn_decoder_loss=0.2933, over 4912.00 frames. ], tot_loss[loss=0.2709, ctc_loss=0.2086, attn_decoder_loss=0.2865, over 972397.54 frames. ], batch size: 19, lr: 2.02e-02,
2024-10-09 06:42:46,329 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=9102.0, ans=0.07
2024-10-09 06:43:10,752 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.27 vs. limit=10.91575
2024-10-09 06:43:10,773 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.25 vs. limit=7.277166666666666
2024-10-09 06:43:29,342 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=9112.0, ans=0.125
2024-10-09 06:43:38,989 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.78 vs. limit=10.91825
2024-10-09 06:43:42,549 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=9115.333333333334, ans=0.025
2024-10-09 06:43:48,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=9115.333333333334, ans=0.5809633333333333
2024-10-09 06:43:51,254 INFO [train.py:1152] Epoch 4, batch 6800, loss[loss=0.2534, ctc_loss=0.1862, attn_decoder_loss=0.2702, over 4909.00 frames. ], tot_loss[loss=0.2691, ctc_loss=0.2058, attn_decoder_loss=0.2849, over 974682.50 frames. ], batch size: 19, lr: 2.02e-02,
2024-10-09 06:44:06,280 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=9122.0, ans=0.125
2024-10-09 06:44:15,240 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=9122.0, ans=0.125
2024-10-09 06:44:15,366 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=9122.0, ans=0.125
2024-10-09 06:44:22,701 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=9125.333333333334, ans=0.04949747468305833
2024-10-09 06:44:33,224 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9125.333333333334, ans=0.20874666666666666
2024-10-09 06:44:42,227 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=9128.666666666666, ans=0.125
2024-10-09 06:44:42,839 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=2.66 vs. limit=10.92325
2024-10-09 06:44:57,049 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.882e+01 6.625e+01 7.886e+01 9.059e+01 1.572e+02, threshold=1.577e+02, percent-clipped=0.0
2024-10-09 06:44:58,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys.whitening_limit, batch_count=9132.0, ans=4.3698
2024-10-09 06:45:01,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=9132.0, ans=0.125
2024-10-09 06:45:03,229 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9132.0, ans=0.20867999999999998
2024-10-09 06:45:06,032 INFO [train.py:1152] Epoch 4, batch 6850, loss[loss=0.2283, ctc_loss=0.1483, attn_decoder_loss=0.2483, over 4978.00 frames. ], tot_loss[loss=0.2663, ctc_loss=0.2032, attn_decoder_loss=0.2821, over 979009.11 frames. ], batch size: 19, lr: 2.01e-02,
2024-10-09 06:45:07,654 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_False/epoch-4.pt
2024-10-09 06:45:44,649 INFO [train.py:1152] Epoch 5, batch 0, loss[loss=0.2504, ctc_loss=0.1675, attn_decoder_loss=0.2712, over 4853.00 frames. ], tot_loss[loss=0.2504, ctc_loss=0.1675, attn_decoder_loss=0.2712, over 4853.00 frames. ], batch size: 19, lr: 1.88e-02,
2024-10-09 06:45:44,649 INFO [train.py:1175] Computing validation loss
2024-10-09 06:45:51,269 INFO [train.py:1184] Epoch 5, validation: loss=0.2211, ctc_loss=0.09148, attn_decoder_loss=0.2535, over 90464.00 frames.
2024-10-09 06:45:51,269 INFO [train.py:1185] Maximum memory allocated so far is 6613MB
2024-10-09 06:45:59,811 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=9136.0, ans=0.125
2024-10-09 06:46:08,690 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=9139.333333333334, ans=0.02858611111111111
2024-10-09 06:46:27,820 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=9142.666666666666, ans=0.125
2024-10-09 06:46:32,068 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=9146.0, ans=0.0
2024-10-09 06:46:44,154 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.17 vs. limit=10.931000000000001
2024-10-09 06:46:58,878 INFO [train.py:1152] Epoch 5, batch 50, loss[loss=0.2551, ctc_loss=0.1882, attn_decoder_loss=0.2718, over 4915.00 frames. ], tot_loss[loss=0.284, ctc_loss=0.2315, attn_decoder_loss=0.2971, over 217819.09 frames. ], batch size: 19, lr: 1.87e-02,
2024-10-09 06:47:43,700 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9162.666666666666, ans=0.20837333333333335
2024-10-09 06:47:43,750 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=9162.666666666666, ans=0.125
2024-10-09 06:47:49,517 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=9162.666666666666, ans=0.125
2024-10-09 06:47:59,427 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.363e+01 7.046e+01 7.967e+01 9.171e+01 2.020e+02, threshold=1.593e+02, percent-clipped=1.0
2024-10-09 06:48:10,973 INFO [train.py:1152] Epoch 5, batch 100, loss[loss=0.2672, ctc_loss=0.2139, attn_decoder_loss=0.2805, over 4749.00 frames. ], tot_loss[loss=0.2865, ctc_loss=0.2324, attn_decoder_loss=0.3001, over 383336.03 frames. ], batch size: 19, lr: 1.87e-02,
2024-10-09 06:48:14,533 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.31 vs. limit=9.584666666666667
2024-10-09 06:48:18,804 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.44 vs. limit=10.9385
2024-10-09 06:48:22,450 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=9169.333333333334, ans=0.008876231884057971
2024-10-09 06:48:33,596 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module1.whiten, num_groups=1, num_channels=192, metric=3.52 vs. limit=10.93975
2024-10-09 06:48:37,939 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.21 vs. limit=4.3759
2024-10-09 06:48:38,461 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=9176.0, ans=0.125
2024-10-09 06:48:48,697 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9176.0, ans=0.20823999999999998
2024-10-09 06:48:54,947 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten.whitening_limit, batch_count=9179.333333333334, ans=10.94225
2024-10-09 06:49:23,401 INFO [train.py:1152] Epoch 5, batch 150, loss[loss=0.2703, ctc_loss=0.1972, attn_decoder_loss=0.2886, over 4914.00 frames. ], tot_loss[loss=0.2806, ctc_loss=0.2245, attn_decoder_loss=0.2947, over 513306.44 frames. ], batch size: 19, lr: 1.87e-02,
2024-10-09 06:49:40,817 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=9189.333333333334, ans=0.125
2024-10-09 06:49:58,054 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=9192.666666666666, ans=0.0
2024-10-09 06:50:02,371 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=9192.666666666666, ans=0.05
2024-10-09 06:50:24,012 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.471e+01 7.055e+01 7.835e+01 8.984e+01 1.325e+02, threshold=1.567e+02, percent-clipped=0.0
2024-10-09 06:50:26,861 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=9199.333333333334, ans=0.125
2024-10-09 06:50:35,264 INFO [train.py:1152] Epoch 5, batch 200, loss[loss=0.3004, ctc_loss=0.2541, attn_decoder_loss=0.312, over 4707.00 frames. ], tot_loss[loss=0.2794, ctc_loss=0.2225, attn_decoder_loss=0.2936, over 613598.53 frames. ], batch size: 45, lr: 1.87e-02,
2024-10-09 06:50:36,021 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.62 vs. limit=14.402000000000001
2024-10-09 06:50:36,844 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=9202.666666666666, ans=0.125
2024-10-09 06:50:47,020 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=9202.666666666666, ans=0.125
2024-10-09 06:51:12,907 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=9209.333333333334, ans=0.125
2024-10-09 06:51:48,407 INFO [train.py:1152] Epoch 5, batch 250, loss[loss=0.2794, ctc_loss=0.2149, attn_decoder_loss=0.2956, over 4840.00 frames. ], tot_loss[loss=0.2797, ctc_loss=0.2225, attn_decoder_loss=0.294, over 692362.94 frames. ], batch size: 38, lr: 1.87e-02,
2024-10-09 06:52:17,609 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=9226.0, ans=0.028225000000000004
2024-10-09 06:52:33,806 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten.whitening_limit, batch_count=9229.333333333334, ans=10.961
2024-10-09 06:52:44,012 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=3.10 vs. limit=10.962250000000001
2024-10-09 06:52:47,274 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.422e+01 7.338e+01 7.997e+01 9.082e+01 1.675e+02, threshold=1.599e+02, percent-clipped=1.0
2024-10-09 06:52:52,400 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=13.01 vs. limit=10.962250000000001
2024-10-09 06:52:58,715 INFO [train.py:1152] Epoch 5, batch 300, loss[loss=0.3376, ctc_loss=0.2927, attn_decoder_loss=0.3489, over 4764.00 frames. ], tot_loss[loss=0.2783, ctc_loss=0.2207, attn_decoder_loss=0.2928, over 752866.47 frames. ], batch size: 32, lr: 1.87e-02,
2024-10-09 06:53:06,528 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.24 vs. limit=14.427
2024-10-09 06:53:08,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=9236.0, ans=0.125
2024-10-09 06:53:20,070 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.75 vs. limit=14.4295
2024-10-09 06:54:01,854 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=10.41 vs. limit=9.624666666666666
2024-10-09 06:54:06,101 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=6.29 vs. limit=10.9685
2024-10-09 06:54:08,200 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-09 06:54:10,767 INFO [train.py:1152] Epoch 5, batch 350, loss[loss=0.2507, ctc_loss=0.1941, attn_decoder_loss=0.2649, over 4883.00 frames. ], tot_loss[loss=0.2775, ctc_loss=0.2192, attn_decoder_loss=0.2921, over 800200.26 frames. ], batch size: 19, lr: 1.86e-02,
2024-10-09 06:54:10,970 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=9252.666666666666, ans=0.05
2024-10-09 06:54:11,551 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.70 vs. limit=14.439499999999999
2024-10-09 06:54:17,077 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=12.51 vs. limit=9.626333333333333
2024-10-09 06:54:31,007 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=9256.0, ans=0.125
2024-10-09 06:54:54,545 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.24 vs. limit=4.3894
2024-10-09 06:55:11,014 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.048e+01 7.243e+01 8.126e+01 9.207e+01 1.302e+02, threshold=1.625e+02, percent-clipped=0.0
2024-10-09 06:55:22,543 INFO [train.py:1152] Epoch 5, batch 400, loss[loss=0.2714, ctc_loss=0.2045, attn_decoder_loss=0.2881, over 4882.00 frames. ], tot_loss[loss=0.2769, ctc_loss=0.2179, attn_decoder_loss=0.2917, over 836873.80 frames. ], batch size: 22, lr: 1.86e-02,
2024-10-09 06:55:51,658 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=13.46 vs. limit=9.638
2024-10-09 06:56:17,749 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.63 vs. limit=14.4595
2024-10-09 06:56:18,585 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=9282.666666666666, ans=0.5751066666666667
2024-10-09 06:56:34,199 INFO [train.py:1152] Epoch 5, batch 450, loss[loss=0.2374, ctc_loss=0.1574, attn_decoder_loss=0.2574, over 4880.00 frames. ], tot_loss[loss=0.2765, ctc_loss=0.2169, attn_decoder_loss=0.2913, over 865468.86 frames. ], batch size: 23, lr: 1.86e-02,
2024-10-09 06:56:44,453 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=9286.0, ans=0.5749900000000001
2024-10-09 06:56:57,235 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=9289.333333333334, ans=0.125
2024-10-09 06:57:14,636 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=9292.666666666666, ans=0.125
2024-10-09 06:57:23,216 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=9296.0, ans=0.125
2024-10-09 06:57:33,346 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=9299.333333333334, ans=0.125
2024-10-09 06:57:34,548 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.209e+01 7.142e+01 8.173e+01 9.172e+01 1.162e+02, threshold=1.635e+02, percent-clipped=0.0
2024-10-09 06:57:40,579 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=9299.333333333334, ans=0.008847971014492754
2024-10-09 06:57:42,056 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=2.586e-03
2024-10-09 06:57:46,218 INFO [train.py:1152] Epoch 5, batch 500, loss[loss=0.3143, ctc_loss=0.248, attn_decoder_loss=0.3308, over 4814.00 frames. ], tot_loss[loss=0.2756, ctc_loss=0.2159, attn_decoder_loss=0.2905, over 888051.78 frames. ], batch size: 34, lr: 1.86e-02,
2024-10-09 06:57:50,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=9302.666666666666, ans=0.0
2024-10-09 06:58:13,758 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=9309.333333333334, ans=0.125
2024-10-09 06:58:22,393 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9309.333333333334, ans=0.20690666666666666
2024-10-09 06:58:28,575 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.27 vs. limit=10.99225
2024-10-09 06:58:32,570 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.max_abs, batch_count=9312.666666666666, ans=10.0
2024-10-09 06:58:58,430 INFO [train.py:1152] Epoch 5, batch 550, loss[loss=0.2728, ctc_loss=0.196, attn_decoder_loss=0.2919, over 4804.00 frames. ], tot_loss[loss=0.2753, ctc_loss=0.2157, attn_decoder_loss=0.2902, over 905589.23 frames. ], batch size: 40, lr: 1.86e-02,
2024-10-09 06:59:04,289 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=9319.333333333334, ans=0.125
2024-10-09 06:59:06,540 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.93 vs. limit=10.99475
2024-10-09 06:59:27,644 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=9326.0, ans=0.125
2024-10-09 06:59:34,361 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=13.05 vs. limit=14.4945
2024-10-09 06:59:57,987 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_False/checkpoint-28000.pt
2024-10-09 07:00:00,004 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.44 vs. limit=9.666333333333334
2024-10-09 07:00:00,449 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.279e+01 7.202e+01 8.191e+01 9.050e+01 1.303e+02, threshold=1.638e+02, percent-clipped=0.0
2024-10-09 07:00:05,685 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=9332.666666666666, ans=0.125
2024-10-09 07:00:11,282 INFO [train.py:1152] Epoch 5, batch 600, loss[loss=0.2593, ctc_loss=0.1869, attn_decoder_loss=0.2774, over 4840.00 frames. ], tot_loss[loss=0.2756, ctc_loss=0.2153, attn_decoder_loss=0.2907, over 919346.63 frames. ], batch size: 38, lr: 1.86e-02,
2024-10-09 07:00:17,002 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=9336.0, ans=10.0
2024-10-09 07:00:30,776 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=9339.333333333334, ans=0.125
2024-10-09 07:00:51,004 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=9342.666666666666, ans=0.025
2024-10-09 07:00:52,547 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=3.134e-02
2024-10-09 07:01:01,079 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=9346.0, ans=0.125
2024-10-09 07:01:02,048 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module1.whiten, num_groups=1, num_channels=192, metric=6.84 vs. limit=11.00475
2024-10-09 07:01:12,718 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=9349.333333333334, ans=0.008837101449275362
2024-10-09 07:01:22,635 INFO [train.py:1152] Epoch 5, batch 650, loss[loss=0.2815, ctc_loss=0.237, attn_decoder_loss=0.2926, over 4836.00 frames. ], tot_loss[loss=0.274, ctc_loss=0.213, attn_decoder_loss=0.2893, over 930289.24 frames. ], batch size: 21, lr: 1.86e-02,
2024-10-09 07:01:31,307 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=9352.666666666666, ans=0.125
2024-10-09 07:01:34,987 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.20 vs. limit=11.007249999999999
2024-10-09 07:01:40,436 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.63 vs. limit=7.339
2024-10-09 07:01:45,636 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=9356.0, ans=0.125
2024-10-09 07:01:59,292 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.77 vs. limit=7.339833333333333
2024-10-09 07:02:10,805 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.46 vs. limit=4.4044
2024-10-09 07:02:22,837 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.934e+01 7.016e+01 7.873e+01 8.611e+01 1.677e+02, threshold=1.575e+02, percent-clipped=1.0
2024-10-09 07:02:34,383 INFO [train.py:1152] Epoch 5, batch 700, loss[loss=0.269, ctc_loss=0.2066, attn_decoder_loss=0.2846, over 4755.00 frames. ], tot_loss[loss=0.2742, ctc_loss=0.2131, attn_decoder_loss=0.2895, over 938192.12 frames. ], batch size: 19, lr: 1.85e-02,
2024-10-09 07:03:05,927 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=9376.0, ans=0.025
2024-10-09 07:03:17,508 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=9379.333333333334, ans=0.07
2024-10-09 07:03:19,713 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.69 vs. limit=9.689666666666668
2024-10-09 07:03:36,379 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=9382.666666666666, ans=0.125
2024-10-09 07:03:46,336 INFO [train.py:1152] Epoch 5, batch 750, loss[loss=0.2707, ctc_loss=0.204, attn_decoder_loss=0.2874, over 4867.00 frames. ], tot_loss[loss=0.2738, ctc_loss=0.2132, attn_decoder_loss=0.2889, over 945006.23 frames. ], batch size: 22, lr: 1.85e-02,
2024-10-09 07:04:00,860 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=9389.333333333334, ans=0.125
2024-10-09 07:04:10,959 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=9389.333333333334, ans=0.025
2024-10-09 07:04:32,896 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=9396.0, ans=0.125
2024-10-09 07:04:40,363 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.36 vs. limit=7.349
2024-10-09 07:04:42,961 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=14.60 vs. limit=11.024750000000001
2024-10-09 07:04:46,707 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.472e+01 7.059e+01 7.859e+01 8.819e+01 1.469e+02, threshold=1.572e+02, percent-clipped=0.0
2024-10-09 07:04:58,107 INFO [train.py:1152] Epoch 5, batch 800, loss[loss=0.2848, ctc_loss=0.2299, attn_decoder_loss=0.2985, over 4857.00 frames. ], tot_loss[loss=0.274, ctc_loss=0.2135, attn_decoder_loss=0.2892, over 949730.96 frames. ], batch size: 19, lr: 1.85e-02,
2024-10-09 07:05:41,192 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=9412.666666666666, ans=0.027447222222222227
2024-10-09 07:05:54,743 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.24 vs. limit=7.354
2024-10-09 07:06:08,747 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=9419.333333333334, ans=0.04949747468305833
2024-10-09 07:06:10,016 INFO [train.py:1152] Epoch 5, batch 850, loss[loss=0.289, ctc_loss=0.2224, attn_decoder_loss=0.3057, over 4803.00 frames. ], tot_loss[loss=0.2744, ctc_loss=0.2133, attn_decoder_loss=0.2897, over 953894.14 frames. ], batch size: 29, lr: 1.85e-02,
2024-10-09 07:06:31,623 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9422.666666666666, ans=0.20577333333333334
2024-10-09 07:06:55,975 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=9429.333333333334, ans=0.008819710144927537
2024-10-09 07:07:02,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.89 vs. limit=14.572
2024-10-09 07:07:10,181 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.857e+01 6.915e+01 7.920e+01 9.132e+01 1.838e+02, threshold=1.584e+02, percent-clipped=1.0
2024-10-09 07:07:10,433 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=9432.666666666666, ans=0.008818985507246377
2024-10-09 07:07:21,231 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.88 vs. limit=11.038499999999999
2024-10-09 07:07:21,921 INFO [train.py:1152] Epoch 5, batch 900, loss[loss=0.2513, ctc_loss=0.1709, attn_decoder_loss=0.2714, over 4852.00 frames. ], tot_loss[loss=0.2747, ctc_loss=0.2137, attn_decoder_loss=0.2899, over 956870.10 frames. ], batch size: 19, lr: 1.85e-02,
2024-10-09 07:07:29,279 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=9436.0, ans=0.125
2024-10-09 07:07:49,095 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.33 vs. limit=9.719666666666667
2024-10-09 07:08:02,166 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module2.whiten, num_groups=1, num_channels=192, metric=4.45 vs. limit=11.041
2024-10-09 07:08:04,174 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=9446.0, ans=0.125
2024-10-09 07:08:10,011 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=9446.0, ans=0.125
2024-10-09 07:08:17,574 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=6.53 vs. limit=11.04225
2024-10-09 07:08:17,657 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=3.68 vs. limit=7.7783999999999995
2024-10-09 07:08:34,191 INFO [train.py:1152] Epoch 5, batch 950, loss[loss=0.2711, ctc_loss=0.1929, attn_decoder_loss=0.2906, over 4819.00 frames. ], tot_loss[loss=0.2751, ctc_loss=0.2146, attn_decoder_loss=0.2902, over 958800.08 frames. ], batch size: 19, lr: 1.85e-02,
2024-10-09 07:08:44,295 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=9452.666666666666, ans=0.5691566666666668
2024-10-09 07:08:45,668 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=9452.666666666666, ans=0.5691566666666668
2024-10-09 07:08:51,494 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9456.0, ans=0.20544
2024-10-09 07:08:57,112 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9456.0, ans=0.20544
2024-10-09 07:09:14,349 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.max_positive, batch_count=9459.333333333334, ans=0.8445933333333333
2024-10-09 07:09:15,828 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=9462.666666666666, ans=0.125
2024-10-09 07:09:34,584 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.941e+01 6.771e+01 7.775e+01 8.995e+01 2.229e+02, threshold=1.555e+02, percent-clipped=2.0
2024-10-09 07:09:45,952 INFO [train.py:1152] Epoch 5, batch 1000, loss[loss=0.2789, ctc_loss=0.2143, attn_decoder_loss=0.2951, over 4932.00 frames. ], tot_loss[loss=0.2748, ctc_loss=0.2138, attn_decoder_loss=0.29, over 960618.39 frames. ], batch size: 20, lr: 1.84e-02,
2024-10-09 07:09:54,905 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=9469.333333333334, ans=0.125
2024-10-09 07:10:22,862 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.57 vs. limit=14.607
2024-10-09 07:10:58,436 INFO [train.py:1152] Epoch 5, batch 1050, loss[loss=0.2531, ctc_loss=0.1892, attn_decoder_loss=0.2691, over 4801.00 frames. ], tot_loss[loss=0.2734, ctc_loss=0.2115, attn_decoder_loss=0.2889, over 962650.70 frames. ], batch size: 25, lr: 1.84e-02,
2024-10-09 07:11:13,190 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=9489.333333333334, ans=0.125
2024-10-09 07:11:16,246 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=9489.333333333334, ans=0.125
2024-10-09 07:11:27,609 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=9492.666666666666, ans=0.125
2024-10-09 07:11:36,249 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=9492.666666666666, ans=0.5677566666666667
2024-10-09 07:11:52,389 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=9496.0, ans=0.125
2024-10-09 07:11:59,449 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.805e+01 6.937e+01 7.778e+01 8.907e+01 1.216e+02, threshold=1.556e+02, percent-clipped=0.0
2024-10-09 07:11:59,630 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=9499.333333333334, ans=0.027086111111111113
2024-10-09 07:12:00,472 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.83 vs. limit=11.06225
2024-10-09 07:12:04,221 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=9499.333333333334, ans=0.0
2024-10-09 07:12:11,101 INFO [train.py:1152] Epoch 5, batch 1100, loss[loss=0.2452, ctc_loss=0.1847, attn_decoder_loss=0.2603, over 4865.00 frames. ], tot_loss[loss=0.2733, ctc_loss=0.2117, attn_decoder_loss=0.2887, over 964054.74 frames. ], batch size: 20, lr: 1.84e-02,
2024-10-09 07:12:11,314 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=9502.666666666666, ans=0.125
2024-10-09 07:12:15,603 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=9502.666666666666, ans=0.5674066666666667
2024-10-09 07:12:42,129 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.90 vs. limit=4.4264
2024-10-09 07:12:50,791 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.43 vs. limit=14.632
2024-10-09 07:13:04,791 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=9512.666666666666, ans=0.027030555555555558
2024-10-09 07:13:10,517 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=9516.0, ans=0.125
2024-10-09 07:13:12,877 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.46 vs. limit=9.758
2024-10-09 07:13:15,413 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=6.30 vs. limit=11.0685
2024-10-09 07:13:19,233 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=9516.0, ans=0.025
2024-10-09 07:13:23,441 INFO [train.py:1152] Epoch 5, batch 1150, loss[loss=0.261, ctc_loss=0.1997, attn_decoder_loss=0.2764, over 4862.00 frames. ], tot_loss[loss=0.2737, ctc_loss=0.2119, attn_decoder_loss=0.2891, over 964333.93 frames. ], batch size: 20, lr: 1.84e-02,
2024-10-09 07:14:18,252 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=9529.333333333334, ans=0.026961111111111113
2024-10-09 07:14:24,015 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.254e+01 7.108e+01 7.919e+01 8.705e+01 1.211e+02, threshold=1.584e+02, percent-clipped=0.0
2024-10-09 07:14:35,589 INFO [train.py:1152] Epoch 5, batch 1200, loss[loss=0.2751, ctc_loss=0.2055, attn_decoder_loss=0.2925, over 4807.00 frames. ], tot_loss[loss=0.2747, ctc_loss=0.2127, attn_decoder_loss=0.2902, over 964233.00 frames. ], batch size: 25, lr: 1.84e-02,
2024-10-09 07:14:50,102 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=9539.333333333334, ans=0.125
2024-10-09 07:15:03,140 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9542.666666666666, ans=0.20457333333333333
2024-10-09 07:15:11,679 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=9542.666666666666, ans=0.026905555555555558
2024-10-09 07:15:16,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=9542.666666666666, ans=0.0
2024-10-09 07:15:18,019 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.64 vs. limit=9.773
2024-10-09 07:15:26,020 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=9546.0, ans=0.008794347826086957
2024-10-09 07:15:38,174 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.10 vs. limit=14.661999999999999
2024-10-09 07:15:47,274 INFO [train.py:1152] Epoch 5, batch 1250, loss[loss=0.2653, ctc_loss=0.2031, attn_decoder_loss=0.2809, over 4757.00 frames. ], tot_loss[loss=0.2742, ctc_loss=0.2119, attn_decoder_loss=0.2898, over 964357.28 frames. ], batch size: 32, lr: 1.84e-02,
2024-10-09 07:16:02,439 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=9556.0, ans=0.008792173913043478
2024-10-09 07:16:09,978 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.17 vs. limit=4.4334
2024-10-09 07:16:23,584 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=9559.333333333334, ans=0.09899494936611666
2024-10-09 07:16:26,331 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=9559.333333333334, ans=0.0
2024-10-09 07:16:48,022 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.411e+01 7.394e+01 8.166e+01 9.311e+01 1.294e+02, threshold=1.633e+02, percent-clipped=0.0
2024-10-09 07:16:50,658 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=15.36 vs. limit=14.6745
2024-10-09 07:16:56,831 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=9566.0, ans=0.125
2024-10-09 07:16:59,624 INFO [train.py:1152] Epoch 5, batch 1300, loss[loss=0.318, ctc_loss=0.2704, attn_decoder_loss=0.33, over 4849.00 frames. ], tot_loss[loss=0.2742, ctc_loss=0.2118, attn_decoder_loss=0.2897, over 965764.91 frames. ], batch size: 43, lr: 1.84e-02,
2024-10-09 07:17:09,725 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=9569.333333333334, ans=0.125
2024-10-09 07:17:19,840 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=9572.666666666666, ans=0.026780555555555557
2024-10-09 07:17:26,979 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=9576.0, ans=0.56484
2024-10-09 07:17:45,929 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9579.333333333334, ans=0.20420666666666665
2024-10-09 07:18:11,830 INFO [train.py:1152] Epoch 5, batch 1350, loss[loss=0.2468, ctc_loss=0.1818, attn_decoder_loss=0.263, over 4844.00 frames. ], tot_loss[loss=0.2729, ctc_loss=0.2103, attn_decoder_loss=0.2885, over 966518.16 frames. ], batch size: 21, lr: 1.83e-02,
2024-10-09 07:18:23,126 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=17.02 vs. limit=14.689499999999999
2024-10-09 07:18:51,614 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.00 vs. limit=11.097249999999999
2024-10-09 07:18:52,348 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=9592.666666666666, ans=0.026697222222222226
2024-10-09 07:18:55,216 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9596.0, ans=0.20404
2024-10-09 07:18:56,682 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=9596.0, ans=0.125
2024-10-09 07:19:06,649 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=9596.0, ans=0.008783478260869566
2024-10-09 07:19:12,420 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.642e+01 6.777e+01 7.805e+01 9.037e+01 1.291e+02, threshold=1.561e+02, percent-clipped=0.0
2024-10-09 07:19:18,248 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=9599.333333333334, ans=0.125
2024-10-09 07:19:21,150 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=9599.333333333334, ans=0.125
2024-10-09 07:19:23,829 INFO [train.py:1152] Epoch 5, batch 1400, loss[loss=0.2382, ctc_loss=0.1678, attn_decoder_loss=0.2557, over 4940.00 frames. ], tot_loss[loss=0.2737, ctc_loss=0.2111, attn_decoder_loss=0.2893, over 966791.66 frames. ], batch size: 19, lr: 1.83e-02,
2024-10-09 07:19:26,116 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.86 vs. limit=14.702
2024-10-09 07:19:31,687 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.02 vs. limit=11.100999999999999
2024-10-09 07:20:02,980 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=9609.333333333334, ans=0.026627777777777778
2024-10-09 07:20:07,836 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.37 vs. limit=14.7095
2024-10-09 07:20:12,129 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.74 vs. limit=11.10475
2024-10-09 07:20:13,069 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=9612.666666666666, ans=0.125
2024-10-09 07:20:17,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=12.48 vs. limit=11.10475
2024-10-09 07:20:36,144 INFO [train.py:1152] Epoch 5, batch 1450, loss[loss=0.2824, ctc_loss=0.2377, attn_decoder_loss=0.2936, over 4834.00 frames. ], tot_loss[loss=0.2739, ctc_loss=0.2116, attn_decoder_loss=0.2894, over 966733.24 frames. ], batch size: 34, lr: 1.83e-02,
2024-10-09 07:20:42,110 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=9619.333333333334, ans=0.5633233333333334
2024-10-09 07:21:05,159 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=9626.0, ans=0.125
2024-10-09 07:21:13,858 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=9626.0, ans=0.125
2024-10-09 07:21:15,142 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=9626.0, ans=0.035
2024-10-09 07:21:16,693 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=9626.0, ans=0.125
2024-10-09 07:21:36,657 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.748e+01 7.322e+01 7.886e+01 8.588e+01 1.304e+02, threshold=1.577e+02, percent-clipped=0.0
2024-10-09 07:21:48,244 INFO [train.py:1152] Epoch 5, batch 1500, loss[loss=0.2793, ctc_loss=0.1895, attn_decoder_loss=0.3017, over 4698.00 frames. ], tot_loss[loss=0.2735, ctc_loss=0.2112, attn_decoder_loss=0.2891, over 966423.47 frames. ], batch size: 26, lr: 1.83e-02,
2024-10-09 07:21:52,781 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9636.0, ans=0.20364
2024-10-09 07:21:58,982 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.82 vs. limit=7.409
2024-10-09 07:22:20,187 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=9642.666666666666, ans=0.125
2024-10-09 07:22:33,298 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 07:22:35,254 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=12.53 vs. limit=11.11725
2024-10-09 07:23:00,346 INFO [train.py:1152] Epoch 5, batch 1550, loss[loss=0.2877, ctc_loss=0.2202, attn_decoder_loss=0.3045, over 4845.00 frames. ], tot_loss[loss=0.274, ctc_loss=0.2128, attn_decoder_loss=0.2894, over 966328.13 frames. ], batch size: 31, lr: 1.83e-02,
2024-10-09 07:23:30,799 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=9659.333333333334, ans=0.008769710144927537
2024-10-09 07:23:40,862 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=9659.333333333334, ans=0.026419444444444443
2024-10-09 07:23:51,819 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.65 vs. limit=14.747
2024-10-09 07:23:55,356 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9662.666666666666, ans=0.20337333333333335
2024-10-09 07:24:01,023 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.518e+01 6.722e+01 7.919e+01 8.788e+01 1.480e+02, threshold=1.584e+02, percent-clipped=0.0
2024-10-09 07:24:03,985 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=9666.0, ans=0.125
2024-10-09 07:24:08,757 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=16.19 vs. limit=14.749500000000001
2024-10-09 07:24:11,723 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.36 vs. limit=14.751999999999999
2024-10-09 07:24:12,469 INFO [train.py:1152] Epoch 5, batch 1600, loss[loss=0.2625, ctc_loss=0.202, attn_decoder_loss=0.2776, over 4818.00 frames. ], tot_loss[loss=0.2726, ctc_loss=0.2108, attn_decoder_loss=0.288, over 966550.04 frames. ], batch size: 25, lr: 1.83e-02,
2024-10-09 07:24:25,016 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.50 vs. limit=9.834666666666667
2024-10-09 07:24:47,099 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=9676.0, ans=0.026350000000000002
2024-10-09 07:25:04,352 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=9679.333333333334, ans=0.026336111111111112
2024-10-09 07:25:07,851 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=5.55 vs. limit=11.12975
2024-10-09 07:25:24,374 INFO [train.py:1152] Epoch 5, batch 1650, loss[loss=0.2677, ctc_loss=0.1828, attn_decoder_loss=0.289, over 4797.00 frames. ], tot_loss[loss=0.2737, ctc_loss=0.2126, attn_decoder_loss=0.289, over 967039.32 frames. ], batch size: 29, lr: 1.83e-02,
2024-10-09 07:25:26,007 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=9686.0, ans=0.00876391304347826
2024-10-09 07:25:29,822 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.67 vs. limit=9.843
2024-10-09 07:25:30,398 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=9686.0, ans=0.125
2024-10-09 07:25:31,074 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.71 vs. limit=14.7645
2024-10-09 07:25:31,332 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=7.23 vs. limit=7.4215
2024-10-09 07:25:38,981 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer_ff2.min_abs, batch_count=9689.333333333334, ans=0.1
2024-10-09 07:25:44,272 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=10.54 vs. limit=11.1335
2024-10-09 07:25:52,546 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.15 vs. limit=14.7695
2024-10-09 07:26:06,586 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=9696.0, ans=0.125
2024-10-09 07:26:11,404 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.53 vs. limit=4.4544
2024-10-09 07:26:12,307 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=9696.0, ans=0.125
2024-10-09 07:26:25,100 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.923e+01 7.014e+01 7.907e+01 8.615e+01 1.184e+02, threshold=1.581e+02, percent-clipped=0.0
2024-10-09 07:26:25,282 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=9699.333333333334, ans=0.0
2024-10-09 07:26:28,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=9699.333333333334, ans=0.5605233333333333
2024-10-09 07:26:36,686 INFO [train.py:1152] Epoch 5, batch 1700, loss[loss=0.2412, ctc_loss=0.1767, attn_decoder_loss=0.2573, over 4940.00 frames. ], tot_loss[loss=0.2727, ctc_loss=0.2102, attn_decoder_loss=0.2883, over 967044.60 frames. ], batch size: 19, lr: 1.82e-02,
2024-10-09 07:26:40,382 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=12.36 vs. limit=11.1385
2024-10-09 07:27:06,076 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=16.02 vs. limit=14.782
2024-10-09 07:27:08,633 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.83 vs. limit=14.782
2024-10-09 07:27:13,578 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.38 vs. limit=14.782
2024-10-09 07:27:18,074 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.31 vs. limit=14.782
2024-10-09 07:27:26,145 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten.whitening_limit, batch_count=9712.666666666666, ans=11.14225
2024-10-09 07:27:28,363 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=9712.666666666666, ans=0.008758115942028986
2024-10-09 07:27:39,600 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=9716.0, ans=0.125
2024-10-09 07:27:48,020 INFO [train.py:1152] Epoch 5, batch 1750, loss[loss=0.2317, ctc_loss=0.149, attn_decoder_loss=0.2524, over 4959.00 frames. ], tot_loss[loss=0.2709, ctc_loss=0.2079, attn_decoder_loss=0.2867, over 967182.88 frames. ], batch size: 19, lr: 1.82e-02,
2024-10-09 07:28:38,797 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=9729.333333333334, ans=0.125
2024-10-09 07:28:40,261 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer_ff3.min_abs, batch_count=9729.333333333334, ans=0.2
2024-10-09 07:28:48,734 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.83 vs. limit=9.866333333333333
2024-10-09 07:28:48,970 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.349e+01 6.419e+01 7.823e+01 9.036e+01 1.403e+02, threshold=1.565e+02, percent-clipped=0.0
2024-10-09 07:29:00,428 INFO [train.py:1152] Epoch 5, batch 1800, loss[loss=0.2504, ctc_loss=0.165, attn_decoder_loss=0.2717, over 4872.00 frames. ], tot_loss[loss=0.2721, ctc_loss=0.2101, attn_decoder_loss=0.2876, over 967936.48 frames. ], batch size: 23, lr: 1.82e-02,
2024-10-09 07:29:10,640 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9736.0, ans=0.20264
2024-10-09 07:29:18,384 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys.whitening_limit, batch_count=9739.333333333334, ans=4.4609000000000005
2024-10-09 07:29:45,468 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=9746.0, ans=0.125
2024-10-09 07:30:04,222 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=9749.333333333334, ans=0.125
2024-10-09 07:30:12,543 INFO [train.py:1152] Epoch 5, batch 1850, loss[loss=0.2874, ctc_loss=0.2341, attn_decoder_loss=0.3007, over 4712.00 frames. ], tot_loss[loss=0.2724, ctc_loss=0.2105, attn_decoder_loss=0.2878, over 968067.77 frames. ], batch size: 26, lr: 1.82e-02,
2024-10-09 07:30:12,760 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=9752.666666666666, ans=0.0
2024-10-09 07:30:15,673 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=9752.666666666666, ans=0.04949747468305833
2024-10-09 07:30:21,393 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=9752.666666666666, ans=0.025
2024-10-09 07:30:22,820 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=9752.666666666666, ans=0.20247333333333334
2024-10-09 07:30:24,368 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=9752.666666666666, ans=0.02603055555555556
2024-10-09 07:30:31,465 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9756.0, ans=0.20244
2024-10-09 07:30:59,328 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.32 vs. limit=4.4643999999999995
2024-10-09 07:31:01,562 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=9762.666666666666, ans=0.125
2024-10-09 07:31:12,806 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.748e+01 7.120e+01 7.886e+01 9.059e+01 1.763e+02, threshold=1.577e+02, percent-clipped=1.0
2024-10-09 07:31:24,235 INFO [train.py:1152] Epoch 5, batch 1900, loss[loss=0.2933, ctc_loss=0.2279, attn_decoder_loss=0.3097, over 4793.00 frames. ], tot_loss[loss=0.2723, ctc_loss=0.2112, attn_decoder_loss=0.2876, over 967890.41 frames. ], batch size: 29, lr: 1.82e-02,
2024-10-09 07:31:24,415 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=9769.333333333334, ans=0.025961111111111112
2024-10-09 07:31:41,584 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=9772.666666666666, ans=0.025947222222222226
2024-10-09 07:31:48,873 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=9772.666666666666, ans=0.125
2024-10-09 07:31:53,761 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.82 vs. limit=11.166
2024-10-09 07:31:55,354 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.05 vs. limit=11.166
2024-10-09 07:32:09,740 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=6.86 vs. limit=11.16725
2024-10-09 07:32:14,492 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=9779.333333333334, ans=0.125
2024-10-09 07:32:33,119 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=9782.666666666666, ans=0.025
2024-10-09 07:32:34,478 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=9786.0, ans=0.025891666666666667
2024-10-09 07:32:35,767 INFO [train.py:1152] Epoch 5, batch 1950, loss[loss=0.2613, ctc_loss=0.1867, attn_decoder_loss=0.2799, over 4856.00 frames. ], tot_loss[loss=0.273, ctc_loss=0.2111, attn_decoder_loss=0.2884, over 966771.07 frames. ], batch size: 20, lr: 1.82e-02,
2024-10-09 07:32:43,243 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=9786.0, ans=0.20214
2024-10-09 07:32:58,846 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 07:33:00,254 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=9789.333333333334, ans=0.5573733333333333
2024-10-09 07:33:00,289 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=9789.333333333334, ans=0.5573733333333333
2024-10-09 07:33:12,649 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=10.88 vs. limit=11.17225
2024-10-09 07:33:28,963 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=9796.0, ans=0.10938700000000001
2024-10-09 07:33:30,568 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=9796.0, ans=0.125
2024-10-09 07:33:36,487 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.634e+01 7.181e+01 7.930e+01 8.998e+01 1.359e+02, threshold=1.586e+02, percent-clipped=0.0
2024-10-09 07:33:38,911 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.29 vs. limit=4.4699
2024-10-09 07:33:42,337 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=9799.333333333334, ans=0.125
2024-10-09 07:33:48,020 INFO [train.py:1152] Epoch 5, batch 2000, loss[loss=0.2495, ctc_loss=0.1842, attn_decoder_loss=0.2658, over 4959.00 frames. ], tot_loss[loss=0.2733, ctc_loss=0.2114, attn_decoder_loss=0.2887, over 966600.73 frames. ], batch size: 19, lr: 1.82e-02,
2024-10-09 07:34:09,799 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=9806.0, ans=0.125
2024-10-09 07:34:17,004 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=9809.333333333334, ans=0.025
2024-10-09 07:34:29,155 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=18.47 vs. limit=14.857
2024-10-09 07:34:38,657 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=9812.666666666666, ans=0.025
2024-10-09 07:34:50,166 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=9816.0, ans=0.025
2024-10-09 07:34:51,591 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=9816.0, ans=0.008735652173913044
2024-10-09 07:34:59,944 INFO [train.py:1152] Epoch 5, batch 2050, loss[loss=0.225, ctc_loss=0.1637, attn_decoder_loss=0.2404, over 4911.00 frames. ], tot_loss[loss=0.2726, ctc_loss=0.2103, attn_decoder_loss=0.2882, over 966992.77 frames. ], batch size: 19, lr: 1.81e-02,
2024-10-09 07:35:18,922 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=9822.666666666666, ans=0.025
2024-10-09 07:35:19,018 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=9822.666666666666, ans=0.125
2024-10-09 07:35:23,314 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=9822.666666666666, ans=0.02573888888888889
2024-10-09 07:35:31,131 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.80 vs. limit=11.184750000000001
2024-10-09 07:35:31,294 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.46 vs. limit=4.4739
2024-10-09 07:35:36,779 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.63 vs. limit=11.184750000000001
2024-10-09 07:35:43,332 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=9829.333333333334, ans=0.125
2024-10-09 07:35:44,810 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=9829.333333333334, ans=0.125
2024-10-09 07:36:00,427 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.417e+01 6.899e+01 7.614e+01 8.267e+01 1.836e+02, threshold=1.523e+02, percent-clipped=1.0
2024-10-09 07:36:12,339 INFO [train.py:1152] Epoch 5, batch 2100, loss[loss=0.2538, ctc_loss=0.1747, attn_decoder_loss=0.2735, over 4848.00 frames. ], tot_loss[loss=0.2726, ctc_loss=0.2106, attn_decoder_loss=0.2882, over 967189.79 frames. ], batch size: 21, lr: 1.81e-02,
2024-10-09 07:36:18,762 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.44 vs. limit=14.876999999999999
2024-10-09 07:36:31,004 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=9839.333333333334, ans=0.008730579710144928
2024-10-09 07:36:39,593 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=9842.666666666666, ans=0.125
2024-10-09 07:36:43,962 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=9842.666666666666, ans=0.02565555555555556
2024-10-09 07:36:55,672 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=9846.0, ans=0.55539
2024-10-09 07:37:13,093 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9849.333333333334, ans=0.20150666666666667
2024-10-09 07:37:24,617 INFO [train.py:1152] Epoch 5, batch 2150, loss[loss=0.247, ctc_loss=0.1705, attn_decoder_loss=0.2661, over 4857.00 frames. ], tot_loss[loss=0.2718, ctc_loss=0.209, attn_decoder_loss=0.2875, over 967989.38 frames. ], batch size: 20, lr: 1.81e-02,
2024-10-09 07:37:26,285 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.prob, batch_count=9852.666666666666, ans=0.125
2024-10-09 07:37:38,203 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=9856.0, ans=0.0256
2024-10-09 07:38:00,787 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=9859.333333333334, ans=0.5549233333333333
2024-10-09 07:38:05,197 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=9859.333333333334, ans=0.125
2024-10-09 07:38:20,201 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.89 vs. limit=9.931333333333333
2024-10-09 07:38:20,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=9866.0, ans=0.125
2024-10-09 07:38:21,606 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.12 vs. limit=11.19975
2024-10-09 07:38:22,191 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=9866.0, ans=0.125
2024-10-09 07:38:24,994 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.423e+01 6.797e+01 7.760e+01 8.776e+01 1.975e+02, threshold=1.552e+02, percent-clipped=1.0
2024-10-09 07:38:36,454 INFO [train.py:1152] Epoch 5, batch 2200, loss[loss=0.281, ctc_loss=0.2047, attn_decoder_loss=0.3, over 4771.00 frames. ], tot_loss[loss=0.2707, ctc_loss=0.2076, attn_decoder_loss=0.2865, over 967731.18 frames. ], batch size: 26, lr: 1.81e-02,
2024-10-09 07:38:40,265 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.05 vs. limit=11.201
2024-10-09 07:38:49,629 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9872.666666666666, ans=0.20127333333333333
2024-10-09 07:39:22,069 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.91 vs. limit=14.909500000000001
2024-10-09 07:39:48,820 INFO [train.py:1152] Epoch 5, batch 2250, loss[loss=0.2391, ctc_loss=0.151, attn_decoder_loss=0.2611, over 4875.00 frames. ], tot_loss[loss=0.2716, ctc_loss=0.2088, attn_decoder_loss=0.2873, over 967696.12 frames. ], batch size: 22, lr: 1.81e-02,
2024-10-09 07:40:11,544 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.46 vs. limit=14.917
2024-10-09 07:40:12,254 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=9889.333333333334, ans=0.008719710144927537
2024-10-09 07:40:23,924 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=9892.666666666666, ans=0.125
2024-10-09 07:40:26,809 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=9892.666666666666, ans=0.0
2024-10-09 07:40:39,299 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.85 vs. limit=14.922
2024-10-09 07:40:49,781 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.314e+01 6.903e+01 7.703e+01 8.615e+01 1.192e+02, threshold=1.541e+02, percent-clipped=0.0
2024-10-09 07:41:01,450 INFO [train.py:1152] Epoch 5, batch 2300, loss[loss=0.2167, ctc_loss=0.1267, attn_decoder_loss=0.2391, over 4883.00 frames. ], tot_loss[loss=0.2707, ctc_loss=0.2075, attn_decoder_loss=0.2865, over 968347.95 frames. ], batch size: 19, lr: 1.81e-02,
2024-10-09 07:41:01,616 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9902.666666666666, ans=0.20097333333333334
2024-10-09 07:41:30,498 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=9909.333333333334, ans=0.125
2024-10-09 07:41:34,861 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=9909.333333333334, ans=0.125
2024-10-09 07:41:44,942 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=9912.666666666666, ans=0.02536388888888889
2024-10-09 07:41:46,411 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=9912.666666666666, ans=0.125
2024-10-09 07:41:54,321 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.48 vs. limit=11.21725
2024-10-09 07:42:02,243 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=9916.0, ans=0.55294
2024-10-09 07:42:09,476 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=9916.0, ans=0.125
2024-10-09 07:42:13,708 INFO [train.py:1152] Epoch 5, batch 2350, loss[loss=0.2778, ctc_loss=0.215, attn_decoder_loss=0.2935, over 4844.00 frames. ], tot_loss[loss=0.27, ctc_loss=0.2061, attn_decoder_loss=0.286, over 968402.15 frames. ], batch size: 23, lr: 1.81e-02,
2024-10-09 07:42:40,555 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=11.32 vs. limit=11.221
2024-10-09 07:42:41,106 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=9926.0, ans=0.125
2024-10-09 07:42:43,924 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=9926.0, ans=0.025308333333333335
2024-10-09 07:42:55,902 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.51 vs. limit=9.964666666666666
2024-10-09 07:43:08,399 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=9929.333333333334, ans=0.008711014492753624
2024-10-09 07:43:14,045 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.566e+01 6.926e+01 7.720e+01 8.590e+01 1.379e+02, threshold=1.544e+02, percent-clipped=0.0
2024-10-09 07:43:14,264 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=9932.666666666666, ans=0.5523566666666667
2024-10-09 07:43:14,304 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=9932.666666666666, ans=0.125
2024-10-09 07:43:25,524 INFO [train.py:1152] Epoch 5, batch 2400, loss[loss=0.2903, ctc_loss=0.2207, attn_decoder_loss=0.3077, over 4750.00 frames. ], tot_loss[loss=0.2715, ctc_loss=0.2082, attn_decoder_loss=0.2873, over 967584.60 frames. ], batch size: 19, lr: 1.80e-02,
2024-10-09 07:43:27,750 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.59 vs. limit=7.484
2024-10-09 07:44:05,628 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=9942.666666666666, ans=0.34914
2024-10-09 07:44:13,450 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.95 vs. limit=7.4864999999999995
2024-10-09 07:44:23,707 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=10.08 vs. limit=9.974666666666668
2024-10-09 07:44:37,023 INFO [train.py:1152] Epoch 5, batch 2450, loss[loss=0.248, ctc_loss=0.178, attn_decoder_loss=0.2655, over 4885.00 frames. ], tot_loss[loss=0.2725, ctc_loss=0.2099, attn_decoder_loss=0.2881, over 966944.55 frames. ], batch size: 22, lr: 1.80e-02,
2024-10-09 07:44:48,751 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=9952.666666666666, ans=0.025197222222222225
2024-10-09 07:45:08,310 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.32 vs. limit=14.9695
2024-10-09 07:45:22,049 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9962.666666666666, ans=0.20037333333333335
2024-10-09 07:45:27,747 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=9962.666666666666, ans=0.125
2024-10-09 07:45:29,700 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.59 vs. limit=14.972000000000001
2024-10-09 07:45:32,166 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=9962.666666666666, ans=0.125
2024-10-09 07:45:37,078 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.51 vs. limit=11.23725
2024-10-09 07:45:37,628 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.415e+01 7.063e+01 8.130e+01 9.400e+01 1.353e+02, threshold=1.626e+02, percent-clipped=0.0
2024-10-09 07:45:39,227 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=9966.0, ans=0.125
2024-10-09 07:45:49,278 INFO [train.py:1152] Epoch 5, batch 2500, loss[loss=0.2571, ctc_loss=0.2183, attn_decoder_loss=0.2668, over 4727.00 frames. ], tot_loss[loss=0.2728, ctc_loss=0.2103, attn_decoder_loss=0.2884, over 966664.12 frames. ], batch size: 26, lr: 1.80e-02,
2024-10-09 07:46:14,016 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=9972.666666666666, ans=0.125
2024-10-09 07:46:29,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=9976.0, ans=0.55084
2024-10-09 07:46:31,798 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=5.91 vs. limit=11.24225
2024-10-09 07:46:59,455 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.30 vs. limit=14.987
2024-10-09 07:47:01,632 INFO [train.py:1152] Epoch 5, batch 2550, loss[loss=0.2465, ctc_loss=0.1867, attn_decoder_loss=0.2614, over 4959.00 frames. ], tot_loss[loss=0.2726, ctc_loss=0.2096, attn_decoder_loss=0.2883, over 966955.94 frames. ], batch size: 19, lr: 1.80e-02,
2024-10-09 07:47:11,755 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=9986.0, ans=0.55049
2024-10-09 07:47:17,702 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 07:47:19,184 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=9989.333333333334, ans=0.125
2024-10-09 07:47:28,997 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.14 vs. limit=7.995733333333334
2024-10-09 07:47:31,979 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=4.06 vs. limit=7.997066666666667
2024-10-09 07:47:56,531 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=9996.0, ans=0.125
2024-10-09 07:48:02,069 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.229e+01 6.977e+01 7.916e+01 8.767e+01 1.613e+02, threshold=1.583e+02, percent-clipped=0.0
2024-10-09 07:48:05,021 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=9999.333333333334, ans=0.125
2024-10-09 07:48:13,377 INFO [train.py:1152] Epoch 5, batch 2600, loss[loss=0.2433, ctc_loss=0.1784, attn_decoder_loss=0.2595, over 4860.00 frames. ], tot_loss[loss=0.2727, ctc_loss=0.21, attn_decoder_loss=0.2884, over 966357.51 frames. ], batch size: 20, lr: 1.80e-02,
2024-10-09 07:48:25,034 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=10002.666666666666, ans=0.125
2024-10-09 07:48:30,623 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=10006.0, ans=0.54979
2024-10-09 07:48:30,673 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=10006.0, ans=0.54979
2024-10-09 07:48:52,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=10009.333333333334, ans=0.125
2024-10-09 07:48:54,899 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=10012.666666666666, ans=0.125
2024-10-09 07:49:25,140 INFO [train.py:1152] Epoch 5, batch 2650, loss[loss=0.3367, ctc_loss=0.3177, attn_decoder_loss=0.3415, over 4833.00 frames. ], tot_loss[loss=0.2728, ctc_loss=0.2096, attn_decoder_loss=0.2886, over 966014.43 frames. ], batch size: 38, lr: 1.80e-02,
2024-10-09 07:49:38,505 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=10022.666666666666, ans=0.125
2024-10-09 07:49:52,941 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=10026.0, ans=0.5490900000000001
2024-10-09 07:49:54,416 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=10026.0, ans=0.125
2024-10-09 07:50:25,746 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.779e+01 7.045e+01 7.972e+01 8.824e+01 1.325e+02, threshold=1.594e+02, percent-clipped=0.0
2024-10-09 07:50:26,067 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=10032.666666666666, ans=0.125
2024-10-09 07:50:37,560 INFO [train.py:1152] Epoch 5, batch 2700, loss[loss=0.2908, ctc_loss=0.2273, attn_decoder_loss=0.3067, over 4877.00 frames. ], tot_loss[loss=0.2732, ctc_loss=0.2106, attn_decoder_loss=0.2889, over 966188.02 frames. ], batch size: 28, lr: 1.80e-02,
2024-10-09 07:50:42,070 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=10036.0, ans=0.02485
2024-10-09 07:50:45,016 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=10036.0, ans=0.125
2024-10-09 07:50:52,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=10039.333333333334, ans=0.008687101449275363
2024-10-09 07:51:18,206 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=10042.666666666666, ans=0.5485066666666667
2024-10-09 07:51:46,207 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.82 vs. limit=11.2685
2024-10-09 07:51:49,914 INFO [train.py:1152] Epoch 5, batch 2750, loss[loss=0.2605, ctc_loss=0.1555, attn_decoder_loss=0.2868, over 4801.00 frames. ], tot_loss[loss=0.2715, ctc_loss=0.2073, attn_decoder_loss=0.2875, over 966945.19 frames. ], batch size: 19, lr: 1.79e-02,
2024-10-09 07:52:29,592 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.80 vs. limit=15.0445
2024-10-09 07:52:37,655 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=10062.666666666666, ans=0.5478066666666668
2024-10-09 07:52:47,650 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=10066.0, ans=0.025
2024-10-09 07:52:50,561 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.905e+01 6.567e+01 7.626e+01 8.860e+01 1.494e+02, threshold=1.525e+02, percent-clipped=0.0
2024-10-09 07:52:50,731 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=10066.0, ans=0.54769
2024-10-09 07:53:02,122 INFO [train.py:1152] Epoch 5, batch 2800, loss[loss=0.335, ctc_loss=0.2932, attn_decoder_loss=0.3455, over 4782.00 frames. ], tot_loss[loss=0.2711, ctc_loss=0.2064, attn_decoder_loss=0.2872, over 966997.33 frames. ], batch size: 53, lr: 1.79e-02,
2024-10-09 07:53:08,545 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.21 vs. limit=15.052
2024-10-09 07:53:19,428 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10072.666666666666, ans=0.19927333333333333
2024-10-09 07:53:28,047 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=10072.666666666666, ans=0.125
2024-10-09 07:53:30,828 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=10076.0, ans=0.125
2024-10-09 07:53:41,173 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.53 vs. limit=10.038
2024-10-09 07:53:41,427 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.07 vs. limit=11.278500000000001
2024-10-09 07:53:46,585 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=10079.333333333334, ans=0.00867840579710145
2024-10-09 07:53:58,211 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10082.666666666666, ans=0.19917333333333334
2024-10-09 07:54:13,843 INFO [train.py:1152] Epoch 5, batch 2850, loss[loss=0.2693, ctc_loss=0.1915, attn_decoder_loss=0.2887, over 4930.00 frames. ], tot_loss[loss=0.2723, ctc_loss=0.2079, attn_decoder_loss=0.2884, over 966831.27 frames. ], batch size: 20, lr: 1.79e-02,
2024-10-09 07:54:36,225 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.04 vs. limit=15.067
2024-10-09 07:54:54,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.20 vs. limit=7.523166666666667
2024-10-09 07:55:12,944 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=10099.333333333334, ans=0.125
2024-10-09 07:55:14,183 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.454e+01 7.032e+01 7.817e+01 8.792e+01 1.714e+02, threshold=1.563e+02, percent-clipped=2.0
2024-10-09 07:55:14,391 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=10099.333333333334, ans=0.19900666666666667
2024-10-09 07:55:25,691 INFO [train.py:1152] Epoch 5, batch 2900, loss[loss=0.2418, ctc_loss=0.1501, attn_decoder_loss=0.2647, over 4759.00 frames. ], tot_loss[loss=0.2719, ctc_loss=0.2075, attn_decoder_loss=0.288, over 965955.50 frames. ], batch size: 20, lr: 1.79e-02,
2024-10-09 07:55:47,697 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=10106.0, ans=0.024558333333333335
2024-10-09 07:55:49,788 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.15 vs. limit=7.5265
2024-10-09 07:56:05,098 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=10109.333333333334, ans=0.5461733333333334
2024-10-09 07:56:09,336 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=10112.666666666666, ans=0.025
2024-10-09 07:56:16,715 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=10112.666666666666, ans=0.125
2024-10-09 07:56:31,929 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=4.12 vs. limit=8.0464
2024-10-09 07:56:36,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=10119.333333333334, ans=0.125
2024-10-09 07:56:37,119 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.38 vs. limit=11.29475
2024-10-09 07:56:37,960 INFO [train.py:1152] Epoch 5, batch 2950, loss[loss=0.2736, ctc_loss=0.1912, attn_decoder_loss=0.2942, over 4797.00 frames. ], tot_loss[loss=0.2711, ctc_loss=0.2066, attn_decoder_loss=0.2873, over 966541.58 frames. ], batch size: 19, lr: 1.79e-02,
2024-10-09 07:56:51,546 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=2.510e-03
2024-10-09 07:56:52,922 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=10122.666666666666, ans=0.5457066666666668
2024-10-09 07:57:38,640 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.794e+01 6.975e+01 7.826e+01 8.367e+01 1.162e+02, threshold=1.565e+02, percent-clipped=0.0
2024-10-09 07:57:44,582 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=10132.666666666666, ans=0.125
2024-10-09 07:57:50,155 INFO [train.py:1152] Epoch 5, batch 3000, loss[loss=0.3011, ctc_loss=0.2581, attn_decoder_loss=0.3118, over 4828.00 frames. ], tot_loss[loss=0.2704, ctc_loss=0.2061, attn_decoder_loss=0.2865, over 967193.04 frames. ], batch size: 21, lr: 1.79e-02,
2024-10-09 07:57:50,155 INFO [train.py:1175] Computing validation loss
2024-10-09 07:57:58,968 INFO [train.py:1184] Epoch 5, validation: loss=0.2152, ctc_loss=0.07864, attn_decoder_loss=0.2493, over 90464.00 frames.
2024-10-09 07:57:58,969 INFO [train.py:1185] Maximum memory allocated so far is 6613MB
2024-10-09 07:58:00,601 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=10136.0, ans=0.024433333333333335
2024-10-09 07:58:04,825 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10136.0, ans=0.19863999999999998
2024-10-09 07:58:26,199 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=10142.666666666666, ans=0.125
2024-10-09 07:58:57,388 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.60 vs. limit=15.112
2024-10-09 07:59:10,688 INFO [train.py:1152] Epoch 5, batch 3050, loss[loss=0.2439, ctc_loss=0.166, attn_decoder_loss=0.2634, over 4751.00 frames. ], tot_loss[loss=0.2718, ctc_loss=0.2076, attn_decoder_loss=0.2878, over 966691.06 frames. ], batch size: 19, lr: 1.79e-02,
2024-10-09 07:59:18,769 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.95 vs. limit=15.1145
2024-10-09 07:59:21,604 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.10 vs. limit=11.30725
2024-10-09 07:59:24,826 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=6.12 vs. limit=6.0312
2024-10-09 07:59:33,108 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=16.50 vs. limit=15.117
2024-10-09 07:59:33,911 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=10156.0, ans=0.125
2024-10-09 07:59:55,405 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10162.666666666666, ans=0.19837333333333335
2024-10-09 08:00:11,208 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.351e+01 7.118e+01 7.771e+01 8.814e+01 6.357e+02, threshold=1.554e+02, percent-clipped=2.0
2024-10-09 08:00:15,730 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=10166.0, ans=0.125
2024-10-09 08:00:17,304 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=10166.0, ans=0.125
2024-10-09 08:00:22,835 INFO [train.py:1152] Epoch 5, batch 3100, loss[loss=0.2973, ctc_loss=0.2335, attn_decoder_loss=0.3132, over 4843.00 frames. ], tot_loss[loss=0.2713, ctc_loss=0.2062, attn_decoder_loss=0.2875, over 966476.26 frames. ], batch size: 38, lr: 1.78e-02,
2024-10-09 08:00:25,727 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=10169.333333333334, ans=0.19830666666666666
2024-10-09 08:00:36,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=16.71 vs. limit=15.1295
2024-10-09 08:00:37,063 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.14 vs. limit=10.086333333333332
2024-10-09 08:01:35,157 INFO [train.py:1152] Epoch 5, batch 3150, loss[loss=0.2772, ctc_loss=0.2364, attn_decoder_loss=0.2874, over 4779.00 frames. ], tot_loss[loss=0.2709, ctc_loss=0.2063, attn_decoder_loss=0.2871, over 966695.35 frames. ], batch size: 40, lr: 1.78e-02,
2024-10-09 08:01:36,798 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=10186.0, ans=0.19813999999999998
2024-10-09 08:01:40,022 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=11.72 vs. limit=11.319749999999999
2024-10-09 08:02:00,158 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 08:02:04,394 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=10192.666666666666, ans=0.024197222222222224
2024-10-09 08:02:18,147 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.37 vs. limit=11.3235
2024-10-09 08:02:28,087 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=16.23 vs. limit=15.147
2024-10-09 08:02:30,571 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=10196.0, ans=0.125
2024-10-09 08:02:36,274 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.433e+01 6.869e+01 7.697e+01 8.692e+01 1.238e+02, threshold=1.539e+02, percent-clipped=0.0
2024-10-09 08:02:47,721 INFO [train.py:1152] Epoch 5, batch 3200, loss[loss=0.2613, ctc_loss=0.1916, attn_decoder_loss=0.2787, over 4750.00 frames. ], tot_loss[loss=0.2712, ctc_loss=0.2074, attn_decoder_loss=0.2872, over 967311.93 frames. ], batch size: 20, lr: 1.78e-02,
2024-10-09 08:03:03,549 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=10206.0, ans=0.125
2024-10-09 08:03:31,406 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.89 vs. limit=15.159500000000001
2024-10-09 08:03:32,205 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=10212.666666666666, ans=0.125
2024-10-09 08:03:32,266 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=10212.666666666666, ans=0.125
2024-10-09 08:03:59,426 INFO [train.py:1152] Epoch 5, batch 3250, loss[loss=0.2957, ctc_loss=0.2459, attn_decoder_loss=0.3081, over 4849.00 frames. ], tot_loss[loss=0.2707, ctc_loss=0.2074, attn_decoder_loss=0.2866, over 967368.35 frames. ], batch size: 24, lr: 1.78e-02,
2024-10-09 08:03:59,645 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=10219.333333333334, ans=0.02408611111111111
2024-10-09 08:04:01,543 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.42 vs. limit=11.33225
2024-10-09 08:04:12,732 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10222.666666666666, ans=0.19777333333333333
2024-10-09 08:04:19,911 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=10222.666666666666, ans=0.008647246376811595
2024-10-09 08:04:22,703 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10222.666666666666, ans=0.19777333333333333
2024-10-09 08:04:22,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=10222.666666666666, ans=0.125
2024-10-09 08:04:27,020 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=10226.0, ans=0.54209
2024-10-09 08:04:40,607 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=6.26 vs. limit=8.090399999999999
2024-10-09 08:04:55,969 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=10232.666666666666, ans=0.125
2024-10-09 08:04:57,391 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=10232.666666666666, ans=0.125
2024-10-09 08:05:00,091 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.598e+01 6.726e+01 7.781e+01 8.796e+01 1.292e+02, threshold=1.556e+02, percent-clipped=0.0
2024-10-09 08:05:03,781 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.94 vs. limit=8.093066666666665
2024-10-09 08:05:11,732 INFO [train.py:1152] Epoch 5, batch 3300, loss[loss=0.2768, ctc_loss=0.2038, attn_decoder_loss=0.2951, over 4836.00 frames. ], tot_loss[loss=0.2694, ctc_loss=0.2055, attn_decoder_loss=0.2854, over 967763.48 frames. ], batch size: 43, lr: 1.78e-02,
2024-10-09 08:05:21,812 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=10236.0, ans=0.125
2024-10-09 08:05:42,026 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=10242.666666666666, ans=0.008642898550724637
2024-10-09 08:06:01,696 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=10246.0, ans=0.125
2024-10-09 08:06:05,147 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.04 vs. limit=15.1845
2024-10-09 08:06:08,743 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=10249.333333333334, ans=0.125
2024-10-09 08:06:11,987 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=11.17 vs. limit=11.3435
2024-10-09 08:06:14,492 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=10249.333333333334, ans=0.125
2024-10-09 08:06:20,153 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 08:06:22,829 INFO [train.py:1152] Epoch 5, batch 3350, loss[loss=0.2787, ctc_loss=0.2157, attn_decoder_loss=0.2944, over 4783.00 frames. ], tot_loss[loss=0.2693, ctc_loss=0.2056, attn_decoder_loss=0.2853, over 967037.70 frames. ], batch size: 40, lr: 1.78e-02,
2024-10-09 08:06:27,741 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.59 vs. limit=8.101066666666666
2024-10-09 08:06:41,523 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10256.0, ans=0.19744
2024-10-09 08:06:43,555 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=20.66 vs. limit=15.192
2024-10-09 08:07:06,623 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.67 vs. limit=15.197
2024-10-09 08:07:15,557 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=6.14 vs. limit=6.052533333333333
2024-10-09 08:07:23,193 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.299e+01 6.790e+01 7.712e+01 8.553e+01 1.216e+02, threshold=1.542e+02, percent-clipped=0.0
2024-10-09 08:07:34,674 INFO [train.py:1152] Epoch 5, batch 3400, loss[loss=0.2486, ctc_loss=0.1714, attn_decoder_loss=0.2679, over 4959.00 frames. ], tot_loss[loss=0.2697, ctc_loss=0.2066, attn_decoder_loss=0.2854, over 966913.67 frames. ], batch size: 19, lr: 1.78e-02,
2024-10-09 08:07:34,778 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=10269.333333333334, ans=0.02387777777777778
2024-10-09 08:08:06,581 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=10276.0, ans=0.023850000000000003
2024-10-09 08:08:14,524 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=2.73 vs. limit=11.3535
2024-10-09 08:08:37,806 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.33 vs. limit=11.356
2024-10-09 08:08:44,108 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=10282.666666666666, ans=0.008634202898550726
2024-10-09 08:08:46,900 INFO [train.py:1152] Epoch 5, batch 3450, loss[loss=0.2895, ctc_loss=0.2416, attn_decoder_loss=0.3014, over 4828.00 frames. ], tot_loss[loss=0.2713, ctc_loss=0.2089, attn_decoder_loss=0.2869, over 967062.09 frames. ], batch size: 43, lr: 1.77e-02,
2024-10-09 08:08:48,550 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=10286.0, ans=0.125
2024-10-09 08:08:55,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10286.0, ans=0.19713999999999998
2024-10-09 08:09:15,773 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=10292.666666666666, ans=0.023780555555555558
2024-10-09 08:09:26,808 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=4.04 vs. limit=8.117066666666666
2024-10-09 08:09:43,306 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=10299.333333333334, ans=0.125
2024-10-09 08:09:47,452 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.888e+01 6.961e+01 8.084e+01 8.867e+01 1.460e+02, threshold=1.617e+02, percent-clipped=0.0
2024-10-09 08:09:59,085 INFO [train.py:1152] Epoch 5, batch 3500, loss[loss=0.2415, ctc_loss=0.154, attn_decoder_loss=0.2633, over 4883.00 frames. ], tot_loss[loss=0.27, ctc_loss=0.2076, attn_decoder_loss=0.2856, over 967424.90 frames. ], batch size: 19, lr: 1.77e-02,
2024-10-09 08:10:01,150 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.66 vs. limit=7.575666666666667
2024-10-09 08:10:16,487 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=10306.0, ans=0.53929
2024-10-09 08:10:40,434 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.08 vs. limit=10.154666666666667
2024-10-09 08:11:02,845 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10316.0, ans=0.19684000000000001
2024-10-09 08:11:11,476 INFO [train.py:1152] Epoch 5, batch 3550, loss[loss=0.2593, ctc_loss=0.1877, attn_decoder_loss=0.2772, over 4771.00 frames. ], tot_loss[loss=0.268, ctc_loss=0.2042, attn_decoder_loss=0.2839, over 967383.51 frames. ], batch size: 29, lr: 1.77e-02,
2024-10-09 08:11:13,596 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.97 vs. limit=15.2395
2024-10-09 08:11:18,010 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.44 vs. limit=15.2395
2024-10-09 08:11:26,112 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=10322.666666666666, ans=0.023655555555555558
2024-10-09 08:11:29,639 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.85 vs. limit=11.371
2024-10-09 08:11:43,559 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10326.0, ans=0.19674
2024-10-09 08:11:47,037 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.37 vs. limit=7.5815
2024-10-09 08:11:47,152 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.09 vs. limit=11.372250000000001
2024-10-09 08:11:57,253 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=6.31 vs. limit=11.3735
2024-10-09 08:12:00,270 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.01 vs. limit=15.247
2024-10-09 08:12:12,443 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.676e+01 6.654e+01 7.562e+01 8.432e+01 1.116e+02, threshold=1.512e+02, percent-clipped=0.0
2024-10-09 08:12:24,016 INFO [train.py:1152] Epoch 5, batch 3600, loss[loss=0.2422, ctc_loss=0.1912, attn_decoder_loss=0.2549, over 4938.00 frames. ], tot_loss[loss=0.2683, ctc_loss=0.2041, attn_decoder_loss=0.2844, over 967481.02 frames. ], batch size: 20, lr: 1.77e-02,
2024-10-09 08:12:31,312 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=10336.0, ans=0.53824
2024-10-09 08:12:34,218 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.attention_skip_rate, batch_count=10336.0, ans=0.023600000000000003
2024-10-09 08:12:38,519 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=10339.333333333334, ans=0.125
2024-10-09 08:13:01,658 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=10342.666666666666, ans=0.125
2024-10-09 08:13:11,794 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=10346.0, ans=0.008620434782608696
2024-10-09 08:13:20,904 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten.whitening_limit, batch_count=10349.333333333334, ans=11.381
2024-10-09 08:13:21,190 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.86 vs. limit=7.5873333333333335
2024-10-09 08:13:36,012 INFO [train.py:1152] Epoch 5, batch 3650, loss[loss=0.2844, ctc_loss=0.2338, attn_decoder_loss=0.2971, over 4879.00 frames. ], tot_loss[loss=0.2678, ctc_loss=0.2038, attn_decoder_loss=0.2838, over 967958.29 frames. ], batch size: 31, lr: 1.77e-02,
2024-10-09 08:14:35,907 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.378e+01 6.799e+01 7.862e+01 8.632e+01 1.164e+02, threshold=1.572e+02, percent-clipped=0.0
2024-10-09 08:14:37,486 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10366.0, ans=0.19634000000000001
2024-10-09 08:14:46,205 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10369.333333333334, ans=0.19630666666666663
2024-10-09 08:14:47,470 INFO [train.py:1152] Epoch 5, batch 3700, loss[loss=0.2534, ctc_loss=0.1805, attn_decoder_loss=0.2716, over 4844.00 frames. ], tot_loss[loss=0.2672, ctc_loss=0.2022, attn_decoder_loss=0.2834, over 967353.81 frames. ], batch size: 24, lr: 1.77e-02,
2024-10-09 08:15:02,003 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=10372.666666666666, ans=0.07
2024-10-09 08:15:13,802 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=10372.666666666666, ans=0.125
2024-10-09 08:15:36,994 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.max_abs, batch_count=10379.333333333334, ans=10.0
2024-10-09 08:15:51,697 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 08:15:59,988 INFO [train.py:1152] Epoch 5, batch 3750, loss[loss=0.2559, ctc_loss=0.192, attn_decoder_loss=0.2719, over 4959.00 frames. ], tot_loss[loss=0.2672, ctc_loss=0.202, attn_decoder_loss=0.2835, over 967661.90 frames. ], batch size: 19, lr: 1.77e-02,
2024-10-09 08:16:05,380 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1.whitening_limit, batch_count=10386.0, ans=7.5965
2024-10-09 08:16:24,891 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=10389.333333333334, ans=0.008611014492753623
2024-10-09 08:16:26,287 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=10389.333333333334, ans=0.5363733333333334
2024-10-09 08:16:27,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=10392.666666666666, ans=0.125
2024-10-09 08:17:00,956 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.369e+01 7.030e+01 7.885e+01 8.691e+01 1.283e+02, threshold=1.577e+02, percent-clipped=0.0
2024-10-09 08:17:09,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=10399.333333333334, ans=0.023336111111111113
2024-10-09 08:17:12,568 INFO [train.py:1152] Epoch 5, batch 3800, loss[loss=0.2709, ctc_loss=0.2098, attn_decoder_loss=0.2861, over 4748.00 frames. ], tot_loss[loss=0.2679, ctc_loss=0.2025, attn_decoder_loss=0.2843, over 967415.33 frames. ], batch size: 26, lr: 1.77e-02,
2024-10-09 08:17:56,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=10412.666666666666, ans=0.025
2024-10-09 08:18:11,831 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=10416.0, ans=0.023266666666666668
2024-10-09 08:18:11,842 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=10416.0, ans=0.53544
2024-10-09 08:18:14,778 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=10416.0, ans=0.023266666666666668
2024-10-09 08:18:24,222 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.20 vs. limit=4.5629
2024-10-09 08:18:24,649 INFO [train.py:1152] Epoch 5, batch 3850, loss[loss=0.2878, ctc_loss=0.2274, attn_decoder_loss=0.3029, over 4820.00 frames. ], tot_loss[loss=0.2675, ctc_loss=0.202, attn_decoder_loss=0.2838, over 967459.87 frames. ], batch size: 38, lr: 1.76e-02,
2024-10-09 08:18:28,226 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.48 vs. limit=7.604833333333334
2024-10-09 08:18:49,995 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.67 vs. limit=15.317
2024-10-09 08:18:50,840 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=10422.666666666666, ans=0.0
2024-10-09 08:19:03,820 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=10426.0, ans=0.19574
2024-10-09 08:19:06,696 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=10429.333333333334, ans=0.025
2024-10-09 08:19:09,541 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=10429.333333333334, ans=0.5349733333333333
2024-10-09 08:19:16,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=10429.333333333334, ans=0.008602318840579711
2024-10-09 08:19:20,941 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=10432.666666666666, ans=0.023197222222222227
2024-10-09 08:19:24,981 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.516e+01 7.099e+01 8.006e+01 9.226e+01 1.322e+02, threshold=1.601e+02, percent-clipped=0.0
2024-10-09 08:19:35,230 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=10436.0, ans=0.00860086956521739
2024-10-09 08:19:36,547 INFO [train.py:1152] Epoch 5, batch 3900, loss[loss=0.2702, ctc_loss=0.2076, attn_decoder_loss=0.2858, over 4748.00 frames. ], tot_loss[loss=0.2683, ctc_loss=0.2021, attn_decoder_loss=0.2848, over 966946.03 frames. ], batch size: 26, lr: 1.76e-02,
2024-10-09 08:19:55,702 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.13 vs. limit=11.41475
2024-10-09 08:20:18,722 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.61 vs. limit=11.41725
2024-10-09 08:20:22,663 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=10446.0, ans=0.125
2024-10-09 08:20:25,518 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=10446.0, ans=0.05
2024-10-09 08:20:29,877 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=10446.0, ans=0.125
2024-10-09 08:20:48,437 INFO [train.py:1152] Epoch 5, batch 3950, loss[loss=0.3028, ctc_loss=0.2402, attn_decoder_loss=0.3184, over 4827.00 frames. ], tot_loss[loss=0.267, ctc_loss=0.2003, attn_decoder_loss=0.2836, over 967138.74 frames. ], batch size: 36, lr: 1.76e-02,
2024-10-09 08:21:01,466 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=10456.0, ans=0.125
2024-10-09 08:21:14,672 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=10456.0, ans=0.023100000000000002
2024-10-09 08:21:20,389 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.min_positive, batch_count=10459.333333333334, ans=0.025
2024-10-09 08:21:20,400 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=10459.333333333334, ans=0.0
2024-10-09 08:21:21,849 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=10459.333333333334, ans=0.025
2024-10-09 08:21:27,574 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=10459.333333333334, ans=0.125
2024-10-09 08:21:43,472 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=10462.666666666666, ans=0.125
2024-10-09 08:21:43,502 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10462.666666666666, ans=0.19537333333333334
2024-10-09 08:21:49,318 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.914e+01 6.493e+01 7.735e+01 8.426e+01 1.130e+02, threshold=1.547e+02, percent-clipped=0.0
2024-10-09 08:22:00,414 INFO [train.py:1152] Epoch 5, batch 4000, loss[loss=0.2795, ctc_loss=0.2004, attn_decoder_loss=0.2992, over 4814.00 frames. ], tot_loss[loss=0.2672, ctc_loss=0.2006, attn_decoder_loss=0.2839, over 967113.19 frames. ], batch size: 19, lr: 1.76e-02,
2024-10-09 08:22:07,640 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=10469.333333333334, ans=0.023044444444444444
2024-10-09 08:22:37,071 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=10476.0, ans=0.09899494936611666
2024-10-09 08:22:44,086 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=10479.333333333334, ans=0.035
2024-10-09 08:22:45,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer1.prob, batch_count=10479.333333333334, ans=0.125
2024-10-09 08:22:51,105 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=10479.333333333334, ans=0.125
2024-10-09 08:23:10,873 INFO [train.py:1152] Epoch 5, batch 4050, loss[loss=0.3309, ctc_loss=0.3049, attn_decoder_loss=0.3374, over 4805.00 frames. ], tot_loss[loss=0.2672, ctc_loss=0.2011, attn_decoder_loss=0.2837, over 967412.66 frames. ], batch size: 53, lr: 1.76e-02,
2024-10-09 08:23:39,040 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10492.666666666666, ans=0.19507333333333335
2024-10-09 08:23:43,290 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=10492.666666666666, ans=0.125
2024-10-09 08:23:51,887 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=10496.0, ans=0.125
2024-10-09 08:24:10,229 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.842e+01 6.839e+01 7.890e+01 9.072e+01 1.310e+02, threshold=1.578e+02, percent-clipped=0.0
2024-10-09 08:24:11,391 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module2.whiten, num_groups=1, num_channels=192, metric=5.10 vs. limit=11.43725
2024-10-09 08:24:13,459 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=10499.333333333334, ans=0.125
2024-10-09 08:24:14,900 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=10499.333333333334, ans=0.022919444444444444
2024-10-09 08:24:19,063 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=10499.333333333334, ans=0.022919444444444444
2024-10-09 08:24:21,845 INFO [train.py:1152] Epoch 5, batch 4100, loss[loss=0.2925, ctc_loss=0.2277, attn_decoder_loss=0.3087, over 4845.00 frames. ], tot_loss[loss=0.2686, ctc_loss=0.2035, attn_decoder_loss=0.2848, over 967006.92 frames. ], batch size: 31, lr: 1.76e-02,
2024-10-09 08:24:26,284 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10502.666666666666, ans=0.19497333333333333
2024-10-09 08:24:31,901 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=10502.666666666666, ans=0.022905555555555557
2024-10-09 08:24:34,688 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=10506.0, ans=0.125
2024-10-09 08:24:36,129 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=10506.0, ans=0.008585652173913043
2024-10-09 08:24:40,443 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=10506.0, ans=0.125
2024-10-09 08:24:40,960 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.65 vs. limit=8.2024
2024-10-09 08:25:13,655 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10512.666666666666, ans=0.19487333333333334
2024-10-09 08:25:22,371 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=10516.0, ans=0.125
2024-10-09 08:25:26,640 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=10516.0, ans=0.0
2024-10-09 08:25:33,861 INFO [train.py:1152] Epoch 5, batch 4150, loss[loss=0.2557, ctc_loss=0.1628, attn_decoder_loss=0.2789, over 4754.00 frames. ], tot_loss[loss=0.2684, ctc_loss=0.2031, attn_decoder_loss=0.2847, over 967187.78 frames. ], batch size: 20, lr: 1.76e-02,
2024-10-09 08:25:34,753 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.16 vs. limit=15.3895
2024-10-09 08:25:37,385 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.30 vs. limit=15.3895
2024-10-09 08:25:38,292 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=10519.333333333334, ans=0.022836111111111113
2024-10-09 08:25:40,503 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=2.83 vs. limit=11.444749999999999
2024-10-09 08:25:53,579 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.69 vs. limit=4.5784
2024-10-09 08:25:57,834 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.70 vs. limit=10.261333333333333
2024-10-09 08:26:19,107 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 08:26:23,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10529.333333333334, ans=0.19470666666666664
2024-10-09 08:26:26,559 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=10529.333333333334, ans=0.025
2024-10-09 08:26:35,191 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.142e+01 6.315e+01 7.738e+01 8.603e+01 1.182e+02, threshold=1.548e+02, percent-clipped=0.0
2024-10-09 08:26:39,521 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=10532.666666666666, ans=0.022780555555555557
2024-10-09 08:26:46,525 INFO [train.py:1152] Epoch 5, batch 4200, loss[loss=0.2684, ctc_loss=0.2071, attn_decoder_loss=0.2838, over 4842.00 frames. ], tot_loss[loss=0.2679, ctc_loss=0.2025, attn_decoder_loss=0.2843, over 967276.65 frames. ], batch size: 31, lr: 1.75e-02,
2024-10-09 08:26:51,026 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=10536.0, ans=0.53124
2024-10-09 08:27:12,807 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=10539.333333333334, ans=0.125
2024-10-09 08:27:28,943 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=10546.0, ans=0.0
2024-10-09 08:27:39,657 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.07 vs. limit=7.6365
2024-10-09 08:27:52,533 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.60 vs. limit=15.411999999999999
2024-10-09 08:27:59,095 INFO [train.py:1152] Epoch 5, batch 4250, loss[loss=0.2215, ctc_loss=0.1371, attn_decoder_loss=0.2426, over 4752.00 frames. ], tot_loss[loss=0.2668, ctc_loss=0.201, attn_decoder_loss=0.2832, over 967061.26 frames. ], batch size: 19, lr: 1.75e-02,
2024-10-09 08:28:19,759 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=10556.0, ans=0.025
2024-10-09 08:29:00,766 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.420e+01 6.613e+01 7.477e+01 8.619e+01 1.177e+02, threshold=1.495e+02, percent-clipped=0.0
2024-10-09 08:29:00,980 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=10566.0, ans=0.05
2024-10-09 08:29:12,045 INFO [train.py:1152] Epoch 5, batch 4300, loss[loss=0.2532, ctc_loss=0.2017, attn_decoder_loss=0.2661, over 4845.00 frames. ], tot_loss[loss=0.2677, ctc_loss=0.2017, attn_decoder_loss=0.2842, over 967368.92 frames. ], batch size: 21, lr: 1.75e-02,
2024-10-09 08:29:27,924 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=10572.666666666666, ans=0.125
2024-10-09 08:29:29,276 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10572.666666666666, ans=0.19427333333333335
2024-10-09 08:29:39,287 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=10576.0, ans=0.0
2024-10-09 08:29:42,181 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=10576.0, ans=0.125
2024-10-09 08:29:45,833 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten.whitening_limit, batch_count=10576.0, ans=11.466000000000001
2024-10-09 08:29:50,860 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=10576.0, ans=0.022600000000000002
2024-10-09 08:29:52,336 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer_ff3.min_abs, batch_count=10576.0, ans=0.2
2024-10-09 08:29:53,173 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=9.56 vs. limit=11.466000000000001
2024-10-09 08:29:53,755 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=10579.333333333334, ans=0.125
2024-10-09 08:30:20,549 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten.whitening_limit, batch_count=10582.666666666666, ans=15.437000000000001
2024-10-09 08:30:24,420 INFO [train.py:1152] Epoch 5, batch 4350, loss[loss=0.2916, ctc_loss=0.2066, attn_decoder_loss=0.3128, over 4850.00 frames. ], tot_loss[loss=0.2676, ctc_loss=0.2018, attn_decoder_loss=0.2841, over 966276.84 frames. ], batch size: 21, lr: 1.75e-02,
2024-10-09 08:30:30,957 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=6.01 vs. limit=11.46975
2024-10-09 08:30:34,103 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=7.29 vs. limit=11.46975
2024-10-09 08:30:50,411 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=10.63 vs. limit=11.471
2024-10-09 08:31:04,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=10592.666666666666, ans=0.125
2024-10-09 08:31:26,851 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.635e+01 6.957e+01 7.930e+01 8.664e+01 1.193e+02, threshold=1.586e+02, percent-clipped=0.0
2024-10-09 08:31:34,343 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=10599.333333333334, ans=0.125
2024-10-09 08:31:38,634 INFO [train.py:1152] Epoch 5, batch 4400, loss[loss=0.2755, ctc_loss=0.2139, attn_decoder_loss=0.2908, over 4735.00 frames. ], tot_loss[loss=0.2677, ctc_loss=0.2015, attn_decoder_loss=0.2843, over 965847.69 frames. ], batch size: 26, lr: 1.75e-02,
2024-10-09 08:32:01,329 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=10606.0, ans=0.125
2024-10-09 08:32:08,618 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=10609.333333333334, ans=0.008563188405797102
2024-10-09 08:32:10,620 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=18.71 vs. limit=11.4785
2024-10-09 08:32:13,164 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=10609.333333333334, ans=0.5286733333333333
2024-10-09 08:32:19,048 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=10609.333333333334, ans=0.125
2024-10-09 08:32:19,539 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.37 vs. limit=15.457
2024-10-09 08:32:32,565 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=10612.666666666666, ans=0.025
2024-10-09 08:32:37,115 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=10616.0, ans=0.125
2024-10-09 08:32:42,323 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.52 vs. limit=10.308
2024-10-09 08:32:42,431 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.13 vs. limit=11.481
2024-10-09 08:32:47,662 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=10616.0, ans=0.19384
2024-10-09 08:32:52,280 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=1.014e-02
2024-10-09 08:32:53,487 INFO [train.py:1152] Epoch 5, batch 4450, loss[loss=0.2425, ctc_loss=0.1684, attn_decoder_loss=0.2611, over 4883.00 frames. ], tot_loss[loss=0.2689, ctc_loss=0.2028, attn_decoder_loss=0.2855, over 966186.83 frames. ], batch size: 19, lr: 1.75e-02,
2024-10-09 08:33:05,597 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=10619.333333333334, ans=0.125
2024-10-09 08:33:08,599 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=10622.666666666666, ans=0.0
2024-10-09 08:33:21,396 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.19 vs. limit=15.466999999999999
2024-10-09 08:33:47,706 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=10629.333333333334, ans=0.5279733333333334
2024-10-09 08:33:55,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=10632.666666666666, ans=0.5278566666666668
2024-10-09 08:33:56,384 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.032e+01 6.788e+01 7.541e+01 8.432e+01 1.318e+02, threshold=1.508e+02, percent-clipped=0.0
2024-10-09 08:34:08,407 INFO [train.py:1152] Epoch 5, batch 4500, loss[loss=0.2899, ctc_loss=0.245, attn_decoder_loss=0.3011, over 4856.00 frames. ], tot_loss[loss=0.2692, ctc_loss=0.2033, attn_decoder_loss=0.2857, over 966043.26 frames. ], batch size: 28, lr: 1.75e-02,
2024-10-09 08:34:19,297 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=10636.0, ans=0.125
2024-10-09 08:34:20,703 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=10636.0, ans=0.125
2024-10-09 08:35:06,722 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=4.34 vs. limit=5.0
2024-10-09 08:35:23,430 INFO [train.py:1152] Epoch 5, batch 4550, loss[loss=0.2757, ctc_loss=0.1988, attn_decoder_loss=0.295, over 4856.00 frames. ], tot_loss[loss=0.2688, ctc_loss=0.2031, attn_decoder_loss=0.2853, over 965929.37 frames. ], batch size: 20, lr: 1.75e-02,
2024-10-09 08:35:52,484 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=1.043e-02
2024-10-09 08:36:24,564 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.68 vs. limit=15.499500000000001
2024-10-09 08:36:25,617 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_False/checkpoint-32000.pt
2024-10-09 08:36:28,084 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.709e+01 7.026e+01 8.056e+01 9.028e+01 1.475e+02, threshold=1.611e+02, percent-clipped=0.0
2024-10-09 08:36:39,077 INFO [train.py:1152] Epoch 5, batch 4600, loss[loss=0.2891, ctc_loss=0.2674, attn_decoder_loss=0.2945, over 4744.00 frames. ], tot_loss[loss=0.2684, ctc_loss=0.2027, attn_decoder_loss=0.2848, over 966235.90 frames. ], batch size: 45, lr: 1.74e-02,
2024-10-09 08:36:40,021 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.86 vs. limit=8.267733333333332
2024-10-09 08:36:52,292 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=10672.666666666666, ans=0.125
2024-10-09 08:36:59,658 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=10672.666666666666, ans=0.04949747468305833
2024-10-09 08:37:12,254 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.98 vs. limit=11.503499999999999
2024-10-09 08:37:12,311 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.32 vs. limit=15.507
2024-10-09 08:37:21,760 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=10679.333333333334, ans=0.0
2024-10-09 08:37:46,540 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.87 vs. limit=11.506
2024-10-09 08:37:52,992 INFO [train.py:1152] Epoch 5, batch 4650, loss[loss=0.2791, ctc_loss=0.2151, attn_decoder_loss=0.2951, over 4813.00 frames. ], tot_loss[loss=0.2679, ctc_loss=0.2017, attn_decoder_loss=0.2845, over 965610.09 frames. ], batch size: 36, lr: 1.74e-02,
2024-10-09 08:38:09,908 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=10689.333333333334, ans=0.125
2024-10-09 08:38:44,348 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=10696.0, ans=0.008544347826086957
2024-10-09 08:38:46,348 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.42 vs. limit=11.511
2024-10-09 08:38:50,490 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=10696.0, ans=0.008544347826086957
2024-10-09 08:38:53,960 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.78 vs. limit=10.349666666666668
2024-10-09 08:38:56,262 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.304e+01 6.986e+01 7.912e+01 8.974e+01 1.775e+02, threshold=1.582e+02, percent-clipped=1.0
2024-10-09 08:38:57,935 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=10699.333333333334, ans=0.022086111111111112
2024-10-09 08:38:59,418 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=10699.333333333334, ans=0.5255233333333333
2024-10-09 08:39:08,105 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=11.62 vs. limit=11.5135
2024-10-09 08:39:08,477 INFO [train.py:1152] Epoch 5, batch 4700, loss[loss=0.2482, ctc_loss=0.1974, attn_decoder_loss=0.2608, over 4940.00 frames. ], tot_loss[loss=0.2691, ctc_loss=0.2042, attn_decoder_loss=0.2853, over 965588.22 frames. ], batch size: 19, lr: 1.74e-02,
2024-10-09 08:39:16,111 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=10702.666666666666, ans=0.022072222222222226
2024-10-09 08:39:16,174 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 08:39:37,270 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=10709.333333333334, ans=0.5251733333333334
2024-10-09 08:39:53,820 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10712.666666666666, ans=0.19287333333333334
2024-10-09 08:40:01,958 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.28 vs. limit=11.51725
2024-10-09 08:40:04,440 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=10712.666666666666, ans=0.5250566666666667
2024-10-09 08:40:05,936 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=10712.666666666666, ans=0.07
2024-10-09 08:40:14,845 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=10716.0, ans=0.025
2024-10-09 08:40:23,300 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys.whitening_limit, batch_count=10719.333333333334, ans=4.6079
2024-10-09 08:40:23,692 INFO [train.py:1152] Epoch 5, batch 4750, loss[loss=0.2764, ctc_loss=0.2048, attn_decoder_loss=0.2943, over 4758.00 frames. ], tot_loss[loss=0.2706, ctc_loss=0.2064, attn_decoder_loss=0.2866, over 965460.25 frames. ], batch size: 45, lr: 1.74e-02,
2024-10-09 08:40:51,431 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=10722.666666666666, ans=0.125
2024-10-09 08:41:01,001 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.01 vs. limit=7.6815
2024-10-09 08:41:27,532 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.535e+01 6.649e+01 7.646e+01 8.507e+01 1.125e+02, threshold=1.529e+02, percent-clipped=0.0
2024-10-09 08:41:39,566 INFO [train.py:1152] Epoch 5, batch 4800, loss[loss=0.2878, ctc_loss=0.2044, attn_decoder_loss=0.3086, over 4880.00 frames. ], tot_loss[loss=0.2691, ctc_loss=0.2039, attn_decoder_loss=0.2855, over 965768.44 frames. ], batch size: 22, lr: 1.74e-02,
2024-10-09 08:42:05,143 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.83 vs. limit=15.5545
2024-10-09 08:42:18,979 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=7.22 vs. limit=7.685666666666666
2024-10-09 08:42:24,833 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=3.27 vs. limit=8.2984
2024-10-09 08:42:55,932 INFO [train.py:1152] Epoch 5, batch 4850, loss[loss=0.2571, ctc_loss=0.1815, attn_decoder_loss=0.276, over 4831.00 frames. ], tot_loss[loss=0.2691, ctc_loss=0.2026, attn_decoder_loss=0.2858, over 966407.89 frames. ], batch size: 28, lr: 1.74e-02,
2024-10-09 08:43:08,050 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=8.214e-03
2024-10-09 08:43:08,083 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=10752.666666666666, ans=0.125
2024-10-09 08:43:11,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=10756.0, ans=0.0
2024-10-09 08:43:13,285 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.58 vs. limit=11.5335
2024-10-09 08:43:22,810 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=10756.0, ans=0.125
2024-10-09 08:43:58,462 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.186e+01 6.896e+01 7.556e+01 8.479e+01 1.326e+02, threshold=1.511e+02, percent-clipped=0.0
2024-10-09 08:44:10,563 INFO [train.py:1152] Epoch 5, batch 4900, loss[loss=0.2803, ctc_loss=0.2055, attn_decoder_loss=0.299, over 4833.00 frames. ], tot_loss[loss=0.269, ctc_loss=0.2024, attn_decoder_loss=0.2857, over 967086.61 frames. ], batch size: 21, lr: 1.74e-02,
2024-10-09 08:44:36,524 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=10772.666666666666, ans=0.125
2024-10-09 08:44:38,001 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10772.666666666666, ans=0.19227333333333335
2024-10-09 08:44:57,768 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=10779.333333333334, ans=0.125
2024-10-09 08:45:12,102 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.43 vs. limit=10.391333333333332
2024-10-09 08:45:12,872 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10782.666666666666, ans=0.19217333333333336
2024-10-09 08:45:18,857 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=10782.666666666666, ans=0.5226066666666667
2024-10-09 08:45:21,131 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.56 vs. limit=15.587
2024-10-09 08:45:26,213 INFO [train.py:1152] Epoch 5, batch 4950, loss[loss=0.3242, ctc_loss=0.2824, attn_decoder_loss=0.3347, over 4793.00 frames. ], tot_loss[loss=0.2692, ctc_loss=0.2026, attn_decoder_loss=0.2859, over 966811.77 frames. ], batch size: 53, lr: 1.74e-02,
2024-10-09 08:45:26,383 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=10786.0, ans=0.85786
2024-10-09 08:45:43,350 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=10789.333333333334, ans=0.125
2024-10-09 08:46:30,193 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.398e+01 6.705e+01 7.725e+01 8.429e+01 1.187e+02, threshold=1.545e+02, percent-clipped=0.0
2024-10-09 08:46:33,380 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=10799.333333333334, ans=0.125
2024-10-09 08:46:42,359 INFO [train.py:1152] Epoch 5, batch 5000, loss[loss=0.2489, ctc_loss=0.1795, attn_decoder_loss=0.2663, over 4786.00 frames. ], tot_loss[loss=0.2683, ctc_loss=0.2014, attn_decoder_loss=0.285, over 967711.15 frames. ], batch size: 29, lr: 1.73e-02,
2024-10-09 08:46:45,614 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=10802.666666666666, ans=0.125
2024-10-09 08:46:57,471 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=10806.0, ans=0.0
2024-10-09 08:47:11,538 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=10809.333333333334, ans=0.125
2024-10-09 08:47:58,406 INFO [train.py:1152] Epoch 5, batch 5050, loss[loss=0.2911, ctc_loss=0.239, attn_decoder_loss=0.3042, over 4855.00 frames. ], tot_loss[loss=0.2668, ctc_loss=0.2001, attn_decoder_loss=0.2835, over 968638.12 frames. ], batch size: 19, lr: 1.73e-02,
2024-10-09 08:47:58,630 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=10819.333333333334, ans=0.5213233333333334
2024-10-09 08:48:06,100 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10819.333333333334, ans=0.19180666666666665
2024-10-09 08:48:25,457 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=10822.666666666666, ans=0.125
2024-10-09 08:48:30,382 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.50 vs. limit=11.559750000000001
2024-10-09 08:48:35,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=10826.0, ans=0.52109
2024-10-09 08:48:39,397 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=11.41 vs. limit=10.413
2024-10-09 08:49:01,623 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.400e+01 6.897e+01 7.969e+01 8.908e+01 1.663e+02, threshold=1.594e+02, percent-clipped=1.0
2024-10-09 08:49:04,815 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=10832.666666666666, ans=0.00851463768115942
2024-10-09 08:49:13,782 INFO [train.py:1152] Epoch 5, batch 5100, loss[loss=0.2342, ctc_loss=0.1656, attn_decoder_loss=0.2513, over 4815.00 frames. ], tot_loss[loss=0.2689, ctc_loss=0.2028, attn_decoder_loss=0.2854, over 967886.19 frames. ], batch size: 19, lr: 1.73e-02,
2024-10-09 08:49:20,195 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=10836.0, ans=0.07
2024-10-09 08:49:52,340 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10842.666666666666, ans=0.19157333333333335
2024-10-09 08:49:54,214 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.65 vs. limit=7.710666666666667
2024-10-09 08:50:01,433 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=10846.0, ans=0.008511739130434782
2024-10-09 08:50:08,995 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=10846.0, ans=0.125
2024-10-09 08:50:29,934 INFO [train.py:1152] Epoch 5, batch 5150, loss[loss=0.2713, ctc_loss=0.2061, attn_decoder_loss=0.2876, over 4816.00 frames. ], tot_loss[loss=0.2691, ctc_loss=0.203, attn_decoder_loss=0.2856, over 967929.06 frames. ], batch size: 36, lr: 1.73e-02,
2024-10-09 08:50:36,112 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10852.666666666666, ans=0.19147333333333333
2024-10-09 08:50:40,807 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=10852.666666666666, ans=0.025
2024-10-09 08:51:01,164 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=15.74 vs. limit=10.429666666666666
2024-10-09 08:51:02,921 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.04 vs. limit=11.57225
2024-10-09 08:51:33,692 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.236e+01 6.690e+01 7.406e+01 8.204e+01 1.117e+02, threshold=1.481e+02, percent-clipped=0.0
2024-10-09 08:51:33,943 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=10866.0, ans=0.51969
2024-10-09 08:51:45,716 INFO [train.py:1152] Epoch 5, batch 5200, loss[loss=0.274, ctc_loss=0.1818, attn_decoder_loss=0.297, over 4777.00 frames. ], tot_loss[loss=0.2687, ctc_loss=0.2028, attn_decoder_loss=0.2852, over 967559.85 frames. ], batch size: 29, lr: 1.73e-02,
2024-10-09 08:51:48,929 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=10869.333333333334, ans=0.5195733333333334
2024-10-09 08:51:48,954 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=10869.333333333334, ans=0.021377777777777777
2024-10-09 08:52:16,624 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=10876.0, ans=0.008505217391304347
2024-10-09 08:52:29,012 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 08:52:39,727 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10879.333333333334, ans=0.19120666666666664
2024-10-09 08:52:59,494 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=10882.666666666666, ans=0.125
2024-10-09 08:53:02,417 INFO [train.py:1152] Epoch 5, batch 5250, loss[loss=0.2225, ctc_loss=0.1604, attn_decoder_loss=0.2381, over 4863.00 frames. ], tot_loss[loss=0.2681, ctc_loss=0.2022, attn_decoder_loss=0.2846, over 967655.21 frames. ], batch size: 20, lr: 1.73e-02,
2024-10-09 08:53:08,604 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.min_abs, batch_count=10886.0, ans=0.36329
2024-10-09 08:53:08,696 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=10886.0, ans=0.04949747468305833
2024-10-09 08:54:03,876 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.88 vs. limit=11.587250000000001
2024-10-09 08:54:06,037 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.755e+01 6.783e+01 7.801e+01 8.792e+01 1.542e+02, threshold=1.560e+02, percent-clipped=1.0
2024-10-09 08:54:18,301 INFO [train.py:1152] Epoch 5, batch 5300, loss[loss=0.2756, ctc_loss=0.2288, attn_decoder_loss=0.2873, over 4825.00 frames. ], tot_loss[loss=0.2672, ctc_loss=0.2011, attn_decoder_loss=0.2837, over 967707.67 frames. ], batch size: 38, lr: 1.73e-02,
2024-10-09 08:54:21,486 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=10902.666666666666, ans=0.07
2024-10-09 08:54:38,277 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.min_abs, batch_count=10906.0, ans=0.36358999999999997
2024-10-09 08:54:41,389 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=10906.0, ans=0.19094
2024-10-09 08:54:42,871 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=10906.0, ans=0.19094
2024-10-09 08:54:42,905 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=10906.0, ans=0.09899494936611666
2024-10-09 08:55:10,323 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=10912.666666666666, ans=0.125
2024-10-09 08:55:24,115 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=10916.0, ans=0.008496521739130436
2024-10-09 08:55:34,850 INFO [train.py:1152] Epoch 5, batch 5350, loss[loss=0.2817, ctc_loss=0.2168, attn_decoder_loss=0.2979, over 4978.00 frames. ], tot_loss[loss=0.2671, ctc_loss=0.2005, attn_decoder_loss=0.2838, over 967177.66 frames. ], batch size: 19, lr: 1.73e-02,
2024-10-09 08:56:02,208 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=10922.666666666666, ans=0.5177066666666668
2024-10-09 08:56:13,680 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.52 vs. limit=15.6945
2024-10-09 08:56:20,564 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=10929.333333333334, ans=0.09899494936611666
2024-10-09 08:56:38,802 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.705e+01 6.936e+01 7.809e+01 8.692e+01 1.278e+02, threshold=1.562e+02, percent-clipped=0.0
2024-10-09 08:56:50,872 INFO [train.py:1152] Epoch 5, batch 5400, loss[loss=0.3027, ctc_loss=0.2593, attn_decoder_loss=0.3135, over 4785.00 frames. ], tot_loss[loss=0.2678, ctc_loss=0.2014, attn_decoder_loss=0.2844, over 966318.24 frames. ], batch size: 49, lr: 1.72e-02,
2024-10-09 08:56:57,065 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=10936.0, ans=0.125
2024-10-09 08:57:19,315 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.44 vs. limit=15.7045
2024-10-09 08:57:20,153 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=10942.666666666666, ans=0.00849072463768116
2024-10-09 08:57:21,121 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=18.70 vs. limit=15.707
2024-10-09 08:57:32,859 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.65 vs. limit=11.6035
2024-10-09 08:58:04,254 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=10949.333333333334, ans=0.07
2024-10-09 08:58:06,277 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys.whitening_limit, batch_count=10952.666666666666, ans=4.6429
2024-10-09 08:58:07,181 INFO [train.py:1152] Epoch 5, batch 5450, loss[loss=0.2623, ctc_loss=0.1867, attn_decoder_loss=0.2812, over 4940.00 frames. ], tot_loss[loss=0.2677, ctc_loss=0.2016, attn_decoder_loss=0.2842, over 967015.37 frames. ], batch size: 19, lr: 1.72e-02,
2024-10-09 08:58:08,990 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=10952.666666666666, ans=0.125
2024-10-09 08:58:11,843 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=10952.666666666666, ans=0.125
2024-10-09 08:58:32,408 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.15 vs. limit=11.6085
2024-10-09 08:58:34,789 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=10956.0, ans=0.125
2024-10-09 08:58:54,819 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=10962.666666666666, ans=0.0
2024-10-09 08:58:56,390 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=10962.666666666666, ans=0.125
2024-10-09 08:59:11,352 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.162e+01 6.800e+01 7.613e+01 8.648e+01 1.316e+02, threshold=1.523e+02, percent-clipped=0.0
2024-10-09 08:59:23,790 INFO [train.py:1152] Epoch 5, batch 5500, loss[loss=0.2823, ctc_loss=0.2157, attn_decoder_loss=0.2989, over 4774.00 frames. ], tot_loss[loss=0.2674, ctc_loss=0.201, attn_decoder_loss=0.284, over 967352.84 frames. ], batch size: 49, lr: 1.72e-02,
2024-10-09 08:59:26,956 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=10969.333333333334, ans=0.5160733333333334
2024-10-09 08:59:31,649 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=10969.333333333334, ans=0.125
2024-10-09 08:59:42,436 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=10972.666666666666, ans=0.008484202898550725
2024-10-09 08:59:51,546 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=10972.666666666666, ans=0.020947222222222225
2024-10-09 08:59:59,138 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=10976.0, ans=0.0
2024-10-09 09:00:06,778 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10976.0, ans=0.19024
2024-10-09 09:00:32,902 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=10982.666666666666, ans=0.125
2024-10-09 09:00:39,437 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.70 vs. limit=10.493
2024-10-09 09:00:40,323 INFO [train.py:1152] Epoch 5, batch 5550, loss[loss=0.2597, ctc_loss=0.1877, attn_decoder_loss=0.2778, over 4800.00 frames. ], tot_loss[loss=0.2668, ctc_loss=0.1994, attn_decoder_loss=0.2836, over 967060.33 frames. ], batch size: 19, lr: 1.72e-02,
2024-10-09 09:00:55,788 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=10989.333333333334, ans=0.125
2024-10-09 09:01:00,506 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=10989.333333333334, ans=0.5153733333333335
2024-10-09 09:01:29,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=10996.0, ans=0.125
2024-10-09 09:01:30,124 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.70 vs. limit=15.747
2024-10-09 09:01:44,576 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.754e+01 6.946e+01 7.552e+01 8.319e+01 1.107e+02, threshold=1.510e+02, percent-clipped=0.0
2024-10-09 09:01:56,776 INFO [train.py:1152] Epoch 5, batch 5600, loss[loss=0.296, ctc_loss=0.2385, attn_decoder_loss=0.3104, over 4848.00 frames. ], tot_loss[loss=0.2667, ctc_loss=0.1997, attn_decoder_loss=0.2834, over 966960.04 frames. ], batch size: 28, lr: 1.72e-02,
2024-10-09 09:02:06,205 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=11002.666666666666, ans=0.125
2024-10-09 09:02:13,839 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=11006.0, ans=0.125
2024-10-09 09:02:21,520 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=11006.0, ans=0.125
2024-10-09 09:02:51,544 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.33 vs. limit=15.7595
2024-10-09 09:02:56,942 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=11016.0, ans=0.125
2024-10-09 09:03:07,604 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=11016.0, ans=0.125
2024-10-09 09:03:12,262 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=11019.333333333334, ans=10.0
2024-10-09 09:03:12,298 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=11019.333333333334, ans=0.125
2024-10-09 09:03:13,574 INFO [train.py:1152] Epoch 5, batch 5650, loss[loss=0.3175, ctc_loss=0.2653, attn_decoder_loss=0.3305, over 4761.00 frames. ], tot_loss[loss=0.2659, ctc_loss=0.1979, attn_decoder_loss=0.2829, over 967045.54 frames. ], batch size: 45, lr: 1.72e-02,
2024-10-09 09:03:15,361 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=11019.333333333334, ans=0.0
2024-10-09 09:03:22,282 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.83 vs. limit=15.7645
2024-10-09 09:03:22,991 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=11019.333333333334, ans=0.125
2024-10-09 09:03:32,024 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=11022.666666666666, ans=0.0
2024-10-09 09:03:51,806 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=11026.0, ans=0.125
2024-10-09 09:04:04,501 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.88 vs. limit=11.636
2024-10-09 09:04:17,419 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.549e+01 6.512e+01 7.455e+01 8.414e+01 1.387e+02, threshold=1.491e+02, percent-clipped=0.0
2024-10-09 09:04:17,666 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=11032.666666666666, ans=0.020697222222222224
2024-10-09 09:04:26,955 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=11032.666666666666, ans=0.125
2024-10-09 09:04:29,796 INFO [train.py:1152] Epoch 5, batch 5700, loss[loss=0.2399, ctc_loss=0.1426, attn_decoder_loss=0.2643, over 4862.00 frames. ], tot_loss[loss=0.2657, ctc_loss=0.1974, attn_decoder_loss=0.2828, over 966522.90 frames. ], batch size: 22, lr: 1.72e-02,
2024-10-09 09:04:37,518 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=11036.0, ans=0.125
2024-10-09 09:05:01,043 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.41 vs. limit=4.6564
2024-10-09 09:05:01,530 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=11042.666666666666, ans=0.125
2024-10-09 09:05:07,584 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 09:05:39,701 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=11049.333333333334, ans=0.02062777777777778
2024-10-09 09:05:44,411 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=11052.666666666666, ans=0.07
2024-10-09 09:05:45,804 INFO [train.py:1152] Epoch 5, batch 5750, loss[loss=0.2996, ctc_loss=0.2542, attn_decoder_loss=0.311, over 4856.00 frames. ], tot_loss[loss=0.2665, ctc_loss=0.1988, attn_decoder_loss=0.2835, over 966824.21 frames. ], batch size: 43, lr: 1.72e-02,
2024-10-09 09:05:59,746 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=11056.0, ans=0.00846608695652174
2024-10-09 09:06:22,292 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=11059.333333333334, ans=0.5129233333333334
2024-10-09 09:06:49,621 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.425e+01 7.017e+01 8.097e+01 9.058e+01 1.392e+02, threshold=1.619e+02, percent-clipped=0.0
2024-10-09 09:07:01,568 INFO [train.py:1152] Epoch 5, batch 5800, loss[loss=0.2931, ctc_loss=0.2356, attn_decoder_loss=0.3074, over 4819.00 frames. ], tot_loss[loss=0.2674, ctc_loss=0.2005, attn_decoder_loss=0.2841, over 966239.77 frames. ], batch size: 43, lr: 1.71e-02,
2024-10-09 09:07:20,250 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 09:07:32,122 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 09:07:58,157 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=11079.333333333334, ans=0.008461014492753624
2024-10-09 09:07:59,641 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=11079.333333333334, ans=0.02050277777777778
2024-10-09 09:08:01,481 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 09:08:02,830 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=11082.666666666666, ans=0.125
2024-10-09 09:08:03,533 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=11.39 vs. limit=11.655999999999999
2024-10-09 09:08:10,558 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=11082.666666666666, ans=0.18917333333333336
2024-10-09 09:08:12,142 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.max_positive, batch_count=11082.666666666666, ans=0.8608266666666666
2024-10-09 09:08:18,011 INFO [train.py:1152] Epoch 5, batch 5850, loss[loss=0.2941, ctc_loss=0.251, attn_decoder_loss=0.3048, over 4745.00 frames. ], tot_loss[loss=0.2681, ctc_loss=0.202, attn_decoder_loss=0.2846, over 966573.55 frames. ], batch size: 45, lr: 1.71e-02,
2024-10-09 09:08:18,230 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=11086.0, ans=0.18914
2024-10-09 09:08:19,690 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=11086.0, ans=0.008459565217391305
2024-10-09 09:08:22,324 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=5.55 vs. limit=6.2172
2024-10-09 09:08:57,615 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=11092.666666666666, ans=0.125
2024-10-09 09:09:11,293 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=11096.0, ans=0.035
2024-10-09 09:09:21,716 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.350e+01 6.672e+01 7.534e+01 8.398e+01 1.273e+02, threshold=1.507e+02, percent-clipped=0.0
2024-10-09 09:09:25,091 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=7.547e-02
2024-10-09 09:09:34,034 INFO [train.py:1152] Epoch 5, batch 5900, loss[loss=0.2863, ctc_loss=0.2125, attn_decoder_loss=0.3047, over 4818.00 frames. ], tot_loss[loss=0.2664, ctc_loss=0.1992, attn_decoder_loss=0.2832, over 966685.35 frames. ], batch size: 34, lr: 1.71e-02,
2024-10-09 09:09:39,781 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module2.whiten, num_groups=1, num_channels=192, metric=3.50 vs. limit=11.663499999999999
2024-10-09 09:09:56,571 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.91 vs. limit=10.553
2024-10-09 09:10:12,281 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff2_skip_rate, batch_count=11109.333333333334, ans=0.008454492753623189
2024-10-09 09:10:32,586 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=11.61 vs. limit=11.66725
2024-10-09 09:10:37,899 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=11116.0, ans=0.09717700000000001
2024-10-09 09:10:38,690 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.55 vs. limit=15.837
2024-10-09 09:10:45,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11116.0, ans=0.18884
2024-10-09 09:10:49,986 INFO [train.py:1152] Epoch 5, batch 5950, loss[loss=0.3101, ctc_loss=0.2596, attn_decoder_loss=0.3228, over 4804.00 frames. ], tot_loss[loss=0.2656, ctc_loss=0.1978, attn_decoder_loss=0.2826, over 966139.14 frames. ], batch size: 34, lr: 1.71e-02,
2024-10-09 09:11:14,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=11122.666666666666, ans=0.125
2024-10-09 09:11:29,568 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.94 vs. limit=11.67225
2024-10-09 09:11:36,527 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=11129.333333333334, ans=0.09899494936611666
2024-10-09 09:11:38,038 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=11129.333333333334, ans=0.125
2024-10-09 09:11:38,082 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11129.333333333334, ans=0.18870666666666663
2024-10-09 09:11:54,729 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.375e+01 6.848e+01 7.647e+01 8.818e+01 2.524e+02, threshold=1.529e+02, percent-clipped=3.0
2024-10-09 09:12:06,792 INFO [train.py:1152] Epoch 5, batch 6000, loss[loss=0.3387, ctc_loss=0.3081, attn_decoder_loss=0.3463, over 4772.00 frames. ], tot_loss[loss=0.2663, ctc_loss=0.1991, attn_decoder_loss=0.2831, over 966715.72 frames. ], batch size: 49, lr: 1.71e-02,
2024-10-09 09:12:06,793 INFO [train.py:1175] Computing validation loss
2024-10-09 09:12:15,849 INFO [train.py:1184] Epoch 5, validation: loss=0.2101, ctc_loss=0.07164, attn_decoder_loss=0.2447, over 90464.00 frames.
2024-10-09 09:12:15,850 INFO [train.py:1185] Maximum memory allocated so far is 6613MB
2024-10-09 09:12:32,214 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=11139.333333333334, ans=0.125
2024-10-09 09:12:42,202 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=2.95 vs. limit=11.67725
2024-10-09 09:12:42,828 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=11139.333333333334, ans=0.025
2024-10-09 09:12:53,922 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.71 vs. limit=10.571333333333332
2024-10-09 09:12:56,550 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=11142.666666666666, ans=0.125
2024-10-09 09:13:03,996 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11146.0, ans=0.18853999999999999
2024-10-09 09:13:04,711 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.56 vs. limit=4.6719
2024-10-09 09:13:05,654 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=11146.0, ans=0.50989
2024-10-09 09:13:31,400 INFO [train.py:1152] Epoch 5, batch 6050, loss[loss=0.2198, ctc_loss=0.1179, attn_decoder_loss=0.2452, over 4817.00 frames. ], tot_loss[loss=0.2658, ctc_loss=0.198, attn_decoder_loss=0.2827, over 966680.87 frames. ], batch size: 19, lr: 1.71e-02,
2024-10-09 09:13:37,661 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=11152.666666666666, ans=0.020197222222222224
2024-10-09 09:13:52,965 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 09:13:57,545 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=11156.0, ans=0.125
2024-10-09 09:14:23,183 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=11162.666666666666, ans=0.125
2024-10-09 09:14:35,113 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.358e+01 6.749e+01 7.591e+01 8.476e+01 1.436e+02, threshold=1.518e+02, percent-clipped=0.0
2024-10-09 09:14:47,365 INFO [train.py:1152] Epoch 5, batch 6100, loss[loss=0.2754, ctc_loss=0.2247, attn_decoder_loss=0.2881, over 4806.00 frames. ], tot_loss[loss=0.2662, ctc_loss=0.1985, attn_decoder_loss=0.2831, over 966341.20 frames. ], batch size: 34, lr: 1.71e-02,
2024-10-09 09:15:01,199 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=11172.666666666666, ans=0.020113888888888893
2024-10-09 09:15:23,954 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=11176.0, ans=0.0
2024-10-09 09:15:27,091 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=11176.0, ans=0.020100000000000003
2024-10-09 09:15:30,288 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=11176.0, ans=0.125
2024-10-09 09:15:36,150 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11179.333333333334, ans=0.18820666666666663
2024-10-09 09:15:42,456 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=11179.333333333334, ans=0.008439275362318841
2024-10-09 09:16:03,675 INFO [train.py:1152] Epoch 5, batch 6150, loss[loss=0.2922, ctc_loss=0.2418, attn_decoder_loss=0.3048, over 4835.00 frames. ], tot_loss[loss=0.2656, ctc_loss=0.197, attn_decoder_loss=0.2827, over 966524.93 frames. ], batch size: 43, lr: 1.71e-02,
2024-10-09 09:16:10,695 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=2.52 vs. limit=11.694749999999999
2024-10-09 09:16:16,013 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=11186.0, ans=0.025
2024-10-09 09:16:35,978 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=11192.666666666666, ans=0.125
2024-10-09 09:16:37,400 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11192.666666666666, ans=0.18807333333333334
2024-10-09 09:16:43,458 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=11192.666666666666, ans=0.125
2024-10-09 09:16:54,045 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=11196.0, ans=0.125
2024-10-09 09:16:55,736 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=11196.0, ans=0.05
2024-10-09 09:17:04,867 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=11199.333333333334, ans=0.125
2024-10-09 09:17:07,980 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.120e+01 6.597e+01 7.494e+01 8.605e+01 1.303e+02, threshold=1.499e+02, percent-clipped=0.0
2024-10-09 09:17:19,979 INFO [train.py:1152] Epoch 5, batch 6200, loss[loss=0.2391, ctc_loss=0.1456, attn_decoder_loss=0.2625, over 4756.00 frames. ], tot_loss[loss=0.2653, ctc_loss=0.197, attn_decoder_loss=0.2824, over 966695.89 frames. ], batch size: 29, lr: 1.70e-02,
2024-10-09 09:17:20,199 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=11202.666666666666, ans=0.5079066666666667
2024-10-09 09:17:30,891 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=11202.666666666666, ans=0.019988888888888893
2024-10-09 09:17:35,402 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=11206.0, ans=0.50779
2024-10-09 09:17:46,065 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=11206.0, ans=0.008433478260869565
2024-10-09 09:17:52,322 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=11209.333333333334, ans=0.01996111111111111
2024-10-09 09:17:56,079 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=16.16 vs. limit=15.907
2024-10-09 09:17:56,851 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=11209.333333333334, ans=0.5076733333333334
2024-10-09 09:18:30,677 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=11216.0, ans=0.125
2024-10-09 09:18:36,565 INFO [train.py:1152] Epoch 5, batch 6250, loss[loss=0.2815, ctc_loss=0.1903, attn_decoder_loss=0.3043, over 4719.00 frames. ], tot_loss[loss=0.2641, ctc_loss=0.1948, attn_decoder_loss=0.2814, over 966988.83 frames. ], batch size: 26, lr: 1.70e-02,
2024-10-09 09:18:42,830 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.max_abs, batch_count=11219.333333333334, ans=10.0
2024-10-09 09:18:55,071 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=11222.666666666666, ans=0.125
2024-10-09 09:19:15,610 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.42 vs. limit=11.70975
2024-10-09 09:19:33,651 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.37 vs. limit=11.711
2024-10-09 09:19:40,433 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.283e+01 6.771e+01 7.708e+01 8.577e+01 1.076e+02, threshold=1.542e+02, percent-clipped=0.0
2024-10-09 09:19:52,745 INFO [train.py:1152] Epoch 5, batch 6300, loss[loss=0.2858, ctc_loss=0.2326, attn_decoder_loss=0.2991, over 4978.00 frames. ], tot_loss[loss=0.2649, ctc_loss=0.1963, attn_decoder_loss=0.2821, over 966697.35 frames. ], batch size: 19, lr: 1.70e-02,
2024-10-09 09:20:03,649 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=11236.0, ans=0.019850000000000003
2024-10-09 09:20:19,119 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=11239.333333333334, ans=0.5066233333333334
2024-10-09 09:20:25,853 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=17.63 vs. limit=15.932
2024-10-09 09:20:45,039 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=11246.0, ans=0.18753999999999998
2024-10-09 09:20:51,242 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=11246.0, ans=0.019808333333333334
2024-10-09 09:20:54,332 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11249.333333333334, ans=0.18750666666666665
2024-10-09 09:21:09,481 INFO [train.py:1152] Epoch 5, batch 6350, loss[loss=0.3102, ctc_loss=0.2699, attn_decoder_loss=0.3203, over 4814.00 frames. ], tot_loss[loss=0.2642, ctc_loss=0.1948, attn_decoder_loss=0.2815, over 966377.00 frames. ], batch size: 36, lr: 1.70e-02,
2024-10-09 09:21:12,654 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=11252.666666666666, ans=0.019780555555555558
2024-10-09 09:21:21,854 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=11252.666666666666, ans=0.125
2024-10-09 09:21:34,052 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=11256.0, ans=0.125
2024-10-09 09:22:14,071 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.875e+01 7.106e+01 7.813e+01 8.561e+01 1.307e+02, threshold=1.563e+02, percent-clipped=0.0
2024-10-09 09:22:23,308 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=11266.0, ans=0.008420434782608696
2024-10-09 09:22:26,254 INFO [train.py:1152] Epoch 5, batch 6400, loss[loss=0.242, ctc_loss=0.1832, attn_decoder_loss=0.2567, over 4870.00 frames. ], tot_loss[loss=0.2639, ctc_loss=0.1942, attn_decoder_loss=0.2813, over 966060.27 frames. ], batch size: 23, lr: 1.70e-02,
2024-10-09 09:22:27,223 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.64 vs. limit=11.725999999999999
2024-10-09 09:23:03,179 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=11276.0, ans=0.008418260869565217
2024-10-09 09:23:04,614 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=11276.0, ans=0.125
2024-10-09 09:23:15,170 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=11279.333333333334, ans=0.019669444444444444
2024-10-09 09:23:16,739 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=11279.333333333334, ans=0.125
2024-10-09 09:23:42,475 INFO [train.py:1152] Epoch 5, batch 6450, loss[loss=0.2378, ctc_loss=0.1594, attn_decoder_loss=0.2574, over 4738.00 frames. ], tot_loss[loss=0.2634, ctc_loss=0.1938, attn_decoder_loss=0.2808, over 965414.42 frames. ], batch size: 26, lr: 1.70e-02,
2024-10-09 09:23:42,739 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11286.0, ans=0.18714
2024-10-09 09:23:50,894 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1.whitening_limit, batch_count=11286.0, ans=7.8215
2024-10-09 09:24:06,697 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=7.31 vs. limit=7.822333333333333
2024-10-09 09:24:07,324 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 09:24:14,884 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=11292.666666666666, ans=0.125
2024-10-09 09:24:24,069 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=11292.666666666666, ans=0.00841463768115942
2024-10-09 09:24:25,696 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=11292.666666666666, ans=0.019613888888888893
2024-10-09 09:24:30,229 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=11296.0, ans=0.13704
2024-10-09 09:24:46,992 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.616e+01 6.881e+01 7.609e+01 8.585e+01 2.801e+02, threshold=1.522e+02, percent-clipped=2.0
2024-10-09 09:24:59,513 INFO [train.py:1152] Epoch 5, batch 6500, loss[loss=0.2804, ctc_loss=0.2217, attn_decoder_loss=0.2951, over 4744.00 frames. ], tot_loss[loss=0.2624, ctc_loss=0.1923, attn_decoder_loss=0.28, over 965068.74 frames. ], batch size: 26, lr: 1.70e-02,
2024-10-09 09:25:01,037 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=11302.666666666666, ans=0.18697333333333335
2024-10-09 09:25:15,168 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11306.0, ans=0.18694
2024-10-09 09:25:16,616 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=11306.0, ans=0.125
2024-10-09 09:25:41,218 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=11309.333333333334, ans=0.008411014492753624
2024-10-09 09:25:51,448 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module2.whiten, num_groups=1, num_channels=192, metric=2.41 vs. limit=11.74225
2024-10-09 09:25:53,565 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 09:25:55,826 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.37 vs. limit=7.828166666666666
2024-10-09 09:25:57,414 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.10 vs. limit=15.9845
2024-10-09 09:26:16,474 INFO [train.py:1152] Epoch 5, batch 6550, loss[loss=0.2423, ctc_loss=0.1691, attn_decoder_loss=0.2606, over 4978.00 frames. ], tot_loss[loss=0.2619, ctc_loss=0.1909, attn_decoder_loss=0.2797, over 964717.63 frames. ], batch size: 19, lr: 1.70e-02,
2024-10-09 09:26:29,109 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=11319.333333333334, ans=0.01950277777777778
2024-10-09 09:26:29,235 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-09 09:26:47,306 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.min_positive, batch_count=11326.0, ans=0.025
2024-10-09 09:26:55,994 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.36 vs. limit=15.9945
2024-10-09 09:27:07,651 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=11329.333333333334, ans=0.5034733333333334
2024-10-09 09:27:10,673 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=11329.333333333334, ans=0.07
2024-10-09 09:27:21,330 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.192e+01 6.431e+01 7.417e+01 8.273e+01 1.192e+02, threshold=1.483e+02, percent-clipped=0.0
2024-10-09 09:27:26,184 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=11332.666666666666, ans=0.125
2024-10-09 09:27:30,633 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=11332.666666666666, ans=0.008405942028985508
2024-10-09 09:27:31,155 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.31 vs. limit=7.833166666666667
2024-10-09 09:27:33,632 INFO [train.py:1152] Epoch 5, batch 6600, loss[loss=0.2612, ctc_loss=0.1901, attn_decoder_loss=0.2789, over 4864.00 frames. ], tot_loss[loss=0.2597, ctc_loss=0.1882, attn_decoder_loss=0.2776, over 965430.86 frames. ], batch size: 23, lr: 1.69e-02,
2024-10-09 09:27:38,509 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=11336.0, ans=0.50324
2024-10-09 09:27:53,119 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.15 vs. limit=11.75225
2024-10-09 09:28:06,431 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=11342.666666666666, ans=0.019405555555555558
2024-10-09 09:28:18,964 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=11346.0, ans=0.025
2024-10-09 09:28:41,350 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=6.33 vs. limit=8.539733333333334
2024-10-09 09:28:51,615 INFO [train.py:1152] Epoch 5, batch 6650, loss[loss=0.2349, ctc_loss=0.1469, attn_decoder_loss=0.257, over 4748.00 frames. ], tot_loss[loss=0.2587, ctc_loss=0.1864, attn_decoder_loss=0.2767, over 967137.21 frames. ], batch size: 20, lr: 1.69e-02,
2024-10-09 09:28:53,353 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=11352.666666666666, ans=0.019363888888888892
2024-10-09 09:29:18,336 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 09:29:40,037 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=11362.666666666666, ans=0.125
2024-10-09 09:29:43,173 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=11362.666666666666, ans=0.025
2024-10-09 09:29:44,787 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=11362.666666666666, ans=0.125
2024-10-09 09:29:57,039 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.195e+01 6.573e+01 7.387e+01 8.481e+01 1.226e+02, threshold=1.477e+02, percent-clipped=0.0
2024-10-09 09:30:00,996 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.85 vs. limit=16.0245
2024-10-09 09:30:09,732 INFO [train.py:1152] Epoch 5, batch 6700, loss[loss=0.2542, ctc_loss=0.1618, attn_decoder_loss=0.2773, over 4935.00 frames. ], tot_loss[loss=0.2582, ctc_loss=0.1852, attn_decoder_loss=0.2764, over 969192.10 frames. ], batch size: 20, lr: 1.69e-02,
2024-10-09 09:30:11,301 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.8.prob, batch_count=11369.333333333334, ans=0.125
2024-10-09 09:30:20,830 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=11369.333333333334, ans=0.125
2024-10-09 09:30:20,879 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=11369.333333333334, ans=0.125
2024-10-09 09:30:20,925 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=11369.333333333334, ans=0.18630666666666665
2024-10-09 09:31:03,323 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=11379.333333333334, ans=0.125
2024-10-09 09:31:11,125 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=11382.666666666666, ans=0.125
2024-10-09 09:31:23,444 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=11382.666666666666, ans=0.008395072463768116
2024-10-09 09:31:24,222 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.33 vs. limit=4.7074
2024-10-09 09:31:27,889 INFO [train.py:1152] Epoch 5, batch 6750, loss[loss=0.2391, ctc_loss=0.1672, attn_decoder_loss=0.2571, over 4909.00 frames. ], tot_loss[loss=0.2548, ctc_loss=0.1815, attn_decoder_loss=0.2731, over 972273.57 frames. ], batch size: 19, lr: 1.69e-02,
2024-10-09 09:31:41,340 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=16.03 vs. limit=16.0395
2024-10-09 09:31:56,248 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=11389.333333333334, ans=0.019211111111111113
2024-10-09 09:32:22,719 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11396.0, ans=0.18603999999999998
2024-10-09 09:32:29,777 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.73 vs. limit=16.049500000000002
2024-10-09 09:32:33,685 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.778e+01 6.187e+01 6.946e+01 7.956e+01 1.353e+02, threshold=1.389e+02, percent-clipped=0.0
2024-10-09 09:32:46,187 INFO [train.py:1152] Epoch 5, batch 6800, loss[loss=0.2596, ctc_loss=0.196, attn_decoder_loss=0.2755, over 4908.00 frames. ], tot_loss[loss=0.2521, ctc_loss=0.1784, attn_decoder_loss=0.2705, over 974593.25 frames. ], batch size: 19, lr: 1.69e-02,
2024-10-09 09:32:51,778 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.00 vs. limit=11.776
2024-10-09 09:32:58,980 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=11402.666666666666, ans=0.019155555555555558
2024-10-09 09:33:01,304 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.00 vs. limit=11.77725
2024-10-09 09:33:03,032 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.83 vs. limit=4.7109000000000005
2024-10-09 09:33:10,236 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=11406.0, ans=0.125
2024-10-09 09:33:20,126 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.66 vs. limit=4.7114
2024-10-09 09:33:27,686 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=11409.333333333334, ans=0.00838927536231884
2024-10-09 09:33:29,289 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=11409.333333333334, ans=0.125
2024-10-09 09:33:35,697 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=11412.666666666666, ans=0.008388550724637681
2024-10-09 09:33:57,870 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=11416.0, ans=0.125
2024-10-09 09:34:05,589 INFO [train.py:1152] Epoch 5, batch 6850, loss[loss=0.2041, ctc_loss=0.1257, attn_decoder_loss=0.2237, over 4978.00 frames. ], tot_loss[loss=0.2495, ctc_loss=0.1758, attn_decoder_loss=0.2679, over 978923.04 frames. ], batch size: 19, lr: 1.69e-02,
2024-10-09 09:34:07,291 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_False/epoch-5.pt
2024-10-09 09:34:35,785 INFO [train.py:1152] Epoch 6, batch 0, loss[loss=0.2707, ctc_loss=0.2075, attn_decoder_loss=0.2865, over 4856.00 frames. ], tot_loss[loss=0.2707, ctc_loss=0.2075, attn_decoder_loss=0.2865, over 4856.00 frames. ], batch size: 19, lr: 1.58e-02,
2024-10-09 09:34:35,786 INFO [train.py:1175] Computing validation loss
2024-10-09 09:34:42,071 INFO [zipformer.py:1858] name=encoder.encoders.0.layers.1.self_attn_weights, attn_weights_entropy = tensor([4.9078, 4.3526, 4.3127, 4.7031], device='cuda:0')
2024-10-09 09:34:43,093 INFO [train.py:1184] Epoch 6, validation: loss=0.2144, ctc_loss=0.07841, attn_decoder_loss=0.2484, over 90464.00 frames.
2024-10-09 09:34:43,093 INFO [train.py:1185] Maximum memory allocated so far is 6613MB
2024-10-09 09:34:56,605 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=11423.333333333334, ans=0.01906944444444444
2024-10-09 09:34:58,016 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=11423.333333333334, ans=0.125
2024-10-09 09:35:15,418 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=11426.666666666666, ans=0.125
2024-10-09 09:35:16,729 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11426.666666666666, ans=0.18573333333333333
2024-10-09 09:35:24,289 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=11430.0, ans=0.125
2024-10-09 09:35:25,239 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=8.80 vs. limit=11.786249999999999
2024-10-09 09:35:35,033 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=11430.0, ans=0.008384782608695653
2024-10-09 09:35:40,783 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.130e+01 5.720e+01 6.957e+01 7.930e+01 1.516e+02, threshold=1.391e+02, percent-clipped=2.0
2024-10-09 09:35:55,953 INFO [train.py:1152] Epoch 6, batch 50, loss[loss=0.2499, ctc_loss=0.1711, attn_decoder_loss=0.2696, over 4908.00 frames. ], tot_loss[loss=0.2674, ctc_loss=0.198, attn_decoder_loss=0.2847, over 217728.37 frames. ], batch size: 19, lr: 1.57e-02,
2024-10-09 09:36:19,448 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=11440.0, ans=0.125
2024-10-09 09:36:19,470 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=11440.0, ans=0.125
2024-10-09 09:36:23,118 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=4.92 vs. limit=11.79
2024-10-09 09:36:26,184 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.25 vs. limit=7.860833333333334
2024-10-09 09:36:27,003 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=11443.333333333334, ans=0.4994833333333334
2024-10-09 09:36:31,501 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=11443.333333333334, ans=0.125
2024-10-09 09:36:37,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=11443.333333333334, ans=0.125
2024-10-09 09:36:52,764 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=11446.666666666666, ans=0.125
2024-10-09 09:36:56,493 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.21 vs. limit=7.8625
2024-10-09 09:36:57,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=11450.0, ans=0.125
2024-10-09 09:37:06,625 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=11450.0, ans=0.018958333333333337
2024-10-09 09:37:12,128 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.05 vs. limit=11.795
2024-10-09 09:37:12,676 INFO [train.py:1152] Epoch 6, batch 100, loss[loss=0.264, ctc_loss=0.1847, attn_decoder_loss=0.2838, over 4746.00 frames. ], tot_loss[loss=0.2684, ctc_loss=0.2014, attn_decoder_loss=0.2852, over 383329.15 frames. ], batch size: 19, lr: 1.57e-02,
2024-10-09 09:37:56,641 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=10.38 vs. limit=11.7975
2024-10-09 09:37:59,209 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=11463.333333333334, ans=0.018902777777777775
2024-10-09 09:38:14,728 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.294e+01 6.497e+01 7.346e+01 8.303e+01 1.394e+02, threshold=1.469e+02, percent-clipped=1.0
2024-10-09 09:38:16,343 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=11466.666666666666, ans=0.018888888888888893
2024-10-09 09:38:23,898 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11466.666666666666, ans=0.18533333333333335
2024-10-09 09:38:30,006 INFO [train.py:1152] Epoch 6, batch 150, loss[loss=0.2286, ctc_loss=0.1527, attn_decoder_loss=0.2476, over 4910.00 frames. ], tot_loss[loss=0.2646, ctc_loss=0.1959, attn_decoder_loss=0.2818, over 513281.80 frames. ], batch size: 19, lr: 1.57e-02,
2024-10-09 09:39:29,467 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=11483.333333333334, ans=0.125
2024-10-09 09:39:30,142 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.92 vs. limit=16.1125
2024-10-09 09:39:31,055 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=11483.333333333334, ans=0.01881944444444444
2024-10-09 09:39:33,179 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=11.02 vs. limit=11.80625
2024-10-09 09:39:35,633 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 09:39:46,230 INFO [train.py:1152] Epoch 6, batch 200, loss[loss=0.3192, ctc_loss=0.2704, attn_decoder_loss=0.3314, over 4740.00 frames. ], tot_loss[loss=0.2631, ctc_loss=0.1948, attn_decoder_loss=0.2802, over 613748.79 frames. ], batch size: 45, lr: 1.57e-02,
2024-10-09 09:40:06,233 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=11490.0, ans=0.01879166666666667
2024-10-09 09:40:15,692 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11493.333333333334, ans=0.18506666666666666
2024-10-09 09:40:18,536 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=11493.333333333334, ans=0.025
2024-10-09 09:40:32,219 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=17.77 vs. limit=16.122500000000002
2024-10-09 09:40:47,809 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.842e+01 6.170e+01 7.264e+01 8.009e+01 1.293e+02, threshold=1.453e+02, percent-clipped=0.0
2024-10-09 09:40:49,562 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11500.0, ans=0.185
2024-10-09 09:40:52,652 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11500.0, ans=0.185
2024-10-09 09:41:02,941 INFO [train.py:1152] Epoch 6, batch 250, loss[loss=0.2801, ctc_loss=0.1956, attn_decoder_loss=0.3012, over 4825.00 frames. ], tot_loss[loss=0.2624, ctc_loss=0.193, attn_decoder_loss=0.2797, over 692431.51 frames. ], batch size: 38, lr: 1.57e-02,
2024-10-09 09:41:15,043 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=11503.333333333334, ans=0.49738333333333334
2024-10-09 09:41:25,680 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=11506.666666666666, ans=0.05
2024-10-09 09:41:27,776 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.31 vs. limit=7.876666666666667
2024-10-09 09:41:28,926 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=11506.666666666666, ans=0.018722222222222223
2024-10-09 09:41:39,640 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.min_positive, batch_count=11510.0, ans=0.13490000000000002
2024-10-09 09:42:13,421 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=11516.666666666666, ans=0.125
2024-10-09 09:42:19,376 INFO [train.py:1152] Epoch 6, batch 300, loss[loss=0.2815, ctc_loss=0.2276, attn_decoder_loss=0.295, over 4752.00 frames. ], tot_loss[loss=0.2612, ctc_loss=0.1908, attn_decoder_loss=0.2789, over 752777.12 frames. ], batch size: 32, lr: 1.57e-02,
2024-10-09 09:42:19,604 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11520.0, ans=0.1848
2024-10-09 09:42:34,363 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=17.37 vs. limit=16.1425
2024-10-09 09:42:51,625 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=11526.666666666666, ans=0.018638888888888892
2024-10-09 09:43:02,315 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=11526.666666666666, ans=0.018638888888888892
2024-10-09 09:43:14,643 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=11530.0, ans=0.49645000000000006
2024-10-09 09:43:20,689 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.085e+01 6.663e+01 7.688e+01 8.468e+01 1.569e+02, threshold=1.538e+02, percent-clipped=2.0
2024-10-09 09:43:36,099 INFO [train.py:1152] Epoch 6, batch 350, loss[loss=0.2445, ctc_loss=0.1543, attn_decoder_loss=0.267, over 4883.00 frames. ], tot_loss[loss=0.2612, ctc_loss=0.1905, attn_decoder_loss=0.2789, over 800327.94 frames. ], batch size: 19, lr: 1.57e-02,
2024-10-09 09:43:54,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=11540.0, ans=0.125
2024-10-09 09:44:19,679 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.93 vs. limit=11.82875
2024-10-09 09:44:23,713 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=11546.666666666666, ans=0.0
2024-10-09 09:44:28,324 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=11546.666666666666, ans=0.125
2024-10-09 09:44:35,070 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.07 vs. limit=16.16
2024-10-09 09:44:52,686 INFO [train.py:1152] Epoch 6, batch 400, loss[loss=0.2492, ctc_loss=0.1748, attn_decoder_loss=0.2678, over 4883.00 frames. ], tot_loss[loss=0.2593, ctc_loss=0.1881, attn_decoder_loss=0.277, over 836993.22 frames. ], batch size: 22, lr: 1.57e-02,
2024-10-09 09:45:00,402 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=11553.333333333334, ans=0.125
2024-10-09 09:45:35,823 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=2.194e-02
2024-10-09 09:45:54,059 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.103e+01 6.102e+01 7.058e+01 7.991e+01 1.163e+02, threshold=1.412e+02, percent-clipped=0.0
2024-10-09 09:46:09,481 INFO [train.py:1152] Epoch 6, batch 450, loss[loss=0.2543, ctc_loss=0.1608, attn_decoder_loss=0.2776, over 4866.00 frames. ], tot_loss[loss=0.2589, ctc_loss=0.1869, attn_decoder_loss=0.2769, over 865666.32 frames. ], batch size: 23, lr: 1.57e-02,
2024-10-09 09:46:12,780 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=11570.0, ans=0.07
2024-10-09 09:46:41,876 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=11576.666666666666, ans=0.125
2024-10-09 09:46:49,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.56 vs. limit=11.841249999999999
2024-10-09 09:47:15,501 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=11583.333333333334, ans=0.04949747468305833
2024-10-09 09:47:20,287 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=11583.333333333334, ans=0.008351449275362318
2024-10-09 09:47:26,170 INFO [train.py:1152] Epoch 6, batch 500, loss[loss=0.2697, ctc_loss=0.1905, attn_decoder_loss=0.2896, over 4822.00 frames. ], tot_loss[loss=0.2577, ctc_loss=0.1854, attn_decoder_loss=0.2758, over 888156.62 frames. ], batch size: 34, lr: 1.56e-02,
2024-10-09 09:47:36,444 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.59 vs. limit=8.634666666666666
2024-10-09 09:47:44,618 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=11590.0, ans=0.125
2024-10-09 09:47:47,679 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11590.0, ans=0.18409999999999999
2024-10-09 09:47:58,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=11593.333333333334, ans=0.49423333333333336
2024-10-09 09:48:08,811 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.71 vs. limit=16.195
2024-10-09 09:48:11,595 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.74 vs. limit=16.197499999999998
2024-10-09 09:48:27,732 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.850e+01 6.450e+01 7.420e+01 8.406e+01 1.220e+02, threshold=1.484e+02, percent-clipped=0.0
2024-10-09 09:48:43,014 INFO [train.py:1152] Epoch 6, batch 550, loss[loss=0.2353, ctc_loss=0.1562, attn_decoder_loss=0.255, over 4804.00 frames. ], tot_loss[loss=0.258, ctc_loss=0.1863, attn_decoder_loss=0.2759, over 905589.97 frames. ], batch size: 40, lr: 1.56e-02,
2024-10-09 09:49:00,201 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=11606.666666666666, ans=0.125
2024-10-09 09:49:09,498 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=11606.666666666666, ans=0.125
2024-10-09 09:49:20,291 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=11610.0, ans=0.008345652173913044
2024-10-09 09:49:29,523 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=11613.333333333334, ans=0.025
2024-10-09 09:49:32,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=11613.333333333334, ans=0.018277777777777775
2024-10-09 09:49:37,163 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=11613.333333333334, ans=0.4935333333333334
2024-10-09 09:50:00,316 INFO [train.py:1152] Epoch 6, batch 600, loss[loss=0.2845, ctc_loss=0.2152, attn_decoder_loss=0.3018, over 4833.00 frames. ], tot_loss[loss=0.2582, ctc_loss=0.1865, attn_decoder_loss=0.2761, over 919510.78 frames. ], batch size: 38, lr: 1.56e-02,
2024-10-09 09:50:11,123 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=11620.0, ans=0.125
2024-10-09 09:50:12,549 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=11620.0, ans=0.49330000000000007
2024-10-09 09:50:13,996 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=11623.333333333334, ans=0.018236111111111106
2024-10-09 09:50:21,758 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer1.prob, batch_count=11623.333333333334, ans=0.125
2024-10-09 09:50:34,142 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=11626.666666666666, ans=0.07
2024-10-09 09:50:38,738 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=11626.666666666666, ans=0.125
2024-10-09 09:51:01,652 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.465e+01 6.130e+01 7.211e+01 8.062e+01 1.123e+02, threshold=1.442e+02, percent-clipped=0.0
2024-10-09 09:51:12,662 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=11633.333333333334, ans=0.125
2024-10-09 09:51:14,122 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=11633.333333333334, ans=0.008340579710144928
2024-10-09 09:51:17,040 INFO [train.py:1152] Epoch 6, batch 650, loss[loss=0.2794, ctc_loss=0.2106, attn_decoder_loss=0.2966, over 4841.00 frames. ], tot_loss[loss=0.2572, ctc_loss=0.1848, attn_decoder_loss=0.2753, over 930358.40 frames. ], batch size: 21, lr: 1.56e-02,
2024-10-09 09:51:37,873 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=9.14 vs. limit=11.865
2024-10-09 09:51:41,424 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=11640.0, ans=0.125
2024-10-09 09:51:52,872 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.44 vs. limit=11.86625
2024-10-09 09:52:14,392 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.59 vs. limit=16.235
2024-10-09 09:52:33,451 INFO [train.py:1152] Epoch 6, batch 700, loss[loss=0.2674, ctc_loss=0.2158, attn_decoder_loss=0.2803, over 4750.00 frames. ], tot_loss[loss=0.2576, ctc_loss=0.1855, attn_decoder_loss=0.2756, over 938185.56 frames. ], batch size: 19, lr: 1.56e-02,
2024-10-09 09:52:34,273 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.55 vs. limit=11.870000000000001
2024-10-09 09:52:45,852 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=11653.333333333334, ans=0.0
2024-10-09 09:52:55,064 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=11656.666666666666, ans=0.125
2024-10-09 09:53:07,329 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=11660.0, ans=0.125
2024-10-09 09:53:19,005 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.16 vs. limit=16.247500000000002
2024-10-09 09:53:35,089 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.281e+01 6.571e+01 7.549e+01 8.216e+01 1.106e+02, threshold=1.510e+02, percent-clipped=0.0
2024-10-09 09:53:51,279 INFO [train.py:1152] Epoch 6, batch 750, loss[loss=0.2408, ctc_loss=0.1716, attn_decoder_loss=0.2581, over 4883.00 frames. ], tot_loss[loss=0.2567, ctc_loss=0.1841, attn_decoder_loss=0.2749, over 944972.82 frames. ], batch size: 22, lr: 1.56e-02,
2024-10-09 09:53:57,721 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.30 vs. limit=16.252499999999998
2024-10-09 09:54:04,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=11673.333333333334, ans=0.008331884057971015
2024-10-09 09:54:16,773 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=11673.333333333334, ans=0.125
2024-10-09 09:54:22,800 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=11676.666666666666, ans=0.025
2024-10-09 09:54:27,926 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=10.95 vs. limit=11.87875
2024-10-09 09:54:51,852 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=11683.333333333334, ans=0.125
2024-10-09 09:54:53,883 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.99 vs. limit=4.7524999999999995
2024-10-09 09:55:06,959 INFO [train.py:1152] Epoch 6, batch 800, loss[loss=0.2111, ctc_loss=0.1488, attn_decoder_loss=0.2266, over 4850.00 frames. ], tot_loss[loss=0.2569, ctc_loss=0.1848, attn_decoder_loss=0.2749, over 949735.99 frames. ], batch size: 19, lr: 1.56e-02,
2024-10-09 09:55:25,614 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=11690.0, ans=0.008328260869565217
2024-10-09 09:55:38,476 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.56 vs. limit=16.27
2024-10-09 09:55:44,277 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=8.39 vs. limit=11.885
2024-10-09 09:56:08,177 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.965e+01 6.375e+01 7.256e+01 8.229e+01 1.995e+02, threshold=1.451e+02, percent-clipped=1.0
2024-10-09 09:56:23,672 INFO [train.py:1152] Epoch 6, batch 850, loss[loss=0.2691, ctc_loss=0.1926, attn_decoder_loss=0.2882, over 4789.00 frames. ], tot_loss[loss=0.2574, ctc_loss=0.1851, attn_decoder_loss=0.2754, over 953982.07 frames. ], batch size: 29, lr: 1.56e-02,
2024-10-09 09:56:26,840 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=11703.333333333334, ans=0.125
2024-10-09 09:56:32,359 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.89 vs. limit=8.681333333333335
2024-10-09 09:56:53,485 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.01 vs. limit=16.2825
2024-10-09 09:57:19,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.32 vs. limit=8.685333333333332
2024-10-09 09:57:20,558 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=11713.333333333334, ans=0.017861111111111105
2024-10-09 09:57:29,880 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=11716.666666666666, ans=0.125
2024-10-09 09:57:33,735 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.76 vs. limit=4.7575
2024-10-09 09:57:40,520 INFO [train.py:1152] Epoch 6, batch 900, loss[loss=0.2466, ctc_loss=0.1501, attn_decoder_loss=0.2708, over 4859.00 frames. ], tot_loss[loss=0.2574, ctc_loss=0.1851, attn_decoder_loss=0.2754, over 956959.21 frames. ], batch size: 19, lr: 1.56e-02,
2024-10-09 09:57:42,233 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=11720.0, ans=0.125
2024-10-09 09:57:46,936 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=11720.0, ans=0.125
2024-10-09 09:57:50,013 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=11720.0, ans=0.0
2024-10-09 09:58:01,593 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.96 vs. limit=11.89625
2024-10-09 09:58:16,832 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.78 vs. limit=11.8975
2024-10-09 09:58:18,437 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.07 vs. limit=16.295
2024-10-09 09:58:26,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=11730.0, ans=0.125
2024-10-09 09:58:31,513 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=11730.0, ans=0.125
2024-10-09 09:58:42,237 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.594e+01 6.118e+01 6.949e+01 8.194e+01 1.132e+02, threshold=1.390e+02, percent-clipped=0.0
2024-10-09 09:58:46,034 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.30 vs. limit=16.3
2024-10-09 09:58:49,514 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.whiten.whitening_limit, batch_count=11733.333333333334, ans=8.693333333333333
2024-10-09 09:58:57,749 INFO [train.py:1152] Epoch 6, batch 950, loss[loss=0.2546, ctc_loss=0.1721, attn_decoder_loss=0.2752, over 4816.00 frames. ], tot_loss[loss=0.2578, ctc_loss=0.1853, attn_decoder_loss=0.276, over 958743.56 frames. ], batch size: 19, lr: 1.56e-02,
2024-10-09 09:59:16,451 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=11740.0, ans=0.125
2024-10-09 09:59:18,624 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.68 vs. limit=16.305
2024-10-09 09:59:27,341 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=11743.333333333334, ans=0.125
2024-10-09 09:59:33,443 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=11743.333333333334, ans=0.4889833333333333
2024-10-09 09:59:41,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=11743.333333333334, ans=0.125
2024-10-09 09:59:50,342 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer_ff3.min_abs, batch_count=11746.666666666666, ans=0.2
2024-10-09 09:59:59,546 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11750.0, ans=0.1825
2024-10-09 10:00:08,502 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=6.19 vs. limit=6.35
2024-10-09 10:00:10,357 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=11750.0, ans=0.48875
2024-10-09 10:00:14,906 INFO [train.py:1152] Epoch 6, batch 1000, loss[loss=0.2448, ctc_loss=0.1743, attn_decoder_loss=0.2624, over 4935.00 frames. ], tot_loss[loss=0.2592, ctc_loss=0.1868, attn_decoder_loss=0.2773, over 960553.03 frames. ], batch size: 20, lr: 1.55e-02,
2024-10-09 10:00:18,147 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 10:00:39,747 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=11756.666666666666, ans=0.125
2024-10-09 10:01:16,708 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.000e+01 6.510e+01 7.187e+01 8.158e+01 9.967e+01, threshold=1.437e+02, percent-clipped=0.0
2024-10-09 10:01:17,688 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.13 vs. limit=11.9125
2024-10-09 10:01:29,397 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=11766.666666666666, ans=0.01763888888888889
2024-10-09 10:01:32,297 INFO [train.py:1152] Epoch 6, batch 1050, loss[loss=0.2217, ctc_loss=0.1362, attn_decoder_loss=0.2431, over 4818.00 frames. ], tot_loss[loss=0.2588, ctc_loss=0.1863, attn_decoder_loss=0.2769, over 962776.70 frames. ], batch size: 25, lr: 1.55e-02,
2024-10-09 10:01:41,708 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=11770.0, ans=0.125
2024-10-09 10:01:44,831 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=11770.0, ans=0.017625000000000002
2024-10-09 10:02:11,152 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=11776.666666666666, ans=0.125
2024-10-09 10:02:17,184 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=11780.0, ans=0.125
2024-10-09 10:02:20,424 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=11780.0, ans=0.125
2024-10-09 10:02:32,610 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=11783.333333333334, ans=0.125
2024-10-09 10:02:49,327 INFO [train.py:1152] Epoch 6, batch 1100, loss[loss=0.2778, ctc_loss=0.2158, attn_decoder_loss=0.2933, over 4853.00 frames. ], tot_loss[loss=0.2592, ctc_loss=0.1871, attn_decoder_loss=0.2772, over 964195.05 frames. ], batch size: 20, lr: 1.55e-02,
2024-10-09 10:02:53,286 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.56 vs. limit=11.92
2024-10-09 10:03:51,025 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.418e+01 6.447e+01 7.035e+01 7.834e+01 1.191e+02, threshold=1.407e+02, percent-clipped=0.0
2024-10-09 10:03:52,638 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=11800.0, ans=0.48700000000000004
2024-10-09 10:03:58,752 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=11800.0, ans=0.125
2024-10-09 10:04:06,251 INFO [train.py:1152] Epoch 6, batch 1150, loss[loss=0.2498, ctc_loss=0.1799, attn_decoder_loss=0.2673, over 4862.00 frames. ], tot_loss[loss=0.258, ctc_loss=0.1849, attn_decoder_loss=0.2763, over 964509.46 frames. ], batch size: 20, lr: 1.55e-02,
2024-10-09 10:04:23,452 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=11806.666666666666, ans=0.025
2024-10-09 10:04:34,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=11806.666666666666, ans=0.01747222222222223
2024-10-09 10:04:45,266 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=11.85 vs. limit=11.92875
2024-10-09 10:05:20,760 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.63 vs. limit=8.726666666666667
2024-10-09 10:05:22,856 INFO [train.py:1152] Epoch 6, batch 1200, loss[loss=0.2722, ctc_loss=0.1899, attn_decoder_loss=0.2928, over 4811.00 frames. ], tot_loss[loss=0.258, ctc_loss=0.1847, attn_decoder_loss=0.2764, over 964453.90 frames. ], batch size: 25, lr: 1.55e-02,
2024-10-09 10:05:30,243 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=3.85 vs. limit=8.728
2024-10-09 10:05:41,553 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=11823.333333333334, ans=0.00829927536231884
2024-10-09 10:05:50,941 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=11823.333333333334, ans=0.125
2024-10-09 10:06:25,306 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.826e+01 6.553e+01 7.575e+01 8.236e+01 1.104e+02, threshold=1.515e+02, percent-clipped=0.0
2024-10-09 10:06:32,662 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11833.333333333334, ans=0.18166666666666664
2024-10-09 10:06:40,035 INFO [train.py:1152] Epoch 6, batch 1250, loss[loss=0.2611, ctc_loss=0.1945, attn_decoder_loss=0.2778, over 4741.00 frames. ], tot_loss[loss=0.2579, ctc_loss=0.1851, attn_decoder_loss=0.2761, over 964373.59 frames. ], batch size: 32, lr: 1.55e-02,
2024-10-09 10:06:56,752 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=11840.0, ans=0.01733333333333334
2024-10-09 10:07:02,202 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.75 vs. limit=10.92
2024-10-09 10:07:07,291 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=11840.0, ans=0.0
2024-10-09 10:07:08,159 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=17.52 vs. limit=10.92
2024-10-09 10:07:13,541 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=11843.333333333334, ans=0.008294927536231884
2024-10-09 10:07:14,426 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.20 vs. limit=7.960833333333333
2024-10-09 10:07:18,066 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11843.333333333334, ans=0.18156666666666665
2024-10-09 10:07:44,384 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=11850.0, ans=0.125
2024-10-09 10:07:45,924 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=11850.0, ans=0.8684999999999999
2024-10-09 10:07:48,921 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=11850.0, ans=0.008293478260869565
2024-10-09 10:07:55,031 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=11853.333333333334, ans=0.125
2024-10-09 10:07:55,732 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.31 vs. limit=8.741333333333333
2024-10-09 10:07:56,405 INFO [train.py:1152] Epoch 6, batch 1300, loss[loss=0.2993, ctc_loss=0.2471, attn_decoder_loss=0.3124, over 4828.00 frames. ], tot_loss[loss=0.2575, ctc_loss=0.1844, attn_decoder_loss=0.2758, over 965644.07 frames. ], batch size: 43, lr: 1.55e-02,
2024-10-09 10:08:07,594 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11853.333333333334, ans=0.18146666666666667
2024-10-09 10:08:09,081 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=11853.333333333334, ans=0.125
2024-10-09 10:08:12,064 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=11856.666666666666, ans=0.125
2024-10-09 10:08:29,167 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=11860.0, ans=0.125
2024-10-09 10:08:35,263 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=11860.0, ans=0.008291304347826087
2024-10-09 10:08:50,868 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=11863.333333333334, ans=0.48478333333333334
2024-10-09 10:08:58,449 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.258e+01 6.446e+01 7.034e+01 7.947e+01 1.271e+02, threshold=1.407e+02, percent-clipped=0.0
2024-10-09 10:09:13,953 INFO [train.py:1152] Epoch 6, batch 1350, loss[loss=0.2449, ctc_loss=0.1604, attn_decoder_loss=0.266, over 4842.00 frames. ], tot_loss[loss=0.256, ctc_loss=0.1827, attn_decoder_loss=0.2743, over 966366.26 frames. ], batch size: 21, lr: 1.55e-02,
2024-10-09 10:09:14,751 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=12.02 vs. limit=11.95125
2024-10-09 10:09:59,372 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=11880.0, ans=0.125
2024-10-09 10:10:05,490 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=11880.0, ans=0.125
2024-10-09 10:10:07,060 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=11880.0, ans=0.4842
2024-10-09 10:10:22,502 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=11883.333333333334, ans=0.00828623188405797
2024-10-09 10:10:31,758 INFO [train.py:1152] Epoch 6, batch 1400, loss[loss=0.2548, ctc_loss=0.2083, attn_decoder_loss=0.2665, over 4940.00 frames. ], tot_loss[loss=0.256, ctc_loss=0.1827, attn_decoder_loss=0.2743, over 966644.81 frames. ], batch size: 19, lr: 1.55e-02,
2024-10-09 10:11:01,602 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=1.069e-01
2024-10-09 10:11:07,745 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=11893.333333333334, ans=0.125
2024-10-09 10:11:20,277 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=11896.666666666666, ans=0.4836166666666667
2024-10-09 10:11:26,490 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=11896.666666666666, ans=0.008283333333333334
2024-10-09 10:11:28,103 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11896.666666666666, ans=0.18103333333333332
2024-10-09 10:11:28,842 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.03 vs. limit=7.974166666666667
2024-10-09 10:11:34,000 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.360e+01 6.297e+01 7.043e+01 7.833e+01 1.256e+02, threshold=1.409e+02, percent-clipped=0.0
2024-10-09 10:11:42,152 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=11900.0, ans=0.48350000000000004
2024-10-09 10:11:49,806 INFO [train.py:1152] Epoch 6, batch 1450, loss[loss=0.2826, ctc_loss=0.2354, attn_decoder_loss=0.2943, over 4810.00 frames. ], tot_loss[loss=0.2571, ctc_loss=0.1838, attn_decoder_loss=0.2754, over 966496.85 frames. ], batch size: 34, lr: 1.54e-02,
2024-10-09 10:12:10,091 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=11906.666666666666, ans=0.008281159420289855
2024-10-09 10:12:10,123 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11906.666666666666, ans=0.18093333333333333
2024-10-09 10:13:07,522 INFO [train.py:1152] Epoch 6, batch 1500, loss[loss=0.237, ctc_loss=0.1615, attn_decoder_loss=0.2559, over 4749.00 frames. ], tot_loss[loss=0.2574, ctc_loss=0.1849, attn_decoder_loss=0.2756, over 966213.36 frames. ], batch size: 26, lr: 1.54e-02,
2024-10-09 10:13:27,287 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.45 vs. limit=7.980833333333333
2024-10-09 10:13:38,127 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.80 vs. limit=11.9725
2024-10-09 10:13:40,564 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=11926.666666666666, ans=0.125
2024-10-09 10:13:48,455 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11926.666666666666, ans=0.18073333333333336
2024-10-09 10:13:48,466 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=11926.666666666666, ans=0.125
2024-10-09 10:13:52,199 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.29 vs. limit=16.445
2024-10-09 10:13:54,701 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=11930.0, ans=0.125
2024-10-09 10:13:59,354 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=11930.0, ans=0.09899494936611666
2024-10-09 10:14:10,179 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.204e+01 6.362e+01 7.216e+01 8.291e+01 2.562e+02, threshold=1.443e+02, percent-clipped=1.0
2024-10-09 10:14:25,658 INFO [train.py:1152] Epoch 6, batch 1550, loss[loss=0.2546, ctc_loss=0.1819, attn_decoder_loss=0.2728, over 4859.00 frames. ], tot_loss[loss=0.2582, ctc_loss=0.1865, attn_decoder_loss=0.2762, over 966062.58 frames. ], batch size: 31, lr: 1.54e-02,
2024-10-09 10:14:49,257 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=11940.0, ans=0.01691666666666667
2024-10-09 10:14:50,626 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=11940.0, ans=0.125
2024-10-09 10:14:50,875 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=11940.0, ans=0.01691666666666667
2024-10-09 10:15:31,993 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.77 vs. limit=11.98125
2024-10-09 10:15:37,688 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=11950.0, ans=0.48175
2024-10-09 10:15:41,553 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=13.02 vs. limit=11.98125
2024-10-09 10:15:43,830 INFO [train.py:1152] Epoch 6, batch 1600, loss[loss=0.253, ctc_loss=0.1482, attn_decoder_loss=0.2792, over 4826.00 frames. ], tot_loss[loss=0.2573, ctc_loss=0.1846, attn_decoder_loss=0.2755, over 966319.82 frames. ], batch size: 25, lr: 1.54e-02,
2024-10-09 10:15:53,418 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=11953.333333333334, ans=0.025
2024-10-09 10:16:30,737 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=11963.333333333334, ans=0.48128333333333334
2024-10-09 10:16:36,994 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=11963.333333333334, ans=0.07
2024-10-09 10:16:40,926 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.32 vs. limit=11.98625
2024-10-09 10:16:44,039 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.46 vs. limit=4.7945
2024-10-09 10:16:46,101 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.986e+01 6.608e+01 7.341e+01 8.401e+01 1.163e+02, threshold=1.468e+02, percent-clipped=0.0
2024-10-09 10:16:46,310 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=11966.666666666666, ans=0.125
2024-10-09 10:16:55,827 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11966.666666666666, ans=0.18033333333333335
2024-10-09 10:17:01,787 INFO [train.py:1152] Epoch 6, batch 1650, loss[loss=0.2663, ctc_loss=0.1691, attn_decoder_loss=0.2905, over 4778.00 frames. ], tot_loss[loss=0.2578, ctc_loss=0.1859, attn_decoder_loss=0.2758, over 966617.45 frames. ], batch size: 29, lr: 1.54e-02,
2024-10-09 10:17:24,275 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=11973.333333333334, ans=0.125
2024-10-09 10:17:25,884 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=11973.333333333334, ans=0.18026666666666666
2024-10-09 10:17:29,483 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=4.76 vs. limit=11.99
2024-10-09 10:17:47,032 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.53 vs. limit=16.4825
2024-10-09 10:17:54,254 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=11980.0, ans=0.05
2024-10-09 10:18:07,456 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=18.81 vs. limit=16.4875
2024-10-09 10:18:10,997 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=9.05 vs. limit=11.99375
2024-10-09 10:18:20,797 INFO [train.py:1152] Epoch 6, batch 1700, loss[loss=0.2284, ctc_loss=0.1722, attn_decoder_loss=0.2425, over 4940.00 frames. ], tot_loss[loss=0.2571, ctc_loss=0.1841, attn_decoder_loss=0.2753, over 966739.34 frames. ], batch size: 19, lr: 1.54e-02,
2024-10-09 10:18:20,944 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=11986.666666666666, ans=0.07
2024-10-09 10:19:08,683 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn2.whiten.whitening_limit, batch_count=11996.666666666666, ans=16.497500000000002
2024-10-09 10:19:21,865 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_False_ctc_True_attdecoder_True_streaming_False/checkpoint-36000.pt
2024-10-09 10:19:24,393 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.295e+01 6.177e+01 7.130e+01 8.519e+01 1.327e+02, threshold=1.426e+02, percent-clipped=0.0
2024-10-09 10:19:39,111 INFO [train.py:1152] Epoch 6, batch 1750, loss[loss=0.2316, ctc_loss=0.1467, attn_decoder_loss=0.2528, over 4959.00 frames. ], tot_loss[loss=0.2564, ctc_loss=0.1826, attn_decoder_loss=0.2749, over 966927.25 frames. ], batch size: 19, lr: 1.54e-02,
2024-10-09 10:20:12,553 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.70 vs. limit=4.8015
2024-10-09 10:20:40,026 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=12016.666666666666, ans=0.125
2024-10-09 10:20:40,634 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.77 vs. limit=12.00625
2024-10-09 10:20:57,084 INFO [train.py:1152] Epoch 6, batch 1800, loss[loss=0.2465, ctc_loss=0.1757, attn_decoder_loss=0.2642, over 4868.00 frames. ], tot_loss[loss=0.258, ctc_loss=0.1844, attn_decoder_loss=0.2764, over 967564.83 frames. ], batch size: 23, lr: 1.54e-02,
2024-10-09 10:20:58,778 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=12020.0, ans=0.125
2024-10-09 10:21:17,747 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=12023.333333333334, ans=0.125
2024-10-09 10:21:19,358 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=12023.333333333334, ans=0.008255797101449275
2024-10-09 10:21:21,061 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12023.333333333334, ans=0.17976666666666666
2024-10-09 10:21:35,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12026.666666666666, ans=0.17973333333333336
2024-10-09 10:21:53,815 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=10.22 vs. limit=12.01125
2024-10-09 10:22:00,459 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.344e+01 6.486e+01 7.150e+01 8.451e+01 1.373e+02, threshold=1.430e+02, percent-clipped=0.0
2024-10-09 10:22:16,092 INFO [train.py:1152] Epoch 6, batch 1850, loss[loss=0.2754, ctc_loss=0.19, attn_decoder_loss=0.2968, over 4757.00 frames. ], tot_loss[loss=0.2578, ctc_loss=0.1844, attn_decoder_loss=0.2761, over 967884.72 frames. ], batch size: 26, lr: 1.54e-02,
2024-10-09 10:22:50,901 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=12043.333333333334, ans=0.008251449275362319
2024-10-09 10:23:01,760 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12046.666666666666, ans=0.17953333333333332
2024-10-09 10:23:34,487 INFO [train.py:1152] Epoch 6, batch 1900, loss[loss=0.2953, ctc_loss=0.2304, attn_decoder_loss=0.3115, over 4802.00 frames. ], tot_loss[loss=0.2574, ctc_loss=0.1835, attn_decoder_loss=0.2758, over 967726.16 frames. ], batch size: 29, lr: 1.54e-02,
2024-10-09 10:24:01,855 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=12056.666666666666, ans=0.025
2024-10-09 10:24:14,933 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.92 vs. limit=11.030000000000001
2024-10-09 10:24:20,659 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=12063.333333333334, ans=0.125
2024-10-09 10:24:37,738 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.077e+01 6.182e+01 7.036e+01 8.137e+01 1.652e+02, threshold=1.407e+02, percent-clipped=1.0
2024-10-09 10:24:37,813 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=12066.666666666666, ans=0.125
2024-10-09 10:24:41,642 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.58 vs. limit=8.016666666666666
2024-10-09 10:24:53,193 INFO [train.py:1152] Epoch 6, batch 1950, loss[loss=0.2403, ctc_loss=0.1554, attn_decoder_loss=0.2615, over 4867.00 frames. ], tot_loss[loss=0.2575, ctc_loss=0.1828, attn_decoder_loss=0.2761, over 966726.57 frames. ], batch size: 20, lr: 1.53e-02,
2024-10-09 10:25:05,916 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 10:25:05,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer_na.min_abs, batch_count=12070.0, ans=0.02
2024-10-09 10:25:35,290 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=12076.666666666666, ans=0.008244202898550724
2024-10-09 10:25:36,866 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=12076.666666666666, ans=0.125
2024-10-09 10:25:42,733 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=9.84 vs. limit=12.030000000000001
2024-10-09 10:26:11,308 INFO [train.py:1152] Epoch 6, batch 2000, loss[loss=0.2219, ctc_loss=0.1347, attn_decoder_loss=0.2437, over 4959.00 frames. ], tot_loss[loss=0.2579, ctc_loss=0.1836, attn_decoder_loss=0.2765, over 966489.75 frames. ], batch size: 19, lr: 1.53e-02,
2024-10-09 10:26:15,189 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.92 vs. limit=12.032499999999999
2024-10-09 10:26:16,112 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12086.666666666666, ans=0.17913333333333334
2024-10-09 10:26:30,104 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=12090.0, ans=0.125
2024-10-09 10:26:44,335 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=12093.333333333334, ans=0.016277777777777773
2024-10-09 10:26:48,187 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.48 vs. limit=11.046666666666667
2024-10-09 10:26:52,169 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=12093.333333333334, ans=0.125
2024-10-09 10:26:53,694 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=12093.333333333334, ans=0.125
2024-10-09 10:26:56,896 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=12096.666666666666, ans=0.125
2024-10-09 10:26:59,247 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=13.18 vs. limit=12.036249999999999
2024-10-09 10:26:59,335 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.43 vs. limit=8.838666666666667
2024-10-09 10:27:04,490 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=12096.666666666666, ans=0.125
2024-10-09 10:27:13,758 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.893e+01 6.514e+01 7.269e+01 7.879e+01 1.287e+02, threshold=1.454e+02, percent-clipped=0.0
2024-10-09 10:27:13,953 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=12100.0, ans=0.125
2024-10-09 10:27:13,987 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=12100.0, ans=0.125
2024-10-09 10:27:21,776 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=12100.0, ans=0.01625
2024-10-09 10:27:29,537 INFO [train.py:1152] Epoch 6, batch 2050, loss[loss=0.2556, ctc_loss=0.1985, attn_decoder_loss=0.2698, over 4910.00 frames. ], tot_loss[loss=0.2572, ctc_loss=0.1829, attn_decoder_loss=0.2758, over 966908.52 frames. ], batch size: 19, lr: 1.53e-02,
2024-10-09 10:27:36,020 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=12103.333333333334, ans=0.125
2024-10-09 10:27:42,222 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=12103.333333333334, ans=0.00823840579710145
2024-10-09 10:28:00,059 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.25 vs. limit=16.5825
2024-10-09 10:28:21,488 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer_ff2.min_abs, batch_count=12113.333333333334, ans=0.1
2024-10-09 10:28:35,517 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=12116.666666666666, ans=0.125
2024-10-09 10:28:48,272 INFO [train.py:1152] Epoch 6, batch 2100, loss[loss=0.2351, ctc_loss=0.1321, attn_decoder_loss=0.2609, over 4852.00 frames. ], tot_loss[loss=0.2568, ctc_loss=0.1825, attn_decoder_loss=0.2754, over 967074.82 frames. ], batch size: 21, lr: 1.53e-02,
2024-10-09 10:28:56,908 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.60 vs. limit=8.03
2024-10-09 10:29:01,074 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=12120.0, ans=0.4758
2024-10-09 10:29:04,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.attention_skip_rate, batch_count=12123.333333333334, ans=0.016152777777777773
2024-10-09 10:29:11,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=12123.333333333334, ans=0.125
2024-10-09 10:29:14,299 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.68 vs. limit=16.5925
2024-10-09 10:29:24,556 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=12126.666666666666, ans=0.125
2024-10-09 10:29:29,185 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 10:29:32,890 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.03 vs. limit=8.031666666666666
2024-10-09 10:29:51,642 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.687e+01 6.258e+01 7.004e+01 8.080e+01 1.077e+02, threshold=1.401e+02, percent-clipped=0.0
2024-10-09 10:29:59,368 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=12133.333333333334, ans=0.125
2024-10-09 10:30:06,849 INFO [train.py:1152] Epoch 6, batch 2150, loss[loss=0.2522, ctc_loss=0.1773, attn_decoder_loss=0.2709, over 4871.00 frames. ], tot_loss[loss=0.2561, ctc_loss=0.182, attn_decoder_loss=0.2746, over 967787.36 frames. ], batch size: 20, lr: 1.53e-02,
2024-10-09 10:30:17,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12136.666666666666, ans=0.17863333333333334
2024-10-09 10:30:33,303 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12140.0, ans=0.17859999999999998
2024-10-09 10:30:36,015 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=10.32 vs. limit=11.07
2024-10-09 10:30:53,845 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 10:31:24,707 INFO [train.py:1152] Epoch 6, batch 2200, loss[loss=0.2568, ctc_loss=0.1854, attn_decoder_loss=0.2747, over 4741.00 frames. ], tot_loss[loss=0.2566, ctc_loss=0.1821, attn_decoder_loss=0.2753, over 967525.41 frames. ], batch size: 26, lr: 1.53e-02,
2024-10-09 10:32:02,791 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=12160.0, ans=0.016
2024-10-09 10:32:04,331 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=12160.0, ans=0.125
2024-10-09 10:32:15,321 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=12163.333333333334, ans=0.125
2024-10-09 10:32:27,818 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.737e+01 6.196e+01 7.244e+01 8.241e+01 1.402e+02, threshold=1.449e+02, percent-clipped=1.0
2024-10-09 10:32:31,234 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer2.prob, batch_count=12166.666666666666, ans=0.125
2024-10-09 10:32:43,646 INFO [train.py:1152] Epoch 6, batch 2250, loss[loss=0.2443, ctc_loss=0.161, attn_decoder_loss=0.2651, over 4892.00 frames. ], tot_loss[loss=0.2572, ctc_loss=0.1833, attn_decoder_loss=0.2757, over 967656.19 frames. ], batch size: 22, lr: 1.53e-02,
2024-10-09 10:32:56,328 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=12170.0, ans=0.0
2024-10-09 10:33:07,134 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=12173.333333333334, ans=0.125
2024-10-09 10:34:01,985 INFO [train.py:1152] Epoch 6, batch 2300, loss[loss=0.219, ctc_loss=0.1234, attn_decoder_loss=0.2429, over 4883.00 frames. ], tot_loss[loss=0.2567, ctc_loss=0.1831, attn_decoder_loss=0.2751, over 968338.17 frames. ], batch size: 19, lr: 1.53e-02,
2024-10-09 10:34:06,947 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=12186.666666666666, ans=0.01588888888888889
2024-10-09 10:34:16,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=12190.0, ans=0.0
2024-10-09 10:34:25,973 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=12190.0, ans=0.17809999999999998
2024-10-09 10:34:56,495 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.27 vs. limit=16.6475
2024-10-09 10:34:57,376 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=12196.666666666666, ans=0.008218115942028986
2024-10-09 10:34:58,997 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=12196.666666666666, ans=0.125
2024-10-09 10:34:59,011 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12196.666666666666, ans=0.17803333333333332
2024-10-09 10:35:05,140 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.638e+01 6.405e+01 7.230e+01 7.930e+01 1.095e+02, threshold=1.446e+02, percent-clipped=0.0
2024-10-09 10:35:16,410 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=12200.0, ans=0.125
2024-10-09 10:35:20,892 INFO [train.py:1152] Epoch 6, batch 2350, loss[loss=0.2433, ctc_loss=0.1594, attn_decoder_loss=0.2643, over 4860.00 frames. ], tot_loss[loss=0.2569, ctc_loss=0.1832, attn_decoder_loss=0.2754, over 968426.32 frames. ], batch size: 23, lr: 1.53e-02,
2024-10-09 10:35:29,482 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=4.34 vs. limit=12.07625
2024-10-09 10:35:58,978 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.16 vs. limit=4.8315
2024-10-09 10:36:06,188 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=12213.333333333334, ans=0.015777777777777773
2024-10-09 10:36:10,992 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=12213.333333333334, ans=0.015777777777777773
2024-10-09 10:36:25,007 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.8.prob, batch_count=12216.666666666666, ans=0.125
2024-10-09 10:36:26,742 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=12216.666666666666, ans=0.4724166666666667
2024-10-09 10:36:36,188 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=12216.666666666666, ans=0.01576388888888889
2024-10-09 10:36:39,057 INFO [train.py:1152] Epoch 6, batch 2400, loss[loss=0.225, ctc_loss=0.1352, attn_decoder_loss=0.2475, over 4750.00 frames. ], tot_loss[loss=0.2566, ctc_loss=0.1827, attn_decoder_loss=0.2751, over 967708.88 frames. ], batch size: 19, lr: 1.53e-02,
2024-10-09 10:36:51,572 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=12220.0, ans=0.125
2024-10-09 10:36:55,403 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=5.63 vs. limit=12.08375
2024-10-09 10:37:22,063 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=3.24 vs. limit=12.085
2024-10-09 10:37:22,850 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12226.666666666666, ans=0.17773333333333335
2024-10-09 10:37:24,464 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=12230.0, ans=0.015708333333333338
2024-10-09 10:37:29,046 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=12230.0, ans=0.47195000000000004
2024-10-09 10:37:32,183 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=12230.0, ans=0.125
2024-10-09 10:37:41,343 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.181e+01 6.495e+01 7.292e+01 8.208e+01 1.156e+02, threshold=1.458e+02, percent-clipped=0.0
2024-10-09 10:37:44,674 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=12233.333333333334, ans=0.4718333333333333
2024-10-09 10:37:57,137 INFO [train.py:1152] Epoch 6, batch 2450, loss[loss=0.2263, ctc_loss=0.1408, attn_decoder_loss=0.2477, over 4878.00 frames. ], tot_loss[loss=0.2568, ctc_loss=0.1831, attn_decoder_loss=0.2752, over 966876.52 frames. ], batch size: 22, lr: 1.52e-02,
2024-10-09 10:38:33,598 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=12243.333333333334, ans=0.4714833333333333
2024-10-09 10:38:53,697 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12246.666666666666, ans=0.17753333333333332
2024-10-09 10:39:01,647 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=12250.0, ans=0.09899494936611666
2024-10-09 10:39:15,468 INFO [train.py:1152] Epoch 6, batch 2500, loss[loss=0.2481, ctc_loss=0.2031, attn_decoder_loss=0.2594, over 4757.00 frames. ], tot_loss[loss=0.2569, ctc_loss=0.1827, attn_decoder_loss=0.2754, over 966490.08 frames. ], batch size: 26, lr: 1.52e-02,
2024-10-09 10:39:17,174 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=12253.333333333334, ans=0.125
2024-10-09 10:39:20,488 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=12253.333333333334, ans=0.01561111111111111
2024-10-09 10:39:36,330 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=12256.666666666666, ans=0.125
2024-10-09 10:39:39,464 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=12256.666666666666, ans=0.035
2024-10-09 10:39:41,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=12256.666666666666, ans=0.125
2024-10-09 10:39:51,154 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=18.11 vs. limit=16.695
2024-10-09 10:40:02,322 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.84 vs. limit=16.697499999999998
2024-10-09 10:40:11,102 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=2.602e-03
2024-10-09 10:40:15,523 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=12263.333333333334, ans=0.125
2024-10-09 10:40:15,611 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=12263.333333333334, ans=0.015569444444444441
2024-10-09 10:40:18,647 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.139e+01 6.511e+01 7.079e+01 7.985e+01 1.128e+02, threshold=1.416e+02, percent-clipped=0.0
2024-10-09 10:40:19,722 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=11.90 vs. limit=12.1
2024-10-09 10:40:22,077 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=12266.666666666666, ans=0.47066666666666673
2024-10-09 10:40:34,145 INFO [train.py:1152] Epoch 6, batch 2550, loss[loss=0.2248, ctc_loss=0.154, attn_decoder_loss=0.2425, over 4959.00 frames. ], tot_loss[loss=0.2564, ctc_loss=0.1824, attn_decoder_loss=0.2749, over 966851.53 frames. ], batch size: 19, lr: 1.52e-02,
2024-10-09 10:40:37,411 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=12270.0, ans=0.125
2024-10-09 10:40:42,779 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.11 vs. limit=16.7025
2024-10-09 10:40:46,753 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.52 vs. limit=16.7025
2024-10-09 10:40:47,440 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 10:41:01,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=12273.333333333334, ans=0.09899494936611666
2024-10-09 10:41:19,661 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=12280.0, ans=0.0155
2024-10-09 10:41:52,504 INFO [train.py:1152] Epoch 6, batch 2600, loss[loss=0.2325, ctc_loss=0.1221, attn_decoder_loss=0.2601, over 4847.00 frames. ], tot_loss[loss=0.2574, ctc_loss=0.1841, attn_decoder_loss=0.2757, over 966280.04 frames. ], batch size: 20, lr: 1.52e-02,
2024-10-09 10:41:55,887 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=12286.666666666666, ans=0.4699666666666667
2024-10-09 10:42:05,279 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12286.666666666666, ans=0.17713333333333334
2024-10-09 10:42:18,292 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.69 vs. limit=16.7175
2024-10-09 10:42:29,439 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.31 vs. limit=12.11
2024-10-09 10:42:55,369 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.469e+01 6.407e+01 7.307e+01 8.557e+01 1.166e+02, threshold=1.461e+02, percent-clipped=0.0
2024-10-09 10:43:11,261 INFO [train.py:1152] Epoch 6, batch 2650, loss[loss=0.3121, ctc_loss=0.2508, attn_decoder_loss=0.3274, over 4832.00 frames. ], tot_loss[loss=0.2579, ctc_loss=0.1849, attn_decoder_loss=0.2762, over 966038.66 frames. ], batch size: 38, lr: 1.52e-02,
2024-10-09 10:43:36,530 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=12306.666666666666, ans=0.015388888888888896
2024-10-09 10:44:27,863 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.05 vs. limit=16.7375
2024-10-09 10:44:28,438 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=12320.0, ans=0.125
2024-10-09 10:44:29,958 INFO [train.py:1152] Epoch 6, batch 2700, loss[loss=0.3176, ctc_loss=0.2529, attn_decoder_loss=0.3338, over 4856.00 frames. ], tot_loss[loss=0.258, ctc_loss=0.185, attn_decoder_loss=0.2762, over 966317.31 frames. ], batch size: 28, lr: 1.52e-02,
2024-10-09 10:44:30,098 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=12320.0, ans=0.0
2024-10-09 10:44:50,727 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=12323.333333333334, ans=0.0
2024-10-09 10:45:09,643 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=12326.666666666666, ans=0.125
2024-10-09 10:45:10,243 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=16.87 vs. limit=12.122499999999999
2024-10-09 10:45:17,876 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=16.42 vs. limit=16.747500000000002
2024-10-09 10:45:33,031 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.020e+01 6.492e+01 7.194e+01 8.178e+01 1.201e+02, threshold=1.439e+02, percent-clipped=0.0
2024-10-09 10:45:48,868 INFO [train.py:1152] Epoch 6, batch 2750, loss[loss=0.2482, ctc_loss=0.1691, attn_decoder_loss=0.268, over 4799.00 frames. ], tot_loss[loss=0.2564, ctc_loss=0.1831, attn_decoder_loss=0.2747, over 966952.46 frames. ], batch size: 19, lr: 1.52e-02,
2024-10-09 10:45:57,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=12336.666666666666, ans=0.015263888888888896
2024-10-09 10:46:13,502 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=5.69 vs. limit=11.17
2024-10-09 10:46:14,346 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=12340.0, ans=0.125
2024-10-09 10:46:37,083 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=4.94 vs. limit=8.086666666666666
2024-10-09 10:46:42,812 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=12346.666666666666, ans=0.125
2024-10-09 10:47:07,880 INFO [train.py:1152] Epoch 6, batch 2800, loss[loss=0.2865, ctc_loss=0.1998, attn_decoder_loss=0.3081, over 4773.00 frames. ], tot_loss[loss=0.2556, ctc_loss=0.1819, attn_decoder_loss=0.274, over 967144.54 frames. ], batch size: 53, lr: 1.52e-02,
2024-10-09 10:47:23,858 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=12356.666666666666, ans=10.0
2024-10-09 10:47:41,187 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=12360.0, ans=0.07
2024-10-09 10:47:52,197 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=12360.0, ans=0.05
2024-10-09 10:48:03,117 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=12363.333333333334, ans=0.125
2024-10-09 10:48:10,796 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.220e+01 6.406e+01 7.206e+01 8.060e+01 1.114e+02, threshold=1.441e+02, percent-clipped=0.0
2024-10-09 10:48:22,328 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=12366.666666666666, ans=0.015138888888888896
2024-10-09 10:48:26,755 INFO [train.py:1152] Epoch 6, batch 2850, loss[loss=0.2342, ctc_loss=0.1556, attn_decoder_loss=0.2538, over 4937.00 frames. ], tot_loss[loss=0.2573, ctc_loss=0.1843, attn_decoder_loss=0.2756, over 966850.50 frames. ], batch size: 20, lr: 1.52e-02,
2024-10-09 10:48:30,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=12370.0, ans=0.125
2024-10-09 10:48:34,916 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=12370.0, ans=0.015125
2024-10-09 10:49:36,503 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=12383.333333333334, ans=0.125
2024-10-09 10:49:43,295 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.05 vs. limit=12.14375
2024-10-09 10:49:45,822 INFO [train.py:1152] Epoch 6, batch 2900, loss[loss=0.2405, ctc_loss=0.1383, attn_decoder_loss=0.266, over 4752.00 frames. ], tot_loss[loss=0.2576, ctc_loss=0.1842, attn_decoder_loss=0.276, over 965923.95 frames. ], batch size: 20, lr: 1.52e-02,
2024-10-09 10:50:42,910 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=12396.666666666666, ans=0.46611666666666673
2024-10-09 10:50:47,826 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=12400.0, ans=0.125
2024-10-09 10:50:50,149 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.559e+01 6.282e+01 7.143e+01 7.786e+01 1.189e+02, threshold=1.429e+02, percent-clipped=0.0
2024-10-09 10:50:55,477 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=12400.0, ans=0.0
2024-10-09 10:51:04,503 INFO [train.py:1152] Epoch 6, batch 2950, loss[loss=0.2349, ctc_loss=0.1442, attn_decoder_loss=0.2576, over 4797.00 frames. ], tot_loss[loss=0.2566, ctc_loss=0.183, attn_decoder_loss=0.2749, over 966428.26 frames. ], batch size: 19, lr: 1.51e-02,
2024-10-09 10:51:37,052 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12410.0, ans=0.1759
2024-10-09 10:52:00,693 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=12413.333333333334, ans=0.09899494936611666
2024-10-09 10:52:22,806 INFO [train.py:1152] Epoch 6, batch 3000, loss[loss=0.2545, ctc_loss=0.1571, attn_decoder_loss=0.2789, over 4855.00 frames. ], tot_loss[loss=0.2555, ctc_loss=0.1803, attn_decoder_loss=0.2743, over 967161.23 frames. ], batch size: 21, lr: 1.51e-02,
2024-10-09 10:52:22,806 INFO [train.py:1175] Computing validation loss
2024-10-09 10:52:31,957 INFO [zipformer.py:1858] name=encoder.encoders.3.encoder.layers.1.self_attn_weights, attn_weights_entropy = tensor([1.1844, 1.6903, 1.5019, 2.0563, 2.2078, 1.6296, 1.5879, 1.9627],
       device='cuda:0')
2024-10-09 10:52:32,243 INFO [train.py:1184] Epoch 6, validation: loss=0.2098, ctc_loss=0.06914, attn_decoder_loss=0.245, over 90464.00 frames.
2024-10-09 10:52:32,243 INFO [train.py:1185] Maximum memory allocated so far is 6613MB
2024-10-09 10:52:34,132 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=12420.0, ans=0.125
2024-10-09 10:52:38,690 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=12420.0, ans=0.125
2024-10-09 10:52:45,586 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.70 vs. limit=12.157499999999999
2024-10-09 10:53:06,973 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=12426.666666666666, ans=0.17573333333333335
2024-10-09 10:53:14,986 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=12426.666666666666, ans=0.014888888888888896
2024-10-09 10:53:18,085 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=12430.0, ans=0.008167391304347826
2024-10-09 10:53:35,279 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.539e+01 6.358e+01 7.275e+01 8.352e+01 1.335e+02, threshold=1.455e+02, percent-clipped=0.0
2024-10-09 10:53:45,780 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.41 vs. limit=12.1625
2024-10-09 10:53:49,581 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=12436.666666666666, ans=0.014847222222222227
2024-10-09 10:53:50,942 INFO [train.py:1152] Epoch 6, batch 3050, loss[loss=0.2186, ctc_loss=0.1266, attn_decoder_loss=0.2416, over 4742.00 frames. ], tot_loss[loss=0.2559, ctc_loss=0.1812, attn_decoder_loss=0.2746, over 966574.69 frames. ], batch size: 19, lr: 1.51e-02,
2024-10-09 10:53:50,954 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=12436.666666666666, ans=0.17563333333333334
2024-10-09 10:53:57,378 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=12436.666666666666, ans=0.014847222222222227
2024-10-09 10:54:17,746 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=12440.0, ans=0.125
2024-10-09 10:54:19,299 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=12440.0, ans=0.008165217391304347
2024-10-09 10:54:36,543 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=12446.666666666666, ans=0.014805555555555558
2024-10-09 10:54:44,337 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=12446.666666666666, ans=0.0
2024-10-09 10:54:48,201 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.00 vs. limit=16.835
2024-10-09 10:54:55,533 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=12450.0, ans=0.09899494936611666
2024-10-09 10:55:09,444 INFO [train.py:1152] Epoch 6, batch 3100, loss[loss=0.277, ctc_loss=0.2146, attn_decoder_loss=0.2926, over 4833.00 frames. ], tot_loss[loss=0.2556, ctc_loss=0.1806, attn_decoder_loss=0.2744, over 966312.07 frames. ], batch size: 38, lr: 1.51e-02,
2024-10-09 10:55:11,144 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=12453.333333333334, ans=0.46413333333333334
2024-10-09 10:55:25,444 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=12456.666666666666, ans=0.125
2024-10-09 10:55:48,085 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.03 vs. limit=12.1725
2024-10-09 10:56:01,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=12463.333333333334, ans=0.125
2024-10-09 10:56:12,557 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.006e+01 6.351e+01 7.108e+01 7.960e+01 1.416e+02, threshold=1.422e+02, percent-clipped=0.0
2024-10-09 10:56:23,782 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=12466.666666666666, ans=0.014722222222222227
2024-10-09 10:56:28,281 INFO [train.py:1152] Epoch 6, batch 3150, loss[loss=0.2782, ctc_loss=0.2122, attn_decoder_loss=0.2947, over 4796.00 frames. ], tot_loss[loss=0.2554, ctc_loss=0.1806, attn_decoder_loss=0.2741, over 966581.13 frames. ], batch size: 40, lr: 1.51e-02,
2024-10-09 10:56:38,039 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=9.479e-02
2024-10-09 10:56:50,284 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=3.98 vs. limit=8.989333333333335
2024-10-09 10:56:50,769 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=12473.333333333334, ans=0.04949747468305833
2024-10-09 10:57:00,211 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12476.666666666666, ans=0.17523333333333335
2024-10-09 10:57:10,521 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=12.25 vs. limit=12.17875
2024-10-09 10:57:17,889 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-09 10:57:27,327 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=12480.0, ans=0.4632
2024-10-09 10:57:38,320 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=12483.333333333334, ans=0.025
2024-10-09 10:57:45,428 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.93 vs. limit=16.8625
2024-10-09 10:57:47,686 INFO [train.py:1152] Epoch 6, batch 3200, loss[loss=0.2531, ctc_loss=0.1933, attn_decoder_loss=0.2681, over 4748.00 frames. ], tot_loss[loss=0.2548, ctc_loss=0.1803, attn_decoder_loss=0.2734, over 967086.26 frames. ], batch size: 20, lr: 1.51e-02,
2024-10-09 10:58:04,515 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten.whitening_limit, batch_count=12490.0, ans=12.18375
2024-10-09 10:58:12,335 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=16.65 vs. limit=16.8675
2024-10-09 10:58:49,946 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-09 10:58:51,090 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.334e+01 6.331e+01 7.146e+01 8.223e+01 1.986e+02, threshold=1.429e+02, percent-clipped=1.0
2024-10-09 10:59:00,860 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=12500.0, ans=0.875
2024-10-09 10:59:07,021 INFO [train.py:1152] Epoch 6, batch 3250, loss[loss=0.2845, ctc_loss=0.1956, attn_decoder_loss=0.3067, over 4860.00 frames. ], tot_loss[loss=0.2551, ctc_loss=0.1799, attn_decoder_loss=0.2739, over 967224.98 frames. ], batch size: 24, lr: 1.51e-02,
2024-10-09 10:59:20,795 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.27 vs. limit=16.877499999999998
2024-10-09 11:00:02,642 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=12513.333333333334, ans=0.125
2024-10-09 11:00:05,874 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=12513.333333333334, ans=0.125
2024-10-09 11:00:08,233 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.82 vs. limit=12.192499999999999
2024-10-09 11:00:23,687 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=12516.666666666666, ans=0.05
2024-10-09 11:00:26,613 INFO [train.py:1152] Epoch 6, batch 3300, loss[loss=0.2779, ctc_loss=0.2039, attn_decoder_loss=0.2963, over 4839.00 frames. ], tot_loss[loss=0.2545, ctc_loss=0.1784, attn_decoder_loss=0.2736, over 967695.14 frames. ], batch size: 43, lr: 1.51e-02,
2024-10-09 11:00:47,300 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=12523.333333333334, ans=0.125
2024-10-09 11:00:56,500 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=12526.666666666666, ans=0.014472222222222227
2024-10-09 11:00:56,511 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=12526.666666666666, ans=0.008146376811594204
2024-10-09 11:01:29,279 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.659e+01 6.325e+01 7.197e+01 8.046e+01 1.144e+02, threshold=1.439e+02, percent-clipped=0.0
2024-10-09 11:01:45,111 INFO [train.py:1152] Epoch 6, batch 3350, loss[loss=0.257, ctc_loss=0.2004, attn_decoder_loss=0.2711, over 4804.00 frames. ], tot_loss[loss=0.2552, ctc_loss=0.1796, attn_decoder_loss=0.274, over 966986.43 frames. ], batch size: 40, lr: 1.51e-02,
2024-10-09 11:01:53,322 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=12536.666666666666, ans=0.04949747468305833
2024-10-09 11:02:22,971 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.min_positive, batch_count=12543.333333333334, ans=0.025
2024-10-09 11:02:22,977 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12543.333333333334, ans=0.17456666666666668
2024-10-09 11:02:37,324 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=12546.666666666666, ans=0.0
2024-10-09 11:03:04,322 INFO [train.py:1152] Epoch 6, batch 3400, loss[loss=0.2419, ctc_loss=0.1744, attn_decoder_loss=0.2588, over 4959.00 frames. ], tot_loss[loss=0.2554, ctc_loss=0.1798, attn_decoder_loss=0.2743, over 966808.60 frames. ], batch size: 19, lr: 1.51e-02,
2024-10-09 11:03:07,591 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=12553.333333333334, ans=0.008140579710144927
2024-10-09 11:03:11,838 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=7.26 vs. limit=12.2075
2024-10-09 11:03:37,606 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=12560.0, ans=0.008139130434782609
2024-10-09 11:03:47,229 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12560.0, ans=0.1744
2024-10-09 11:04:07,618 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.826e+01 6.187e+01 7.135e+01 8.013e+01 1.171e+02, threshold=1.427e+02, percent-clipped=0.0
2024-10-09 11:04:11,712 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten.whitening_limit, batch_count=12566.666666666666, ans=12.2125
2024-10-09 11:04:12,029 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.28 vs. limit=11.283333333333333
2024-10-09 11:04:16,044 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=12566.666666666666, ans=0.00813768115942029
2024-10-09 11:04:18,317 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.19 vs. limit=12.2125
2024-10-09 11:04:23,802 INFO [train.py:1152] Epoch 6, batch 3450, loss[loss=0.2891, ctc_loss=0.2391, attn_decoder_loss=0.3016, over 4847.00 frames. ], tot_loss[loss=0.2553, ctc_loss=0.1795, attn_decoder_loss=0.2742, over 967059.85 frames. ], batch size: 43, lr: 1.51e-02,
2024-10-09 11:04:23,936 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12570.0, ans=0.17429999999999998
2024-10-09 11:04:35,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=12570.0, ans=0.125
2024-10-09 11:04:38,912 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.59 vs. limit=12.215
2024-10-09 11:04:41,502 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12573.333333333334, ans=0.17426666666666665
2024-10-09 11:04:49,363 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=12573.333333333334, ans=0.4599333333333333
2024-10-09 11:04:51,636 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.85 vs. limit=16.93
2024-10-09 11:05:03,573 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=12576.666666666666, ans=0.125
2024-10-09 11:05:38,613 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=12583.333333333334, ans=0.035
2024-10-09 11:05:43,356 INFO [train.py:1152] Epoch 6, batch 3500, loss[loss=0.2358, ctc_loss=0.1496, attn_decoder_loss=0.2574, over 4883.00 frames. ], tot_loss[loss=0.2555, ctc_loss=0.1803, attn_decoder_loss=0.2743, over 967418.48 frames. ], batch size: 19, lr: 1.50e-02,
2024-10-09 11:06:47,084 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.894e+01 6.054e+01 6.948e+01 7.839e+01 1.191e+02, threshold=1.390e+02, percent-clipped=0.0
2024-10-09 11:06:51,873 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=12600.0, ans=0.459
2024-10-09 11:07:02,847 INFO [train.py:1152] Epoch 6, batch 3550, loss[loss=0.2584, ctc_loss=0.1738, attn_decoder_loss=0.2796, over 4778.00 frames. ], tot_loss[loss=0.2543, ctc_loss=0.1789, attn_decoder_loss=0.2732, over 967455.86 frames. ], batch size: 29, lr: 1.50e-02,
2024-10-09 11:07:18,816 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=12606.666666666666, ans=0.125
2024-10-09 11:07:45,678 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=12610.0, ans=0.125
2024-10-09 11:07:52,001 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=12613.333333333334, ans=0.014111111111111109
2024-10-09 11:08:21,973 INFO [train.py:1152] Epoch 6, batch 3600, loss[loss=0.2291, ctc_loss=0.1439, attn_decoder_loss=0.2504, over 4935.00 frames. ], tot_loss[loss=0.2546, ctc_loss=0.179, attn_decoder_loss=0.2735, over 967649.45 frames. ], batch size: 20, lr: 1.50e-02,
2024-10-09 11:08:22,131 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=12620.0, ans=0.00812608695652174
2024-10-09 11:08:24,515 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.87 vs. limit=4.893
2024-10-09 11:08:36,502 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 11:08:42,996 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=12623.333333333334, ans=0.00812536231884058
2024-10-09 11:09:20,933 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=12630.0, ans=0.38945
2024-10-09 11:09:25,293 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.074e+01 6.232e+01 7.008e+01 8.109e+01 1.176e+02, threshold=1.402e+02, percent-clipped=0.0
2024-10-09 11:09:33,553 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=12633.333333333334, ans=0.0081231884057971
2024-10-09 11:09:35,207 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-09 11:09:41,090 INFO [train.py:1152] Epoch 6, batch 3650, loss[loss=0.2595, ctc_loss=0.1892, attn_decoder_loss=0.2771, over 4868.00 frames. ], tot_loss[loss=0.2537, ctc_loss=0.1783, attn_decoder_loss=0.2725, over 968022.85 frames. ], batch size: 31, lr: 1.50e-02,
2024-10-09 11:09:47,906 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=12636.666666666666, ans=0.008122463768115942
2024-10-09 11:09:58,348 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=12640.0, ans=0.0
2024-10-09 11:10:04,443 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=12640.0, ans=0.125
2024-10-09 11:10:18,369 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=12643.333333333334, ans=0.07
2024-10-09 11:10:47,989 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=12650.0, ans=0.125
2024-10-09 11:10:54,652 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=11.85 vs. limit=12.24375
2024-10-09 11:10:58,818 INFO [train.py:1152] Epoch 6, batch 3700, loss[loss=0.2516, ctc_loss=0.1803, attn_decoder_loss=0.2694, over 4839.00 frames. ], tot_loss[loss=0.2548, ctc_loss=0.1794, attn_decoder_loss=0.2737, over 967382.46 frames. ], batch size: 24, lr: 1.50e-02,
2024-10-09 11:11:05,637 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=8.28 vs. limit=12.245000000000001
2024-10-09 11:11:09,289 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.31 vs. limit=12.245000000000001
2024-10-09 11:11:13,762 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.30 vs. limit=8.164166666666667
2024-10-09 11:11:22,105 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=5.25 vs. limit=12.24625
2024-10-09 11:11:22,729 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=12656.666666666666, ans=0.125
2024-10-09 11:11:24,783 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.88 vs. limit=16.9925
2024-10-09 11:11:30,545 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=12660.0, ans=0.45690000000000003
2024-10-09 11:11:30,574 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=12660.0, ans=0.125
2024-10-09 11:11:33,696 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=12660.0, ans=0.3899
2024-10-09 11:12:01,958 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.298e+01 6.418e+01 7.306e+01 8.135e+01 1.167e+02, threshold=1.461e+02, percent-clipped=0.0
2024-10-09 11:12:17,439 INFO [train.py:1152] Epoch 6, batch 3750, loss[loss=0.2027, ctc_loss=0.1191, attn_decoder_loss=0.2236, over 4959.00 frames. ], tot_loss[loss=0.255, ctc_loss=0.1792, attn_decoder_loss=0.274, over 967775.69 frames. ], batch size: 19, lr: 1.50e-02,
2024-10-09 11:12:19,114 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=12670.0, ans=0.125
2024-10-09 11:12:44,985 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=11.16 vs. limit=12.252500000000001
2024-10-09 11:12:59,694 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=12676.666666666666, ans=0.125
2024-10-09 11:13:09,202 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=12680.0, ans=0.013833333333333336
2024-10-09 11:13:19,355 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=5.42 vs. limit=6.536666666666667
2024-10-09 11:13:21,498 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=12683.333333333334, ans=0.125
2024-10-09 11:13:24,485 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.max_abs, batch_count=12683.333333333334, ans=10.0
2024-10-09 11:13:34,973 INFO [train.py:1152] Epoch 6, batch 3800, loss[loss=0.2525, ctc_loss=0.1681, attn_decoder_loss=0.2736, over 4732.00 frames. ], tot_loss[loss=0.2551, ctc_loss=0.1789, attn_decoder_loss=0.2741, over 967592.00 frames. ], batch size: 26, lr: 1.50e-02,
2024-10-09 11:13:39,789 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=12686.666666666666, ans=0.013805555555555557
2024-10-09 11:13:47,502 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=12686.666666666666, ans=0.0
2024-10-09 11:14:26,735 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=12.19 vs. limit=12.26125
2024-10-09 11:14:28,054 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=15.83 vs. limit=12.26125
2024-10-09 11:14:36,470 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.933e+01 6.423e+01 7.205e+01 8.213e+01 1.275e+02, threshold=1.441e+02, percent-clipped=0.0
2024-10-09 11:14:47,647 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-09 11:14:51,836 INFO [train.py:1152] Epoch 6, batch 3850, loss[loss=0.2792, ctc_loss=0.2207, attn_decoder_loss=0.2938, over 4820.00 frames. ], tot_loss[loss=0.2543, ctc_loss=0.1786, attn_decoder_loss=0.2732, over 967510.09 frames. ], batch size: 38, lr: 1.50e-02,
2024-10-09 11:14:55,098 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=12703.333333333334, ans=0.008107971014492753
2024-10-09 11:15:01,151 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=12703.333333333334, ans=0.125
2024-10-09 11:15:07,324 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=12706.666666666666, ans=0.125
2024-10-09 11:15:08,991 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=12706.666666666666, ans=0.4552666666666667
2024-10-09 11:15:19,400 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=12706.666666666666, ans=0.17293333333333333
2024-10-09 11:15:19,491 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=12706.666666666666, ans=0.125
2024-10-09 11:15:22,584 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=12710.0, ans=0.013708333333333336
2024-10-09 11:15:32,269 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=17.56 vs. limit=17.0325
2024-10-09 11:15:55,643 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=12716.666666666666, ans=0.008105072463768116
2024-10-09 11:16:07,373 INFO [train.py:1152] Epoch 6, batch 3900, loss[loss=0.2334, ctc_loss=0.1433, attn_decoder_loss=0.2559, over 4728.00 frames. ], tot_loss[loss=0.254, ctc_loss=0.1781, attn_decoder_loss=0.2729, over 967184.00 frames. ], batch size: 26, lr: 1.50e-02,
2024-10-09 11:16:13,493 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=12720.0, ans=0.125
2024-10-09 11:16:21,744 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.87 vs. limit=11.361666666666668
2024-10-09 11:16:49,737 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12726.666666666666, ans=0.17273333333333335
2024-10-09 11:16:59,475 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=6.61 vs. limit=9.091999999999999
2024-10-09 11:17:07,777 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.471e+01 6.344e+01 6.988e+01 8.185e+01 2.545e+02, threshold=1.398e+02, percent-clipped=3.0
2024-10-09 11:17:14,641 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.92 vs. limit=12.275
2024-10-09 11:17:18,343 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=12733.333333333334, ans=0.008101449275362318
2024-10-09 11:17:22,664 INFO [train.py:1152] Epoch 6, batch 3950, loss[loss=0.2503, ctc_loss=0.1807, attn_decoder_loss=0.2678, over 4824.00 frames. ], tot_loss[loss=0.2535, ctc_loss=0.1777, attn_decoder_loss=0.2725, over 967323.90 frames. ], batch size: 36, lr: 1.50e-02,
2024-10-09 11:17:28,023 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.08 vs. limit=17.052500000000002
2024-10-09 11:17:34,702 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=12736.666666666666, ans=0.00810072463768116
2024-10-09 11:17:45,917 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=5.57 vs. limit=11.370000000000001
2024-10-09 11:18:01,683 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=12743.333333333334, ans=0.025
2024-10-09 11:18:08,935 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.34 vs. limit=12.280000000000001
2024-10-09 11:18:21,209 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=12750.0, ans=0.125
2024-10-09 11:18:21,947 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten.whitening_limit, batch_count=12750.0, ans=12.28125
2024-10-09 11:18:30,574 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.42 vs. limit=8.1875
2024-10-09 11:18:34,171 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=12750.0, ans=0.0
2024-10-09 11:18:36,832 INFO [train.py:1152] Epoch 6, batch 4000, loss[loss=0.2584, ctc_loss=0.1703, attn_decoder_loss=0.2805, over 4818.00 frames. ], tot_loss[loss=0.2541, ctc_loss=0.179, attn_decoder_loss=0.2729, over 967349.27 frames. ], batch size: 19, lr: 1.49e-02,
2024-10-09 11:18:48,785 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=12753.333333333334, ans=0.125
2024-10-09 11:18:57,540 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12756.666666666666, ans=0.17243333333333333
2024-10-09 11:19:35,985 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.056e+01 6.536e+01 7.312e+01 8.234e+01 1.666e+02, threshold=1.462e+02, percent-clipped=1.0
2024-10-09 11:19:39,680 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.96 vs. limit=17.075
2024-10-09 11:19:42,959 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.05 vs. limit=17.075
2024-10-09 11:19:50,897 INFO [train.py:1152] Epoch 6, batch 4050, loss[loss=0.274, ctc_loss=0.2167, attn_decoder_loss=0.2884, over 4794.00 frames. ], tot_loss[loss=0.2564, ctc_loss=0.1818, attn_decoder_loss=0.275, over 967587.69 frames. ], batch size: 53, lr: 1.49e-02,
2024-10-09 11:20:05,776 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=12773.333333333334, ans=0.008092753623188407
2024-10-09 11:20:23,202 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=12776.666666666666, ans=0.025
2024-10-09 11:20:23,261 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=12776.666666666666, ans=0.125
2024-10-09 11:20:30,494 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=12776.666666666666, ans=0.125
2024-10-09 11:20:36,384 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=12780.0, ans=0.125
2024-10-09 11:21:04,137 INFO [train.py:1152] Epoch 6, batch 4100, loss[loss=0.2387, ctc_loss=0.159, attn_decoder_loss=0.2587, over 4843.00 frames. ], tot_loss[loss=0.2563, ctc_loss=0.1819, attn_decoder_loss=0.2749, over 966987.75 frames. ], batch size: 31, lr: 1.49e-02,
