2024-10-07 20:06:32,609 INFO [train.py:1231] Training started
2024-10-07 20:06:32,610 INFO [train.py:1241] Device: cuda:0
2024-10-07 20:06:32,611 INFO [train.py:1272] Using dtype=torch.float32
2024-10-07 20:06:32,611 INFO [train.py:1273] Use AMP=False
2024-10-07 20:06:32,611 INFO [train.py:1275] {'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 50, 'reset_interval': 200, 'valid_interval': 3000, 'feature_dim': 80, 'subsampling_factor': 4, 'ignore_id': -1, 'label_smoothing': 0.1, 'warm_step': 2000, 'env_info': {'k2-version': '1.24.3', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': 'e400fa3b456faf8afe0ee5bfe572946b4921a3db', 'k2-git-date': 'Sat Jul 15 04:21:50 2023', 'lhotse-version': '1.25.0', 'torch-version': '2.0.1+cu118', 'torch-cuda-available': True, 'torch-cuda-version': '11.8', 'python-version': '3.9', 'icefall-git-branch': 'master', 'icefall-git-sha1': 'cabeaf7f-dirty', 'icefall-git-date': 'Thu Oct 3 12:53:52 2024', 'icefall-path': '/mnt/ahogpu_ldisk2/adriang/icefall', 'k2-path': '/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/k2/__init__.py', 'lhotse-path': '/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/lhotse/__init__.py', 'hostname': 'ahogpu', 'IP address': '192.168.1.130'}, 'world_size': 1, 'master_port': 12354, 'tensorboard': True, 'num_epochs': 30, 'start_epoch': 1, 'start_batch': 0, 'exp_dir': PosixPath('zipformer/exp1'), 'bpe_model': '/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/data/lang_bpe_256/bpe.model', 'base_lr': 0.045, 'lr_batches': 7500, 'lr_epochs': 3.5, 'ref_duration': 600, 'context_size': 2, 'prune_range': 5, 'lm_scale': 0.25, 'am_scale': 0.0, 'simple_loss_scale': 0.5, 'ctc_loss_scale': 0.2, 'attention_decoder_loss_scale': 0.8, 'seed': 42, 'print_diagnostics': False, 'inf_check': False, 'save_every_n': 4000, 'keep_last_k': 30, 'average_period': 200, 'use_bf16': False, 'use_fp16': False, 'num_encoder_layers': '2,2,3,4,3,2', 'downsampling_factor': '1,2,4,8,4,2', 'feedforward_dim': '512,768,1024,1536,1024,768', 'num_heads': '4,4,4,8,4,4', 'encoder_dim': '192,256,384,512,384,256', 'query_head_dim': '32', 'value_head_dim': '12', 'pos_head_dim': '4', 'pos_dim': 48, 'encoder_unmasked_dim': '192,192,256,256,256,192', 'cnn_module_kernel': '31,31,15,15,15,31', 'decoder_dim': 512, 'joiner_dim': 512, 'attention_decoder_dim': 512, 'attention_decoder_num_layers': 6, 'attention_decoder_attention_dim': 512, 'attention_decoder_num_heads': 8, 'attention_decoder_feedforward_dim': 2048, 'causal': False, 'chunk_size': '16,32,64,-1', 'left_context_frames': '64,128,256,-1', 'use_transducer': True, 'use_ctc': True, 'use_attention_decoder': False, 'manifest_dir': PosixPath('/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/data/fbank'), 'max_duration': 200.0, 'bucketing_sampler': True, 'num_buckets': 30, 'concatenate_cuts': False, 'duration_factor': 1.0, 'gap': 1.0, 'on_the_fly_feats': False, 'shuffle': True, 'drop_last': True, 'return_cuts': True, 'num_workers': 2, 'enable_spec_aug': True, 'spec_aug_time_warp_factor': 80, 'enable_musan': True, 'input_strategy': 'PrecomputedFeatures', 'blank_id': 0, 'sos_id': 1, 'eos_id': 1, 'vocab_size': 256, 'dtype': torch.float32, 'use_autocast': False}
2024-10-07 20:06:32,612 INFO [train.py:1277] About to create model
2024-10-07 20:06:33,026 INFO [train.py:1281] Number of model parameters: 65179895
2024-10-07 20:06:34,820 INFO [train.py:1299] Using single GPU
2024-10-07 20:06:34,835 INFO [custom_asr_data_module.py:394] About to load train cuts
2024-10-07 20:06:34,836 INFO [custom_asr_data_module.py:204] Enable MUSAN
2024-10-07 20:06:34,836 INFO [custom_asr_data_module.py:205] About to get Musan cuts
2024-10-07 20:06:36,530 INFO [custom_asr_data_module.py:234] Enable SpecAugment
2024-10-07 20:06:36,530 INFO [custom_asr_data_module.py:235] Time warp factor: 80
2024-10-07 20:06:36,530 INFO [custom_asr_data_module.py:245] Num frame mask: 10
2024-10-07 20:06:36,531 INFO [custom_asr_data_module.py:260] About to create train dataset
2024-10-07 20:06:36,531 INFO [custom_asr_data_module.py:287] Using DynamicBucketingSampler.
2024-10-07 20:06:36,973 INFO [custom_asr_data_module.py:304] About to create train dataloader
2024-10-07 20:06:36,974 INFO [custom_asr_data_module.py:402] About to load valid cuts
2024-10-07 20:06:36,974 INFO [custom_asr_data_module.py:338] About to create dev dataset
2024-10-07 20:06:36,999 INFO [custom_asr_data_module.py:355] About to create dev dataloader
2024-10-07 20:06:36,999 INFO [train.py:1491] Sanity check -- see if any of the batches in epoch 1 would cause OOM.
/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
2024-10-07 20:07:11,050 INFO [scaling.py:1024] Whitening: name=None, num_groups=1, num_channels=384, metric=76.55 vs. limit=7.5
2024-10-07 20:07:11,267 INFO [train.py:1519] Maximum memory allocated so far is 4542MB
2024-10-07 20:07:12,107 INFO [train.py:1519] Maximum memory allocated so far is 4670MB
2024-10-07 20:07:12,982 INFO [train.py:1519] Maximum memory allocated so far is 4670MB
2024-10-07 20:07:13,510 INFO [scaling.py:1024] Whitening: name=None, num_groups=1, num_channels=512, metric=71.81 vs. limit=7.5
2024-10-07 20:07:13,815 INFO [train.py:1519] Maximum memory allocated so far is 4670MB
2024-10-07 20:07:14,646 INFO [train.py:1519] Maximum memory allocated so far is 4670MB
2024-10-07 20:07:15,572 INFO [train.py:1519] Maximum memory allocated so far is 4670MB
2024-10-07 20:07:27,529 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=18.58 vs. limit=7.5
2024-10-07 20:07:27,938 INFO [train.py:1153] Epoch 1, batch 0, loss[loss=7.719, simple_loss=6.24, pruned_loss=6.239, ctc_loss=4.266, over 4853.00 frames. ], tot_loss[loss=7.719, simple_loss=6.24, pruned_loss=6.239, ctc_loss=4.266, over 4853.00 frames. ], batch size: 19, lr: 2.25e-02,
2024-10-07 20:07:27,938 INFO [train.py:1176] Computing validation loss
2024-10-07 20:07:33,437 INFO [train.py:1185] Epoch 1, validation: loss=7.516, simple_loss=6.045, pruned_loss=6.096, ctc_loss=4.301, over 90464.00 frames.
2024-10-07 20:07:33,438 INFO [train.py:1186] Maximum memory allocated so far is 4670MB
2024-10-07 20:07:34,708 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=11.15 vs. limit=5.0
2024-10-07 20:07:34,863 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=29.37 vs. limit=7.5
2024-10-07 20:07:36,346 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=0.0, ans=0.1
2024-10-07 20:07:37,580 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten.whitening_limit, batch_count=0.0, ans=7.5
2024-10-07 20:07:41,468 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=0.0, ans=0.5
2024-10-07 20:07:43,383 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.210e+02 1.060e+03 1.195e+03 1.298e+03 1.489e+03, threshold=4.781e+03, percent-clipped=0.0
2024-10-07 20:07:43,507 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=3.3333333333333335, ans=0.19987500000000002
2024-10-07 20:07:45,014 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=17.44 vs. limit=7.50125
2024-10-07 20:07:47,757 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=3.3333333333333335, ans=0.49984375
2024-10-07 20:07:49,258 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=33.27 vs. limit=7.50125
2024-10-07 20:07:49,611 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.25 vs. limit=7.5025
2024-10-07 20:07:54,319 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.287e+02 4.299e+02 9.210e+02 1.251e+03 1.524e+03, threshold=3.684e+03, percent-clipped=0.0
2024-10-07 20:07:56,965 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=6.666666666666667, ans=0.249625
2024-10-07 20:07:59,634 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=51.04 vs. limit=7.5025
2024-10-07 20:08:07,076 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=23.85 vs. limit=4.004
2024-10-07 20:08:09,401 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=156.66 vs. limit=7.50375
2024-10-07 20:08:15,628 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=63.92 vs. limit=5.005
2024-10-07 20:08:18,741 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.287e+02 1.955e+02 2.879e+02 9.210e+02 1.524e+03, threshold=1.151e+03, percent-clipped=0.0
2024-10-07 20:08:20,116 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=13.333333333333334, ans=0.499375
2024-10-07 20:08:21,051 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=2.04 vs. limit=3.002
2024-10-07 20:08:24,430 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=144.66 vs. limit=7.505
2024-10-07 20:08:25,714 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=21.77 vs. limit=4.005333333333334
2024-10-07 20:08:29,468 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=21.96 vs. limit=4.005333333333334
2024-10-07 20:08:29,613 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=10.78 vs. limit=4.005333333333334
2024-10-07 20:08:31,406 INFO [train.py:1153] Epoch 1, batch 50, loss[loss=1.432, simple_loss=1.06, pruned_loss=1.209, ctc_loss=1.183, over 4912.00 frames. ], tot_loss[loss=2.953, simple_loss=2.4, pruned_loss=2.049, ctc_loss=1.713, over 217782.93 frames. ], batch size: 19, lr: 2.48e-02,
2024-10-07 20:08:36,635 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=16.666666666666668, ans=0.8994166666666666
2024-10-07 20:08:39,255 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=16.666666666666668, ans=0.19937500000000002
2024-10-07 20:08:43,224 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer_ff3.min_abs, batch_count=20.0, ans=0.001
2024-10-07 20:08:47,587 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=70.56 vs. limit=7.5075
2024-10-07 20:09:00,343 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=23.333333333333332, ans=0.49890625
2024-10-07 20:09:01,432 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=23.333333333333332, ans=0.19912500000000002
2024-10-07 20:09:08,206 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=44.30 vs. limit=7.5175
2024-10-07 20:09:10,528 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=178.93 vs. limit=7.51
2024-10-07 20:09:12,191 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=182.07 vs. limit=7.51
2024-10-07 20:09:22,192 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=88.76 vs. limit=7.51
2024-10-07 20:09:23,616 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=107.12 vs. limit=7.51125
2024-10-07 20:09:24,255 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=30.0, ans=0.49625
2024-10-07 20:09:27,290 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=17.79 vs. limit=5.0075
2024-10-07 20:09:28,750 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=22.75 vs. limit=5.0075
2024-10-07 20:09:33,842 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=173.33 vs. limit=7.51125
2024-10-07 20:09:37,075 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.672e+01 6.074e+01 1.287e+02 2.451e+02 1.524e+03, threshold=2.575e+02, percent-clipped=0.0
2024-10-07 20:09:37,124 INFO [train.py:1153] Epoch 1, batch 100, loss[loss=1.217, simple_loss=0.8717, pruned_loss=1.07, ctc_loss=1.059, over 4752.00 frames. ], tot_loss[loss=2.087, simple_loss=1.634, pruned_loss=1.568, ctc_loss=1.408, over 383572.91 frames. ], batch size: 19, lr: 2.70e-02,
2024-10-07 20:09:39,003 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=163.70 vs. limit=7.5125
2024-10-07 20:09:44,162 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=16.58 vs. limit=5.008333333333334
2024-10-07 20:09:46,464 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=33.333333333333336, ans=0.4984375
2024-10-07 20:09:49,172 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=36.666666666666664, ans=0.04988541666666667
2024-10-07 20:09:56,468 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=36.80 vs. limit=7.51375
2024-10-07 20:10:01,913 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=32.69 vs. limit=7.51375
2024-10-07 20:10:04,283 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=57.31 vs. limit=7.515
2024-10-07 20:10:15,170 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=27.21 vs. limit=4.016
2024-10-07 20:10:15,401 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=38.35 vs. limit=7.53
2024-10-07 20:10:15,535 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=179.30 vs. limit=7.515
2024-10-07 20:10:25,178 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=19.08 vs. limit=4.017333333333333
2024-10-07 20:10:27,269 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=43.333333333333336, ans=0.49796875
2024-10-07 20:10:29,909 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=46.666666666666664, ans=0.7504666666666666
2024-10-07 20:10:30,279 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=28.33 vs. limit=7.535
2024-10-07 20:10:37,988 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=46.666666666666664, ans=0.4978125
2024-10-07 20:10:41,310 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=53.30 vs. limit=7.5175
2024-10-07 20:10:43,942 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=40.52 vs. limit=4.02
2024-10-07 20:10:44,054 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=295.75 vs. limit=7.51875
2024-10-07 20:10:44,615 INFO [train.py:1153] Epoch 1, batch 150, loss[loss=1.138, simple_loss=0.7916, pruned_loss=0.9548, ctc_loss=1.08, over 4910.00 frames. ], tot_loss[loss=1.717, simple_loss=1.304, pruned_loss=1.34, ctc_loss=1.286, over 513480.80 frames. ], batch size: 19, lr: 2.93e-02,
2024-10-07 20:10:47,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=50.0, ans=0.49765625
2024-10-07 20:11:01,887 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=37.11 vs. limit=7.52
2024-10-07 20:11:08,376 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=160.23 vs. limit=7.52
2024-10-07 20:11:20,782 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=22.16 vs. limit=5.014166666666667
2024-10-07 20:11:20,899 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=322.71 vs. limit=7.52125
2024-10-07 20:11:24,432 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=64.27 vs. limit=7.5225
2024-10-07 20:11:36,439 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=60.0, ans=0.4971875
2024-10-07 20:11:37,327 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=46.47 vs. limit=7.5225
2024-10-07 20:11:39,175 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=63.333333333333336, ans=0.49703125
2024-10-07 20:11:49,081 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=174.37 vs. limit=7.52375
2024-10-07 20:11:49,175 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=53.21 vs. limit=7.52375
2024-10-07 20:11:53,131 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.155e+01 5.534e+01 6.238e+01 7.614e+01 1.755e+02, threshold=1.248e+02, percent-clipped=0.0
2024-10-07 20:11:53,175 INFO [train.py:1153] Epoch 1, batch 200, loss[loss=1.136, simple_loss=0.773, pruned_loss=0.8932, ctc_loss=1.161, over 4760.00 frames. ], tot_loss[loss=1.509, simple_loss=1.117, pruned_loss=1.191, ctc_loss=1.227, over 613838.26 frames. ], batch size: 45, lr: 3.15e-02,
2024-10-07 20:12:02,644 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=66.66666666666667, ans=0.496875
2024-10-07 20:12:04,375 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=155.76 vs. limit=7.525
2024-10-07 20:12:12,140 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=70.0, ans=0.49671875
2024-10-07 20:12:12,201 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=70.0, ans=0.8975500000000001
2024-10-07 20:12:16,729 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=48.30 vs. limit=7.52625
2024-10-07 20:12:17,932 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=86.06 vs. limit=7.52625
2024-10-07 20:12:19,454 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.48 vs. limit=3.011
2024-10-07 20:12:26,109 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=203.00 vs. limit=7.5275
2024-10-07 20:12:26,228 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=57.11 vs. limit=5.036666666666667
2024-10-07 20:12:26,401 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=44.88 vs. limit=7.555
2024-10-07 20:12:27,723 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=105.03 vs. limit=7.5275
2024-10-07 20:12:28,106 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=173.84 vs. limit=7.5275
2024-10-07 20:12:35,928 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=304.92 vs. limit=7.52875
2024-10-07 20:12:41,472 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=85.75 vs. limit=7.52875
2024-10-07 20:12:44,126 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=33.25 vs. limit=7.5575
2024-10-07 20:12:45,956 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=24.77 vs. limit=7.5575
2024-10-07 20:12:49,384 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=42.40 vs. limit=5.04
2024-10-07 20:12:52,224 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=229.12 vs. limit=7.53
2024-10-07 20:12:52,690 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn2.whiten.whitening_limit, batch_count=80.0, ans=7.56
2024-10-07 20:12:53,273 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=80.0, ans=0.49625
2024-10-07 20:12:57,847 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=49.14 vs. limit=7.53
2024-10-07 20:13:01,376 INFO [train.py:1153] Epoch 1, batch 250, loss[loss=1.148, simple_loss=0.7743, pruned_loss=0.8884, ctc_loss=1.167, over 4819.00 frames. ], tot_loss[loss=1.372, simple_loss=0.993, pruned_loss=1.084, ctc_loss=1.185, over 692578.36 frames. ], batch size: 38, lr: 3.38e-02,
2024-10-07 20:13:03,220 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=59.70 vs. limit=7.5625
2024-10-07 20:13:11,008 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=83.33333333333333, ans=0.098125
2024-10-07 20:13:18,137 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=57.78 vs. limit=7.565
2024-10-07 20:13:22,603 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=32.89 vs. limit=7.565
2024-10-07 20:13:27,389 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=90.0, ans=0.2991
2024-10-07 20:13:30,958 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=25.92 vs. limit=4.036
2024-10-07 20:13:32,021 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=33.51 vs. limit=7.53375
2024-10-07 20:13:36,567 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=25.00 vs. limit=7.53375
2024-10-07 20:13:41,674 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=69.65 vs. limit=7.57
2024-10-07 20:13:44,767 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=23.63 vs. limit=7.535
2024-10-07 20:13:47,371 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=93.33333333333333, ans=7.57
2024-10-07 20:13:51,688 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=12.81 vs. limit=4.037333333333334
2024-10-07 20:13:55,019 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=96.66666666666667, ans=5.060416666666667
2024-10-07 20:14:02,012 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.prob, batch_count=96.66666666666667, ans=0.49546875
2024-10-07 20:14:09,949 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.148e+01 6.249e+01 7.238e+01 8.627e+01 2.546e+02, threshold=1.448e+02, percent-clipped=3.0
2024-10-07 20:14:09,995 INFO [train.py:1153] Epoch 1, batch 300, loss[loss=1.154, simple_loss=0.7697, pruned_loss=0.8838, ctc_loss=1.171, over 4773.00 frames. ], tot_loss[loss=1.283, simple_loss=0.9109, pruned_loss=1.009, ctc_loss=1.158, over 753007.62 frames. ], batch size: 32, lr: 3.60e-02,
2024-10-07 20:14:13,145 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=14.84 vs. limit=5.025
2024-10-07 20:14:15,107 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=23.59 vs. limit=7.5375
2024-10-07 20:14:19,009 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=211.47 vs. limit=7.5375
2024-10-07 20:14:33,913 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=103.33333333333333, ans=0.29896666666666666
2024-10-07 20:14:34,570 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=27.37 vs. limit=7.5775
2024-10-07 20:14:38,593 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=37.43 vs. limit=7.58
2024-10-07 20:14:46,841 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=91.76 vs. limit=7.54
2024-10-07 20:14:48,424 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.77 vs. limit=3.016
2024-10-07 20:14:51,199 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=71.29 vs. limit=7.54125
2024-10-07 20:14:52,867 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=29.77 vs. limit=7.54125
2024-10-07 20:15:00,915 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=73.71 vs. limit=7.54125
2024-10-07 20:15:07,731 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=46.51 vs. limit=7.5425
2024-10-07 20:15:08,769 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=113.33333333333333, ans=0.4946875
2024-10-07 20:15:18,468 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=116.66666666666667, ans=0.29883333333333334
2024-10-07 20:15:19,621 INFO [train.py:1153] Epoch 1, batch 350, loss[loss=1.069, simple_loss=0.7057, pruned_loss=0.8307, ctc_loss=1.056, over 4883.00 frames. ], tot_loss[loss=1.22, simple_loss=0.8517, pruned_loss=0.9537, ctc_loss=1.138, over 800374.59 frames. ], batch size: 19, lr: 3.83e-02,
2024-10-07 20:15:30,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=116.66666666666667, ans=0.8959166666666667
2024-10-07 20:15:43,400 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=75.78 vs. limit=7.545
2024-10-07 20:15:50,337 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=68.95 vs. limit=7.54625
2024-10-07 20:15:51,041 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=123.33333333333333, ans=0.2987666666666667
2024-10-07 20:15:54,124 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=42.42 vs. limit=7.54625
2024-10-07 20:15:56,897 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=10.41 vs. limit=5.030833333333334
2024-10-07 20:16:00,226 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=166.16 vs. limit=7.5475
2024-10-07 20:16:01,133 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=41.04 vs. limit=7.595
2024-10-07 20:16:02,006 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=126.66666666666667, ans=0.09920833333333334
2024-10-07 20:16:07,205 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=11.84 vs. limit=4.025333333333333
2024-10-07 20:16:12,143 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=63.10 vs. limit=7.5475
2024-10-07 20:16:20,785 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=11.22 vs. limit=5.0325
2024-10-07 20:16:27,731 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=27.79 vs. limit=7.6
2024-10-07 20:16:28,287 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.559e+01 6.628e+01 7.394e+01 7.887e+01 1.409e+02, threshold=1.479e+02, percent-clipped=0.0
2024-10-07 20:16:28,329 INFO [train.py:1153] Epoch 1, batch 400, loss[loss=1.056, simple_loss=0.6732, pruned_loss=0.7989, ctc_loss=1.129, over 4874.00 frames. ], tot_loss[loss=1.177, simple_loss=0.8085, pruned_loss=0.9135, ctc_loss=1.127, over 837078.96 frames. ], batch size: 22, lr: 4.05e-02,
2024-10-07 20:16:29,742 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=133.33333333333334, ans=0.49375
2024-10-07 20:16:38,313 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=41.18 vs. limit=7.6
2024-10-07 20:16:38,687 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=37.72 vs. limit=5.066666666666666
2024-10-07 20:16:39,808 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=512, metric=140.37 vs. limit=7.55
2024-10-07 20:16:46,081 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=136.66666666666666, ans=0.8952166666666667
2024-10-07 20:16:46,108 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=136.66666666666666, ans=0.49359375
2024-10-07 20:16:56,217 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=55.92 vs. limit=7.5525
2024-10-07 20:16:57,708 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=57.25 vs. limit=7.5525
2024-10-07 20:17:03,616 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=35.44 vs. limit=7.5525
2024-10-07 20:17:04,398 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=49.33 vs. limit=7.5525
2024-10-07 20:17:08,193 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=143.33333333333334, ans=0.19462500000000002
2024-10-07 20:17:14,508 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module1.whiten, num_groups=1, num_channels=192, metric=15.00 vs. limit=7.55375
2024-10-07 20:17:27,140 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=146.66666666666666, ans=0.8948666666666667
2024-10-07 20:17:36,677 INFO [train.py:1153] Epoch 1, batch 450, loss[loss=1.071, simple_loss=0.677, pruned_loss=0.8084, ctc_loss=1.129, over 4873.00 frames. ], tot_loss[loss=1.143, simple_loss=0.7738, pruned_loss=0.8789, ctc_loss=1.115, over 865679.54 frames. ], batch size: 23, lr: 4.28e-02,
2024-10-07 20:17:46,629 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.min_positive, batch_count=150.0, ans=0.049531250000000006
2024-10-07 20:17:46,686 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=150.0, ans=0.2485
2024-10-07 20:17:46,932 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=34.08 vs. limit=7.55625
2024-10-07 20:17:49,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1.whitening_limit, batch_count=153.33333333333334, ans=5.038333333333333
2024-10-07 20:17:57,873 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=125.81 vs. limit=7.5575
2024-10-07 20:18:03,469 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=120.62 vs. limit=7.55875
2024-10-07 20:18:05,882 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=156.66666666666666, ans=0.49265625
2024-10-07 20:18:26,598 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=160.0, ans=0.4925
2024-10-07 20:18:27,901 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=160.0, ans=0.8944
2024-10-07 20:18:43,618 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.23 vs. limit=7.6225
2024-10-07 20:18:44,422 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=166.66666666666666, ans=0.8941666666666667
2024-10-07 20:18:45,582 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.138e+01 7.009e+01 7.667e+01 8.224e+01 1.088e+02, threshold=1.533e+02, percent-clipped=0.0
2024-10-07 20:18:45,628 INFO [train.py:1153] Epoch 1, batch 500, loss[loss=1.05, simple_loss=0.672, pruned_loss=0.7724, ctc_loss=1.055, over 4806.00 frames. ], tot_loss[loss=1.122, simple_loss=0.7498, pruned_loss=0.8556, ctc_loss=1.108, over 888220.59 frames. ], batch size: 34, lr: 4.49e-02,
2024-10-07 20:18:47,496 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=38.03 vs. limit=7.5625
2024-10-07 20:18:53,453 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=204.92 vs. limit=7.5625
2024-10-07 20:18:55,021 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=17.45 vs. limit=7.625
2024-10-07 20:19:01,304 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=128.54 vs. limit=7.56375
2024-10-07 20:19:07,324 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=102.51 vs. limit=7.56375
2024-10-07 20:19:13,907 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=7.12 vs. limit=4.069333333333334
2024-10-07 20:19:16,527 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=38.23 vs. limit=7.565
2024-10-07 20:19:21,133 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module1.whiten, num_groups=1, num_channels=192, metric=15.17 vs. limit=7.565
2024-10-07 20:19:26,423 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=30.80 vs. limit=7.6325
2024-10-07 20:19:32,002 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=49.09 vs. limit=7.6325
2024-10-07 20:19:44,530 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=70.12 vs. limit=7.5675
2024-10-07 20:19:49,444 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=180.0, ans=0.4915625
2024-10-07 20:19:52,141 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=180.0, ans=0.19325
2024-10-07 20:19:56,224 INFO [train.py:1153] Epoch 1, batch 550, loss[loss=1.1, simple_loss=0.6965, pruned_loss=0.7837, ctc_loss=1.135, over 4821.00 frames. ], tot_loss[loss=1.106, simple_loss=0.7302, pruned_loss=0.8352, ctc_loss=1.102, over 905711.34 frames. ], batch size: 40, lr: 4.49e-02,
2024-10-07 20:19:57,652 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=5.73 vs. limit=4.073333333333333
2024-10-07 20:19:58,190 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.74 vs. limit=5.091666666666667
2024-10-07 20:20:09,611 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=9.28 vs. limit=4.074666666666666
2024-10-07 20:20:10,485 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=186.66666666666666, ans=0.49125
2024-10-07 20:20:13,316 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=23.79 vs. limit=7.57
2024-10-07 20:20:14,850 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=19.14 vs. limit=5.093333333333334
2024-10-07 20:20:17,042 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=186.66666666666666, ans=0.09580000000000001
2024-10-07 20:20:17,751 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=10.08 vs. limit=7.57
2024-10-07 20:20:21,552 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=112.16 vs. limit=7.57125
2024-10-07 20:20:25,314 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=97.20 vs. limit=7.57125
2024-10-07 20:20:27,054 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=103.66 vs. limit=7.57125
2024-10-07 20:20:34,966 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=43.53 vs. limit=7.5725
2024-10-07 20:20:43,786 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=193.33333333333334, ans=0.29806666666666665
2024-10-07 20:20:44,077 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.59 vs. limit=4.077333333333334
2024-10-07 20:20:45,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=193.33333333333334, ans=0.24806666666666666
2024-10-07 20:20:54,016 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=45.06 vs. limit=7.6475
2024-10-07 20:21:01,944 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=200.0, ans=0.1925
2024-10-07 20:21:02,997 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.406e+01 7.388e+01 8.051e+01 8.890e+01 1.333e+02, threshold=1.610e+02, percent-clipped=0.0
2024-10-07 20:21:03,039 INFO [train.py:1153] Epoch 1, batch 600, loss[loss=1.115, simple_loss=0.6989, pruned_loss=0.7847, ctc_loss=1.154, over 4834.00 frames. ], tot_loss[loss=1.094, simple_loss=0.7142, pruned_loss=0.8174, ctc_loss=1.095, over 919511.50 frames. ], batch size: 38, lr: 4.49e-02,
2024-10-07 20:21:07,105 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=200.0, ans=0.203
2024-10-07 20:21:10,143 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=19.08 vs. limit=7.575
2024-10-07 20:21:11,645 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=114.12 vs. limit=7.575
2024-10-07 20:21:25,654 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=8.80 vs. limit=7.57625
2024-10-07 20:21:27,018 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=25.11 vs. limit=7.57625
2024-10-07 20:21:49,763 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=9.422e-01
2024-10-07 20:21:57,127 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=82.06 vs. limit=7.58
2024-10-07 20:21:58,283 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=42.09 vs. limit=7.66
2024-10-07 20:22:08,180 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=31.75 vs. limit=7.58
2024-10-07 20:22:11,738 INFO [train.py:1153] Epoch 1, batch 650, loss[loss=1.058, simple_loss=0.6508, pruned_loss=0.7696, ctc_loss=1.052, over 4844.00 frames. ], tot_loss[loss=1.088, simple_loss=0.7026, pruned_loss=0.8043, ctc_loss=1.091, over 930350.51 frames. ], batch size: 21, lr: 4.49e-02,
2024-10-07 20:22:12,505 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=38.73 vs. limit=7.6625
2024-10-07 20:22:14,742 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=216.66666666666666, ans=0.29783333333333334
2024-10-07 20:22:37,989 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=223.33333333333334, ans=0.48953125
2024-10-07 20:22:41,217 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=56.82 vs. limit=7.58375
2024-10-07 20:22:44,090 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=21.61 vs. limit=7.58375
2024-10-07 20:22:47,674 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=223.33333333333334, ans=0.094975
2024-10-07 20:22:50,438 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer2.prob, batch_count=223.33333333333334, ans=0.48953125
2024-10-07 20:22:53,107 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=226.66666666666666, ans=0.29773333333333335
2024-10-07 20:22:56,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1.whitening_limit, batch_count=226.66666666666666, ans=5.056666666666667
2024-10-07 20:22:58,073 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.71 vs. limit=7.67
2024-10-07 20:23:05,661 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=230.0, ans=0.2477
2024-10-07 20:23:11,557 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=53.97 vs. limit=7.6725
2024-10-07 20:23:17,800 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=230.0, ans=0.20345000000000002
2024-10-07 20:23:20,276 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.472e+01 7.855e+01 8.696e+01 1.010e+02 1.886e+02, threshold=1.739e+02, percent-clipped=1.0
2024-10-07 20:23:20,322 INFO [train.py:1153] Epoch 1, batch 700, loss[loss=1.012, simple_loss=0.6282, pruned_loss=0.706, ctc_loss=1.001, over 4758.00 frames. ], tot_loss[loss=1.084, simple_loss=0.6939, pruned_loss=0.7917, ctc_loss=1.09, over 938206.07 frames. ], batch size: 19, lr: 4.49e-02,
2024-10-07 20:23:21,099 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=24.75 vs. limit=7.675
2024-10-07 20:23:26,672 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.77 vs. limit=7.675
2024-10-07 20:23:30,585 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=10.24 vs. limit=5.058333333333334
2024-10-07 20:23:32,020 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=16.66 vs. limit=7.5875
2024-10-07 20:23:38,151 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=236.66666666666666, ans=0.48890625
2024-10-07 20:23:42,617 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=48.42 vs. limit=7.6775
2024-10-07 20:23:51,594 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=240.0, ans=0.2976
2024-10-07 20:23:55,767 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=240.0, ans=0.5
2024-10-07 20:23:59,993 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.min_positive, batch_count=243.33333333333334, ans=0.24756666666666666
2024-10-07 20:24:00,720 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=2.50 vs. limit=3.0365
2024-10-07 20:24:07,663 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=12.94 vs. limit=7.59125
2024-10-07 20:24:11,202 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=243.33333333333334, ans=0.04923958333333334
2024-10-07 20:24:11,840 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=66.73 vs. limit=7.59125
2024-10-07 20:24:13,967 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=246.66666666666666, ans=0.2975333333333333
2024-10-07 20:24:25,503 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=37.80 vs. limit=7.685
2024-10-07 20:24:29,088 INFO [train.py:1153] Epoch 1, batch 750, loss[loss=0.9531, simple_loss=0.582, pruned_loss=0.6472, ctc_loss=0.9846, over 4880.00 frames. ], tot_loss[loss=1.075, simple_loss=0.6825, pruned_loss=0.7749, ctc_loss=1.083, over 944974.19 frames. ], batch size: 22, lr: 4.49e-02,
2024-10-07 20:24:43,673 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.62 vs. limit=5.126666666666667
2024-10-07 20:24:45,038 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.32 vs. limit=5.126666666666667
2024-10-07 20:24:53,950 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=253.33333333333334, ans=0.488125
2024-10-07 20:24:54,516 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.48 vs. limit=5.126666666666667
2024-10-07 20:24:55,267 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=256.6666666666667, ans=0.46791666666666665
2024-10-07 20:25:10,146 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.78 vs. limit=7.695
2024-10-07 20:25:14,178 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=31.69 vs. limit=7.5975
2024-10-07 20:25:17,701 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=44.61 vs. limit=7.695
2024-10-07 20:25:21,727 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=260.0, ans=0.19025
2024-10-07 20:25:31,687 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=16.70 vs. limit=7.59875
2024-10-07 20:25:35,341 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=263.3333333333333, ans=0.48765625
2024-10-07 20:25:38,127 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.668e+01 7.797e+01 8.862e+01 1.053e+02 2.444e+02, threshold=1.772e+02, percent-clipped=1.0
2024-10-07 20:25:38,169 INFO [train.py:1153] Epoch 1, batch 800, loss[loss=0.9932, simple_loss=0.6145, pruned_loss=0.6617, ctc_loss=0.9854, over 4855.00 frames. ], tot_loss[loss=1.072, simple_loss=0.6751, pruned_loss=0.7614, ctc_loss=1.08, over 949819.77 frames. ], batch size: 19, lr: 4.49e-02,
2024-10-07 20:25:38,794 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=91.50 vs. limit=7.6
2024-10-07 20:25:47,041 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=40.97 vs. limit=7.6
2024-10-07 20:26:00,114 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=2.330e+01
2024-10-07 20:26:02,011 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.54 vs. limit=7.7025
2024-10-07 20:26:07,489 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=64.06 vs. limit=7.6025
2024-10-07 20:26:12,552 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=273.3333333333333, ans=0.2972666666666667
2024-10-07 20:26:12,986 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=31.70 vs. limit=7.705
2024-10-07 20:26:21,514 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=5.69 vs. limit=7.60375
2024-10-07 20:26:28,025 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=39.13 vs. limit=7.7075
2024-10-07 20:26:36,205 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=33.03 vs. limit=7.605
2024-10-07 20:26:45,796 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.03 vs. limit=5.141666666666667
2024-10-07 20:26:46,534 INFO [train.py:1153] Epoch 1, batch 850, loss[loss=0.9583, simple_loss=0.5857, pruned_loss=0.6238, ctc_loss=0.9797, over 4793.00 frames. ], tot_loss[loss=1.068, simple_loss=0.6689, pruned_loss=0.7477, ctc_loss=1.076, over 954090.52 frames. ], batch size: 29, lr: 4.49e-02,
2024-10-07 20:26:51,375 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.11 vs. limit=7.7125
2024-10-07 20:26:52,675 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=12.67 vs. limit=5.141666666666667
2024-10-07 20:26:53,426 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=283.3333333333333, ans=0.48671875
2024-10-07 20:26:56,526 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=57.24 vs. limit=7.60625
2024-10-07 20:26:58,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=286.6666666666667, ans=0.4865625
2024-10-07 20:27:04,777 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=14.26 vs. limit=7.6075
2024-10-07 20:27:05,894 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=33.89 vs. limit=7.6075
2024-10-07 20:27:09,664 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=286.6666666666667, ans=0.46416666666666667
2024-10-07 20:27:11,721 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=8.36 vs. limit=7.6075
2024-10-07 20:27:14,140 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=79.15 vs. limit=7.60875
2024-10-07 20:27:22,600 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=10.28 vs. limit=7.60875
2024-10-07 20:27:28,853 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=293.3333333333333, ans=0.48625
2024-10-07 20:27:34,884 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=17.58 vs. limit=7.61
2024-10-07 20:27:48,604 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=28.45 vs. limit=7.7225
2024-10-07 20:27:50,166 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=21.90 vs. limit=7.61125
2024-10-07 20:27:54,667 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.694e+01 9.348e+01 1.123e+02 1.448e+02 2.788e+02, threshold=2.247e+02, percent-clipped=12.0
2024-10-07 20:27:54,712 INFO [train.py:1153] Epoch 1, batch 900, loss[loss=1.009, simple_loss=0.6199, pruned_loss=0.6425, ctc_loss=1.02, over 4854.00 frames. ], tot_loss[loss=1.065, simple_loss=0.6634, pruned_loss=0.733, ctc_loss=1.071, over 956897.83 frames. ], batch size: 19, lr: 4.48e-02,
2024-10-07 20:27:56,174 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=300.0, ans=0.4859375
2024-10-07 20:27:57,607 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=300.0, ans=0.4859375
2024-10-07 20:27:59,130 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=300.0, ans=0.4859375
2024-10-07 20:28:02,299 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=44.05 vs. limit=7.6125
2024-10-07 20:28:05,147 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=13.92 vs. limit=7.6125
2024-10-07 20:28:08,695 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=303.3333333333333, ans=0.46208333333333335
2024-10-07 20:28:33,488 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=26.87 vs. limit=7.615
2024-10-07 20:28:34,647 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.min_abs, batch_count=310.0, ans=0.20465
2024-10-07 20:28:35,989 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=310.0, ans=0.2969
2024-10-07 20:28:40,384 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=35.87 vs. limit=7.7325
2024-10-07 20:28:45,927 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=30.16 vs. limit=7.7325
2024-10-07 20:28:52,845 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.whiten.whitening_limit, batch_count=313.3333333333333, ans=4.125333333333334
2024-10-07 20:28:54,327 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.06 vs. limit=3.047
2024-10-07 20:28:57,241 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.03 vs. limit=7.735
2024-10-07 20:28:57,288 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.62 vs. limit=7.735
2024-10-07 20:29:03,162 INFO [train.py:1153] Epoch 1, batch 950, loss[loss=1.098, simple_loss=0.6663, pruned_loss=0.7084, ctc_loss=1.082, over 4817.00 frames. ], tot_loss[loss=1.063, simple_loss=0.6597, pruned_loss=0.7204, ctc_loss=1.067, over 958663.64 frames. ], batch size: 19, lr: 4.48e-02,
2024-10-07 20:29:15,491 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=320.0, ans=5.2
2024-10-07 20:29:15,807 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.68 vs. limit=4.128
2024-10-07 20:29:18,792 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=47.03 vs. limit=7.62
2024-10-07 20:29:26,207 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=320.0, ans=0.8888
2024-10-07 20:29:28,909 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=323.3333333333333, ans=0.48484375
2024-10-07 20:29:38,457 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=323.3333333333333, ans=0.48484375
2024-10-07 20:29:49,874 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=8.83 vs. limit=7.6225
2024-10-07 20:29:50,816 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=326.6666666666667, ans=0.09265000000000001
2024-10-07 20:29:52,740 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=45.92 vs. limit=7.745
2024-10-07 20:29:57,593 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=330.0, ans=0.5
2024-10-07 20:30:01,199 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=17.29 vs. limit=7.62375
2024-10-07 20:30:02,078 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.51 vs. limit=5.165
2024-10-07 20:30:06,600 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.89 vs. limit=7.62375
2024-10-07 20:30:07,492 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=8.98 vs. limit=7.62375
2024-10-07 20:30:10,406 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=333.3333333333333, ans=0.484375
2024-10-07 20:30:11,065 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=23.08 vs. limit=7.625
2024-10-07 20:30:11,425 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.715e+01 1.058e+02 1.215e+02 1.405e+02 4.733e+02, threshold=2.431e+02, percent-clipped=6.0
2024-10-07 20:30:11,468 INFO [train.py:1153] Epoch 1, batch 1000, loss[loss=0.9731, simple_loss=0.5948, pruned_loss=0.6088, ctc_loss=0.96, over 4940.00 frames. ], tot_loss[loss=1.063, simple_loss=0.6583, pruned_loss=0.7091, ctc_loss=1.062, over 960466.08 frames. ], batch size: 20, lr: 4.48e-02,
2024-10-07 20:30:12,748 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=333.3333333333333, ans=0.23125
2024-10-07 20:30:17,530 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=35.56 vs. limit=7.625
2024-10-07 20:30:22,108 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=29.61 vs. limit=7.75
2024-10-07 20:30:27,552 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=336.6666666666667, ans=7.7525
2024-10-07 20:30:28,643 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=6.42 vs. limit=4.134666666666667
2024-10-07 20:30:30,214 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.55 vs. limit=7.7525
2024-10-07 20:30:30,963 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=336.6666666666667, ans=0.187375
2024-10-07 20:30:37,953 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=49.23 vs. limit=7.6275
2024-10-07 20:30:39,118 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=30.17 vs. limit=7.6275
2024-10-07 20:30:43,242 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=11.65 vs. limit=5.085
2024-10-07 20:30:51,939 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=343.3333333333333, ans=0.48390625
2024-10-07 20:30:53,937 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=37.26 vs. limit=7.7575
2024-10-07 20:30:54,013 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.93 vs. limit=5.171666666666667
2024-10-07 20:31:09,183 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module1.whiten, num_groups=1, num_channels=192, metric=9.56 vs. limit=7.63
2024-10-07 20:31:17,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=350.0, ans=0.186875
2024-10-07 20:31:18,143 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten.whitening_limit, batch_count=350.0, ans=7.63125
2024-10-07 20:31:18,692 INFO [train.py:1153] Epoch 1, batch 1050, loss[loss=0.9936, simple_loss=0.6198, pruned_loss=0.6025, ctc_loss=0.9573, over 4792.00 frames. ], tot_loss[loss=1.054, simple_loss=0.6523, pruned_loss=0.6907, ctc_loss=1.05, over 962579.73 frames. ], batch size: 25, lr: 4.48e-02,
2024-10-07 20:31:30,232 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.84 vs. limit=5.175
2024-10-07 20:31:41,678 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=353.3333333333333, ans=0.4834375
2024-10-07 20:31:41,692 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=353.3333333333333, ans=0.04889583333333333
2024-10-07 20:31:43,460 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=16.02 vs. limit=5.176666666666667
2024-10-07 20:31:48,352 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=356.6666666666667, ans=0.09777083333333333
2024-10-07 20:31:55,862 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=9.29 vs. limit=7.63375
2024-10-07 20:31:56,922 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=8.08 vs. limit=7.63375
2024-10-07 20:32:02,440 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.56 vs. limit=7.77
2024-10-07 20:32:15,328 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-07 20:32:24,685 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=366.6666666666667, ans=0.09175
2024-10-07 20:32:25,799 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.691e+01 9.898e+01 1.128e+02 1.267e+02 2.260e+02, threshold=2.256e+02, percent-clipped=0.0
2024-10-07 20:32:25,844 INFO [train.py:1153] Epoch 1, batch 1100, loss[loss=0.9755, simple_loss=0.6043, pruned_loss=0.5887, ctc_loss=0.9348, over 4863.00 frames. ], tot_loss[loss=1.046, simple_loss=0.6476, pruned_loss=0.6732, ctc_loss=1.037, over 964036.58 frames. ], batch size: 20, lr: 4.48e-02,
2024-10-07 20:32:27,936 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.09 vs. limit=7.775
2024-10-07 20:32:30,616 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=44.61 vs. limit=7.6375
2024-10-07 20:32:42,887 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.34 vs. limit=4.148
2024-10-07 20:32:49,616 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=42.84 vs. limit=7.7775
2024-10-07 20:32:50,492 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=370.0, ans=0.48265625
2024-10-07 20:32:54,517 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=373.3333333333333, ans=0.4825
2024-10-07 20:32:56,352 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=33.22 vs. limit=7.64
2024-10-07 20:33:04,395 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=45.30 vs. limit=7.64
2024-10-07 20:33:13,876 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.01 vs. limit=5.1883333333333335
2024-10-07 20:33:15,688 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=8.72 vs. limit=7.7825
2024-10-07 20:33:17,541 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=376.6666666666667, ans=0.8868166666666667
2024-10-07 20:33:25,841 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=40.66 vs. limit=7.785
2024-10-07 20:33:26,035 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.68 vs. limit=5.095
2024-10-07 20:33:33,509 INFO [train.py:1153] Epoch 1, batch 1150, loss[loss=1.032, simple_loss=0.6388, pruned_loss=0.6009, ctc_loss=1.028, over 4850.00 frames. ], tot_loss[loss=1.039, simple_loss=0.6442, pruned_loss=0.6564, ctc_loss=1.027, over 964294.11 frames. ], batch size: 20, lr: 4.47e-02,
2024-10-07 20:33:45,976 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=1.939e-01
2024-10-07 20:33:59,694 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=5.93 vs. limit=4.156
2024-10-07 20:34:03,264 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=390.0, ans=0.185375
2024-10-07 20:34:05,390 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.11 vs. limit=7.7925
2024-10-07 20:34:07,427 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=390.0, ans=0.185375
2024-10-07 20:34:07,496 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=390.0, ans=0.2961
2024-10-07 20:34:12,069 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.16 vs. limit=7.7925
2024-10-07 20:34:29,404 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=17.15 vs. limit=7.64875
2024-10-07 20:34:31,879 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.56 vs. limit=3.0595
2024-10-07 20:34:32,271 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.91 vs. limit=5.099166666666667
2024-10-07 20:34:40,127 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=26.08 vs. limit=7.8
2024-10-07 20:34:40,880 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.290e+01 1.030e+02 1.147e+02 1.262e+02 1.925e+02, threshold=2.295e+02, percent-clipped=0.0
2024-10-07 20:34:40,924 INFO [train.py:1153] Epoch 1, batch 1200, loss[loss=0.9912, simple_loss=0.6133, pruned_loss=0.5686, ctc_loss=0.9896, over 4810.00 frames. ], tot_loss[loss=1.027, simple_loss=0.6379, pruned_loss=0.6363, ctc_loss=1.012, over 964164.94 frames. ], batch size: 25, lr: 4.47e-02,
2024-10-07 20:34:43,504 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.8.prob, batch_count=400.0, ans=0.48125
2024-10-07 20:34:49,032 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=400.0, ans=0.20600000000000002
2024-10-07 20:34:53,778 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.48 vs. limit=7.8025
2024-10-07 20:35:11,956 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=406.6666666666667, ans=0.4809375
2024-10-07 20:35:14,958 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=20.65 vs. limit=7.6525
2024-10-07 20:35:16,650 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.04 vs. limit=7.805
2024-10-07 20:35:20,166 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=4.70 vs. limit=4.164
2024-10-07 20:35:29,771 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.17 vs. limit=3.0615
2024-10-07 20:35:42,611 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=413.3333333333333, ans=0.44833333333333336
2024-10-07 20:35:47,750 INFO [train.py:1153] Epoch 1, batch 1250, loss[loss=0.9159, simple_loss=0.5781, pruned_loss=0.5118, ctc_loss=0.8966, over 4754.00 frames. ], tot_loss[loss=1.015, simple_loss=0.6319, pruned_loss=0.6172, ctc_loss=0.998, over 964201.34 frames. ], batch size: 32, lr: 4.47e-02,
2024-10-07 20:35:50,938 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.47 vs. limit=3.0625
2024-10-07 20:35:58,883 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=0.000e+00
2024-10-07 20:36:17,188 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=8.98 vs. limit=7.65875
2024-10-07 20:36:23,044 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=423.3333333333333, ans=0.44708333333333333
2024-10-07 20:36:26,236 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.30 vs. limit=3.0635
2024-10-07 20:36:29,841 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=426.6666666666667, ans=0.48
2024-10-07 20:36:33,806 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=426.6666666666667, ans=0.04866666666666667
2024-10-07 20:36:35,141 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.min_abs, batch_count=426.6666666666667, ans=0.2064
2024-10-07 20:36:35,656 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=11.71 vs. limit=5.1066666666666665
2024-10-07 20:36:52,716 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=430.0, ans=0.47984375
2024-10-07 20:36:55,210 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.843e+01 1.073e+02 1.230e+02 1.441e+02 2.924e+02, threshold=2.461e+02, percent-clipped=2.0
2024-10-07 20:36:55,258 INFO [train.py:1153] Epoch 1, batch 1300, loss[loss=1.01, simple_loss=0.6378, pruned_loss=0.5621, ctc_loss=0.9697, over 4845.00 frames. ], tot_loss[loss=0.9973, simple_loss=0.6234, pruned_loss=0.5954, ctc_loss=0.9788, over 965501.64 frames. ], batch size: 43, lr: 4.47e-02,
2024-10-07 20:36:57,254 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=42.38 vs. limit=7.825
2024-10-07 20:37:20,463 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=436.6666666666667, ans=7.8275
2024-10-07 20:37:21,719 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=35.51 vs. limit=7.83
2024-10-07 20:37:22,825 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.09 vs. limit=7.665
2024-10-07 20:37:23,338 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.27 vs. limit=7.83
2024-10-07 20:37:35,002 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=14.97 vs. limit=7.66625
2024-10-07 20:37:40,579 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=37.22 vs. limit=7.8325
2024-10-07 20:37:43,537 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=34.60 vs. limit=7.8325
2024-10-07 20:38:03,018 INFO [train.py:1153] Epoch 1, batch 1350, loss[loss=0.9506, simple_loss=0.6107, pruned_loss=0.5248, ctc_loss=0.873, over 4829.00 frames. ], tot_loss[loss=0.9787, simple_loss=0.6147, pruned_loss=0.5736, ctc_loss=0.9575, over 966236.50 frames. ], batch size: 21, lr: 4.46e-02,
2024-10-07 20:38:05,252 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.96 vs. limit=7.8375
2024-10-07 20:38:15,293 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=453.3333333333333, ans=0.47875
2024-10-07 20:38:17,957 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=453.3333333333333, ans=0.47875
2024-10-07 20:38:22,589 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=15.26 vs. limit=5.226666666666667
2024-10-07 20:38:28,800 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=456.6666666666667, ans=0.24543333333333334
2024-10-07 20:38:41,065 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=456.6666666666667, ans=0.182875
2024-10-07 20:38:43,663 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=460.0, ans=0.224125
2024-10-07 20:38:56,309 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=21.81 vs. limit=7.67375
2024-10-07 20:39:00,486 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.63 vs. limit=7.67375
2024-10-07 20:39:00,662 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=10.03 vs. limit=7.67375
2024-10-07 20:39:10,706 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.278e+01 1.167e+02 1.287e+02 1.492e+02 2.573e+02, threshold=2.575e+02, percent-clipped=1.0
2024-10-07 20:39:10,750 INFO [train.py:1153] Epoch 1, batch 1400, loss[loss=0.858, simple_loss=0.5602, pruned_loss=0.4444, ctc_loss=0.8471, over 4940.00 frames. ], tot_loss[loss=0.9633, simple_loss=0.6077, pruned_loss=0.5544, ctc_loss=0.9406, over 966729.80 frames. ], batch size: 19, lr: 4.46e-02,
2024-10-07 20:39:18,201 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.62 vs. limit=5.116666666666666
2024-10-07 20:39:23,376 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten.whitening_limit, batch_count=470.0, ans=7.67625
2024-10-07 20:39:28,241 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=470.0, ans=0.182375
2024-10-07 20:39:32,748 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=16.46 vs. limit=5.235
2024-10-07 20:39:48,556 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=473.3333333333333, ans=0.4778125
2024-10-07 20:40:05,857 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=18.52 vs. limit=7.68
2024-10-07 20:40:12,353 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=6.47 vs. limit=4.192
2024-10-07 20:40:14,906 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.15 vs. limit=3.072
2024-10-07 20:40:15,135 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.53 vs. limit=7.86
2024-10-07 20:40:18,285 INFO [train.py:1153] Epoch 1, batch 1450, loss[loss=0.8689, simple_loss=0.5506, pruned_loss=0.4624, ctc_loss=0.8495, over 4798.00 frames. ], tot_loss[loss=0.9469, simple_loss=0.5997, pruned_loss=0.536, ctc_loss=0.922, over 966641.08 frames. ], batch size: 34, lr: 4.46e-02,
2024-10-07 20:40:19,649 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=483.3333333333333, ans=0.47734375
2024-10-07 20:40:20,131 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=44.47 vs. limit=7.8625
2024-10-07 20:40:31,532 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=486.6666666666667, ans=0.8829666666666667
2024-10-07 20:40:40,084 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.87 vs. limit=7.6825
2024-10-07 20:40:42,399 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=15.86 vs. limit=7.6825
2024-10-07 20:40:47,753 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=19.20 vs. limit=7.68375
2024-10-07 20:40:58,410 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=31.37 vs. limit=7.87
2024-10-07 20:41:05,900 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=493.3333333333333, ans=0.7549333333333333
2024-10-07 20:41:11,441 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=23.54 vs. limit=7.8725
2024-10-07 20:41:17,905 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=496.6666666666667, ans=0.181375
2024-10-07 20:41:18,084 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=16.27 vs. limit=7.68625
2024-10-07 20:41:22,298 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.28 vs. limit=5.1241666666666665
2024-10-07 20:41:24,238 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.004e+02 1.219e+02 1.341e+02 1.568e+02 2.312e+02, threshold=2.682e+02, percent-clipped=0.0
2024-10-07 20:41:24,285 INFO [train.py:1153] Epoch 1, batch 1500, loss[loss=0.7944, simple_loss=0.5196, pruned_loss=0.3991, ctc_loss=0.8015, over 4743.00 frames. ], tot_loss[loss=0.9301, simple_loss=0.5925, pruned_loss=0.5171, ctc_loss=0.9048, over 966360.75 frames. ], batch size: 26, lr: 4.46e-02,
2024-10-07 20:41:27,870 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.35 vs. limit=7.875
2024-10-07 20:41:46,043 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=18.59 vs. limit=5.251666666666667
2024-10-07 20:41:47,404 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=28.04 vs. limit=7.8775
2024-10-07 20:41:54,168 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.80 vs. limit=7.88
2024-10-07 20:42:12,987 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=8.07 vs. limit=7.69125
2024-10-07 20:42:17,445 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=513.3333333333334, ans=0.8820333333333333
2024-10-07 20:42:17,477 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=513.3333333333334, ans=0.29486666666666667
2024-10-07 20:42:26,917 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=513.3333333333334, ans=0.221125
2024-10-07 20:42:30,644 INFO [train.py:1153] Epoch 1, batch 1550, loss[loss=0.8477, simple_loss=0.5431, pruned_loss=0.4413, ctc_loss=0.8149, over 4857.00 frames. ], tot_loss[loss=0.9128, simple_loss=0.5841, pruned_loss=0.4996, ctc_loss=0.8863, over 966180.64 frames. ], batch size: 31, lr: 4.45e-02,
2024-10-07 20:42:46,737 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=520.0, ans=0.2948
2024-10-07 20:42:51,056 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=4.07 vs. limit=4.208
2024-10-07 20:43:05,544 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=523.3333333333334, ans=0.180375
2024-10-07 20:43:14,333 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.05 vs. limit=5.131666666666667
2024-10-07 20:43:27,429 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=33.88 vs. limit=7.8975
2024-10-07 20:43:37,798 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.022e+02 1.271e+02 1.431e+02 1.637e+02 2.989e+02, threshold=2.862e+02, percent-clipped=1.0
2024-10-07 20:43:37,840 INFO [train.py:1153] Epoch 1, batch 1600, loss[loss=0.8377, simple_loss=0.5621, pruned_loss=0.4164, ctc_loss=0.7945, over 4810.00 frames. ], tot_loss[loss=0.8943, simple_loss=0.575, pruned_loss=0.4821, ctc_loss=0.8667, over 966378.52 frames. ], batch size: 25, lr: 4.45e-02,
2024-10-07 20:43:48,807 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=6.34 vs. limit=4.213333333333333
2024-10-07 20:43:50,074 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.30 vs. limit=5.134166666666666
2024-10-07 20:43:52,181 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=536.6666666666666, ans=0.47484375
2024-10-07 20:44:06,375 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=540.0, ans=0.8811
2024-10-07 20:44:07,740 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=540.0, ans=0.21962500000000001
2024-10-07 20:44:08,254 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=38.72 vs. limit=7.905
2024-10-07 20:44:10,357 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=540.0, ans=0.21962500000000001
2024-10-07 20:44:16,898 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-07 20:44:20,657 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=543.3333333333334, ans=0.47453125
2024-10-07 20:44:24,721 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=543.3333333333334, ans=0.087775
2024-10-07 20:44:34,025 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=546.6666666666666, ans=0.43166666666666664
2024-10-07 20:44:37,255 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=34.31 vs. limit=7.91
2024-10-07 20:44:39,936 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=13.48 vs. limit=7.705
2024-10-07 20:44:39,968 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.78 vs. limit=7.705
2024-10-07 20:44:41,272 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=30.54 vs. limit=7.91
2024-10-07 20:44:43,331 INFO [train.py:1153] Epoch 1, batch 1650, loss[loss=0.8579, simple_loss=0.564, pruned_loss=0.4249, ctc_loss=0.8426, over 4792.00 frames. ], tot_loss[loss=0.8764, simple_loss=0.5666, pruned_loss=0.4654, ctc_loss=0.8485, over 966732.12 frames. ], batch size: 29, lr: 4.45e-02,
2024-10-07 20:44:49,261 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=5.15 vs. limit=7.70625
2024-10-07 20:44:50,921 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.17 vs. limit=7.9125
2024-10-07 20:44:58,844 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.51 vs. limit=3.083
2024-10-07 20:45:02,700 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.16 vs. limit=7.7075
2024-10-07 20:45:05,563 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=33.04 vs. limit=7.915
2024-10-07 20:45:08,488 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.41 vs. limit=5.138333333333334
2024-10-07 20:45:10,956 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.62 vs. limit=5.139166666666666
2024-10-07 20:45:13,233 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=556.6666666666666, ans=0.179125
2024-10-07 20:45:27,048 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=11.97 vs. limit=5.28
2024-10-07 20:45:30,228 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=4.34 vs. limit=4.112
2024-10-07 20:45:32,606 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=15.01 vs. limit=7.71
2024-10-07 20:45:40,190 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=563.3333333333334, ans=0.47359375
2024-10-07 20:45:49,926 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.85 vs. limit=5.141666666666667
2024-10-07 20:45:50,762 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.056e+02 1.360e+02 1.470e+02 1.680e+02 2.609e+02, threshold=2.940e+02, percent-clipped=0.0
2024-10-07 20:45:50,809 INFO [train.py:1153] Epoch 1, batch 1700, loss[loss=0.7474, simple_loss=0.4916, pruned_loss=0.3658, ctc_loss=0.7412, over 4940.00 frames. ], tot_loss[loss=0.8608, simple_loss=0.5593, pruned_loss=0.4505, ctc_loss=0.833, over 966952.97 frames. ], batch size: 19, lr: 4.44e-02,
2024-10-07 20:45:55,606 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=21.60 vs. limit=7.7125
2024-10-07 20:46:07,724 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.35 vs. limit=3.0855
2024-10-07 20:46:14,099 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=570.0, ans=0.20855
2024-10-07 20:46:18,418 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=29.57 vs. limit=7.93
2024-10-07 20:46:21,096 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=15.55 vs. limit=5.286666666666667
2024-10-07 20:46:36,674 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=576.6666666666666, ans=0.29423333333333335
2024-10-07 20:46:38,050 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=576.6666666666666, ans=0.178375
2024-10-07 20:46:39,399 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=576.6666666666666, ans=0.42791666666666667
2024-10-07 20:46:41,362 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.53 vs. limit=7.71625
2024-10-07 20:46:44,692 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=580.0, ans=0.8797
2024-10-07 20:46:47,705 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.48 vs. limit=5.145
2024-10-07 20:46:50,463 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=20.86 vs. limit=5.29
2024-10-07 20:46:51,331 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=580.0, ans=0.4728125
2024-10-07 20:46:52,771 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=580.0, ans=0.17825000000000002
2024-10-07 20:46:54,092 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=580.0, ans=0.4728125
2024-10-07 20:46:58,052 INFO [train.py:1153] Epoch 1, batch 1750, loss[loss=0.7701, simple_loss=0.5109, pruned_loss=0.3851, ctc_loss=0.7047, over 4959.00 frames. ], tot_loss[loss=0.844, simple_loss=0.5506, pruned_loss=0.4361, ctc_loss=0.8158, over 967116.99 frames. ], batch size: 19, lr: 4.44e-02,
2024-10-07 20:47:10,295 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=586.6666666666666, ans=0.17800000000000002
2024-10-07 20:47:12,249 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=10.31 vs. limit=7.72
2024-10-07 20:47:12,471 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.95 vs. limit=5.1466666666666665
2024-10-07 20:47:20,547 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.97 vs. limit=5.1466666666666665
2024-10-07 20:47:45,225 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer1.prob, batch_count=593.3333333333334, ans=0.4721875
2024-10-07 20:47:47,941 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=593.3333333333334, ans=0.17775000000000002
2024-10-07 20:47:59,904 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=596.6666666666666, ans=0.47203125
2024-10-07 20:48:05,506 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.051e+02 1.310e+02 1.489e+02 1.723e+02 3.144e+02, threshold=2.977e+02, percent-clipped=1.0
2024-10-07 20:48:05,550 INFO [train.py:1153] Epoch 1, batch 1800, loss[loss=0.7814, simple_loss=0.523, pruned_loss=0.3731, ctc_loss=0.7713, over 4864.00 frames. ], tot_loss[loss=0.8326, simple_loss=0.5456, pruned_loss=0.425, ctc_loss=0.8032, over 967907.35 frames. ], batch size: 23, lr: 4.44e-02,
2024-10-07 20:48:09,528 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=600.0, ans=0.17750000000000002
2024-10-07 20:48:10,846 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=600.0, ans=5.375
2024-10-07 20:48:11,515 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=7.82 vs. limit=7.725
2024-10-07 20:48:23,289 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=17.23 vs. limit=7.72625
2024-10-07 20:48:29,501 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=603.3333333333334, ans=0.29396666666666665
2024-10-07 20:48:35,193 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=37.82 vs. limit=7.955
2024-10-07 20:48:43,349 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=34.10 vs. limit=7.955
2024-10-07 20:48:45,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=610.0, ans=0.5
2024-10-07 20:48:59,826 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.25 vs. limit=7.96
2024-10-07 20:49:00,846 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=16.27 vs. limit=7.73
2024-10-07 20:49:03,161 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=613.3333333333334, ans=0.29386666666666666
2024-10-07 20:49:11,507 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.69 vs. limit=7.9625
2024-10-07 20:49:12,064 INFO [train.py:1153] Epoch 1, batch 1850, loss[loss=0.7208, simple_loss=0.4909, pruned_loss=0.3387, ctc_loss=0.7057, over 4737.00 frames. ], tot_loss[loss=0.8172, simple_loss=0.5383, pruned_loss=0.4121, ctc_loss=0.7877, over 968162.53 frames. ], batch size: 26, lr: 4.43e-02,
2024-10-07 20:49:16,210 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=12.24 vs. limit=7.73125
2024-10-07 20:49:18,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=616.6666666666666, ans=0.08612500000000001
2024-10-07 20:49:27,744 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.09 vs. limit=7.7325
2024-10-07 20:49:46,205 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.40 vs. limit=3.0935
2024-10-07 20:49:49,909 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.86 vs. limit=7.73375
2024-10-07 20:49:52,728 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=8.18 vs. limit=7.735
2024-10-07 20:49:53,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=626.6666666666666, ans=0.0859
2024-10-07 20:49:53,184 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=626.6666666666666, ans=0.17650000000000002
2024-10-07 20:49:58,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=10.72 vs. limit=7.735
2024-10-07 20:50:09,398 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=30.03 vs. limit=7.9725
2024-10-07 20:50:18,065 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.096e+02 1.385e+02 1.504e+02 1.716e+02 2.513e+02, threshold=3.007e+02, percent-clipped=0.0
2024-10-07 20:50:18,113 INFO [train.py:1153] Epoch 1, batch 1900, loss[loss=0.7796, simple_loss=0.5235, pruned_loss=0.3706, ctc_loss=0.7542, over 4773.00 frames. ], tot_loss[loss=0.8076, simple_loss=0.534, pruned_loss=0.4028, ctc_loss=0.7782, over 967894.55 frames. ], batch size: 29, lr: 4.43e-02,
2024-10-07 20:50:18,808 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.02 vs. limit=5.158333333333333
2024-10-07 20:50:20,920 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=633.3333333333334, ans=0.20950000000000002
2024-10-07 20:50:25,415 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.52 vs. limit=7.7375
2024-10-07 20:50:38,997 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn2.whiten.whitening_limit, batch_count=636.6666666666666, ans=7.9775
2024-10-07 20:50:45,161 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=15.32 vs. limit=5.32
2024-10-07 20:50:53,240 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.16 vs. limit=5.32
2024-10-07 20:51:06,464 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.19 vs. limit=7.74125
2024-10-07 20:51:14,991 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=646.6666666666666, ans=0.4696875
2024-10-07 20:51:18,131 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.62 vs. limit=5.323333333333333
2024-10-07 20:51:24,102 INFO [train.py:1153] Epoch 1, batch 1950, loss[loss=0.6938, simple_loss=0.4724, pruned_loss=0.3263, ctc_loss=0.6634, over 4863.00 frames. ], tot_loss[loss=0.7982, simple_loss=0.53, pruned_loss=0.3939, ctc_loss=0.7685, over 966894.31 frames. ], batch size: 20, lr: 4.43e-02,
2024-10-07 20:51:33,889 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=14.68 vs. limit=7.74375
2024-10-07 20:51:37,457 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=653.3333333333334, ans=0.04795833333333334
2024-10-07 20:51:38,760 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=653.3333333333334, ans=0.17550000000000002
2024-10-07 20:51:52,326 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.75 vs. limit=7.9925
2024-10-07 20:52:05,345 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.53 vs. limit=3.099
2024-10-07 20:52:15,641 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=663.3333333333334, ans=0.175125
2024-10-07 20:52:18,470 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=663.3333333333334, ans=0.8767833333333334
2024-10-07 20:52:19,618 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=663.3333333333334, ans=0.047927083333333335
2024-10-07 20:52:30,405 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.168e+02 1.390e+02 1.486e+02 1.748e+02 3.538e+02, threshold=2.972e+02, percent-clipped=1.0
2024-10-07 20:52:30,449 INFO [train.py:1153] Epoch 1, batch 2000, loss[loss=0.662, simple_loss=0.4499, pruned_loss=0.3103, ctc_loss=0.6336, over 4959.00 frames. ], tot_loss[loss=0.7884, simple_loss=0.5255, pruned_loss=0.3855, ctc_loss=0.7579, over 966732.32 frames. ], batch size: 19, lr: 4.42e-02,
2024-10-07 20:52:31,031 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.82 vs. limit=5.166666666666667
2024-10-07 20:52:46,414 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.min_positive, batch_count=670.0, ans=0.2433
2024-10-07 20:52:57,706 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=24.62 vs. limit=8.005
2024-10-07 20:53:01,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.57 vs. limit=5.168333333333333
2024-10-07 20:53:05,827 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.78 vs. limit=8.005
2024-10-07 20:53:15,141 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.94 vs. limit=7.75375
2024-10-07 20:53:16,355 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.85 vs. limit=4.270666666666667
2024-10-07 20:53:18,890 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.56 vs. limit=5.1691666666666665
2024-10-07 20:53:29,710 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.41 vs. limit=5.17
2024-10-07 20:53:35,682 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-07 20:53:36,859 INFO [train.py:1153] Epoch 1, batch 2050, loss[loss=0.6764, simple_loss=0.4583, pruned_loss=0.317, ctc_loss=0.651, over 4914.00 frames. ], tot_loss[loss=0.7781, simple_loss=0.5206, pruned_loss=0.3774, ctc_loss=0.7462, over 967008.64 frames. ], batch size: 19, lr: 4.42e-02,
2024-10-07 20:53:37,077 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=683.3333333333334, ans=0.174375
2024-10-07 20:53:38,325 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=683.3333333333334, ans=0.084625
2024-10-07 20:53:55,196 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=20.20 vs. limit=5.343333333333334
2024-10-07 20:54:14,916 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=19.50 vs. limit=7.75875
2024-10-07 20:54:15,138 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.67 vs. limit=5.1725
2024-10-07 20:54:18,381 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=693.3333333333334, ans=0.41333333333333333
2024-10-07 20:54:31,606 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=696.6666666666666, ans=0.2108125
2024-10-07 20:54:31,909 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.76 vs. limit=7.76125
2024-10-07 20:54:43,443 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.002e+02 1.399e+02 1.521e+02 1.665e+02 2.329e+02, threshold=3.041e+02, percent-clipped=0.0
2024-10-07 20:54:43,489 INFO [train.py:1153] Epoch 1, batch 2100, loss[loss=0.7594, simple_loss=0.5207, pruned_loss=0.3588, ctc_loss=0.7009, over 4841.00 frames. ], tot_loss[loss=0.7647, simple_loss=0.5144, pruned_loss=0.3679, ctc_loss=0.7326, over 967171.22 frames. ], batch size: 21, lr: 4.42e-02,
2024-10-07 20:54:44,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=700.0, ans=0.4671875
2024-10-07 20:54:45,620 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=16.36 vs. limit=8.025
2024-10-07 20:55:11,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=706.6666666666666, ans=0.17350000000000002
2024-10-07 20:55:13,504 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.25 vs. limit=3.106
2024-10-07 20:55:17,591 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.16 vs. limit=5.176666666666667
2024-10-07 20:55:19,851 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=706.6666666666666, ans=0.466875
2024-10-07 20:55:20,752 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=14.45 vs. limit=8.03
2024-10-07 20:55:25,691 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=35.11 vs. limit=8.0325
2024-10-07 20:55:28,518 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=10.03 vs. limit=7.76625
2024-10-07 20:55:28,725 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=14.30 vs. limit=8.0325
2024-10-07 20:55:35,164 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.42 vs. limit=7.76625
2024-10-07 20:55:50,410 INFO [train.py:1153] Epoch 1, batch 2150, loss[loss=0.6231, simple_loss=0.4415, pruned_loss=0.2758, ctc_loss=0.6329, over 4862.00 frames. ], tot_loss[loss=0.7531, simple_loss=0.5096, pruned_loss=0.3592, ctc_loss=0.7223, over 968006.36 frames. ], batch size: 20, lr: 4.41e-02,
2024-10-07 20:55:52,504 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.79 vs. limit=4.286666666666667
2024-10-07 20:56:05,595 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=17.30 vs. limit=5.36
2024-10-07 20:56:15,984 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=723.3333333333334, ans=0.4095833333333333
2024-10-07 20:56:21,617 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.94 vs. limit=5.180833333333333
2024-10-07 20:56:21,650 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=6.45 vs. limit=4.289333333333333
2024-10-07 20:56:29,813 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=13.61 vs. limit=8.045
2024-10-07 20:56:37,135 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-07 20:56:37,368 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=33.88 vs. limit=8.045
2024-10-07 20:56:45,127 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=21.04 vs. limit=8.0475
2024-10-07 20:56:54,527 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=16.85 vs. limit=5.365
2024-10-07 20:56:55,854 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=733.3333333333334, ans=0.0835
2024-10-07 20:56:56,933 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.135e+02 1.367e+02 1.584e+02 1.807e+02 3.869e+02, threshold=3.169e+02, percent-clipped=2.0
2024-10-07 20:56:56,976 INFO [train.py:1153] Epoch 1, batch 2200, loss[loss=0.7815, simple_loss=0.5252, pruned_loss=0.3699, ctc_loss=0.7449, over 4741.00 frames. ], tot_loss[loss=0.7425, simple_loss=0.5051, pruned_loss=0.3516, ctc_loss=0.7127, over 967775.49 frames. ], batch size: 26, lr: 4.41e-02,
2024-10-07 20:57:09,107 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=19.93 vs. limit=8.0525
2024-10-07 20:57:11,477 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=736.6666666666666, ans=0.172375
2024-10-07 20:57:14,762 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=14.09 vs. limit=7.77625
2024-10-07 20:57:15,530 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.min_positive, batch_count=736.6666666666666, ans=0.09539583333333333
2024-10-07 20:57:20,887 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=736.6666666666666, ans=0.8742166666666666
2024-10-07 20:57:32,900 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=740.0, ans=0.17225000000000001
2024-10-07 20:57:34,225 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=740.0, ans=0.08335000000000001
2024-10-07 20:57:34,940 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.43 vs. limit=7.7775
2024-10-07 20:57:46,284 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=743.3333333333334, ans=0.8739833333333333
2024-10-07 20:57:54,824 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten.whitening_limit, batch_count=746.6666666666666, ans=7.78
2024-10-07 20:58:04,081 INFO [train.py:1153] Epoch 1, batch 2250, loss[loss=0.7556, simple_loss=0.5322, pruned_loss=0.3471, ctc_loss=0.7125, over 4870.00 frames. ], tot_loss[loss=0.732, simple_loss=0.5008, pruned_loss=0.3441, ctc_loss=0.7037, over 967682.50 frames. ], batch size: 22, lr: 4.40e-02,
2024-10-07 20:58:08,890 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.77 vs. limit=5.375
2024-10-07 20:58:09,816 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten.whitening_limit, batch_count=750.0, ans=7.78125
2024-10-07 20:58:12,708 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.98 vs. limit=5.375
2024-10-07 20:58:13,332 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=750.0, ans=0.083125
2024-10-07 20:58:18,457 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=753.3333333333334, ans=0.08305
2024-10-07 20:58:30,921 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.32 vs. limit=5.378333333333333
2024-10-07 20:58:34,871 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=6.33 vs. limit=4.302666666666667
2024-10-07 20:58:53,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=760.0, ans=0.5
2024-10-07 20:59:01,500 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.94 vs. limit=4.3053333333333335
2024-10-07 20:59:01,526 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=14.79 vs. limit=5.381666666666667
2024-10-07 20:59:05,549 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=4.43 vs. limit=7.78625
2024-10-07 20:59:10,383 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.162e+02 1.353e+02 1.536e+02 1.806e+02 3.135e+02, threshold=3.071e+02, percent-clipped=0.0
2024-10-07 20:59:10,429 INFO [train.py:1153] Epoch 1, batch 2300, loss[loss=0.7425, simple_loss=0.5174, pruned_loss=0.3489, ctc_loss=0.6747, over 4883.00 frames. ], tot_loss[loss=0.7198, simple_loss=0.4954, pruned_loss=0.3362, ctc_loss=0.6921, over 968174.42 frames. ], batch size: 19, lr: 4.40e-02,
2024-10-07 20:59:10,967 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.22 vs. limit=5.191666666666666
2024-10-07 20:59:23,126 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=30.22 vs. limit=8.0775
2024-10-07 20:59:24,311 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=21.17 vs. limit=8.0775
2024-10-07 20:59:26,915 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=20.76 vs. limit=8.0775
2024-10-07 20:59:28,403 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=15.54 vs. limit=5.385
2024-10-07 20:59:38,729 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=773.3333333333334, ans=0.46375
2024-10-07 20:59:40,477 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=13.08 vs. limit=7.79
2024-10-07 20:59:41,421 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=773.3333333333334, ans=0.2116
2024-10-07 20:59:47,330 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.02 vs. limit=7.79
2024-10-07 20:59:56,676 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=14.45 vs. limit=7.79125
2024-10-07 21:00:00,747 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.32 vs. limit=3.1165
2024-10-07 21:00:00,803 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.25 vs. limit=7.79125
2024-10-07 21:00:03,263 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.94 vs. limit=5.195
2024-10-07 21:00:04,188 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=780.0, ans=0.4634375
2024-10-07 21:00:17,585 INFO [train.py:1153] Epoch 1, batch 2350, loss[loss=0.662, simple_loss=0.4705, pruned_loss=0.2941, ctc_loss=0.6636, over 4865.00 frames. ], tot_loss[loss=0.7132, simple_loss=0.4926, pruned_loss=0.3316, ctc_loss=0.6864, over 968126.48 frames. ], batch size: 23, lr: 4.40e-02,
2024-10-07 21:00:19,130 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=783.3333333333334, ans=0.7578333333333334
2024-10-07 21:00:30,330 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.91 vs. limit=5.196666666666666
2024-10-07 21:00:36,589 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=786.6666666666666, ans=0.8724666666666667
2024-10-07 21:00:37,856 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=786.6666666666666, ans=0.463125
2024-10-07 21:00:45,109 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=7.80 vs. limit=7.79625
2024-10-07 21:00:45,987 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=790.0, ans=0.46296875
2024-10-07 21:00:47,758 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.76 vs. limit=7.79625
2024-10-07 21:00:51,368 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-07 21:00:54,635 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.49 vs. limit=8.0925
2024-10-07 21:00:57,042 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.56 vs. limit=8.095
2024-10-07 21:00:58,003 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=13.64 vs. limit=5.3966666666666665
2024-10-07 21:01:05,775 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=793.3333333333334, ans=0.2119
2024-10-07 21:01:15,378 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.48 vs. limit=4.318666666666667
2024-10-07 21:01:17,662 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=796.6666666666666, ans=0.2920333333333333
2024-10-07 21:01:24,315 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.149e+02 1.410e+02 1.574e+02 1.726e+02 2.591e+02, threshold=3.148e+02, percent-clipped=0.0
2024-10-07 21:01:24,359 INFO [train.py:1153] Epoch 1, batch 2400, loss[loss=0.7475, simple_loss=0.5309, pruned_loss=0.3451, ctc_loss=0.685, over 4749.00 frames. ], tot_loss[loss=0.7074, simple_loss=0.4905, pruned_loss=0.3276, ctc_loss=0.6806, over 967387.13 frames. ], batch size: 19, lr: 4.39e-02,
2024-10-07 21:01:25,276 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.84 vs. limit=8.1
2024-10-07 21:01:28,374 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=800.0, ans=0.872
2024-10-07 21:01:39,264 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.58 vs. limit=4.3213333333333335
2024-10-07 21:01:42,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=803.3333333333334, ans=0.8718833333333333
2024-10-07 21:01:43,283 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.74 vs. limit=4.3213333333333335
2024-10-07 21:01:43,405 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=21.75 vs. limit=8.1025
2024-10-07 21:02:07,139 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.24 vs. limit=3.1215
2024-10-07 21:02:07,156 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=11.95 vs. limit=7.80375
2024-10-07 21:02:08,787 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.19 vs. limit=3.1215
2024-10-07 21:02:23,933 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=813.3333333333334, ans=0.09491666666666668
2024-10-07 21:02:25,788 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=4.85 vs. limit=7.805
2024-10-07 21:02:30,507 INFO [train.py:1153] Epoch 1, batch 2450, loss[loss=0.6425, simple_loss=0.4612, pruned_loss=0.2849, ctc_loss=0.6349, over 4865.00 frames. ], tot_loss[loss=0.7028, simple_loss=0.489, pruned_loss=0.3242, ctc_loss=0.6761, over 966635.17 frames. ], batch size: 22, lr: 4.39e-02,
2024-10-07 21:02:33,869 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=28.24 vs. limit=8.1125
2024-10-07 21:02:40,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=816.6666666666666, ans=0.169375
2024-10-07 21:02:42,853 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=820.0, ans=0.4615625
2024-10-07 21:02:56,573 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.53 vs. limit=7.80875
2024-10-07 21:02:58,952 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-07 21:02:59,051 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=11.99 vs. limit=7.80875
2024-10-07 21:03:04,488 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.87 vs. limit=7.80875
2024-10-07 21:03:23,173 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.05 vs. limit=7.81125
2024-10-07 21:03:32,371 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=31.27 vs. limit=8.1225
2024-10-07 21:03:36,952 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.209e+02 1.414e+02 1.559e+02 1.736e+02 2.456e+02, threshold=3.119e+02, percent-clipped=0.0
2024-10-07 21:03:36,997 INFO [train.py:1153] Epoch 1, batch 2500, loss[loss=0.7278, simple_loss=0.5123, pruned_loss=0.3332, ctc_loss=0.6925, over 4740.00 frames. ], tot_loss[loss=0.6949, simple_loss=0.4859, pruned_loss=0.3192, ctc_loss=0.6687, over 966280.93 frames. ], batch size: 26, lr: 4.38e-02,
2024-10-07 21:03:38,967 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.93 vs. limit=4.333333333333333
2024-10-07 21:03:44,338 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.whiten.whitening_limit, batch_count=833.3333333333334, ans=4.333333333333333
2024-10-07 21:03:48,540 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.29 vs. limit=3.125
2024-10-07 21:03:51,912 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=836.6666666666666, ans=0.46078125000000003
2024-10-07 21:03:53,215 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=836.6666666666666, ans=0.21255000000000002
2024-10-07 21:04:06,879 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.33 vs. limit=5.21
2024-10-07 21:04:14,699 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=28.65 vs. limit=8.13
2024-10-07 21:04:22,193 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.64 vs. limit=7.81625
2024-10-07 21:04:23,618 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.35 vs. limit=5.210833333333333
2024-10-07 21:04:28,460 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=846.6666666666666, ans=0.08095000000000001
2024-10-07 21:04:28,748 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.66 vs. limit=7.8175
2024-10-07 21:04:42,999 INFO [train.py:1153] Epoch 1, batch 2550, loss[loss=0.5834, simple_loss=0.4259, pruned_loss=0.2601, ctc_loss=0.5519, over 4959.00 frames. ], tot_loss[loss=0.6886, simple_loss=0.4833, pruned_loss=0.3153, ctc_loss=0.6619, over 966882.73 frames. ], batch size: 19, lr: 4.38e-02,
2024-10-07 21:04:51,632 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.11 vs. limit=3.1275
2024-10-07 21:04:55,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=38.36 vs. limit=8.14
2024-10-07 21:04:56,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.53 vs. limit=5.426666666666667
2024-10-07 21:05:06,914 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=853.3333333333334, ans=0.08080000000000001
2024-10-07 21:05:09,191 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.53 vs. limit=5.428333333333334
2024-10-07 21:05:12,759 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=30.56 vs. limit=8.1425
2024-10-07 21:05:20,149 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=856.6666666666666, ans=0.45984375
2024-10-07 21:05:31,262 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=14.15 vs. limit=5.43
2024-10-07 21:05:35,165 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=5.08 vs. limit=4.3453333333333335
2024-10-07 21:05:41,419 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=863.3333333333334, ans=0.45953125
2024-10-07 21:05:44,022 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=863.3333333333334, ans=0.45953125
2024-10-07 21:05:46,682 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=863.3333333333334, ans=0.45953125
2024-10-07 21:05:46,704 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=863.3333333333334, ans=5.539583333333334
2024-10-07 21:05:49,368 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.029e+02 1.366e+02 1.539e+02 1.832e+02 2.983e+02, threshold=3.077e+02, percent-clipped=0.0
2024-10-07 21:05:49,411 INFO [train.py:1153] Epoch 1, batch 2600, loss[loss=0.6545, simple_loss=0.4762, pruned_loss=0.2881, ctc_loss=0.6412, over 4859.00 frames. ], tot_loss[loss=0.6801, simple_loss=0.4798, pruned_loss=0.31, ctc_loss=0.6542, over 966370.44 frames. ], batch size: 20, lr: 4.37e-02,
2024-10-07 21:05:49,588 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=866.6666666666666, ans=0.459375
2024-10-07 21:05:50,129 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.68 vs. limit=8.15
2024-10-07 21:05:52,310 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=17.62 vs. limit=8.15
2024-10-07 21:05:53,911 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.74 vs. limit=7.825
2024-10-07 21:05:57,690 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=23.99 vs. limit=8.15
2024-10-07 21:05:57,773 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys.whitening_limit, batch_count=866.6666666666666, ans=3.13
2024-10-07 21:05:58,583 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=866.6666666666666, ans=0.459375
2024-10-07 21:06:15,260 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.00 vs. limit=5.218333333333334
2024-10-07 21:06:17,762 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.15 vs. limit=8.155
2024-10-07 21:06:23,249 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1.whitening_limit, batch_count=873.3333333333334, ans=5.218333333333334
2024-10-07 21:06:26,973 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.13 vs. limit=5.218333333333334
2024-10-07 21:06:36,200 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=15.03 vs. limit=8.1575
2024-10-07 21:06:40,216 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=7.01 vs. limit=7.82875
2024-10-07 21:06:45,787 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=880.0, ans=0.2912
2024-10-07 21:06:46,172 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=34.16 vs. limit=8.16
2024-10-07 21:06:46,175 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=15.72 vs. limit=5.44
2024-10-07 21:06:48,345 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=880.0, ans=0.5
2024-10-07 21:06:54,625 INFO [train.py:1153] Epoch 1, batch 2650, loss[loss=0.63, simple_loss=0.4505, pruned_loss=0.2762, ctc_loss=0.6426, over 4827.00 frames. ], tot_loss[loss=0.6779, simple_loss=0.4796, pruned_loss=0.3082, ctc_loss=0.6517, over 966102.73 frames. ], batch size: 38, lr: 4.37e-02,
2024-10-07 21:06:58,122 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=24.16 vs. limit=8.1625
2024-10-07 21:07:01,348 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-07 21:07:19,824 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=890.0, ans=0.45828125
2024-10-07 21:07:27,712 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=890.0, ans=0.45828125
2024-10-07 21:07:38,403 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=893.3333333333334, ans=0.29106666666666664
2024-10-07 21:07:43,609 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=893.3333333333334, ans=0.29106666666666664
2024-10-07 21:07:50,821 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.14 vs. limit=3.1345
2024-10-07 21:07:53,474 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.00 vs. limit=5.224166666666667
2024-10-07 21:08:00,534 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.212e+02 1.439e+02 1.572e+02 1.808e+02 2.982e+02, threshold=3.144e+02, percent-clipped=0.0
2024-10-07 21:08:00,580 INFO [train.py:1153] Epoch 1, batch 2700, loss[loss=0.628, simple_loss=0.4549, pruned_loss=0.2746, ctc_loss=0.6299, over 4854.00 frames. ], tot_loss[loss=0.6698, simple_loss=0.4761, pruned_loss=0.3031, ctc_loss=0.6446, over 966383.78 frames. ], batch size: 28, lr: 4.36e-02,
2024-10-07 21:08:17,310 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.18 vs. limit=3.1355
2024-10-07 21:08:21,372 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=7.93 vs. limit=7.83875
2024-10-07 21:08:40,509 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=910.0, ans=0.45734375
2024-10-07 21:08:51,192 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=910.0, ans=0.165875
2024-10-07 21:09:06,980 INFO [train.py:1153] Epoch 1, batch 2750, loss[loss=0.6628, simple_loss=0.4789, pruned_loss=0.2987, ctc_loss=0.6233, over 4799.00 frames. ], tot_loss[loss=0.665, simple_loss=0.4749, pruned_loss=0.2999, ctc_loss=0.6396, over 967020.74 frames. ], batch size: 19, lr: 4.36e-02,
2024-10-07 21:09:18,187 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=31.78 vs. limit=8.1875
2024-10-07 21:09:23,582 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=30.69 vs. limit=8.19
2024-10-07 21:09:25,084 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=5.86 vs. limit=7.845
2024-10-07 21:09:25,733 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=920.0, ans=0.047125
2024-10-07 21:09:26,281 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=6.62 vs. limit=4.368
2024-10-07 21:09:29,724 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=920.0, ans=0.1655
2024-10-07 21:09:43,268 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=24.33 vs. limit=8.1925
2024-10-07 21:09:53,592 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=926.6666666666666, ans=0.4565625
2024-10-07 21:09:58,725 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=930.0, ans=0.19768750000000002
2024-10-07 21:09:58,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=930.0, ans=0.165125
2024-10-07 21:10:00,085 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=930.0, ans=0.45640625
2024-10-07 21:10:05,285 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=930.0, ans=0.079075
2024-10-07 21:10:13,374 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.168e+02 1.422e+02 1.509e+02 1.727e+02 2.557e+02, threshold=3.019e+02, percent-clipped=0.0
2024-10-07 21:10:13,416 INFO [train.py:1153] Epoch 1, batch 2800, loss[loss=0.6724, simple_loss=0.4709, pruned_loss=0.3022, ctc_loss=0.6733, over 4771.00 frames. ], tot_loss[loss=0.6611, simple_loss=0.4734, pruned_loss=0.2974, ctc_loss=0.6362, over 967098.21 frames. ], batch size: 53, lr: 4.36e-02,
2024-10-07 21:10:17,245 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=933.3333333333334, ans=0.45625
2024-10-07 21:10:19,964 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=933.3333333333334, ans=0.21400000000000002
2024-10-07 21:10:20,266 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=27.27 vs. limit=8.2
2024-10-07 21:10:25,739 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=24.34 vs. limit=8.2025
2024-10-07 21:10:28,429 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.78 vs. limit=5.468333333333334
2024-10-07 21:10:41,250 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.13 vs. limit=7.8525
2024-10-07 21:10:46,975 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=14.24 vs. limit=8.205
2024-10-07 21:10:55,908 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.09 vs. limit=5.471666666666667
2024-10-07 21:11:05,873 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=946.6666666666666, ans=0.455625
2024-10-07 21:11:05,915 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=946.6666666666666, ans=0.1645
2024-10-07 21:11:07,800 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=17.39 vs. limit=8.21
2024-10-07 21:11:09,963 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=946.6666666666666, ans=0.2905333333333333
2024-10-07 21:11:11,308 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=946.6666666666666, ans=0.1645
2024-10-07 21:11:18,999 INFO [train.py:1153] Epoch 1, batch 2850, loss[loss=0.6097, simple_loss=0.4595, pruned_loss=0.2666, ctc_loss=0.5666, over 4932.00 frames. ], tot_loss[loss=0.6577, simple_loss=0.472, pruned_loss=0.2953, ctc_loss=0.6327, over 966900.70 frames. ], batch size: 20, lr: 4.35e-02,
2024-10-07 21:11:22,345 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.85 vs. limit=8.2125
2024-10-07 21:11:24,366 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=950.0, ans=0.45546875
2024-10-07 21:11:30,503 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=4.46 vs. limit=7.85625
2024-10-07 21:11:44,134 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=956.6666666666666, ans=0.45515625
2024-10-07 21:11:44,748 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.46 vs. limit=5.239166666666667
2024-10-07 21:11:49,901 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.19 vs. limit=7.85875
2024-10-07 21:11:52,579 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.30 vs. limit=7.85875
2024-10-07 21:11:55,195 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.16 vs. limit=3.1435
2024-10-07 21:11:56,035 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=956.6666666666666, ans=0.078475
2024-10-07 21:11:56,041 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=956.6666666666666, ans=0.5
2024-10-07 21:11:57,338 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=960.0, ans=0.455
2024-10-07 21:11:58,662 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=960.0, ans=0.0784
2024-10-07 21:12:02,919 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=26.88 vs. limit=8.22
2024-10-07 21:12:15,531 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=963.3333333333334, ans=0.8662833333333334
2024-10-07 21:12:24,512 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.105e+02 1.395e+02 1.511e+02 1.761e+02 2.294e+02, threshold=3.022e+02, percent-clipped=0.0
2024-10-07 21:12:24,558 INFO [train.py:1153] Epoch 1, batch 2900, loss[loss=0.5276, simple_loss=0.4102, pruned_loss=0.2187, ctc_loss=0.5186, over 4746.00 frames. ], tot_loss[loss=0.6525, simple_loss=0.4697, pruned_loss=0.2921, ctc_loss=0.6285, over 965979.15 frames. ], batch size: 20, lr: 4.35e-02,
2024-10-07 21:12:26,098 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=966.6666666666666, ans=0.5
2024-10-07 21:12:26,540 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.15 vs. limit=5.483333333333333
2024-10-07 21:12:28,064 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.37 vs. limit=5.483333333333333
2024-10-07 21:12:29,135 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=32.97 vs. limit=8.225
2024-10-07 21:12:35,089 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.78 vs. limit=8.225
2024-10-07 21:12:43,499 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.max_abs, batch_count=970.0, ans=5.60625
2024-10-07 21:12:43,854 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=15.81 vs. limit=8.2275
2024-10-07 21:12:46,752 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.22 vs. limit=7.86375
2024-10-07 21:12:47,999 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.56 vs. limit=5.2425
2024-10-07 21:12:48,395 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.55 vs. limit=8.2275
2024-10-07 21:13:09,062 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=976.6666666666666, ans=0.45421875
2024-10-07 21:13:10,824 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=22.38 vs. limit=8.2325
2024-10-07 21:13:10,849 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2.whitening_limit, batch_count=976.6666666666666, ans=5.488333333333333
2024-10-07 21:13:28,689 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=980.0, ans=0.4540625
2024-10-07 21:13:30,092 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=983.3333333333334, ans=0.45390625
2024-10-07 21:13:31,197 INFO [train.py:1153] Epoch 1, batch 2950, loss[loss=0.5851, simple_loss=0.4361, pruned_loss=0.2528, ctc_loss=0.5713, over 4798.00 frames. ], tot_loss[loss=0.645, simple_loss=0.4664, pruned_loss=0.2876, ctc_loss=0.6213, over 966555.58 frames. ], batch size: 19, lr: 4.34e-02,
2024-10-07 21:13:32,918 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.44 vs. limit=5.491666666666667
2024-10-07 21:13:33,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=983.3333333333334, ans=0.3770833333333333
2024-10-07 21:13:36,686 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=983.3333333333334, ans=0.8655833333333334
2024-10-07 21:14:02,959 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.10 vs. limit=5.495
2024-10-07 21:14:10,065 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=993.3333333333334, ans=0.4534375
2024-10-07 21:14:16,478 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=993.3333333333334, ans=0.4534375
2024-10-07 21:14:21,079 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.31 vs. limit=3.149
2024-10-07 21:14:27,182 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.61 vs. limit=4.398666666666666
2024-10-07 21:14:32,582 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=6.02 vs. limit=4.398666666666666
2024-10-07 21:14:34,957 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.00 vs. limit=3.1495
2024-10-07 21:14:37,301 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.188e+02 1.411e+02 1.512e+02 1.749e+02 3.323e+02, threshold=3.023e+02, percent-clipped=1.0
2024-10-07 21:14:37,343 INFO [train.py:1153] Epoch 1, batch 3000, loss[loss=0.6283, simple_loss=0.473, pruned_loss=0.2747, ctc_loss=0.5856, over 4849.00 frames. ], tot_loss[loss=0.6408, simple_loss=0.4648, pruned_loss=0.2852, ctc_loss=0.6165, over 967211.69 frames. ], batch size: 21, lr: 4.34e-02,
2024-10-07 21:14:37,344 INFO [train.py:1176] Computing validation loss
2024-10-07 21:14:41,191 INFO [zipformer.py:1858] name=encoder.encoders.5.encoder.layers.0.self_attn_weights, attn_weights_entropy = tensor([4.8423, 5.0496, 5.1203, 4.9798], device='cuda:0')
2024-10-07 21:14:44,676 INFO [train.py:1185] Epoch 1, validation: loss=0.4721, simple_loss=0.4143, pruned_loss=0.1815, ctc_loss=0.4174, over 90464.00 frames.
2024-10-07 21:14:44,676 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-07 21:14:46,232 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=1000.0, ans=0.19375
2024-10-07 21:14:53,678 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=1000.0, ans=0.1625
2024-10-07 21:14:56,265 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=1003.3333333333334, ans=5.627083333333333
2024-10-07 21:14:59,297 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.82 vs. limit=5.2508333333333335
2024-10-07 21:15:00,894 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.13 vs. limit=5.501666666666667
2024-10-07 21:15:04,503 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.67 vs. limit=8.2525
2024-10-07 21:15:07,672 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=1003.3333333333334, ans=0.45296875000000003
2024-10-07 21:15:20,866 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=1006.6666666666666, ans=0.8647666666666667
2024-10-07 21:15:21,425 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.22 vs. limit=7.8775
2024-10-07 21:15:25,097 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=13.01 vs. limit=5.505
2024-10-07 21:15:49,398 INFO [train.py:1153] Epoch 1, batch 3050, loss[loss=0.5604, simple_loss=0.4311, pruned_loss=0.2365, ctc_loss=0.5416, over 4750.00 frames. ], tot_loss[loss=0.6375, simple_loss=0.464, pruned_loss=0.2831, ctc_loss=0.6122, over 966736.06 frames. ], batch size: 19, lr: 4.33e-02,
2024-10-07 21:15:51,380 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=22.84 vs. limit=8.2625
2024-10-07 21:15:59,153 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.00 vs. limit=5.254166666666666
2024-10-07 21:16:02,917 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=13.21 vs. limit=5.51
2024-10-07 21:16:05,285 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=1020.0, ans=0.4521875
2024-10-07 21:16:16,325 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.49 vs. limit=7.88375
2024-10-07 21:16:21,832 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.71 vs. limit=7.88375
2024-10-07 21:16:22,587 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=1023.3333333333334, ans=0.45203125
2024-10-07 21:16:23,031 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.32 vs. limit=7.88375
2024-10-07 21:16:25,122 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=1023.3333333333334, ans=5.639583333333333
2024-10-07 21:16:26,487 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=1023.3333333333334, ans=0.16162500000000002
2024-10-07 21:16:29,138 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.min_positive, batch_count=1026.6666666666667, ans=0.09358333333333334
2024-10-07 21:16:39,907 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=1026.6666666666667, ans=0.8640666666666666
2024-10-07 21:16:42,459 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=1030.0, ans=0.45171875
2024-10-07 21:16:53,587 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.21 vs. limit=3.1545
2024-10-07 21:16:55,019 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.10 vs. limit=5.258333333333333
2024-10-07 21:16:55,686 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.173e+02 1.386e+02 1.559e+02 1.743e+02 2.284e+02, threshold=3.117e+02, percent-clipped=0.0
2024-10-07 21:16:55,732 INFO [train.py:1153] Epoch 1, batch 3100, loss[loss=0.6176, simple_loss=0.4536, pruned_loss=0.2754, ctc_loss=0.577, over 4807.00 frames. ], tot_loss[loss=0.6299, simple_loss=0.4611, pruned_loss=0.2786, ctc_loss=0.6044, over 966426.09 frames. ], batch size: 38, lr: 4.33e-02,
2024-10-07 21:17:04,151 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=30.87 vs. limit=8.275
2024-10-07 21:17:04,403 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=20.20 vs. limit=8.275
2024-10-07 21:17:05,818 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=31.41 vs. limit=8.275
2024-10-07 21:17:11,181 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.24 vs. limit=5.5183333333333335
2024-10-07 21:17:12,447 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.30 vs. limit=8.2775
2024-10-07 21:17:13,692 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.30 vs. limit=8.2775
2024-10-07 21:17:20,479 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=13.64 vs. limit=8.2775
2024-10-07 21:17:25,552 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.18 vs. limit=7.89
2024-10-07 21:17:28,006 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.23 vs. limit=5.52
2024-10-07 21:17:33,462 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.78 vs. limit=5.52
2024-10-07 21:17:35,303 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=1043.3333333333333, ans=0.45109375
2024-10-07 21:17:50,529 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn1.whiten.whitening_limit, batch_count=1046.6666666666667, ans=8.285
2024-10-07 21:17:55,136 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=1046.6666666666667, ans=0.5
2024-10-07 21:17:55,490 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.63 vs. limit=5.523333333333333
2024-10-07 21:18:01,402 INFO [train.py:1153] Epoch 1, batch 3150, loss[loss=0.6448, simple_loss=0.4699, pruned_loss=0.287, ctc_loss=0.6145, over 4800.00 frames. ], tot_loss[loss=0.6249, simple_loss=0.4582, pruned_loss=0.2761, ctc_loss=0.5986, over 966760.36 frames. ], batch size: 40, lr: 4.32e-02,
2024-10-07 21:18:06,734 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=1050.0, ans=0.36875
2024-10-07 21:18:09,435 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=1050.0, ans=0.36875
2024-10-07 21:18:12,053 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=1050.0, ans=0.8632500000000001
2024-10-07 21:18:15,198 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.92 vs. limit=4.421333333333333
2024-10-07 21:18:24,418 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.30 vs. limit=3.158
2024-10-07 21:18:38,458 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-07 21:18:46,103 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=1060.0, ans=0.4503125
2024-10-07 21:18:47,878 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.11 vs. limit=7.8975
2024-10-07 21:18:52,028 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=26.67 vs. limit=8.295
2024-10-07 21:18:57,191 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.61 vs. limit=8.2975
2024-10-07 21:19:03,060 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=1063.3333333333333, ans=0.1901875
2024-10-07 21:19:06,767 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.94 vs. limit=8.3
2024-10-07 21:19:07,096 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.145e+02 1.421e+02 1.534e+02 1.684e+02 3.087e+02, threshold=3.069e+02, percent-clipped=0.0
2024-10-07 21:19:07,138 INFO [train.py:1153] Epoch 1, batch 3200, loss[loss=0.642, simple_loss=0.4707, pruned_loss=0.2875, ctc_loss=0.5958, over 4743.00 frames. ], tot_loss[loss=0.62, simple_loss=0.4563, pruned_loss=0.2731, ctc_loss=0.5937, over 967211.05 frames. ], batch size: 20, lr: 4.32e-02,
2024-10-07 21:19:11,069 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=1066.6666666666667, ans=0.5
2024-10-07 21:19:16,158 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=1066.6666666666667, ans=0.04666666666666667
2024-10-07 21:19:18,284 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.61 vs. limit=5.266666666666667
2024-10-07 21:19:21,901 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=9.42 vs. limit=7.90125
2024-10-07 21:19:25,303 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=1070.0, ans=0.86255
2024-10-07 21:19:34,446 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=1073.3333333333333, ans=0.4496875
2024-10-07 21:19:44,273 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.27 vs. limit=7.9025
2024-10-07 21:20:00,760 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1080.0, ans=0.2892
2024-10-07 21:20:03,331 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=1080.0, ans=0.365
2024-10-07 21:20:09,080 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.71 vs. limit=7.905
2024-10-07 21:20:09,792 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=1080.0, ans=0.5
2024-10-07 21:20:12,250 INFO [train.py:1153] Epoch 1, batch 3250, loss[loss=0.6245, simple_loss=0.4535, pruned_loss=0.2811, ctc_loss=0.5831, over 4855.00 frames. ], tot_loss[loss=0.6158, simple_loss=0.4549, pruned_loss=0.2704, ctc_loss=0.5896, over 967283.25 frames. ], batch size: 24, lr: 4.31e-02,
2024-10-07 21:20:17,825 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=1083.3333333333333, ans=0.075625
2024-10-07 21:20:24,915 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.63 vs. limit=5.543333333333333
2024-10-07 21:20:31,275 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.63 vs. limit=7.9075
2024-10-07 21:20:40,547 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten.whitening_limit, batch_count=1090.0, ans=7.90875
2024-10-07 21:20:42,953 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=1090.0, ans=0.15912500000000002
2024-10-07 21:21:01,218 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1093.3333333333333, ans=0.28906666666666664
2024-10-07 21:21:07,153 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.24 vs. limit=5.274166666666667
2024-10-07 21:21:11,021 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.76 vs. limit=5.274166666666667
2024-10-07 21:21:18,442 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=1100.0, ans=0.5
2024-10-07 21:21:19,153 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.065e+02 1.377e+02 1.490e+02 1.754e+02 3.271e+02, threshold=2.979e+02, percent-clipped=1.0
2024-10-07 21:21:19,190 INFO [train.py:1153] Epoch 1, batch 3300, loss[loss=0.6528, simple_loss=0.4806, pruned_loss=0.2943, ctc_loss=0.5911, over 4840.00 frames. ], tot_loss[loss=0.61, simple_loss=0.4526, pruned_loss=0.267, ctc_loss=0.5833, over 967719.24 frames. ], batch size: 43, lr: 4.31e-02,
2024-10-07 21:21:19,321 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=1100.0, ans=0.4484375
2024-10-07 21:21:19,324 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=1100.0, ans=0.4484375
2024-10-07 21:21:24,487 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.34 vs. limit=8.325
2024-10-07 21:21:27,144 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=17.64 vs. limit=8.325
2024-10-07 21:21:29,603 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.58 vs. limit=4.44
2024-10-07 21:21:30,913 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.13 vs. limit=7.91375
2024-10-07 21:21:39,188 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=1103.3333333333333, ans=0.1879375
2024-10-07 21:21:50,942 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=16.77 vs. limit=8.33
2024-10-07 21:22:03,760 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1110.0, ans=0.2889
2024-10-07 21:22:04,182 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.81 vs. limit=5.555
2024-10-07 21:22:06,678 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.29 vs. limit=3.1665
2024-10-07 21:22:07,015 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.13 vs. limit=8.3325
2024-10-07 21:22:12,791 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=1113.3333333333333, ans=5.695833333333333
2024-10-07 21:22:23,181 INFO [train.py:1153] Epoch 1, batch 3350, loss[loss=0.6615, simple_loss=0.4788, pruned_loss=0.2921, ctc_loss=0.6497, over 4802.00 frames. ], tot_loss[loss=0.6115, simple_loss=0.4535, pruned_loss=0.2679, ctc_loss=0.5843, over 966981.25 frames. ], batch size: 40, lr: 4.30e-02,
2024-10-07 21:22:23,320 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=1116.6666666666667, ans=0.44765625
2024-10-07 21:22:25,871 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=1116.6666666666667, ans=0.36041666666666666
2024-10-07 21:22:25,948 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=1116.6666666666667, ans=0.074875
2024-10-07 21:22:31,336 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=11.58 vs. limit=5.558333333333334
2024-10-07 21:22:40,664 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.34 vs. limit=3.168
2024-10-07 21:22:45,654 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1120.0, ans=0.2888
2024-10-07 21:22:46,566 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.83 vs. limit=8.34
2024-10-07 21:22:50,792 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=1123.3333333333333, ans=0.15787500000000002
2024-10-07 21:23:05,368 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=1126.6666666666667, ans=0.07465
2024-10-07 21:23:17,236 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1130.0, ans=0.2887
2024-10-07 21:23:17,349 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=1130.0, ans=0.7613
2024-10-07 21:23:29,376 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.207e+02 1.377e+02 1.476e+02 1.608e+02 2.713e+02, threshold=2.953e+02, percent-clipped=0.0
2024-10-07 21:23:29,418 INFO [train.py:1153] Epoch 1, batch 3400, loss[loss=0.5247, simple_loss=0.4143, pruned_loss=0.2159, ctc_loss=0.508, over 4959.00 frames. ], tot_loss[loss=0.6088, simple_loss=0.4523, pruned_loss=0.2663, ctc_loss=0.5817, over 966766.51 frames. ], batch size: 19, lr: 4.29e-02,
2024-10-07 21:23:33,977 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.29 vs. limit=7.925
2024-10-07 21:23:48,379 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=10.18 vs. limit=7.92625
2024-10-07 21:23:56,504 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.57 vs. limit=8.355
2024-10-07 21:24:19,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=1143.3333333333333, ans=0.15712500000000001
2024-10-07 21:24:26,053 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=1146.6666666666667, ans=0.2172
2024-10-07 21:24:31,341 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1146.6666666666667, ans=0.44625
2024-10-07 21:24:31,370 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=1146.6666666666667, ans=0.8598666666666667
2024-10-07 21:24:35,164 INFO [train.py:1153] Epoch 1, batch 3450, loss[loss=0.7178, simple_loss=0.5217, pruned_loss=0.3257, ctc_loss=0.6563, over 4854.00 frames. ], tot_loss[loss=0.6069, simple_loss=0.4516, pruned_loss=0.2652, ctc_loss=0.5794, over 967096.41 frames. ], batch size: 43, lr: 4.29e-02,
2024-10-07 21:24:43,898 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.96 vs. limit=7.93125
2024-10-07 21:24:45,948 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=1150.0, ans=0.21725
2024-10-07 21:24:46,777 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.54 vs. limit=8.3625
2024-10-07 21:24:54,586 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.56 vs. limit=3.173
2024-10-07 21:25:02,944 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=1156.6666666666667, ans=0.073975
2024-10-07 21:25:05,540 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1156.6666666666667, ans=0.2884333333333333
2024-10-07 21:25:07,371 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.06 vs. limit=7.93375
2024-10-07 21:25:08,265 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=1156.6666666666667, ans=0.5
2024-10-07 21:25:18,681 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=1160.0, ans=0.1565
2024-10-07 21:25:25,227 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=1160.0, ans=0.445625
2024-10-07 21:25:30,844 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=22.94 vs. limit=8.3725
2024-10-07 21:25:33,431 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.35 vs. limit=7.93625
2024-10-07 21:25:34,465 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.min_positive, batch_count=1163.3333333333333, ans=0.09272916666666667
2024-10-07 21:25:35,125 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.99 vs. limit=4.465333333333334
2024-10-07 21:25:40,154 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.27 vs. limit=8.375
2024-10-07 21:25:40,842 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.149e+02 1.390e+02 1.555e+02 1.748e+02 3.338e+02, threshold=3.109e+02, percent-clipped=2.0
2024-10-07 21:25:40,889 INFO [train.py:1153] Epoch 1, batch 3500, loss[loss=0.5517, simple_loss=0.4264, pruned_loss=0.2356, ctc_loss=0.5144, over 4883.00 frames. ], tot_loss[loss=0.6029, simple_loss=0.4498, pruned_loss=0.2631, ctc_loss=0.5749, over 967403.75 frames. ], batch size: 19, lr: 4.28e-02,
2024-10-07 21:25:47,532 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=1166.6666666666667, ans=0.4453125
2024-10-07 21:25:54,089 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=1170.0, ans=0.44515625000000003
2024-10-07 21:25:56,261 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.15 vs. limit=3.1755
2024-10-07 21:26:12,369 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=1173.3333333333333, ans=0.09266666666666667
2024-10-07 21:26:12,830 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.17 vs. limit=5.586666666666667
2024-10-07 21:26:24,286 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-07 21:26:29,429 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1176.6666666666667, ans=0.44484375
2024-10-07 21:26:46,345 INFO [train.py:1153] Epoch 1, batch 3550, loss[loss=0.5888, simple_loss=0.4387, pruned_loss=0.2613, ctc_loss=0.5406, over 4792.00 frames. ], tot_loss[loss=0.5983, simple_loss=0.4481, pruned_loss=0.2602, ctc_loss=0.5703, over 967338.54 frames. ], batch size: 29, lr: 4.28e-02,
2024-10-07 21:26:59,736 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1186.6666666666667, ans=0.2881333333333333
2024-10-07 21:27:09,612 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.08 vs. limit=5.593333333333334
2024-10-07 21:27:12,276 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.94 vs. limit=8.3925
2024-10-07 21:27:12,843 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=1190.0, ans=0.073225
2024-10-07 21:27:18,084 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=1190.0, ans=0.073225
2024-10-07 21:27:18,105 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=1190.0, ans=0.44421875
2024-10-07 21:27:29,986 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=1193.3333333333333, ans=0.4440625
2024-10-07 21:27:32,503 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=1193.3333333333333, ans=0.4440625
2024-10-07 21:27:33,820 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=1193.3333333333333, ans=0.4440625
2024-10-07 21:27:36,781 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.93 vs. limit=5.298333333333333
2024-10-07 21:27:38,272 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.04 vs. limit=7.94875
2024-10-07 21:27:47,608 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=35.61 vs. limit=8.3975
2024-10-07 21:27:49,844 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1196.6666666666667, ans=0.2880333333333333
2024-10-07 21:27:52,672 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.070e+02 1.421e+02 1.562e+02 1.734e+02 3.171e+02, threshold=3.124e+02, percent-clipped=1.0
2024-10-07 21:27:52,715 INFO [train.py:1153] Epoch 1, batch 3600, loss[loss=0.5469, simple_loss=0.4177, pruned_loss=0.231, ctc_loss=0.5356, over 4946.00 frames. ], tot_loss[loss=0.5984, simple_loss=0.4489, pruned_loss=0.2601, ctc_loss=0.5695, over 967405.82 frames. ], batch size: 20, lr: 4.27e-02,
2024-10-07 21:27:54,015 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=1200.0, ans=0.858
2024-10-07 21:27:58,025 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=1200.0, ans=0.44375
2024-10-07 21:28:01,250 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.74 vs. limit=5.3
2024-10-07 21:28:07,427 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=1203.3333333333333, ans=0.154875
2024-10-07 21:28:16,434 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=1203.3333333333333, ans=0.072925
2024-10-07 21:28:21,913 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=1206.6666666666667, ans=0.4434375
2024-10-07 21:28:38,793 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=1210.0, ans=0.44328125
2024-10-07 21:28:41,735 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.85 vs. limit=8.4075
2024-10-07 21:28:51,933 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=1213.3333333333333, ans=0.443125
2024-10-07 21:28:54,976 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.80 vs. limit=8.41
2024-10-07 21:28:54,992 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.91 vs. limit=5.6066666666666665
2024-10-07 21:28:58,230 INFO [train.py:1153] Epoch 1, batch 3650, loss[loss=0.4837, simple_loss=0.3819, pruned_loss=0.1991, ctc_loss=0.4685, over 4842.00 frames. ], tot_loss[loss=0.5963, simple_loss=0.4478, pruned_loss=0.2589, ctc_loss=0.5672, over 967903.82 frames. ], batch size: 31, lr: 4.27e-02,
2024-10-07 21:29:08,883 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=1216.6666666666667, ans=0.5
2024-10-07 21:29:17,109 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.88 vs. limit=7.9575
2024-10-07 21:29:19,361 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=1220.0, ans=0.4428125
2024-10-07 21:29:21,060 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.15 vs. limit=7.9575
2024-10-07 21:29:36,326 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=1226.6666666666667, ans=0.4425
2024-10-07 21:29:51,248 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=17.89 vs. limit=8.4225
2024-10-07 21:29:54,698 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=1230.0, ans=0.44234375
2024-10-07 21:30:03,678 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.199e+02 1.441e+02 1.538e+02 1.721e+02 3.781e+02, threshold=3.075e+02, percent-clipped=1.0
2024-10-07 21:30:03,724 INFO [train.py:1153] Epoch 1, batch 3700, loss[loss=0.6828, simple_loss=0.5046, pruned_loss=0.3047, ctc_loss=0.6291, over 4827.00 frames. ], tot_loss[loss=0.5921, simple_loss=0.4461, pruned_loss=0.2563, ctc_loss=0.5636, over 967394.99 frames. ], batch size: 24, lr: 4.26e-02,
2024-10-07 21:30:05,837 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.46 vs. limit=7.9625
2024-10-07 21:30:21,629 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.44 vs. limit=8.4275
2024-10-07 21:30:30,342 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=1240.0, ans=0.8566
2024-10-07 21:30:30,343 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=1240.0, ans=0.1535
2024-10-07 21:30:32,995 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=1240.0, ans=0.441875
2024-10-07 21:30:35,536 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=1240.0, ans=0.441875
2024-10-07 21:30:39,974 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.91 vs. limit=4.496
2024-10-07 21:30:41,176 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.88 vs. limit=7.965
2024-10-07 21:30:55,063 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=1246.6666666666667, ans=0.44156249999999997
2024-10-07 21:30:58,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=1246.6666666666667, ans=0.44156249999999997
2024-10-07 21:31:00,291 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=1246.6666666666667, ans=0.5
2024-10-07 21:31:09,130 INFO [train.py:1153] Epoch 1, batch 3750, loss[loss=0.5432, simple_loss=0.4165, pruned_loss=0.2308, ctc_loss=0.5204, over 4959.00 frames. ], tot_loss[loss=0.586, simple_loss=0.4429, pruned_loss=0.253, ctc_loss=0.5575, over 967724.70 frames. ], batch size: 19, lr: 4.26e-02,
2024-10-07 21:31:15,276 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.04 vs. limit=8.4375
2024-10-07 21:31:15,981 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.min_positive, batch_count=1250.0, ans=0.04609375
2024-10-07 21:31:17,377 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=1250.0, ans=0.44140625
2024-10-07 21:31:17,412 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=1250.0, ans=0.44140625
2024-10-07 21:31:33,041 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1253.3333333333333, ans=0.28746666666666665
2024-10-07 21:31:45,211 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.66 vs. limit=3.1885
2024-10-07 21:31:46,464 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.67 vs. limit=7.97125
2024-10-07 21:31:51,290 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.92 vs. limit=5.63
2024-10-07 21:32:00,033 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=1263.3333333333333, ans=0.44078125
2024-10-07 21:32:00,504 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=24.60 vs. limit=8.4475
2024-10-07 21:32:14,395 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.150e+02 1.431e+02 1.581e+02 1.729e+02 2.911e+02, threshold=3.161e+02, percent-clipped=0.0
2024-10-07 21:32:14,436 INFO [train.py:1153] Epoch 1, batch 3800, loss[loss=0.6067, simple_loss=0.4534, pruned_loss=0.264, ctc_loss=0.5802, over 4760.00 frames. ], tot_loss[loss=0.583, simple_loss=0.4418, pruned_loss=0.2512, ctc_loss=0.5547, over 967570.03 frames. ], batch size: 26, lr: 4.25e-02,
2024-10-07 21:32:26,816 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.19 vs. limit=3.1905
2024-10-07 21:32:28,238 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.86 vs. limit=4.508
2024-10-07 21:32:37,315 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.27 vs. limit=8.4525
2024-10-07 21:32:38,212 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=1270.0, ans=0.152375
2024-10-07 21:32:42,799 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.44 vs. limit=5.636666666666667
2024-10-07 21:32:43,953 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.19 vs. limit=7.9775
2024-10-07 21:32:50,052 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=1273.3333333333333, ans=0.15225
2024-10-07 21:32:52,544 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=1276.6666666666667, ans=0.152125
2024-10-07 21:32:57,303 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.08 vs. limit=7.97875
2024-10-07 21:33:09,776 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=2.781e-03
2024-10-07 21:33:14,878 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=1280.0, ans=0.178
2024-10-07 21:33:15,446 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.73 vs. limit=7.98
2024-10-07 21:33:20,017 INFO [train.py:1153] Epoch 1, batch 3850, loss[loss=0.6221, simple_loss=0.474, pruned_loss=0.2698, ctc_loss=0.5762, over 4830.00 frames. ], tot_loss[loss=0.58, simple_loss=0.4408, pruned_loss=0.2492, ctc_loss=0.5516, over 967529.49 frames. ], batch size: 38, lr: 4.24e-02,
2024-10-07 21:33:28,022 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=1283.3333333333333, ans=0.43984375
2024-10-07 21:33:35,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=1286.6666666666667, ans=0.8549666666666667
2024-10-07 21:33:36,371 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.78 vs. limit=5.6433333333333335
2024-10-07 21:33:43,887 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=1286.6666666666667, ans=0.21930000000000002
2024-10-07 21:33:45,260 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-07 21:33:45,633 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2.whitening_limit, batch_count=1290.0, ans=5.645
2024-10-07 21:33:57,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.65 vs. limit=5.3225
2024-10-07 21:34:03,786 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=1293.3333333333333, ans=0.8547333333333333
2024-10-07 21:34:06,403 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=1293.3333333333333, ans=0.439375
2024-10-07 21:34:09,606 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.50 vs. limit=7.985
2024-10-07 21:34:10,899 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.34 vs. limit=7.985
2024-10-07 21:34:20,891 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=1296.6666666666667, ans=0.8546166666666667
2024-10-07 21:34:25,992 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.138e+02 1.444e+02 1.592e+02 1.770e+02 3.292e+02, threshold=3.185e+02, percent-clipped=1.0
2024-10-07 21:34:26,037 INFO [train.py:1153] Epoch 1, batch 3900, loss[loss=0.5093, simple_loss=0.406, pruned_loss=0.2083, ctc_loss=0.4901, over 4743.00 frames. ], tot_loss[loss=0.5802, simple_loss=0.4412, pruned_loss=0.2495, ctc_loss=0.5504, over 967015.45 frames. ], batch size: 26, lr: 4.24e-02,
2024-10-07 21:34:31,350 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=1300.0, ans=0.8545
2024-10-07 21:34:39,548 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.80 vs. limit=5.651666666666666
2024-10-07 21:34:45,683 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=1303.3333333333333, ans=0.33708333333333335
2024-10-07 21:34:55,497 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.59 vs. limit=8.48
2024-10-07 21:35:02,561 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=1306.6666666666667, ans=0.1765
2024-10-07 21:35:23,668 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=1313.3333333333333, ans=0.15075
2024-10-07 21:35:31,164 INFO [train.py:1153] Epoch 1, batch 3950, loss[loss=0.5523, simple_loss=0.4183, pruned_loss=0.2358, ctc_loss=0.5361, over 4839.00 frames. ], tot_loss[loss=0.5752, simple_loss=0.4389, pruned_loss=0.2467, ctc_loss=0.545, over 967247.74 frames. ], batch size: 36, lr: 4.23e-02,
2024-10-07 21:35:44,216 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=1320.0, ans=0.1505
2024-10-07 21:35:48,153 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.attention_skip_rate, batch_count=1320.0, ans=0.1505
2024-10-07 21:35:51,105 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.73 vs. limit=5.66
2024-10-07 21:35:53,318 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1320.0, ans=0.2868
2024-10-07 21:35:53,839 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=42.57 vs. limit=8.49
2024-10-07 21:36:05,142 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=1323.3333333333333, ans=0.43796875
2024-10-07 21:36:05,615 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.25 vs. limit=5.3308333333333335
2024-10-07 21:36:10,744 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.39 vs. limit=4.530666666666667
2024-10-07 21:36:30,197 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.96 vs. limit=5.3325
2024-10-07 21:36:34,101 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.78 vs. limit=7.99875
2024-10-07 21:36:35,151 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/checkpoint-4000.pt
2024-10-07 21:36:37,271 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.204e+02 1.418e+02 1.542e+02 1.675e+02 2.075e+02, threshold=3.084e+02, percent-clipped=0.0
2024-10-07 21:36:37,345 INFO [train.py:1153] Epoch 1, batch 4000, loss[loss=0.4918, simple_loss=0.3914, pruned_loss=0.2037, ctc_loss=0.4622, over 4815.00 frames. ], tot_loss[loss=0.5742, simple_loss=0.4386, pruned_loss=0.2463, ctc_loss=0.5432, over 967198.32 frames. ], batch size: 19, lr: 4.23e-02,
2024-10-07 21:36:54,970 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.74 vs. limit=5.3341666666666665
2024-10-07 21:37:03,883 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=34.88 vs. limit=8.504999999999999
2024-10-07 21:37:05,190 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.54 vs. limit=5.67
2024-10-07 21:37:11,864 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.93 vs. limit=5.67
2024-10-07 21:37:12,096 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.08 vs. limit=5.67
2024-10-07 21:37:14,179 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=11.55 vs. limit=5.671666666666667
2024-10-07 21:37:15,956 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.03 vs. limit=8.5075
2024-10-07 21:37:20,905 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.28 vs. limit=8.00375
2024-10-07 21:37:36,498 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.48 vs. limit=5.336666666666667
2024-10-07 21:37:41,012 INFO [train.py:1153] Epoch 1, batch 4050, loss[loss=0.7111, simple_loss=0.5189, pruned_loss=0.3142, ctc_loss=0.6871, over 4777.00 frames. ], tot_loss[loss=0.5744, simple_loss=0.4394, pruned_loss=0.2462, ctc_loss=0.5424, over 967549.96 frames. ], batch size: 53, lr: 4.22e-02,
2024-10-07 21:37:45,092 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=1350.0, ans=0.149375
2024-10-07 21:37:51,675 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=1350.0, ans=0.149375
2024-10-07 21:37:58,716 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.49 vs. limit=8.515
2024-10-07 21:38:10,070 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=25.81 vs. limit=8.5175
2024-10-07 21:38:20,033 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=1360.0, ans=5.85
2024-10-07 21:38:21,286 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1360.0, ans=0.43625
2024-10-07 21:38:25,957 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.24 vs. limit=8.01
2024-10-07 21:38:28,626 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=5.43 vs. limit=8.01
2024-10-07 21:38:30,434 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=1360.0, ans=0.1735
2024-10-07 21:38:34,322 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=1363.3333333333333, ans=0.32958333333333334
2024-10-07 21:38:35,724 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=1363.3333333333333, ans=0.32958333333333334
2024-10-07 21:38:46,071 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.143e+02 1.405e+02 1.572e+02 1.762e+02 4.094e+02, threshold=3.144e+02, percent-clipped=2.0
2024-10-07 21:38:46,119 INFO [train.py:1153] Epoch 1, batch 4100, loss[loss=0.5027, simple_loss=0.3896, pruned_loss=0.2096, ctc_loss=0.4916, over 4861.00 frames. ], tot_loss[loss=0.5733, simple_loss=0.4397, pruned_loss=0.2454, ctc_loss=0.5406, over 966936.24 frames. ], batch size: 31, lr: 4.22e-02,
2024-10-07 21:38:50,269 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=1366.6666666666667, ans=0.06925
2024-10-07 21:38:52,875 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=1366.6666666666667, ans=0.4359375
2024-10-07 21:38:55,886 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.97 vs. limit=8.525
2024-10-07 21:38:57,667 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.68 vs. limit=5.341666666666667
2024-10-07 21:38:59,461 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=1370.0, ans=0.43578125
2024-10-07 21:39:04,720 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=1370.0, ans=0.43578125
2024-10-07 21:39:07,280 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=1370.0, ans=0.85205
2024-10-07 21:39:14,227 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=12.52 vs. limit=5.6866666666666665
2024-10-07 21:39:40,648 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=1380.0, ans=8.535
2024-10-07 21:39:51,746 INFO [train.py:1153] Epoch 1, batch 4150, loss[loss=0.5767, simple_loss=0.438, pruned_loss=0.2472, ctc_loss=0.5526, over 4749.00 frames. ], tot_loss[loss=0.5702, simple_loss=0.4376, pruned_loss=0.2439, ctc_loss=0.5375, over 967084.83 frames. ], batch size: 20, lr: 4.21e-02,
2024-10-07 21:39:57,744 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.67 vs. limit=8.5375
2024-10-07 21:39:58,912 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.80 vs. limit=5.691666666666666
2024-10-07 21:40:00,316 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.61 vs. limit=8.5375
2024-10-07 21:40:02,361 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1383.3333333333333, ans=0.2861666666666667
2024-10-07 21:40:12,747 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=1386.6666666666667, ans=0.32666666666666666
2024-10-07 21:40:17,923 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=1390.0, ans=0.28609999999999997
2024-10-07 21:40:22,220 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.40 vs. limit=5.695
2024-10-07 21:40:24,478 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=1390.0, ans=0.147875
2024-10-07 21:40:27,131 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=1390.0, ans=0.43484375
2024-10-07 21:40:33,241 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.13 vs. limit=5.696666666666666
2024-10-07 21:40:35,480 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=8.93 vs. limit=8.0225
2024-10-07 21:40:46,050 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=3.01 vs. limit=4.558666666666666
2024-10-07 21:40:56,899 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.188e+02 1.453e+02 1.573e+02 1.790e+02 3.259e+02, threshold=3.146e+02, percent-clipped=2.0
2024-10-07 21:40:56,943 INFO [train.py:1153] Epoch 1, batch 4200, loss[loss=0.6061, simple_loss=0.452, pruned_loss=0.2719, ctc_loss=0.5408, over 4858.00 frames. ], tot_loss[loss=0.5696, simple_loss=0.4377, pruned_loss=0.2436, ctc_loss=0.5354, over 967267.35 frames. ], batch size: 31, lr: 4.20e-02,
2024-10-07 21:41:00,977 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=1400.0, ans=0.221
2024-10-07 21:41:10,654 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.82 vs. limit=8.5525
2024-10-07 21:41:40,264 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.58 vs. limit=8.557500000000001
2024-10-07 21:41:42,741 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.32 vs. limit=3.2115
2024-10-07 21:41:44,174 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=35.97 vs. limit=8.557500000000001
2024-10-07 21:41:47,669 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=1413.3333333333333, ans=0.43374999999999997
2024-10-07 21:41:52,817 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=1413.3333333333333, ans=0.14700000000000002
2024-10-07 21:42:01,801 INFO [train.py:1153] Epoch 1, batch 4250, loss[loss=0.528, simple_loss=0.4234, pruned_loss=0.2206, ctc_loss=0.4786, over 4756.00 frames. ], tot_loss[loss=0.5667, simple_loss=0.4361, pruned_loss=0.2424, ctc_loss=0.5314, over 967210.84 frames. ], batch size: 19, lr: 4.20e-02,
2024-10-07 21:42:14,041 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.40 vs. limit=5.355
2024-10-07 21:42:16,743 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=512, metric=3.66 vs. limit=8.0325
2024-10-07 21:42:19,533 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.27 vs. limit=3.213
2024-10-07 21:42:21,798 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=21.68 vs. limit=8.565
2024-10-07 21:42:23,720 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=4.52 vs. limit=4.284
2024-10-07 21:42:49,098 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=27.89 vs. limit=8.57
2024-10-07 21:42:53,545 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.33 vs. limit=3.2145
2024-10-07 21:42:56,837 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.48 vs. limit=8.03625
2024-10-07 21:43:06,852 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.199e+02 1.418e+02 1.568e+02 1.725e+02 2.441e+02, threshold=3.135e+02, percent-clipped=0.0
2024-10-07 21:43:06,898 INFO [train.py:1153] Epoch 1, batch 4300, loss[loss=0.5018, simple_loss=0.392, pruned_loss=0.2097, ctc_loss=0.4807, over 4849.00 frames. ], tot_loss[loss=0.564, simple_loss=0.435, pruned_loss=0.2405, ctc_loss=0.5296, over 967391.96 frames. ], batch size: 21, lr: 4.19e-02,
2024-10-07 21:43:23,809 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=1436.6666666666667, ans=0.8497166666666667
2024-10-07 21:43:26,746 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=16.22 vs. limit=8.5775
2024-10-07 21:43:28,996 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1436.6666666666667, ans=0.2856333333333333
2024-10-07 21:43:38,539 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.05 vs. limit=5.72
2024-10-07 21:43:44,547 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1443.3333333333333, ans=0.28556666666666664
2024-10-07 21:43:47,676 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=7.45 vs. limit=8.04125
2024-10-07 21:43:50,270 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.46 vs. limit=8.04125
2024-10-07 21:43:54,104 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=26.84 vs. limit=8.5825
2024-10-07 21:44:02,821 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-07 21:44:03,975 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=1446.6666666666667, ans=0.4321875
2024-10-07 21:44:05,597 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.59 vs. limit=8.585
2024-10-07 21:44:11,637 INFO [train.py:1153] Epoch 1, batch 4350, loss[loss=0.5008, simple_loss=0.4063, pruned_loss=0.2009, ctc_loss=0.4837, over 4851.00 frames. ], tot_loss[loss=0.5607, simple_loss=0.4338, pruned_loss=0.2385, ctc_loss=0.5264, over 966230.05 frames. ], batch size: 21, lr: 4.19e-02,
2024-10-07 21:44:29,549 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.43 vs. limit=5.363333333333333
2024-10-07 21:44:35,361 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=1453.3333333333333, ans=0.431875
2024-10-07 21:44:38,091 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=1456.6666666666667, ans=0.43171875
2024-10-07 21:44:49,477 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=1460.0, ans=0.4315625
2024-10-07 21:44:54,832 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=1460.0, ans=0.14525
2024-10-07 21:45:04,278 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.21 vs. limit=8.5975
2024-10-07 21:45:11,488 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=1463.3333333333333, ans=0.43140625
2024-10-07 21:45:16,648 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.123e+02 1.409e+02 1.593e+02 1.799e+02 4.324e+02, threshold=3.185e+02, percent-clipped=2.0
2024-10-07 21:45:16,692 INFO [train.py:1153] Epoch 1, batch 4400, loss[loss=0.5583, simple_loss=0.4347, pruned_loss=0.2383, ctc_loss=0.5133, over 4735.00 frames. ], tot_loss[loss=0.5615, simple_loss=0.4346, pruned_loss=0.2389, ctc_loss=0.5261, over 965771.58 frames. ], batch size: 26, lr: 4.18e-02,
2024-10-07 21:45:21,866 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=1466.6666666666667, ans=0.5
2024-10-07 21:45:31,015 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=1470.0, ans=0.43109375
2024-10-07 21:45:32,974 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.84 vs. limit=8.6025
2024-10-07 21:45:53,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.38 vs. limit=5.736666666666666
2024-10-07 21:46:04,020 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=2.55 vs. limit=8.05375
2024-10-07 21:46:12,395 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=1480.0, ans=0.430625
2024-10-07 21:46:12,904 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.61 vs. limit=5.37
2024-10-07 21:46:21,190 INFO [train.py:1153] Epoch 1, batch 4450, loss[loss=0.4696, simple_loss=0.3827, pruned_loss=0.1866, ctc_loss=0.4584, over 4883.00 frames. ], tot_loss[loss=0.5604, simple_loss=0.4336, pruned_loss=0.2385, ctc_loss=0.5254, over 966090.40 frames. ], batch size: 19, lr: 4.17e-02,
2024-10-07 21:46:29,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1483.3333333333333, ans=0.2851666666666667
2024-10-07 21:46:29,718 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.53 vs. limit=5.370833333333334
2024-10-07 21:46:32,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=1486.6666666666667, ans=0.14425
2024-10-07 21:46:33,632 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten.whitening_limit, batch_count=1486.6666666666667, ans=8.615
2024-10-07 21:46:37,139 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=24.84 vs. limit=8.615
2024-10-07 21:46:38,835 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.05 vs. limit=8.615
2024-10-07 21:47:01,333 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=1493.3333333333333, ans=0.43
2024-10-07 21:47:01,895 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.15 vs. limit=8.06
2024-10-07 21:47:09,012 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1493.3333333333333, ans=0.28506666666666663
2024-10-07 21:47:15,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.97 vs. limit=8.6225
2024-10-07 21:47:19,758 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.69 vs. limit=5.748333333333333
2024-10-07 21:47:25,648 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.174e+02 1.481e+02 1.615e+02 1.764e+02 2.911e+02, threshold=3.230e+02, percent-clipped=0.0
2024-10-07 21:47:25,694 INFO [train.py:1153] Epoch 1, batch 4500, loss[loss=0.5398, simple_loss=0.4169, pruned_loss=0.2304, ctc_loss=0.5049, over 4861.00 frames. ], tot_loss[loss=0.5611, simple_loss=0.4346, pruned_loss=0.2388, ctc_loss=0.5254, over 966208.89 frames. ], batch size: 28, lr: 4.17e-02,
2024-10-07 21:47:27,132 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1500.0, ans=0.285
2024-10-07 21:47:37,709 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.64 vs. limit=5.751666666666667
2024-10-07 21:48:05,725 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=34.74 vs. limit=8.6325
2024-10-07 21:48:08,312 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.25 vs. limit=8.6325
2024-10-07 21:48:17,723 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.01 vs. limit=5.756666666666667
2024-10-07 21:48:20,674 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=1513.3333333333333, ans=0.4290625
2024-10-07 21:48:29,460 INFO [train.py:1153] Epoch 1, batch 4550, loss[loss=0.5127, simple_loss=0.4048, pruned_loss=0.2123, ctc_loss=0.49, over 4846.00 frames. ], tot_loss[loss=0.5595, simple_loss=0.4334, pruned_loss=0.2382, ctc_loss=0.5233, over 966014.55 frames. ], batch size: 20, lr: 4.16e-02,
2024-10-07 21:48:32,775 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.89 vs. limit=8.6375
2024-10-07 21:48:45,124 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=1520.0, ans=0.42875
2024-10-07 21:48:49,466 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.12 vs. limit=4.608
2024-10-07 21:48:56,771 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=1523.3333333333333, ans=0.8466833333333333
2024-10-07 21:48:56,779 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=1523.3333333333333, ans=0.42859375
2024-10-07 21:48:58,500 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=4.88 vs. limit=4.609333333333334
2024-10-07 21:49:05,746 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=1523.3333333333333, ans=0.09047916666666667
2024-10-07 21:49:14,059 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.17 vs. limit=8.645
2024-10-07 21:49:14,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=1526.6666666666667, ans=0.4284375
2024-10-07 21:49:19,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.68 vs. limit=8.645
2024-10-07 21:49:20,813 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.64 vs. limit=5.765
2024-10-07 21:49:24,191 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.95 vs. limit=8.6475
2024-10-07 21:49:25,114 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=1530.0, ans=0.42828125
2024-10-07 21:49:30,858 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.06 vs. limit=8.07375
2024-10-07 21:49:34,172 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.209e+02 1.466e+02 1.637e+02 1.804e+02 2.894e+02, threshold=3.273e+02, percent-clipped=0.0
2024-10-07 21:49:34,216 INFO [train.py:1153] Epoch 1, batch 4600, loss[loss=0.6572, simple_loss=0.4969, pruned_loss=0.2873, ctc_loss=0.6073, over 4767.00 frames. ], tot_loss[loss=0.557, simple_loss=0.4324, pruned_loss=0.2367, ctc_loss=0.5204, over 966350.88 frames. ], batch size: 45, lr: 4.15e-02,
2024-10-07 21:49:36,800 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=1533.3333333333333, ans=0.428125
2024-10-07 21:49:40,231 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.41 vs. limit=5.383333333333333
2024-10-07 21:49:45,875 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=1536.6666666666667, ans=0.22305000000000003
2024-10-07 21:49:47,160 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=1536.6666666666667, ans=0.142375
2024-10-07 21:50:11,775 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1543.3333333333333, ans=0.42765625
2024-10-07 21:50:15,057 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.17 vs. limit=3.2315
2024-10-07 21:50:24,666 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer_na.min_abs, batch_count=1546.6666666666667, ans=0.010186666666666667
2024-10-07 21:50:30,107 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=32.04 vs. limit=8.66
2024-10-07 21:50:31,906 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=5.75 vs. limit=5.773333333333333
2024-10-07 21:50:33,113 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.64 vs. limit=5.773333333333333
2024-10-07 21:50:38,678 INFO [train.py:1153] Epoch 1, batch 4650, loss[loss=0.5833, simple_loss=0.4608, pruned_loss=0.2464, ctc_loss=0.5324, over 4824.00 frames. ], tot_loss[loss=0.5565, simple_loss=0.4329, pruned_loss=0.2363, ctc_loss=0.5189, over 965794.19 frames. ], batch size: 36, lr: 4.15e-02,
2024-10-07 21:50:42,770 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-07 21:50:45,998 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.43 vs. limit=8.6625
2024-10-07 21:50:47,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1550.0, ans=0.2845
2024-10-07 21:50:52,259 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.17 vs. limit=8.0825
2024-10-07 21:51:16,407 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1560.0, ans=0.2844
2024-10-07 21:51:16,852 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=35.08 vs. limit=8.67
2024-10-07 21:51:33,551 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=16.69 vs. limit=8.08625
2024-10-07 21:51:43,349 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.254e+02 1.450e+02 1.625e+02 1.850e+02 3.978e+02, threshold=3.250e+02, percent-clipped=2.0
2024-10-07 21:51:43,396 INFO [train.py:1153] Epoch 1, batch 4700, loss[loss=0.524, simple_loss=0.4178, pruned_loss=0.2197, ctc_loss=0.4769, over 4940.00 frames. ], tot_loss[loss=0.5564, simple_loss=0.4332, pruned_loss=0.2364, ctc_loss=0.5169, over 965687.48 frames. ], batch size: 19, lr: 4.14e-02,
2024-10-07 21:51:53,918 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=1566.6666666666667, ans=0.04510416666666667
2024-10-07 21:52:21,994 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-07 21:52:23,300 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=1576.6666666666667, ans=0.035
2024-10-07 21:52:25,045 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.32 vs. limit=3.2365
2024-10-07 21:52:26,396 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.17 vs. limit=4.6306666666666665
2024-10-07 21:52:33,220 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2.whitening_limit, batch_count=1576.6666666666667, ans=5.788333333333333
2024-10-07 21:52:35,931 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.39 vs. limit=3.237
2024-10-07 21:52:41,395 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=1580.0, ans=0.2842
2024-10-07 21:52:47,711 INFO [train.py:1153] Epoch 1, batch 4750, loss[loss=0.6008, simple_loss=0.4545, pruned_loss=0.2599, ctc_loss=0.5687, over 4769.00 frames. ], tot_loss[loss=0.5566, simple_loss=0.4332, pruned_loss=0.2367, ctc_loss=0.5169, over 965640.65 frames. ], batch size: 45, lr: 4.14e-02,
2024-10-07 21:52:49,137 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=1583.3333333333333, ans=0.8445833333333334
2024-10-07 21:52:49,692 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.06 vs. limit=4.633333333333333
2024-10-07 21:52:56,273 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.43 vs. limit=8.6875
2024-10-07 21:52:56,536 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.97 vs. limit=5.791666666666667
2024-10-07 21:52:59,544 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=1586.6666666666667, ans=0.42562500000000003
2024-10-07 21:53:10,143 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.64 vs. limit=8.69
2024-10-07 21:53:11,041 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-07 21:53:14,933 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.29 vs. limit=8.692499999999999
2024-10-07 21:53:22,305 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.min_positive, batch_count=1590.0, ans=0.04503125
2024-10-07 21:53:24,511 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=14.31 vs. limit=8.692499999999999
2024-10-07 21:53:34,174 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=4.92 vs. limit=4.637333333333333
2024-10-07 21:53:38,824 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=1596.6666666666667, ans=0.8441166666666666
2024-10-07 21:53:51,515 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.167e+02 1.454e+02 1.614e+02 1.780e+02 5.373e+02, threshold=3.227e+02, percent-clipped=2.0
2024-10-07 21:53:51,559 INFO [train.py:1153] Epoch 1, batch 4800, loss[loss=0.4887, simple_loss=0.3965, pruned_loss=0.1962, ctc_loss=0.4714, over 4873.00 frames. ], tot_loss[loss=0.5518, simple_loss=0.4306, pruned_loss=0.234, ctc_loss=0.5124, over 965918.29 frames. ], batch size: 22, lr: 4.13e-02,
2024-10-07 21:53:59,486 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.25 vs. limit=8.7
2024-10-07 21:54:00,443 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.max_abs, batch_count=1600.0, ans=6.0
2024-10-07 21:54:07,313 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.57 vs. limit=5.400833333333333
2024-10-07 21:54:12,706 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.51 vs. limit=8.10125
2024-10-07 21:54:16,454 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.47 vs. limit=8.1025
2024-10-07 21:54:20,142 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.77 vs. limit=8.705
2024-10-07 21:54:39,913 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=1610.0, ans=8.7075
2024-10-07 21:54:53,820 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=1613.3333333333333, ans=0.08991666666666667
2024-10-07 21:54:56,262 INFO [train.py:1153] Epoch 1, batch 4850, loss[loss=0.5577, simple_loss=0.4247, pruned_loss=0.2391, ctc_loss=0.5311, over 4848.00 frames. ], tot_loss[loss=0.5479, simple_loss=0.4288, pruned_loss=0.2319, ctc_loss=0.5085, over 966613.53 frames. ], batch size: 28, lr: 4.12e-02,
2024-10-07 21:55:00,757 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=7.84 vs. limit=8.10625
2024-10-07 21:55:02,300 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.28 vs. limit=5.808333333333334
2024-10-07 21:55:09,477 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=1620.0, ans=0.4240625
2024-10-07 21:55:13,942 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.05 vs. limit=5.8100000000000005
2024-10-07 21:55:16,822 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.29 vs. limit=5.405
2024-10-07 21:55:17,351 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=1620.0, ans=0.4240625
2024-10-07 21:55:29,329 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.62 vs. limit=8.10875
2024-10-07 21:55:44,458 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=1626.6666666666667, ans=0.42375
2024-10-07 21:55:49,018 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=24.99 vs. limit=8.7225
2024-10-07 21:55:53,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=1630.0, ans=0.2337
2024-10-07 21:55:55,541 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.33 vs. limit=3.2445
2024-10-07 21:56:01,276 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.261e+02 1.446e+02 1.559e+02 1.689e+02 3.524e+02, threshold=3.117e+02, percent-clipped=2.0
2024-10-07 21:56:01,320 INFO [train.py:1153] Epoch 1, batch 4900, loss[loss=0.5017, simple_loss=0.3842, pruned_loss=0.2157, ctc_loss=0.4692, over 4854.00 frames. ], tot_loss[loss=0.5472, simple_loss=0.4283, pruned_loss=0.2316, ctc_loss=0.5072, over 967195.10 frames. ], batch size: 21, lr: 4.12e-02,
2024-10-07 21:56:02,762 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=1633.3333333333333, ans=0.13875
2024-10-07 21:56:07,060 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.87 vs. limit=5.408333333333333
2024-10-07 21:56:15,649 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=1636.6666666666667, ans=0.138625
2024-10-07 21:56:23,528 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=1636.6666666666667, ans=0.42328125
2024-10-07 21:56:27,752 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.03 vs. limit=8.73
2024-10-07 21:56:30,263 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.05 vs. limit=5.82
2024-10-07 21:56:32,046 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.55 vs. limit=5.41
2024-10-07 21:56:41,799 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1643.3333333333333, ans=0.28356666666666663
2024-10-07 21:56:43,597 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.23 vs. limit=3.2465
2024-10-07 21:56:48,365 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1643.3333333333333, ans=0.28356666666666663
2024-10-07 21:56:49,583 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=1643.3333333333333, ans=0.8424833333333334
2024-10-07 21:56:51,698 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.87 vs. limit=5.821666666666666
2024-10-07 21:57:06,159 INFO [train.py:1153] Epoch 1, batch 4950, loss[loss=0.5695, simple_loss=0.4289, pruned_loss=0.244, ctc_loss=0.5553, over 4775.00 frames. ], tot_loss[loss=0.5504, simple_loss=0.43, pruned_loss=0.2336, ctc_loss=0.5091, over 966685.70 frames. ], batch size: 53, lr: 4.11e-02,
2024-10-07 21:57:17,148 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.71 vs. limit=8.7375
2024-10-07 21:57:28,680 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1653.3333333333333, ans=0.28346666666666664
2024-10-07 21:57:33,832 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=1656.6666666666667, ans=0.137875
2024-10-07 21:57:55,183 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.20 vs. limit=8.1225
2024-10-07 21:57:57,558 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.84 vs. limit=5.831666666666667
2024-10-07 21:57:58,939 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.27 vs. limit=5.831666666666667
2024-10-07 21:57:59,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=1663.3333333333333, ans=0.42203124999999997
2024-10-07 21:58:00,573 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.73 vs. limit=5.831666666666667
2024-10-07 21:58:00,691 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.86 vs. limit=5.831666666666667
2024-10-07 21:58:11,515 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.179e+02 1.482e+02 1.591e+02 1.831e+02 3.911e+02, threshold=3.183e+02, percent-clipped=2.0
2024-10-07 21:58:11,559 INFO [train.py:1153] Epoch 1, batch 5000, loss[loss=0.5295, simple_loss=0.4289, pruned_loss=0.2231, ctc_loss=0.4595, over 4786.00 frames. ], tot_loss[loss=0.5447, simple_loss=0.427, pruned_loss=0.2304, ctc_loss=0.5039, over 967691.25 frames. ], batch size: 29, lr: 4.10e-02,
2024-10-07 21:58:22,205 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=1666.6666666666667, ans=0.1375
2024-10-07 21:58:26,690 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.23 vs. limit=8.7525
2024-10-07 21:58:29,106 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.13 vs. limit=8.7525
2024-10-07 21:58:31,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.44 vs. limit=8.7525
2024-10-07 21:58:34,195 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.85 vs. limit=8.7525
2024-10-07 21:58:38,200 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.41 vs. limit=8.1275
2024-10-07 21:58:42,097 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.89 vs. limit=8.1275
2024-10-07 21:58:56,867 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.99 vs. limit=8.7575
2024-10-07 21:58:57,541 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=1676.6666666666667, ans=0.04476041666666667
2024-10-07 21:59:01,394 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=1676.6666666666667, ans=0.137125
2024-10-07 21:59:02,689 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=1680.0, ans=0.42125
2024-10-07 21:59:15,826 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=1683.3333333333333, ans=0.42109375
2024-10-07 21:59:17,065 INFO [train.py:1153] Epoch 1, batch 5050, loss[loss=0.4748, simple_loss=0.3915, pruned_loss=0.1923, ctc_loss=0.434, over 4854.00 frames. ], tot_loss[loss=0.5414, simple_loss=0.4252, pruned_loss=0.2287, ctc_loss=0.5006, over 968668.79 frames. ], batch size: 19, lr: 4.10e-02,
2024-10-07 21:59:24,925 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=1683.3333333333333, ans=0.42109375
2024-10-07 21:59:26,362 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1683.3333333333333, ans=0.2831666666666667
2024-10-07 21:59:38,554 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys.whitening_limit, batch_count=1686.6666666666667, ans=3.253
2024-10-07 21:59:47,604 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.13 vs. limit=8.7675
2024-10-07 21:59:49,671 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1690.0, ans=0.2831
2024-10-07 21:59:58,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=1693.3333333333333, ans=0.8407333333333333
2024-10-07 22:00:12,419 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.66 vs. limit=8.7725
2024-10-07 22:00:22,162 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.207e+02 1.484e+02 1.617e+02 1.851e+02 2.944e+02, threshold=3.234e+02, percent-clipped=0.0
2024-10-07 22:00:22,209 INFO [train.py:1153] Epoch 1, batch 5100, loss[loss=0.4963, simple_loss=0.4063, pruned_loss=0.2014, ctc_loss=0.4586, over 4816.00 frames. ], tot_loss[loss=0.5457, simple_loss=0.4277, pruned_loss=0.2309, ctc_loss=0.5047, over 967968.84 frames. ], batch size: 19, lr: 4.09e-02,
2024-10-07 22:00:28,997 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=1700.0, ans=0.5
2024-10-07 22:00:30,780 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.35 vs. limit=8.1375
2024-10-07 22:00:56,529 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=1706.6666666666667, ans=0.42
2024-10-07 22:00:56,927 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.77 vs. limit=8.78
2024-10-07 22:01:06,460 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.74 vs. limit=8.7825
2024-10-07 22:01:13,695 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=1713.3333333333333, ans=0.4196875
2024-10-07 22:01:17,601 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1713.3333333333333, ans=0.28286666666666666
2024-10-07 22:01:21,604 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=1713.3333333333333, ans=0.4196875
2024-10-07 22:01:23,497 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.09 vs. limit=8.1425
2024-10-07 22:01:26,841 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=1716.6666666666667, ans=0.1534375
2024-10-07 22:01:27,998 INFO [train.py:1153] Epoch 1, batch 5150, loss[loss=0.5842, simple_loss=0.4515, pruned_loss=0.2503, ctc_loss=0.5409, over 4814.00 frames. ], tot_loss[loss=0.5445, simple_loss=0.4268, pruned_loss=0.2303, ctc_loss=0.5041, over 968096.19 frames. ], batch size: 36, lr: 4.09e-02,
2024-10-07 22:01:33,758 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.30 vs. limit=3.2575
2024-10-07 22:01:38,543 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=1716.6666666666667, ans=0.41953125
2024-10-07 22:01:42,441 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=1720.0, ans=0.419375
2024-10-07 22:01:48,774 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.min_abs, batch_count=1720.0, ans=0.2258
2024-10-07 22:02:03,468 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=27.47 vs. limit=8.7925
2024-10-07 22:02:06,462 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys.whitening_limit, batch_count=1726.6666666666667, ans=3.259
2024-10-07 22:02:10,023 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.49 vs. limit=5.863333333333333
2024-10-07 22:02:22,647 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=1730.0, ans=0.41890625
2024-10-07 22:02:27,008 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=17.48 vs. limit=8.14875
2024-10-07 22:02:29,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=1730.0, ans=0.061075000000000004
2024-10-07 22:02:30,026 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.85 vs. limit=5.865
2024-10-07 22:02:30,985 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten.whitening_limit, batch_count=1730.0, ans=8.7975
2024-10-07 22:02:32,958 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.123e+02 1.514e+02 1.654e+02 1.912e+02 3.372e+02, threshold=3.308e+02, percent-clipped=2.0
2024-10-07 22:02:33,002 INFO [train.py:1153] Epoch 1, batch 5200, loss[loss=0.5526, simple_loss=0.4204, pruned_loss=0.2408, ctc_loss=0.5082, over 4793.00 frames. ], tot_loss[loss=0.5417, simple_loss=0.4253, pruned_loss=0.2287, ctc_loss=0.5013, over 967730.18 frames. ], batch size: 29, lr: 4.08e-02,
2024-10-07 22:02:33,142 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=1733.3333333333333, ans=0.8393333333333334
2024-10-07 22:02:46,107 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=1736.6666666666667, ans=0.04457291666666667
2024-10-07 22:02:51,493 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.53 vs. limit=8.8025
2024-10-07 22:02:52,782 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.44 vs. limit=4.6946666666666665
2024-10-07 22:02:56,238 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1736.6666666666667, ans=0.28263333333333335
2024-10-07 22:02:56,818 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=7.69 vs. limit=8.15125
2024-10-07 22:02:58,043 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.64 vs. limit=8.805
2024-10-07 22:03:03,473 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=7.12 vs. limit=8.1525
2024-10-07 22:03:23,409 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=1746.6666666666667, ans=0.41812499999999997
2024-10-07 22:03:35,136 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=1746.6666666666667, ans=0.8388666666666666
2024-10-07 22:03:37,568 INFO [train.py:1153] Epoch 1, batch 5250, loss[loss=0.5442, simple_loss=0.4356, pruned_loss=0.2281, ctc_loss=0.4912, over 4857.00 frames. ], tot_loss[loss=0.5375, simple_loss=0.4233, pruned_loss=0.2265, ctc_loss=0.497, over 967808.47 frames. ], batch size: 20, lr: 4.07e-02,
2024-10-07 22:03:40,773 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.52 vs. limit=8.15625
2024-10-07 22:03:52,579 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.75 vs. limit=8.815
2024-10-07 22:03:55,512 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=1753.3333333333333, ans=0.41781250000000003
2024-10-07 22:03:58,188 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=1753.3333333333333, ans=0.8386333333333333
2024-10-07 22:04:07,509 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.37 vs. limit=8.817499999999999
2024-10-07 22:04:14,264 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.63 vs. limit=8.817499999999999
2024-10-07 22:04:29,575 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.16 vs. limit=5.881666666666667
2024-10-07 22:04:32,168 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.40 vs. limit=8.16125
2024-10-07 22:04:37,202 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.32 vs. limit=3.2645
2024-10-07 22:04:41,843 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.049e+02 1.447e+02 1.601e+02 1.886e+02 4.474e+02, threshold=3.203e+02, percent-clipped=1.0
2024-10-07 22:04:41,888 INFO [train.py:1153] Epoch 1, batch 5300, loss[loss=0.5221, simple_loss=0.4237, pruned_loss=0.2153, ctc_loss=0.4748, over 4825.00 frames. ], tot_loss[loss=0.538, simple_loss=0.424, pruned_loss=0.2267, ctc_loss=0.4966, over 967887.53 frames. ], batch size: 38, lr: 4.07e-02,
2024-10-07 22:04:52,673 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.01 vs. limit=5.883333333333334
2024-10-07 22:04:54,434 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=13.33 vs. limit=8.8275
2024-10-07 22:04:58,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=1770.0, ans=0.5
2024-10-07 22:04:59,357 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=9.36 vs. limit=8.8275
2024-10-07 22:05:05,204 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=1770.0, ans=0.5
2024-10-07 22:05:08,289 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.41 vs. limit=8.83
2024-10-07 22:05:14,570 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.40 vs. limit=8.165
2024-10-07 22:05:17,084 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.53 vs. limit=8.165
2024-10-07 22:05:20,836 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.39 vs. limit=8.8325
2024-10-07 22:05:41,321 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.55 vs. limit=8.835
2024-10-07 22:05:46,208 INFO [train.py:1153] Epoch 1, batch 5350, loss[loss=0.4745, simple_loss=0.3845, pruned_loss=0.1995, ctc_loss=0.4139, over 4978.00 frames. ], tot_loss[loss=0.5384, simple_loss=0.4244, pruned_loss=0.2267, ctc_loss=0.4973, over 967293.95 frames. ], batch size: 19, lr: 4.06e-02,
2024-10-07 22:05:47,945 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.82 vs. limit=8.8375
2024-10-07 22:05:52,768 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=1783.3333333333333, ans=0.41640625
2024-10-07 22:05:56,845 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=24.02 vs. limit=8.8375
2024-10-07 22:06:00,842 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.77 vs. limit=8.84
2024-10-07 22:06:07,146 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.90 vs. limit=8.84
2024-10-07 22:06:32,813 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.90 vs. limit=8.1725
2024-10-07 22:06:35,633 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.04 vs. limit=8.845
2024-10-07 22:06:39,854 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.44 vs. limit=3.2695
2024-10-07 22:06:41,556 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=1796.6666666666667, ans=0.059575
2024-10-07 22:06:45,352 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-07 22:06:50,418 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.106e+02 1.546e+02 1.693e+02 1.856e+02 3.269e+02, threshold=3.386e+02, percent-clipped=1.0
2024-10-07 22:06:50,460 INFO [train.py:1153] Epoch 1, batch 5400, loss[loss=0.6043, simple_loss=0.461, pruned_loss=0.2618, ctc_loss=0.56, over 4773.00 frames. ], tot_loss[loss=0.5422, simple_loss=0.4264, pruned_loss=0.229, ctc_loss=0.4999, over 966532.54 frames. ], batch size: 49, lr: 4.05e-02,
2024-10-07 22:07:02,014 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-07 22:07:05,127 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=8.02 vs. limit=8.17625
2024-10-07 22:07:19,307 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.81 vs. limit=8.855
2024-10-07 22:07:39,442 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=1810.0, ans=6.13125
2024-10-07 22:07:42,878 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.86 vs. limit=5.906666666666666
2024-10-07 22:07:49,600 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1813.3333333333333, ans=0.28186666666666665
2024-10-07 22:07:52,829 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.67 vs. limit=8.86
2024-10-07 22:07:54,710 INFO [train.py:1153] Epoch 1, batch 5450, loss[loss=0.4437, simple_loss=0.367, pruned_loss=0.1781, ctc_loss=0.4107, over 4940.00 frames. ], tot_loss[loss=0.5364, simple_loss=0.4235, pruned_loss=0.2259, ctc_loss=0.4936, over 967208.69 frames. ], batch size: 19, lr: 4.05e-02,
2024-10-07 22:07:57,996 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=1816.6666666666667, ans=0.059125
2024-10-07 22:07:58,378 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.44 vs. limit=8.8625
2024-10-07 22:08:13,406 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.14 vs. limit=8.865
2024-10-07 22:08:14,176 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=1820.0, ans=0.4146875
2024-10-07 22:08:15,485 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=1820.0, ans=0.7682
2024-10-07 22:08:18,476 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.65 vs. limit=5.455
2024-10-07 22:08:18,643 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.53 vs. limit=5.455
2024-10-07 22:08:23,329 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=1823.3333333333333, ans=0.131625
2024-10-07 22:08:52,443 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.14 vs. limit=5.915
2024-10-07 22:08:53,547 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.64 vs. limit=4.732
2024-10-07 22:08:59,368 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.113e+02 1.485e+02 1.655e+02 1.818e+02 2.585e+02, threshold=3.311e+02, percent-clipped=0.0
2024-10-07 22:08:59,415 INFO [train.py:1153] Epoch 1, batch 5500, loss[loss=0.5424, simple_loss=0.4158, pruned_loss=0.2356, ctc_loss=0.4943, over 4811.00 frames. ], tot_loss[loss=0.5351, simple_loss=0.4229, pruned_loss=0.2252, ctc_loss=0.492, over 967472.79 frames. ], batch size: 49, lr: 4.04e-02,
2024-10-07 22:09:03,410 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-07 22:09:11,249 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=1836.6666666666667, ans=0.131125
2024-10-07 22:09:32,734 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.92 vs. limit=5.92
2024-10-07 22:09:33,299 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=1840.0, ans=0.7684
2024-10-07 22:09:36,208 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten.whitening_limit, batch_count=1840.0, ans=8.879999999999999
2024-10-07 22:09:40,953 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=1843.3333333333333, ans=0.04949747468305833
2024-10-07 22:09:45,381 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=22.18 vs. limit=8.8825
2024-10-07 22:09:45,398 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.56 vs. limit=3.2765
2024-10-07 22:09:56,957 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=27.66 vs. limit=8.885
2024-10-07 22:10:03,949 INFO [train.py:1153] Epoch 1, batch 5550, loss[loss=0.4595, simple_loss=0.3741, pruned_loss=0.184, ctc_loss=0.4421, over 4799.00 frames. ], tot_loss[loss=0.5335, simple_loss=0.4225, pruned_loss=0.2243, ctc_loss=0.4899, over 967087.24 frames. ], batch size: 19, lr: 4.03e-02,
2024-10-07 22:10:04,558 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=31.20 vs. limit=8.8875
2024-10-07 22:10:11,150 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.98 vs. limit=8.8875
2024-10-07 22:10:18,345 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=1853.3333333333333, ans=0.413125
2024-10-07 22:10:18,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.45 vs. limit=8.89
2024-10-07 22:10:26,668 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.75 vs. limit=8.195
2024-10-07 22:10:32,491 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=1856.6666666666667, ans=0.41296875
2024-10-07 22:10:45,434 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1860.0, ans=0.2814
2024-10-07 22:10:52,482 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.62 vs. limit=5.465
2024-10-07 22:10:54,428 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1863.3333333333333, ans=0.28136666666666665
2024-10-07 22:11:08,504 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.128e+02 1.462e+02 1.612e+02 1.870e+02 3.678e+02, threshold=3.224e+02, percent-clipped=4.0
2024-10-07 22:11:08,547 INFO [train.py:1153] Epoch 1, batch 5600, loss[loss=0.5013, simple_loss=0.4099, pruned_loss=0.2019, ctc_loss=0.4723, over 4862.00 frames. ], tot_loss[loss=0.5329, simple_loss=0.4221, pruned_loss=0.224, ctc_loss=0.4893, over 967173.94 frames. ], batch size: 28, lr: 4.03e-02,
2024-10-07 22:11:10,669 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=3.57 vs. limit=5.0
2024-10-07 22:11:17,694 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=1866.6666666666667, ans=0.057999999999999996
2024-10-07 22:11:39,933 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=1873.3333333333333, ans=0.5
2024-10-07 22:11:50,330 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=1876.6666666666667, ans=0.5
2024-10-07 22:11:51,578 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=1876.6666666666667, ans=0.41203125
2024-10-07 22:12:03,631 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.05 vs. limit=5.9399999999999995
2024-10-07 22:12:11,354 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=11.82 vs. limit=8.205
2024-10-07 22:12:13,354 INFO [train.py:1153] Epoch 1, batch 5650, loss[loss=0.5852, simple_loss=0.4626, pruned_loss=0.2456, ctc_loss=0.5415, over 4739.00 frames. ], tot_loss[loss=0.5286, simple_loss=0.42, pruned_loss=0.2216, ctc_loss=0.4852, over 967022.70 frames. ], batch size: 45, lr: 4.02e-02,
2024-10-07 22:12:24,040 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=1883.3333333333333, ans=0.41171875
2024-10-07 22:12:30,467 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1886.6666666666667, ans=0.28113333333333335
2024-10-07 22:12:33,503 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.71 vs. limit=5.471666666666667
2024-10-07 22:12:35,670 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1886.6666666666667, ans=0.28113333333333335
2024-10-07 22:12:38,373 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer_ff2.min_abs, batch_count=1890.0, ans=0.04725
2024-10-07 22:12:44,671 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=1890.0, ans=0.41140625
2024-10-07 22:12:55,032 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=1893.3333333333333, ans=0.05740000000000001
2024-10-07 22:13:04,034 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=1896.6666666666667, ans=0.41109375
2024-10-07 22:13:04,621 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.62 vs. limit=8.21125
2024-10-07 22:13:13,773 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.02 vs. limit=8.21125
2024-10-07 22:13:18,215 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.242e+02 1.481e+02 1.641e+02 1.868e+02 2.615e+02, threshold=3.282e+02, percent-clipped=0.0
2024-10-07 22:13:18,262 INFO [train.py:1153] Epoch 1, batch 5700, loss[loss=0.5636, simple_loss=0.4354, pruned_loss=0.238, ctc_loss=0.5396, over 4888.00 frames. ], tot_loss[loss=0.5258, simple_loss=0.4182, pruned_loss=0.2201, ctc_loss=0.4831, over 966420.61 frames. ], batch size: 22, lr: 4.02e-02,
2024-10-07 22:13:20,436 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.49 vs. limit=8.2125
2024-10-07 22:13:22,583 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=10.67 vs. limit=8.925
2024-10-07 22:13:35,218 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=1903.3333333333333, ans=0.41078125
2024-10-07 22:13:42,133 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.08 vs. limit=5.951666666666666
2024-10-07 22:13:42,338 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.30 vs. limit=4.761333333333333
2024-10-07 22:13:44,905 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.11 vs. limit=8.215
2024-10-07 22:13:50,969 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=1906.6666666666667, ans=0.0571
2024-10-07 22:13:58,702 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=1910.0, ans=0.12837500000000002
2024-10-07 22:14:01,903 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.73 vs. limit=8.21625
2024-10-07 22:14:08,094 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.25 vs. limit=8.932500000000001
2024-10-07 22:14:18,883 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.47 vs. limit=5.4783333333333335
2024-10-07 22:14:23,265 INFO [train.py:1153] Epoch 1, batch 5750, loss[loss=0.6796, simple_loss=0.5004, pruned_loss=0.3001, ctc_loss=0.6467, over 4851.00 frames. ], tot_loss[loss=0.5271, simple_loss=0.4189, pruned_loss=0.221, ctc_loss=0.4831, over 966852.46 frames. ], batch size: 43, lr: 4.01e-02,
2024-10-07 22:14:30,564 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=7.76 vs. limit=8.21875
2024-10-07 22:14:31,178 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=1916.6666666666667, ans=0.128125
2024-10-07 22:14:36,707 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.50 vs. limit=8.94
2024-10-07 22:14:55,979 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.13 vs. limit=4.769333333333333
2024-10-07 22:15:01,810 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1926.6666666666667, ans=0.4096875
2024-10-07 22:15:15,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten.whitening_limit, batch_count=1930.0, ans=8.22375
2024-10-07 22:15:16,087 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=1930.0, ans=0.40953125
2024-10-07 22:15:16,518 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.39 vs. limit=8.22375
2024-10-07 22:15:19,439 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.13 vs. limit=3.2895
2024-10-07 22:15:20,248 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=10.38 vs. limit=8.9475
2024-10-07 22:15:25,700 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.59 vs. limit=5.4825
2024-10-07 22:15:27,549 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.158e+02 1.477e+02 1.611e+02 1.867e+02 3.326e+02, threshold=3.221e+02, percent-clipped=1.0
2024-10-07 22:15:27,592 INFO [train.py:1153] Epoch 1, batch 5800, loss[loss=0.5684, simple_loss=0.4349, pruned_loss=0.2451, ctc_loss=0.5291, over 4846.00 frames. ], tot_loss[loss=0.5254, simple_loss=0.4174, pruned_loss=0.2206, ctc_loss=0.4803, over 966144.94 frames. ], batch size: 43, lr: 4.00e-02,
2024-10-07 22:15:28,896 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=1933.3333333333333, ans=0.409375
2024-10-07 22:15:29,348 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.38 vs. limit=8.95
2024-10-07 22:15:33,259 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.85 vs. limit=5.483333333333333
2024-10-07 22:15:45,696 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=1936.6666666666667, ans=0.12737500000000002
2024-10-07 22:16:01,446 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.39 vs. limit=5.97
2024-10-07 22:16:06,602 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.35 vs. limit=5.971666666666667
2024-10-07 22:16:11,433 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.min_abs, batch_count=1943.3333333333333, ans=0.22915000000000002
2024-10-07 22:16:21,874 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=1946.6666666666667, ans=0.1405
2024-10-07 22:16:23,409 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.47 vs. limit=8.23
2024-10-07 22:16:32,111 INFO [train.py:1153] Epoch 1, batch 5850, loss[loss=0.5837, simple_loss=0.4484, pruned_loss=0.2562, ctc_loss=0.5165, over 4733.00 frames. ], tot_loss[loss=0.5241, simple_loss=0.417, pruned_loss=0.2198, ctc_loss=0.4788, over 966531.89 frames. ], batch size: 45, lr: 4.00e-02,
2024-10-07 22:16:37,376 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1950.0, ans=0.40859375
2024-10-07 22:16:46,215 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1953.3333333333333, ans=0.28046666666666664
2024-10-07 22:16:49,317 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.30 vs. limit=4.781333333333333
2024-10-07 22:16:50,073 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.min_positive, batch_count=1953.3333333333333, ans=0.043895833333333335
2024-10-07 22:17:09,274 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.57 vs. limit=5.49
2024-10-07 22:17:20,979 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.44 vs. limit=3.294
2024-10-07 22:17:23,840 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=9.23 vs. limit=8.9725
2024-10-07 22:17:30,702 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.min_abs, batch_count=1963.3333333333333, ans=0.22945000000000002
2024-10-07 22:17:35,706 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.102e+02 1.508e+02 1.631e+02 1.824e+02 4.196e+02, threshold=3.262e+02, percent-clipped=2.0
2024-10-07 22:17:35,752 INFO [train.py:1153] Epoch 1, batch 5900, loss[loss=0.5342, simple_loss=0.4264, pruned_loss=0.2231, ctc_loss=0.4895, over 4808.00 frames. ], tot_loss[loss=0.5222, simple_loss=0.4163, pruned_loss=0.2189, ctc_loss=0.4758, over 966715.50 frames. ], batch size: 34, lr: 3.99e-02,
2024-10-07 22:17:46,010 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=1966.6666666666667, ans=0.4078125
2024-10-07 22:17:51,102 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=1970.0, ans=0.40765625
2024-10-07 22:18:08,356 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.24 vs. limit=5.986666666666666
2024-10-07 22:18:08,381 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=17.89 vs. limit=8.98
2024-10-07 22:18:13,736 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.04 vs. limit=8.9825
2024-10-07 22:18:17,875 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.39 vs. limit=3.2965
2024-10-07 22:18:35,191 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1980.0, ans=0.4071875
2024-10-07 22:18:40,114 INFO [train.py:1153] Epoch 1, batch 5950, loss[loss=0.525, simple_loss=0.411, pruned_loss=0.2267, ctc_loss=0.464, over 4823.00 frames. ], tot_loss[loss=0.5194, simple_loss=0.415, pruned_loss=0.2174, ctc_loss=0.4727, over 966052.83 frames. ], batch size: 34, lr: 3.98e-02,
2024-10-07 22:19:07,098 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=22.11 vs. limit=8.9925
2024-10-07 22:19:10,095 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.04 vs. limit=8.9925
2024-10-07 22:19:13,656 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.20 vs. limit=3.2984999999999998
2024-10-07 22:19:15,091 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.70 vs. limit=8.9925
2024-10-07 22:19:19,259 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=4.53 vs. limit=4.398666666666666
2024-10-07 22:19:25,639 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.49 vs. limit=8.995000000000001
2024-10-07 22:19:26,981 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.46 vs. limit=5.996666666666666
2024-10-07 22:19:33,984 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=1996.6666666666667, ans=0.055075
2024-10-07 22:19:35,992 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.25 vs. limit=8.9975
2024-10-07 22:19:45,641 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.206e+02 1.525e+02 1.698e+02 1.910e+02 4.246e+02, threshold=3.396e+02, percent-clipped=4.0
2024-10-07 22:19:45,685 INFO [train.py:1153] Epoch 1, batch 6000, loss[loss=0.5749, simple_loss=0.4501, pruned_loss=0.2399, ctc_loss=0.5501, over 4795.00 frames. ], tot_loss[loss=0.518, simple_loss=0.4147, pruned_loss=0.2163, ctc_loss=0.4717, over 966652.71 frames. ], batch size: 49, lr: 3.98e-02,
2024-10-07 22:19:45,685 INFO [train.py:1176] Computing validation loss
2024-10-07 22:19:53,034 INFO [train.py:1185] Epoch 1, validation: loss=0.349, simple_loss=0.3605, pruned_loss=0.1181, ctc_loss=0.2528, over 90464.00 frames.
2024-10-07 22:19:53,035 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-07 22:19:53,624 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.39 vs. limit=8.25
2024-10-07 22:19:55,163 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.48 vs. limit=9.0
2024-10-07 22:19:56,278 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.37 vs. limit=8.25
2024-10-07 22:20:05,284 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.42 vs. limit=3.3005
2024-10-07 22:20:18,447 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=2006.6666666666667, ans=0.24916666666666665
2024-10-07 22:20:24,022 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.81 vs. limit=8.2525
2024-10-07 22:20:24,864 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=2006.6666666666667, ans=0.4059375
2024-10-07 22:20:35,837 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=2.59 vs. limit=8.25375
2024-10-07 22:20:38,262 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.75 vs. limit=5.5024999999999995
2024-10-07 22:20:45,936 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.90 vs. limit=9.01
2024-10-07 22:20:50,988 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=2013.3333333333333, ans=0.405625
2024-10-07 22:20:57,015 INFO [train.py:1153] Epoch 1, batch 6050, loss[loss=0.4964, simple_loss=0.4378, pruned_loss=0.1899, ctc_loss=0.4381, over 4813.00 frames. ], tot_loss[loss=0.5172, simple_loss=0.4147, pruned_loss=0.2159, ctc_loss=0.4699, over 966584.47 frames. ], batch size: 19, lr: 3.97e-02,
2024-10-07 22:21:09,017 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2.whitening_limit, batch_count=2020.0, ans=6.01
2024-10-07 22:21:19,435 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.17 vs. limit=8.2575
2024-10-07 22:21:43,891 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.16 vs. limit=9.02
2024-10-07 22:21:52,929 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.54 vs. limit=9.0225
2024-10-07 22:21:55,022 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=2030.0, ans=0.035
2024-10-07 22:22:01,159 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.145e+02 1.524e+02 1.683e+02 1.989e+02 4.010e+02, threshold=3.367e+02, percent-clipped=1.0
2024-10-07 22:22:01,207 INFO [train.py:1153] Epoch 1, batch 6100, loss[loss=0.5521, simple_loss=0.432, pruned_loss=0.239, ctc_loss=0.4859, over 4787.00 frames. ], tot_loss[loss=0.5179, simple_loss=0.4147, pruned_loss=0.2166, ctc_loss=0.4699, over 966083.82 frames. ], batch size: 34, lr: 3.96e-02,
2024-10-07 22:22:07,226 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=6.79 vs. limit=8.2625
2024-10-07 22:22:10,427 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=2033.3333333333333, ans=0.05425
2024-10-07 22:22:13,876 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=2036.6666666666667, ans=0.40453125
2024-10-07 22:22:15,357 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=2036.6666666666667, ans=0.24541666666666667
2024-10-07 22:22:16,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=2036.6666666666667, ans=0.04363541666666667
2024-10-07 22:22:19,210 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=2036.6666666666667, ans=0.123625
2024-10-07 22:22:24,945 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.44 vs. limit=6.0183333333333335
2024-10-07 22:22:33,475 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=2040.0, ans=0.2306
2024-10-07 22:22:34,827 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=2040.0, ans=0.8286
2024-10-07 22:22:36,270 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.21 vs. limit=4.816
2024-10-07 22:22:42,024 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=14.48 vs. limit=9.0325
2024-10-07 22:22:42,862 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.01 vs. limit=9.0325
2024-10-07 22:22:52,213 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.48 vs. limit=9.035
2024-10-07 22:22:52,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=9.48 vs. limit=9.035
2024-10-07 22:23:00,605 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=2046.6666666666667, ans=0.12325
2024-10-07 22:23:04,909 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.56 vs. limit=8.26875
2024-10-07 22:23:05,583 INFO [train.py:1153] Epoch 1, batch 6150, loss[loss=0.576, simple_loss=0.4454, pruned_loss=0.2467, ctc_loss=0.5332, over 4829.00 frames. ], tot_loss[loss=0.5168, simple_loss=0.4145, pruned_loss=0.2158, ctc_loss=0.4688, over 966182.97 frames. ], batch size: 43, lr: 3.96e-02,
2024-10-07 22:23:05,682 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=2050.0, ans=0.123125
2024-10-07 22:23:06,301 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.09 vs. limit=5.5125
2024-10-07 22:23:10,373 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.56 vs. limit=9.0375
2024-10-07 22:23:12,275 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=2050.0, ans=0.04949747468305833
2024-10-07 22:23:16,843 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=6.27 vs. limit=8.26875
2024-10-07 22:23:26,573 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=2053.3333333333335, ans=0.40375
2024-10-07 22:23:37,717 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.31 vs. limit=9.0425
2024-10-07 22:24:10,908 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.046e+02 1.535e+02 1.703e+02 1.962e+02 3.104e+02, threshold=3.407e+02, percent-clipped=0.0
2024-10-07 22:24:10,952 INFO [train.py:1153] Epoch 1, batch 6200, loss[loss=0.6693, simple_loss=0.5075, pruned_loss=0.2994, ctc_loss=0.5803, over 4786.00 frames. ], tot_loss[loss=0.5157, simple_loss=0.4143, pruned_loss=0.2153, ctc_loss=0.4666, over 966399.01 frames. ], batch size: 29, lr: 3.95e-02,
2024-10-07 22:24:14,851 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=2066.6666666666665, ans=0.12250000000000001
2024-10-07 22:24:29,102 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=2070.0, ans=0.40296875
2024-10-07 22:24:29,140 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.min_positive, batch_count=2070.0, ans=0.2293
2024-10-07 22:24:47,566 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.30 vs. limit=4.8293333333333335
2024-10-07 22:24:51,063 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=2076.6666666666665, ans=0.40265625
2024-10-07 22:24:57,790 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=29.38 vs. limit=9.0575
2024-10-07 22:25:03,445 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=9.57 vs. limit=9.06
2024-10-07 22:25:06,613 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=2080.0, ans=0.8272
2024-10-07 22:25:15,538 INFO [train.py:1153] Epoch 1, batch 6250, loss[loss=0.5114, simple_loss=0.411, pruned_loss=0.2085, ctc_loss=0.4871, over 4731.00 frames. ], tot_loss[loss=0.5118, simple_loss=0.4122, pruned_loss=0.2132, ctc_loss=0.4627, over 966780.96 frames. ], batch size: 26, lr: 3.94e-02,
2024-10-07 22:25:32,882 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.29 vs. limit=9.065
2024-10-07 22:25:48,339 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.67 vs. limit=5.5225
2024-10-07 22:25:51,661 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=17.06 vs. limit=9.067499999999999
2024-10-07 22:25:52,008 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.34 vs. limit=8.28375
2024-10-07 22:26:01,000 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.75 vs. limit=6.046666666666667
2024-10-07 22:26:04,213 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2093.3333333333335, ans=0.27906666666666663
2024-10-07 22:26:12,225 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=3.94 vs. limit=8.286249999999999
2024-10-07 22:26:13,781 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.95 vs. limit=9.0725
2024-10-07 22:26:19,569 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.217e+02 1.452e+02 1.629e+02 1.776e+02 3.659e+02, threshold=3.258e+02, percent-clipped=1.0
2024-10-07 22:26:19,615 INFO [train.py:1153] Epoch 1, batch 6300, loss[loss=0.5186, simple_loss=0.4186, pruned_loss=0.2146, ctc_loss=0.474, over 4978.00 frames. ], tot_loss[loss=0.51, simple_loss=0.4111, pruned_loss=0.2123, ctc_loss=0.4608, over 966447.34 frames. ], batch size: 19, lr: 3.94e-02,
2024-10-07 22:26:22,221 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=2100.0, ans=0.4015625
2024-10-07 22:26:38,687 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=13.57 vs. limit=9.0775
2024-10-07 22:26:43,478 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=24.99 vs. limit=9.0775
2024-10-07 22:26:49,394 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=2106.6666666666665, ans=0.8262666666666667
2024-10-07 22:27:01,147 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2110.0, ans=0.40109375
2024-10-07 22:27:02,278 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=2110.0, ans=0.40109375
2024-10-07 22:27:11,834 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.12 vs. limit=9.085
2024-10-07 22:27:19,472 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.40 vs. limit=9.085
2024-10-07 22:27:23,940 INFO [train.py:1153] Epoch 1, batch 6350, loss[loss=0.6074, simple_loss=0.462, pruned_loss=0.2627, ctc_loss=0.5684, over 4817.00 frames. ], tot_loss[loss=0.5073, simple_loss=0.41, pruned_loss=0.2107, ctc_loss=0.4579, over 966135.03 frames. ], batch size: 36, lr: 3.93e-02,
2024-10-07 22:27:26,630 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=2116.6666666666665, ans=0.40078125
2024-10-07 22:27:28,813 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.50 vs. limit=6.058333333333334
2024-10-07 22:27:39,092 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.99 vs. limit=6.0600000000000005
2024-10-07 22:27:42,591 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.75 vs. limit=9.09
2024-10-07 22:27:53,657 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2123.3333333333335, ans=0.40046875
2024-10-07 22:27:53,965 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.37 vs. limit=3.3185000000000002
2024-10-07 22:27:54,217 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.56 vs. limit=9.092500000000001
2024-10-07 22:28:12,118 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.63 vs. limit=5.531666666666666
2024-10-07 22:28:15,623 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys.whitening_limit, batch_count=2130.0, ans=3.3195
2024-10-07 22:28:15,820 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.40 vs. limit=4.852
2024-10-07 22:28:19,892 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.07 vs. limit=9.0975
2024-10-07 22:28:27,574 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=9.78 vs. limit=9.1
2024-10-07 22:28:28,335 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.221e+02 1.534e+02 1.730e+02 1.874e+02 3.707e+02, threshold=3.461e+02, percent-clipped=1.0
2024-10-07 22:28:28,379 INFO [train.py:1153] Epoch 1, batch 6400, loss[loss=0.4752, simple_loss=0.3977, pruned_loss=0.1901, ctc_loss=0.4315, over 4882.00 frames. ], tot_loss[loss=0.5053, simple_loss=0.4089, pruned_loss=0.2097, ctc_loss=0.4557, over 965893.85 frames. ], batch size: 23, lr: 3.92e-02,
2024-10-07 22:28:28,579 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=2133.3333333333335, ans=0.12
2024-10-07 22:28:44,002 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=2136.6666666666665, ans=0.8252166666666667
2024-10-07 22:28:53,499 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.52 vs. limit=3.321
2024-10-07 22:29:15,508 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.05 vs. limit=9.1075
2024-10-07 22:29:22,624 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2146.6666666666665, ans=0.2785333333333333
2024-10-07 22:29:32,755 INFO [train.py:1153] Epoch 1, batch 6450, loss[loss=0.4922, simple_loss=0.3994, pruned_loss=0.2025, ctc_loss=0.4502, over 4740.00 frames. ], tot_loss[loss=0.5041, simple_loss=0.4082, pruned_loss=0.2091, ctc_loss=0.4544, over 965238.39 frames. ], batch size: 26, lr: 3.92e-02,
2024-10-07 22:29:39,405 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=2150.0, ans=0.23125
2024-10-07 22:29:45,373 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.37 vs. limit=9.115
2024-10-07 22:29:46,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=2153.3333333333335, ans=0.043270833333333335
2024-10-07 22:29:49,155 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.90 vs. limit=9.115
2024-10-07 22:29:52,325 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=2153.3333333333335, ans=0.3990625
2024-10-07 22:29:59,381 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.10 vs. limit=5.539166666666667
2024-10-07 22:30:01,999 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.05 vs. limit=9.1175
2024-10-07 22:30:05,584 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=9.69 vs. limit=9.1175
2024-10-07 22:30:05,886 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.15 vs. limit=8.30875
2024-10-07 22:30:11,775 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=2160.0, ans=0.39875
2024-10-07 22:30:34,792 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=2163.3333333333335, ans=0.8242833333333334
2024-10-07 22:30:37,173 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.179e+02 1.471e+02 1.622e+02 1.818e+02 2.817e+02, threshold=3.243e+02, percent-clipped=0.0
2024-10-07 22:30:37,218 INFO [train.py:1153] Epoch 1, batch 6500, loss[loss=0.4969, simple_loss=0.4086, pruned_loss=0.2089, ctc_loss=0.4187, over 4735.00 frames. ], tot_loss[loss=0.4996, simple_loss=0.4062, pruned_loss=0.2066, ctc_loss=0.4497, over 964973.97 frames. ], batch size: 26, lr: 3.91e-02,
2024-10-07 22:30:38,654 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.min_abs, batch_count=2166.6666666666665, ans=0.2325
2024-10-07 22:30:40,173 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=19.99 vs. limit=9.125
2024-10-07 22:30:49,447 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.36 vs. limit=3.3255
2024-10-07 22:30:52,851 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2170.0, ans=0.2783
2024-10-07 22:31:05,765 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=2173.3333333333335, ans=0.398125
2024-10-07 22:31:10,080 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.02 vs. limit=9.13
2024-10-07 22:31:12,082 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.max_positive, batch_count=2173.3333333333335, ans=0.7717333333333334
2024-10-07 22:31:15,074 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.41 vs. limit=9.1325
2024-10-07 22:31:28,028 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.80 vs. limit=5.545
2024-10-07 22:31:30,003 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=2180.0, ans=0.3978125
2024-10-07 22:31:33,278 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.91 vs. limit=6.09
2024-10-07 22:31:35,852 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=9.82 vs. limit=9.135
2024-10-07 22:31:41,599 INFO [train.py:1153] Epoch 1, batch 6550, loss[loss=0.4557, simple_loss=0.3869, pruned_loss=0.182, ctc_loss=0.4009, over 4978.00 frames. ], tot_loss[loss=0.4974, simple_loss=0.4052, pruned_loss=0.2056, ctc_loss=0.446, over 964827.84 frames. ], batch size: 19, lr: 3.91e-02,
2024-10-07 22:31:47,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.82 vs. limit=8.31875
2024-10-07 22:31:48,816 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.53 vs. limit=9.1375
2024-10-07 22:31:50,163 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.18 vs. limit=6.091666666666667
2024-10-07 22:31:52,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=2183.3333333333335, ans=6.364583333333334
2024-10-07 22:31:57,400 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.prob, batch_count=2186.6666666666665, ans=0.3975
2024-10-07 22:31:58,017 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.52 vs. limit=9.14
2024-10-07 22:32:00,060 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=2186.6666666666665, ans=0.3975
2024-10-07 22:32:00,698 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.73 vs. limit=9.14
2024-10-07 22:32:08,314 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.90 vs. limit=9.1425
2024-10-07 22:32:46,570 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.206e+02 1.544e+02 1.710e+02 1.920e+02 3.739e+02, threshold=3.421e+02, percent-clipped=2.0
2024-10-07 22:32:46,614 INFO [train.py:1153] Epoch 1, batch 6600, loss[loss=0.593, simple_loss=0.4643, pruned_loss=0.2492, ctc_loss=0.558, over 4835.00 frames. ], tot_loss[loss=0.4967, simple_loss=0.4049, pruned_loss=0.2052, ctc_loss=0.4454, over 965321.62 frames. ], batch size: 23, lr: 3.90e-02,
2024-10-07 22:32:47,229 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.59 vs. limit=5.55
2024-10-07 22:32:48,018 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=2200.0, ans=0.12625
2024-10-07 22:32:48,520 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.36 vs. limit=3.33
2024-10-07 22:32:48,769 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.74 vs. limit=5.55
2024-10-07 22:32:55,888 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=2200.0, ans=0.1175
2024-10-07 22:32:58,352 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=2203.3333333333335, ans=0.2245833333333333
2024-10-07 22:32:58,434 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2203.3333333333335, ans=0.27796666666666664
2024-10-07 22:32:59,992 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=10.42 vs. limit=9.1525
2024-10-07 22:33:00,930 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=2203.3333333333335, ans=0.39671875
2024-10-07 22:33:10,892 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=7.64 vs. limit=8.32625
2024-10-07 22:33:14,036 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=2206.6666666666665, ans=0.8227666666666666
2024-10-07 22:33:21,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2206.6666666666665, ans=0.2779333333333333
2024-10-07 22:33:24,773 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.01 vs. limit=8.32875
2024-10-07 22:33:28,833 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.07 vs. limit=5.5525
2024-10-07 22:33:29,935 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.35 vs. limit=9.1575
2024-10-07 22:33:32,923 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=30.08 vs. limit=9.1575
2024-10-07 22:33:50,617 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=2216.6666666666665, ans=0.39609375
2024-10-07 22:33:50,635 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2216.6666666666665, ans=0.39609375
2024-10-07 22:33:51,784 INFO [train.py:1153] Epoch 1, batch 6650, loss[loss=0.4702, simple_loss=0.3981, pruned_loss=0.1883, ctc_loss=0.4139, over 4748.00 frames. ], tot_loss[loss=0.4941, simple_loss=0.4036, pruned_loss=0.2038, ctc_loss=0.4423, over 967033.23 frames. ], batch size: 20, lr: 3.89e-02,
2024-10-07 22:33:54,578 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=2216.6666666666665, ans=0.1253125
2024-10-07 22:34:18,208 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2223.3333333333335, ans=0.27776666666666666
2024-10-07 22:34:18,750 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.15 vs. limit=9.1675
2024-10-07 22:34:21,263 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.74 vs. limit=6.111666666666666
2024-10-07 22:34:22,593 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.48 vs. limit=6.111666666666666
2024-10-07 22:34:31,378 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2226.6666666666665, ans=0.27773333333333333
2024-10-07 22:34:43,373 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=2230.0, ans=0.11637499999999999
2024-10-07 22:34:43,860 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.43 vs. limit=3.3345000000000002
2024-10-07 22:34:53,973 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-07 22:34:57,665 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.267e+02 1.512e+02 1.711e+02 1.934e+02 3.319e+02, threshold=3.423e+02, percent-clipped=0.0
2024-10-07 22:34:57,713 INFO [train.py:1153] Epoch 1, batch 6700, loss[loss=0.4832, simple_loss=0.4059, pruned_loss=0.1958, ctc_loss=0.4223, over 4938.00 frames. ], tot_loss[loss=0.4888, simple_loss=0.401, pruned_loss=0.2009, ctc_loss=0.4369, over 969213.20 frames. ], batch size: 20, lr: 3.89e-02,
2024-10-07 22:34:58,268 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.93 vs. limit=9.175
2024-10-07 22:35:02,529 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=21.45 vs. limit=9.175
2024-10-07 22:35:08,746 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.68 vs. limit=9.175
2024-10-07 22:35:14,469 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.44 vs. limit=3.3355
2024-10-07 22:35:19,102 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=2236.6666666666665, ans=0.116125
2024-10-07 22:35:40,379 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=2243.3333333333335, ans=0.39484375
2024-10-07 22:35:45,093 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.98 vs. limit=5.560833333333333
2024-10-07 22:35:46,658 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.78 vs. limit=6.121666666666667
2024-10-07 22:35:59,151 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=5.77 vs. limit=6.123333333333333
2024-10-07 22:36:01,101 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.43 vs. limit=9.185
2024-10-07 22:36:03,942 INFO [train.py:1153] Epoch 1, batch 6750, loss[loss=0.4569, simple_loss=0.3654, pruned_loss=0.1955, ctc_loss=0.3937, over 4911.00 frames. ], tot_loss[loss=0.4793, simple_loss=0.3951, pruned_loss=0.1963, ctc_loss=0.4271, over 972265.95 frames. ], batch size: 19, lr: 3.88e-02,
2024-10-07 22:36:17,490 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=2253.3333333333335, ans=0.39437500000000003
2024-10-07 22:36:24,355 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=5.09 vs. limit=8.345
2024-10-07 22:36:38,623 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2256.6666666666665, ans=0.2774333333333333
2024-10-07 22:36:40,407 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.50 vs. limit=9.192499999999999
2024-10-07 22:36:46,578 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2260.0, ans=0.2774
2024-10-07 22:36:54,469 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=2260.0, ans=0.3940625
2024-10-07 22:36:56,393 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.34 vs. limit=8.34875
2024-10-07 22:36:59,577 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=2263.3333333333335, ans=0.049074999999999994
2024-10-07 22:37:07,359 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=2263.3333333333335, ans=0.07
2024-10-07 22:37:08,878 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=2266.6666666666665, ans=0.1225
2024-10-07 22:37:08,890 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=2266.6666666666665, ans=0.8206666666666667
2024-10-07 22:37:09,886 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.209e+02 1.464e+02 1.585e+02 1.812e+02 3.064e+02, threshold=3.170e+02, percent-clipped=0.0
2024-10-07 22:37:09,931 INFO [train.py:1153] Epoch 1, batch 6800, loss[loss=0.4513, simple_loss=0.3727, pruned_loss=0.1873, ctc_loss=0.388, over 4912.00 frames. ], tot_loss[loss=0.4768, simple_loss=0.394, pruned_loss=0.195, ctc_loss=0.4239, over 974591.32 frames. ], batch size: 19, lr: 3.87e-02,
2024-10-07 22:37:23,150 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=2270.0, ans=0.11487499999999999
2024-10-07 22:37:27,693 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.70 vs. limit=5.5675
2024-10-07 22:37:31,518 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=28.65 vs. limit=9.2025
2024-10-07 22:37:45,037 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=20.56 vs. limit=9.205
2024-10-07 22:37:49,673 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=2276.6666666666665, ans=0.2154166666666667
2024-10-07 22:38:00,000 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=2276.6666666666665, ans=0.27723333333333333
2024-10-07 22:38:02,792 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=2280.0, ans=0.11449999999999999
2024-10-07 22:38:04,150 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=2280.0, ans=0.23420000000000002
2024-10-07 22:38:04,552 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=6.77 vs. limit=8.355
2024-10-07 22:38:15,980 INFO [train.py:1153] Epoch 1, batch 6850, loss[loss=0.4383, simple_loss=0.3709, pruned_loss=0.172, ctc_loss=0.4042, over 4978.00 frames. ], tot_loss[loss=0.4717, simple_loss=0.3906, pruned_loss=0.1926, ctc_loss=0.419, over 978927.87 frames. ], batch size: 19, lr: 3.87e-02,
2024-10-07 22:38:17,457 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/epoch-1.pt
2024-10-07 22:38:32,506 INFO [train.py:1153] Epoch 2, batch 0, loss[loss=0.4212, simple_loss=0.357, pruned_loss=0.1678, ctc_loss=0.3743, over 4855.00 frames. ], tot_loss[loss=0.4212, simple_loss=0.357, pruned_loss=0.1678, ctc_loss=0.3743, over 4855.00 frames. ], batch size: 19, lr: 3.79e-02,
2024-10-07 22:38:32,507 INFO [train.py:1176] Computing validation loss
2024-10-07 22:38:38,955 INFO [train.py:1185] Epoch 2, validation: loss=0.3475, simple_loss=0.3638, pruned_loss=0.1149, ctc_loss=0.2534, over 90464.00 frames.
2024-10-07 22:38:38,955 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-07 22:38:39,062 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=2284.0, ans=0.11435000000000001
2024-10-07 22:39:03,077 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=2290.6666666666665, ans=0.04846
2024-10-07 22:39:03,297 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-07 22:39:08,728 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.39 vs. limit=3.3436
2024-10-07 22:39:14,591 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2294.0, ans=0.39246875000000003
2024-10-07 22:39:31,066 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=2297.3333333333335, ans=0.3923125
2024-10-07 22:39:37,188 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.57 vs. limit=6.148666666666667
2024-10-07 22:39:38,849 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.189e+02 1.441e+02 1.630e+02 1.820e+02 3.194e+02, threshold=3.260e+02, percent-clipped=1.0
2024-10-07 22:39:41,469 INFO [train.py:1153] Epoch 2, batch 50, loss[loss=0.4556, simple_loss=0.3734, pruned_loss=0.1931, ctc_loss=0.379, over 4909.00 frames. ], tot_loss[loss=0.5092, simple_loss=0.4104, pruned_loss=0.2126, ctc_loss=0.4571, over 217758.01 frames. ], batch size: 19, lr: 3.79e-02,
2024-10-07 22:39:41,623 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=2300.6666666666665, ans=0.39215625
2024-10-07 22:39:42,893 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=2300.6666666666665, ans=0.113725
2024-10-07 22:39:45,623 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2300.6666666666665, ans=0.2769933333333333
2024-10-07 22:40:11,565 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=2307.3333333333335, ans=0.048085
2024-10-07 22:40:24,376 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=2310.6666666666665, ans=0.08555833333333333
2024-10-07 22:40:38,659 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=2314.0, ans=0.39153125
2024-10-07 22:40:43,708 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2314.0, ans=0.27686
2024-10-07 22:40:45,990 INFO [train.py:1153] Epoch 2, batch 100, loss[loss=0.4615, simple_loss=0.3895, pruned_loss=0.1852, ctc_loss=0.4077, over 4755.00 frames. ], tot_loss[loss=0.5182, simple_loss=0.4164, pruned_loss=0.217, ctc_loss=0.4646, over 383327.18 frames. ], batch size: 19, lr: 3.78e-02,
2024-10-07 22:40:46,123 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=2317.3333333333335, ans=0.39137500000000003
2024-10-07 22:40:54,291 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=18.33 vs. limit=9.238
2024-10-07 22:40:54,318 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.42 vs. limit=8.369
2024-10-07 22:41:01,970 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.64 vs. limit=8.37025
2024-10-07 22:41:08,265 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=2320.6666666666665, ans=0.8187766666666667
2024-10-07 22:41:09,997 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.00 vs. limit=9.2405
2024-10-07 22:41:16,064 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer1.prob, batch_count=2324.0, ans=0.3910625
2024-10-07 22:41:17,874 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.92 vs. limit=5.5809999999999995
2024-10-07 22:41:18,018 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.17 vs. limit=5.5809999999999995
2024-10-07 22:41:18,759 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=2324.0, ans=0.3910625
2024-10-07 22:41:38,953 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.03 vs. limit=5.582666666666666
2024-10-07 22:41:39,165 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.63 vs. limit=3.3496
2024-10-07 22:41:39,717 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=2330.6666666666665, ans=0.39075
2024-10-07 22:41:42,360 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=2330.6666666666665, ans=0.1126
2024-10-07 22:41:46,188 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=2330.6666666666665, ans=0.04271666666666667
2024-10-07 22:41:48,732 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.141e+02 1.527e+02 1.740e+02 1.963e+02 3.556e+02, threshold=3.481e+02, percent-clipped=1.0
2024-10-07 22:41:51,253 INFO [train.py:1153] Epoch 2, batch 150, loss[loss=0.4942, simple_loss=0.395, pruned_loss=0.2083, ctc_loss=0.442, over 4908.00 frames. ], tot_loss[loss=0.504, simple_loss=0.4084, pruned_loss=0.2097, ctc_loss=0.4506, over 513287.79 frames. ], batch size: 19, lr: 3.77e-02,
2024-10-07 22:42:11,137 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=26.35 vs. limit=9.253
2024-10-07 22:42:17,173 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=2340.6666666666665, ans=0.39028125
2024-10-07 22:42:33,833 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=2344.0, ans=0.81796
2024-10-07 22:42:55,366 INFO [train.py:1153] Epoch 2, batch 200, loss[loss=0.5989, simple_loss=0.461, pruned_loss=0.2588, ctc_loss=0.5481, over 4761.00 frames. ], tot_loss[loss=0.4978, simple_loss=0.4047, pruned_loss=0.2067, ctc_loss=0.4438, over 613727.61 frames. ], batch size: 45, lr: 3.77e-02,
2024-10-07 22:43:03,323 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten.whitening_limit, batch_count=2350.6666666666665, ans=8.381499999999999
2024-10-07 22:43:25,255 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=2357.3333333333335, ans=0.3895
2024-10-07 22:43:31,004 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.93 vs. limit=6.1786666666666665
2024-10-07 22:43:31,626 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2357.3333333333335, ans=0.3895
2024-10-07 22:43:32,085 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=7.54 vs. limit=8.384
2024-10-07 22:43:38,442 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.96 vs. limit=9.2705
2024-10-07 22:43:49,336 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-07 22:43:50,019 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.88 vs. limit=5.591
2024-10-07 22:43:52,235 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=7.13 vs. limit=8.3865
2024-10-07 22:43:52,414 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.45 vs. limit=8.3865
2024-10-07 22:43:56,429 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.42 vs. limit=3.3546
2024-10-07 22:43:56,905 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.942e+01 1.501e+02 1.649e+02 1.845e+02 2.715e+02, threshold=3.298e+02, percent-clipped=0.0
2024-10-07 22:43:59,436 INFO [train.py:1153] Epoch 2, batch 250, loss[loss=0.5274, simple_loss=0.4013, pruned_loss=0.2294, ctc_loss=0.4868, over 4832.00 frames. ], tot_loss[loss=0.4954, simple_loss=0.4036, pruned_loss=0.2055, ctc_loss=0.4408, over 692427.42 frames. ], batch size: 38, lr: 3.76e-02,
2024-10-07 22:43:59,592 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=2367.3333333333335, ans=0.11122499999999999
2024-10-07 22:44:05,824 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.min_abs, batch_count=2367.3333333333335, ans=0.23551
2024-10-07 22:44:21,724 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=2370.6666666666665, ans=0.04666
2024-10-07 22:44:24,319 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=2.694e-01
2024-10-07 22:44:25,519 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=2374.0, ans=0.046585
2024-10-07 22:44:28,263 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=9.32 vs. limit=9.2805
2024-10-07 22:44:28,344 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.84 vs. limit=9.2805
2024-10-07 22:44:29,825 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.32 vs. limit=9.2805
2024-10-07 22:44:35,460 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=2374.0, ans=0.38871875
2024-10-07 22:44:40,471 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=2377.3333333333335, ans=0.2028333333333333
2024-10-07 22:44:43,000 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=2377.3333333333335, ans=0.11627499999999999
2024-10-07 22:44:59,893 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=14.17 vs. limit=8.39275
2024-10-07 22:45:03,022 INFO [train.py:1153] Epoch 2, batch 300, loss[loss=0.4983, simple_loss=0.3981, pruned_loss=0.2084, ctc_loss=0.4544, over 4786.00 frames. ], tot_loss[loss=0.493, simple_loss=0.4021, pruned_loss=0.2041, ctc_loss=0.4394, over 752845.68 frames. ], batch size: 32, lr: 3.75e-02,
2024-10-07 22:45:29,227 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=12.12 vs. limit=6.195333333333333
2024-10-07 22:45:29,902 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=2390.6666666666665, ans=0.20116666666666666
2024-10-07 22:45:36,440 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=2390.6666666666665, ans=0.3879375
2024-10-07 22:45:45,242 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=2394.0, ans=0.81621
2024-10-07 22:45:47,006 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.68 vs. limit=9.2955
2024-10-07 22:46:04,481 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.210e+02 1.539e+02 1.730e+02 1.932e+02 3.841e+02, threshold=3.460e+02, percent-clipped=2.0
2024-10-07 22:46:06,876 INFO [train.py:1153] Epoch 2, batch 350, loss[loss=0.4823, simple_loss=0.4017, pruned_loss=0.1937, ctc_loss=0.4389, over 4883.00 frames. ], tot_loss[loss=0.4908, simple_loss=0.4014, pruned_loss=0.2026, ctc_loss=0.4372, over 800170.97 frames. ], batch size: 19, lr: 3.75e-02,
2024-10-07 22:46:11,554 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=9.08 vs. limit=8.40025
2024-10-07 22:46:17,916 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.70 vs. limit=4.960266666666667
2024-10-07 22:46:19,838 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=2404.0, ans=0.81586
2024-10-07 22:46:28,866 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=2404.0, ans=0.81586
2024-10-07 22:46:32,048 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.22 vs. limit=9.3055
2024-10-07 22:46:35,369 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=2407.3333333333335, ans=0.045835
2024-10-07 22:46:45,426 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=2410.6666666666665, ans=0.19866666666666666
2024-10-07 22:46:58,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2414.0, ans=0.27586
2024-10-07 22:47:10,880 INFO [train.py:1153] Epoch 2, batch 400, loss[loss=0.4295, simple_loss=0.376, pruned_loss=0.1694, ctc_loss=0.3604, over 4879.00 frames. ], tot_loss[loss=0.4917, simple_loss=0.4024, pruned_loss=0.2033, ctc_loss=0.4357, over 836930.10 frames. ], batch size: 22, lr: 3.74e-02,
2024-10-07 22:47:35,715 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.20 vs. limit=6.212
2024-10-07 22:47:40,541 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2424.0, ans=0.27576
2024-10-07 22:47:51,158 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.03 vs. limit=6.213666666666667
2024-10-07 22:47:51,458 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.87 vs. limit=9.3205
2024-10-07 22:48:04,231 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.09 vs. limit=5.607666666666667
2024-10-07 22:48:05,774 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.72 vs. limit=5.607666666666667
2024-10-07 22:48:08,026 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.70 vs. limit=6.215333333333334
2024-10-07 22:48:08,765 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=2430.6666666666665, ans=0.04240416666666667
2024-10-07 22:48:10,499 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.85 vs. limit=5.607666666666667
2024-10-07 22:48:10,989 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.26 vs. limit=6.215333333333334
2024-10-07 22:48:12,438 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.144e+02 1.588e+02 1.754e+02 1.902e+02 5.298e+02, threshold=3.508e+02, percent-clipped=2.0
2024-10-07 22:48:15,011 INFO [train.py:1153] Epoch 2, batch 450, loss[loss=0.5008, simple_loss=0.4203, pruned_loss=0.2038, ctc_loss=0.4342, over 4871.00 frames. ], tot_loss[loss=0.4915, simple_loss=0.4029, pruned_loss=0.203, ctc_loss=0.435, over 865484.37 frames. ], batch size: 23, lr: 3.73e-02,
2024-10-07 22:48:15,617 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.64 vs. limit=8.412749999999999
2024-10-07 22:48:21,507 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=2434.0, ans=0.10872499999999999
2024-10-07 22:48:25,433 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=2434.0, ans=0.81481
2024-10-07 22:48:31,807 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2437.3333333333335, ans=0.38575
2024-10-07 22:48:45,549 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.51 vs. limit=6.2203333333333335
2024-10-07 22:48:59,376 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.82 vs. limit=5.611
2024-10-07 22:49:02,996 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=6.97 vs. limit=8.4165
2024-10-07 22:49:05,330 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2447.3333333333335, ans=0.38528125
2024-10-07 22:49:12,194 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.03 vs. limit=9.3355
2024-10-07 22:49:13,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.84 vs. limit=9.3355
2024-10-07 22:49:16,874 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=2447.3333333333335, ans=0.8143433333333333
2024-10-07 22:49:18,154 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2450.6666666666665, ans=0.2754933333333333
2024-10-07 22:49:19,227 INFO [train.py:1153] Epoch 2, batch 500, loss[loss=0.4759, simple_loss=0.4103, pruned_loss=0.186, ctc_loss=0.4239, over 4837.00 frames. ], tot_loss[loss=0.4871, simple_loss=0.4001, pruned_loss=0.2008, ctc_loss=0.4311, over 888098.43 frames. ], batch size: 34, lr: 3.73e-02,
2024-10-07 22:49:23,225 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.min_positive, batch_count=2450.6666666666665, ans=0.22549333333333332
2024-10-07 22:49:25,786 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2450.6666666666665, ans=0.2754933333333333
2024-10-07 22:49:33,324 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=2454.0, ans=0.38496874999999997
2024-10-07 22:49:33,335 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=2454.0, ans=0.38496874999999997
2024-10-07 22:49:46,032 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.min_positive, batch_count=2457.3333333333335, ans=0.042320833333333335
2024-10-07 22:50:01,825 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.40 vs. limit=3.3691
2024-10-07 22:50:10,463 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.69 vs. limit=9.348
2024-10-07 22:50:10,763 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=15.58 vs. limit=9.348
2024-10-07 22:50:20,196 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.184e+02 1.513e+02 1.717e+02 1.874e+02 3.835e+02, threshold=3.435e+02, percent-clipped=1.0
2024-10-07 22:50:22,650 INFO [train.py:1153] Epoch 2, batch 550, loss[loss=0.5136, simple_loss=0.4195, pruned_loss=0.2164, ctc_loss=0.4377, over 4790.00 frames. ], tot_loss[loss=0.4863, simple_loss=0.4002, pruned_loss=0.2003, ctc_loss=0.4299, over 905465.58 frames. ], batch size: 40, lr: 3.72e-02,
2024-10-07 22:50:35,007 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.46 vs. limit=8.4265
2024-10-07 22:50:41,134 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=9.10 vs. limit=8.4265
2024-10-07 22:50:52,132 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=2474.0, ans=0.044335
2024-10-07 22:51:04,824 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=2477.3333333333335, ans=0.8132933333333333
2024-10-07 22:51:06,200 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2477.3333333333335, ans=0.2752266666666667
2024-10-07 22:51:07,409 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-07 22:51:25,754 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.26 vs. limit=9.363
2024-10-07 22:51:26,538 INFO [train.py:1153] Epoch 2, batch 600, loss[loss=0.5181, simple_loss=0.4159, pruned_loss=0.2178, ctc_loss=0.4619, over 4810.00 frames. ], tot_loss[loss=0.4851, simple_loss=0.3998, pruned_loss=0.1996, ctc_loss=0.4283, over 919259.30 frames. ], batch size: 38, lr: 3.72e-02,
2024-10-07 22:51:37,655 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=10.83 vs. limit=9.363
2024-10-07 22:51:53,595 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2490.6666666666665, ans=0.2750933333333333
2024-10-07 22:52:04,969 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.min_positive, batch_count=2494.0, ans=0.04220625
2024-10-07 22:52:12,478 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=2494.0, ans=0.81271
2024-10-07 22:52:20,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=2497.3333333333335, ans=0.8125933333333334
2024-10-07 22:52:20,090 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2497.3333333333335, ans=0.27502666666666664
2024-10-07 22:52:23,698 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=2497.3333333333335, ans=0.3829375
2024-10-07 22:52:25,857 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.08 vs. limit=6.248666666666667
2024-10-07 22:52:27,379 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.164e+02 1.580e+02 1.679e+02 1.880e+02 3.358e+02, threshold=3.357e+02, percent-clipped=0.0
2024-10-07 22:52:30,017 INFO [train.py:1153] Epoch 2, batch 650, loss[loss=0.4076, simple_loss=0.3511, pruned_loss=0.1567, ctc_loss=0.3768, over 4843.00 frames. ], tot_loss[loss=0.4833, simple_loss=0.3984, pruned_loss=0.1988, ctc_loss=0.4264, over 930188.13 frames. ], batch size: 21, lr: 3.71e-02,
2024-10-07 22:52:30,177 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=2500.6666666666665, ans=0.18741666666666668
2024-10-07 22:52:32,208 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.75 vs. limit=6.250333333333334
2024-10-07 22:52:49,105 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2504.0, ans=0.27496
2024-10-07 22:53:05,956 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=2507.3333333333335, ans=0.38246875
2024-10-07 22:53:13,443 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=2510.6666666666665, ans=0.04351000000000001
2024-10-07 22:53:17,328 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=2510.6666666666665, ans=0.3823125
2024-10-07 22:53:28,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=2514.0, ans=0.22486
2024-10-07 22:53:33,640 INFO [train.py:1153] Epoch 2, batch 700, loss[loss=0.4736, simple_loss=0.407, pruned_loss=0.1882, ctc_loss=0.4093, over 4740.00 frames. ], tot_loss[loss=0.4807, simple_loss=0.3971, pruned_loss=0.1973, ctc_loss=0.4241, over 937971.52 frames. ], batch size: 19, lr: 3.70e-02,
2024-10-07 22:53:34,599 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.19 vs. limit=6.258666666666667
2024-10-07 22:53:36,669 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.45 vs. limit=3.3776
2024-10-07 22:53:38,772 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=2517.3333333333335, ans=0.10839999999999997
2024-10-07 22:53:39,396 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.56 vs. limit=9.388
2024-10-07 22:53:43,014 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.94 vs. limit=8.443999999999999
2024-10-07 22:53:52,843 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=2520.6666666666665, ans=0.8117766666666667
2024-10-07 22:53:56,126 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.27 vs. limit=3.3781
2024-10-07 22:54:03,334 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.93 vs. limit=9.393
2024-10-07 22:54:08,084 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=2524.0, ans=0.3816875
2024-10-07 22:54:20,816 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=2527.3333333333335, ans=0.043135
2024-10-07 22:54:23,861 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.27 vs. limit=9.398
2024-10-07 22:54:30,247 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.78 vs. limit=9.398
2024-10-07 22:54:34,751 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.062e+02 1.554e+02 1.672e+02 1.819e+02 2.975e+02, threshold=3.344e+02, percent-clipped=0.0
2024-10-07 22:54:34,889 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=2530.6666666666665, ans=0.381375
2024-10-07 22:54:37,174 INFO [train.py:1153] Epoch 2, batch 750, loss[loss=0.4465, simple_loss=0.3733, pruned_loss=0.1832, ctc_loss=0.3835, over 4894.00 frames. ], tot_loss[loss=0.4774, simple_loss=0.3953, pruned_loss=0.1957, ctc_loss=0.4201, over 944733.25 frames. ], batch size: 22, lr: 3.70e-02,
2024-10-07 22:54:40,402 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.02 vs. limit=8.45025
2024-10-07 22:55:23,286 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=2544.0, ans=0.182
2024-10-07 22:55:24,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.min_abs, batch_count=2544.0, ans=0.23816
2024-10-07 22:55:26,952 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2547.3333333333335, ans=0.27452666666666664
2024-10-07 22:55:27,545 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.22 vs. limit=5.018933333333333
2024-10-07 22:55:40,924 INFO [train.py:1153] Epoch 2, batch 800, loss[loss=0.4381, simple_loss=0.3561, pruned_loss=0.1819, ctc_loss=0.3907, over 4852.00 frames. ], tot_loss[loss=0.4761, simple_loss=0.3943, pruned_loss=0.1952, ctc_loss=0.4185, over 949630.50 frames. ], batch size: 19, lr: 3.69e-02,
2024-10-07 22:55:41,119 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2550.6666666666665, ans=0.2744933333333333
2024-10-07 22:55:42,751 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=12.26 vs. limit=8.4565
2024-10-07 22:55:47,301 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=2550.6666666666665, ans=0.8107266666666667
2024-10-07 22:55:48,047 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.90 vs. limit=5.637666666666666
2024-10-07 22:55:50,679 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.80 vs. limit=6.275333333333333
2024-10-07 22:55:52,509 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=2554.0, ans=0.042535
2024-10-07 22:56:00,553 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=19.28 vs. limit=8.45775
2024-10-07 22:56:24,680 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.50 vs. limit=3.3841
2024-10-07 22:56:28,659 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten.whitening_limit, batch_count=2560.6666666666665, ans=8.46025
2024-10-07 22:56:29,707 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.78 vs. limit=9.4205
2024-10-07 22:56:35,158 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.95 vs. limit=6.282
2024-10-07 22:56:41,996 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.181e+02 1.556e+02 1.693e+02 1.906e+02 3.503e+02, threshold=3.386e+02, percent-clipped=1.0
2024-10-07 22:56:44,653 INFO [train.py:1153] Epoch 2, batch 850, loss[loss=0.4826, simple_loss=0.3999, pruned_loss=0.1989, ctc_loss=0.4188, over 4804.00 frames. ], tot_loss[loss=0.4736, simple_loss=0.3926, pruned_loss=0.1941, ctc_loss=0.4164, over 953989.62 frames. ], batch size: 29, lr: 3.69e-02,
2024-10-07 22:56:57,932 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.32 vs. limit=3.3856
2024-10-07 22:56:58,997 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys.whitening_limit, batch_count=2570.6666666666665, ans=3.3856
2024-10-07 22:57:04,372 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.20 vs. limit=9.428
2024-10-07 22:57:06,376 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.min_abs, batch_count=2570.6666666666665, ans=0.23856
2024-10-07 22:57:06,420 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=2570.6666666666665, ans=0.23856
2024-10-07 22:57:06,801 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.72 vs. limit=5.642666666666667
2024-10-07 22:57:15,904 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.73 vs. limit=9.4305
2024-10-07 22:57:17,736 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2574.0, ans=0.37934375
2024-10-07 22:57:38,082 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=2580.6666666666665, ans=0.8096766666666667
2024-10-07 22:57:39,325 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=2580.6666666666665, ans=0.17741666666666667
2024-10-07 22:57:48,203 INFO [train.py:1153] Epoch 2, batch 900, loss[loss=0.5148, simple_loss=0.4206, pruned_loss=0.2174, ctc_loss=0.4357, over 4855.00 frames. ], tot_loss[loss=0.4734, simple_loss=0.3927, pruned_loss=0.1939, ctc_loss=0.416, over 956955.00 frames. ], batch size: 19, lr: 3.68e-02,
2024-10-07 22:57:49,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=2584.0, ans=0.378875
2024-10-07 22:58:08,733 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=2587.3333333333335, ans=0.37871875
2024-10-07 22:58:14,056 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.16 vs. limit=8.4715
2024-10-07 22:58:21,650 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.01 vs. limit=6.295333333333334
2024-10-07 22:58:23,966 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=2590.6666666666665, ans=0.04171
2024-10-07 22:58:40,808 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.42 vs. limit=3.3896
2024-10-07 22:58:43,606 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.53 vs. limit=8.474
2024-10-07 22:58:45,685 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=2597.3333333333335, ans=0.37825
2024-10-07 22:58:47,446 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.64 vs. limit=9.448
2024-10-07 22:58:48,475 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=2597.3333333333335, ans=0.37825
2024-10-07 22:58:49,512 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.254e+02 1.560e+02 1.749e+02 1.925e+02 4.039e+02, threshold=3.498e+02, percent-clipped=2.0
2024-10-07 22:58:51,976 INFO [train.py:1153] Epoch 2, batch 950, loss[loss=0.4562, simple_loss=0.3747, pruned_loss=0.1909, ctc_loss=0.3892, over 4817.00 frames. ], tot_loss[loss=0.4739, simple_loss=0.3929, pruned_loss=0.1941, ctc_loss=0.4164, over 958786.93 frames. ], batch size: 19, lr: 3.67e-02,
2024-10-07 22:59:00,978 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=2600.6666666666665, ans=0.37809375
2024-10-07 22:59:09,410 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.45 vs. limit=6.302
2024-10-07 22:59:10,092 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.52 vs. limit=8.4765
2024-10-07 22:59:14,306 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten.whitening_limit, batch_count=2604.0, ans=8.4765
2024-10-07 22:59:18,416 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.79 vs. limit=9.4555
2024-10-07 22:59:28,432 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=4.06 vs. limit=8.47775
2024-10-07 22:59:34,316 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2610.6666666666665, ans=0.2738933333333333
2024-10-07 22:59:44,514 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=2614.0, ans=0.37746875
2024-10-07 22:59:47,039 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=2614.0, ans=0.37746875
2024-10-07 22:59:55,843 INFO [train.py:1153] Epoch 2, batch 1000, loss[loss=0.3945, simple_loss=0.3576, pruned_loss=0.1479, ctc_loss=0.3392, over 4946.00 frames. ], tot_loss[loss=0.4762, simple_loss=0.3944, pruned_loss=0.1953, ctc_loss=0.4183, over 960713.58 frames. ], batch size: 20, lr: 3.67e-02,
2024-10-07 23:00:01,138 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=2617.3333333333335, ans=0.8083933333333334
2024-10-07 23:00:02,371 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=2617.3333333333335, ans=0.04111
2024-10-07 23:00:04,956 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=2617.3333333333335, ans=0.8083933333333334
2024-10-07 23:00:13,368 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.91 vs. limit=9.4655
2024-10-07 23:00:15,505 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=0.000e+00
2024-10-07 23:00:21,073 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.31 vs. limit=9.468
2024-10-07 23:00:22,919 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=2624.0, ans=0.22376
2024-10-07 23:00:26,861 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2624.0, ans=0.27376
2024-10-07 23:00:53,990 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=2630.6666666666665, ans=0.3766875
2024-10-07 23:00:57,567 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.216e+02 1.601e+02 1.726e+02 1.997e+02 3.167e+02, threshold=3.451e+02, percent-clipped=0.0
2024-10-07 23:00:58,562 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.56 vs. limit=9.473
2024-10-07 23:01:00,167 INFO [train.py:1153] Epoch 2, batch 1050, loss[loss=0.5342, simple_loss=0.428, pruned_loss=0.2306, ctc_loss=0.4479, over 4823.00 frames. ], tot_loss[loss=0.4734, simple_loss=0.393, pruned_loss=0.1939, ctc_loss=0.4151, over 962809.92 frames. ], batch size: 25, lr: 3.66e-02,
2024-10-07 23:01:11,316 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.31 vs. limit=3.3951000000000002
2024-10-07 23:01:23,501 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=2637.3333333333335, ans=0.1011
2024-10-07 23:01:28,765 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2640.6666666666665, ans=0.27359333333333336
2024-10-07 23:01:43,141 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=2644.0, ans=0.37606249999999997
2024-10-07 23:01:54,231 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.42 vs. limit=6.323666666666667
2024-10-07 23:02:03,781 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=2650.6666666666665, ans=0.37575000000000003
2024-10-07 23:02:04,290 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=512, metric=5.58 vs. limit=8.494
2024-10-07 23:02:04,957 INFO [train.py:1153] Epoch 2, batch 1100, loss[loss=0.4971, simple_loss=0.3879, pruned_loss=0.2172, ctc_loss=0.4301, over 4862.00 frames. ], tot_loss[loss=0.4716, simple_loss=0.392, pruned_loss=0.1929, ctc_loss=0.4134, over 964135.11 frames. ], batch size: 20, lr: 3.65e-02,
2024-10-07 23:02:06,921 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.13 vs. limit=9.488
2024-10-07 23:02:32,656 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=10.96 vs. limit=9.493
2024-10-07 23:02:35,794 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=2657.3333333333335, ans=0.3754375
2024-10-07 23:02:38,394 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=2657.3333333333335, ans=0.8069933333333333
2024-10-07 23:02:42,997 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.25 vs. limit=8.49775
2024-10-07 23:02:48,690 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=2660.6666666666665, ans=0.37528125
2024-10-07 23:02:56,704 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=17.52 vs. limit=8.499
2024-10-07 23:03:05,184 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/checkpoint-8000.pt
2024-10-07 23:03:07,226 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.095e+02 1.588e+02 1.735e+02 2.019e+02 4.267e+02, threshold=3.471e+02, percent-clipped=1.0
2024-10-07 23:03:09,394 INFO [train.py:1153] Epoch 2, batch 1150, loss[loss=0.4174, simple_loss=0.3561, pruned_loss=0.1672, ctc_loss=0.3608, over 4865.00 frames. ], tot_loss[loss=0.4731, simple_loss=0.3927, pruned_loss=0.194, ctc_loss=0.4137, over 964398.55 frames. ], batch size: 20, lr: 3.65e-02,
2024-10-07 23:03:09,509 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2667.3333333333335, ans=0.27332666666666666
2024-10-07 23:03:09,570 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=2667.3333333333335, ans=0.8066433333333334
2024-10-07 23:03:29,397 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2670.6666666666665, ans=0.27329333333333333
2024-10-07 23:03:30,706 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=2670.6666666666665, ans=0.3748125
2024-10-07 23:03:34,485 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2674.0, ans=0.27326
2024-10-07 23:03:36,872 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2674.0, ans=0.37465625
2024-10-07 23:04:09,684 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=2680.6666666666665, ans=0.22319333333333333
2024-10-07 23:04:12,135 INFO [train.py:1153] Epoch 2, batch 1200, loss[loss=0.4773, simple_loss=0.395, pruned_loss=0.1971, ctc_loss=0.4134, over 4808.00 frames. ], tot_loss[loss=0.4744, simple_loss=0.3936, pruned_loss=0.1946, ctc_loss=0.4146, over 964262.33 frames. ], batch size: 25, lr: 3.64e-02,
2024-10-07 23:04:15,966 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=2684.0, ans=0.099025
2024-10-07 23:04:18,425 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=2684.0, ans=0.3741875
2024-10-07 23:04:19,810 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=2684.0, ans=0.099025
2024-10-07 23:04:20,214 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.59 vs. limit=8.506499999999999
2024-10-07 23:04:39,086 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.47 vs. limit=3.4036
2024-10-07 23:04:39,261 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.22 vs. limit=8.509
2024-10-07 23:04:40,115 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=2690.6666666666665, ans=6.681666666666667
2024-10-07 23:04:40,974 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.06 vs. limit=6.3453333333333335
2024-10-07 23:04:42,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=2690.6666666666665, ans=0.8058266666666667
2024-10-07 23:05:12,959 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.225e+02 1.621e+02 1.739e+02 1.998e+02 3.677e+02, threshold=3.478e+02, percent-clipped=2.0
2024-10-07 23:05:15,426 INFO [train.py:1153] Epoch 2, batch 1250, loss[loss=0.4948, simple_loss=0.4026, pruned_loss=0.2051, ctc_loss=0.4419, over 4745.00 frames. ], tot_loss[loss=0.4731, simple_loss=0.393, pruned_loss=0.1939, ctc_loss=0.4134, over 964185.52 frames. ], batch size: 32, lr: 3.64e-02,
2024-10-07 23:05:16,075 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.29 vs. limit=9.525500000000001
2024-10-07 23:05:19,775 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.52 vs. limit=8.51275
2024-10-07 23:05:27,086 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2704.0, ans=0.27296
2024-10-07 23:05:28,820 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.72 vs. limit=9.528
2024-10-07 23:05:40,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=2707.3333333333335, ans=0.37309375
2024-10-07 23:05:43,410 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=2707.3333333333335, ans=0.37309375
2024-10-07 23:05:43,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2707.3333333333335, ans=0.37309375
2024-10-07 23:05:43,980 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.34 vs. limit=8.51525
2024-10-07 23:05:46,329 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten.whitening_limit, batch_count=2707.3333333333335, ans=8.51525
2024-10-07 23:06:05,173 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2714.0, ans=0.27286
2024-10-07 23:06:11,976 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.29 vs. limit=8.51775
2024-10-07 23:06:13,486 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=6.10 vs. limit=8.51775
2024-10-07 23:06:18,711 INFO [train.py:1153] Epoch 2, batch 1300, loss[loss=0.5193, simple_loss=0.4136, pruned_loss=0.2235, ctc_loss=0.4447, over 4826.00 frames. ], tot_loss[loss=0.4729, simple_loss=0.3929, pruned_loss=0.194, ctc_loss=0.4126, over 965636.55 frames. ], batch size: 43, lr: 3.63e-02,
2024-10-07 23:06:18,823 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=2717.3333333333335, ans=0.372625
2024-10-07 23:06:20,533 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.57 vs. limit=9.538
2024-10-07 23:06:25,646 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.36 vs. limit=8.519
2024-10-07 23:06:25,750 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=10.59 vs. limit=9.538
2024-10-07 23:06:27,022 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=10.39 vs. limit=9.538
2024-10-07 23:06:40,544 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=2720.6666666666665, ans=0.038785
2024-10-07 23:06:42,070 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.49 vs. limit=8.52025
2024-10-07 23:06:42,583 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.06 vs. limit=9.5405
2024-10-07 23:07:01,240 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.88 vs. limit=9.5455
2024-10-07 23:07:10,599 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.14 vs. limit=9.548
2024-10-07 23:07:15,220 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-07 23:07:15,869 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=7.18 vs. limit=8.524000000000001
2024-10-07 23:07:20,097 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.169e+02 1.637e+02 1.780e+02 2.003e+02 3.361e+02, threshold=3.559e+02, percent-clipped=0.0
2024-10-07 23:07:20,322 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=2730.6666666666665, ans=0.24096
2024-10-07 23:07:21,749 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=9.99 vs. limit=9.5505
2024-10-07 23:07:22,615 INFO [train.py:1153] Epoch 2, batch 1350, loss[loss=0.5075, simple_loss=0.4121, pruned_loss=0.2124, ctc_loss=0.4455, over 4833.00 frames. ], tot_loss[loss=0.4714, simple_loss=0.3922, pruned_loss=0.1932, ctc_loss=0.4107, over 966306.21 frames. ], batch size: 21, lr: 3.62e-02,
2024-10-07 23:07:22,741 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=2734.0, ans=0.37184375000000003
2024-10-07 23:07:27,026 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.62 vs. limit=8.52525
2024-10-07 23:07:29,240 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=2734.0, ans=0.0962125
2024-10-07 23:07:35,564 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2737.3333333333335, ans=0.27262666666666663
2024-10-07 23:08:07,592 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=2744.0, ans=0.80396
2024-10-07 23:08:14,537 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=10.62 vs. limit=9.560500000000001
2024-10-07 23:08:16,563 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=2747.3333333333335, ans=0.37121875
2024-10-07 23:08:17,685 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2747.3333333333335, ans=0.27252666666666664
2024-10-07 23:08:21,601 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=2747.3333333333335, ans=0.09697499999999999
2024-10-07 23:08:22,186 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.74 vs. limit=6.373666666666667
2024-10-07 23:08:26,659 INFO [train.py:1153] Epoch 2, batch 1400, loss[loss=0.4044, simple_loss=0.3533, pruned_loss=0.1541, ctc_loss=0.3683, over 4940.00 frames. ], tot_loss[loss=0.4704, simple_loss=0.3916, pruned_loss=0.1927, ctc_loss=0.4096, over 966654.78 frames. ], batch size: 19, lr: 3.62e-02,
2024-10-07 23:08:26,930 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-07 23:08:33,177 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=2750.6666666666665, ans=0.37106249999999996
2024-10-07 23:08:35,551 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2750.6666666666665, ans=0.2724933333333333
2024-10-07 23:08:36,914 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=2750.6666666666665, ans=0.15616666666666668
2024-10-07 23:08:37,370 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=10.53 vs. limit=9.563
2024-10-07 23:08:39,916 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=10.60 vs. limit=9.5655
2024-10-07 23:08:42,006 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2754.0, ans=0.37090625
2024-10-07 23:09:02,463 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.43 vs. limit=3.4136
2024-10-07 23:09:08,445 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=2760.6666666666665, ans=0.096475
2024-10-07 23:09:08,727 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.35 vs. limit=3.4141
2024-10-07 23:09:11,479 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=15.11 vs. limit=8.53525
2024-10-07 23:09:12,287 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=2760.6666666666665, ans=6.725416666666667
2024-10-07 23:09:14,095 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.42 vs. limit=3.4141
2024-10-07 23:09:18,484 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=2764.0, ans=0.3704375
2024-10-07 23:09:19,756 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=2764.0, ans=0.3704375
2024-10-07 23:09:27,216 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.023e+02 1.635e+02 1.860e+02 2.197e+02 5.529e+02, threshold=3.721e+02, percent-clipped=2.0
2024-10-07 23:09:29,317 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.09 vs. limit=9.5755
2024-10-07 23:09:29,825 INFO [train.py:1153] Epoch 2, batch 1450, loss[loss=0.4741, simple_loss=0.3938, pruned_loss=0.1969, ctc_loss=0.4013, over 4795.00 frames. ], tot_loss[loss=0.4701, simple_loss=0.3911, pruned_loss=0.1928, ctc_loss=0.4089, over 966538.71 frames. ], batch size: 34, lr: 3.61e-02,
2024-10-07 23:09:34,116 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=41.62 vs. limit=8.537749999999999
2024-10-07 23:09:39,995 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=2767.3333333333335, ans=0.09622499999999999
2024-10-07 23:09:54,117 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=2774.0, ans=0.36996875
2024-10-07 23:09:56,598 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=2774.0, ans=0.36996875
2024-10-07 23:10:06,805 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2777.3333333333335, ans=0.3698125
2024-10-07 23:10:20,764 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=2780.6666666666665, ans=0.8026766666666667
2024-10-07 23:10:26,464 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.09 vs. limit=9.5855
2024-10-07 23:10:33,454 INFO [train.py:1153] Epoch 2, batch 1500, loss[loss=0.4219, simple_loss=0.3607, pruned_loss=0.1689, ctc_loss=0.363, over 4751.00 frames. ], tot_loss[loss=0.4712, simple_loss=0.3919, pruned_loss=0.1932, ctc_loss=0.4099, over 966209.40 frames. ], batch size: 26, lr: 3.61e-02,
2024-10-07 23:10:39,071 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.67 vs. limit=9.588000000000001
2024-10-07 23:10:47,882 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.46 vs. limit=8.54525
2024-10-07 23:10:51,308 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=2787.3333333333335, ans=0.037285
2024-10-07 23:10:58,264 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.68 vs. limit=9.593
2024-10-07 23:11:00,499 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.82 vs. limit=9.593
2024-10-07 23:11:04,022 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=2790.6666666666665, ans=0.8023266666666667
2024-10-07 23:11:19,257 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2794.0, ans=0.27205999999999997
2024-10-07 23:11:23,602 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.88 vs. limit=5.699333333333334
2024-10-07 23:11:31,897 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.min_abs, batch_count=2797.3333333333335, ans=0.24196
2024-10-07 23:11:34,397 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.261e+02 1.643e+02 1.763e+02 2.031e+02 4.023e+02, threshold=3.526e+02, percent-clipped=2.0
2024-10-07 23:11:36,809 INFO [train.py:1153] Epoch 2, batch 1550, loss[loss=0.5467, simple_loss=0.43, pruned_loss=0.2346, ctc_loss=0.4853, over 4840.00 frames. ], tot_loss[loss=0.4703, simple_loss=0.3914, pruned_loss=0.1928, ctc_loss=0.409, over 966004.73 frames. ], batch size: 31, lr: 3.60e-02,
2024-10-07 23:11:50,366 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.47 vs. limit=8.5515
2024-10-07 23:12:00,485 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=33.59 vs. limit=9.603
2024-10-07 23:12:02,851 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=6.78 vs. limit=8.55275
2024-10-07 23:12:06,341 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=2807.3333333333335, ans=0.03683499999999999
2024-10-07 23:12:09,004 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2807.3333333333335, ans=0.36840625
2024-10-07 23:12:20,924 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.04 vs. limit=6.405333333333333
2024-10-07 23:12:21,232 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.05 vs. limit=5.7026666666666666
2024-10-07 23:12:28,296 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.13 vs. limit=9.6105
2024-10-07 23:12:40,660 INFO [train.py:1153] Epoch 2, batch 1600, loss[loss=0.4475, simple_loss=0.3924, pruned_loss=0.1748, ctc_loss=0.3824, over 4799.00 frames. ], tot_loss[loss=0.4692, simple_loss=0.3914, pruned_loss=0.192, ctc_loss=0.4075, over 966251.12 frames. ], batch size: 25, lr: 3.59e-02,
2024-10-07 23:12:45,118 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.80 vs. limit=5.7043333333333335
2024-10-07 23:13:06,038 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=2824.0, ans=0.367625
2024-10-07 23:13:08,572 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=2824.0, ans=0.367625
2024-10-07 23:13:15,533 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.75 vs. limit=9.618
2024-10-07 23:13:16,305 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2824.0, ans=0.27176
2024-10-07 23:13:19,261 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.53 vs. limit=9.6205
2024-10-07 23:13:24,222 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=20.74 vs. limit=9.6205
2024-10-07 23:13:25,043 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=2827.3333333333335, ans=0.09899494936611666
2024-10-07 23:13:28,872 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=2827.3333333333335, ans=0.8010433333333333
2024-10-07 23:13:33,920 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=2830.6666666666665, ans=0.09385
2024-10-07 23:13:39,254 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=2830.6666666666665, ans=0.04115416666666667
2024-10-07 23:13:41,653 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.292e+02 1.641e+02 1.803e+02 2.005e+02 3.412e+02, threshold=3.606e+02, percent-clipped=0.0
2024-10-07 23:13:44,157 INFO [train.py:1153] Epoch 2, batch 1650, loss[loss=0.4998, simple_loss=0.4083, pruned_loss=0.2083, ctc_loss=0.4369, over 4780.00 frames. ], tot_loss[loss=0.4677, simple_loss=0.3904, pruned_loss=0.1913, ctc_loss=0.4062, over 966771.45 frames. ], batch size: 29, lr: 3.59e-02,
2024-10-07 23:13:51,442 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.07 vs. limit=9.6255
2024-10-07 23:13:52,376 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.71 vs. limit=8.56275
2024-10-07 23:13:59,860 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2837.3333333333335, ans=0.2716266666666667
2024-10-07 23:13:59,921 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2837.3333333333335, ans=0.2716266666666667
2024-10-07 23:14:02,385 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=2837.3333333333335, ans=0.14533333333333331
2024-10-07 23:14:04,419 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.17 vs. limit=6.418666666666667
2024-10-07 23:14:08,014 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.35 vs. limit=9.628
2024-10-07 23:14:17,163 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.00 vs. limit=9.6305
2024-10-07 23:14:17,838 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=2840.6666666666665, ans=0.36684375
2024-10-07 23:14:21,947 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.11 vs. limit=8.56525
2024-10-07 23:14:25,893 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2844.0, ans=0.27155999999999997
2024-10-07 23:14:38,393 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=2847.3333333333335, ans=0.27152666666666664
2024-10-07 23:14:45,912 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=2847.3333333333335, ans=0.36653125
2024-10-07 23:14:47,957 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.31 vs. limit=6.425333333333333
2024-10-07 23:14:48,398 INFO [train.py:1153] Epoch 2, batch 1700, loss[loss=0.3955, simple_loss=0.3553, pruned_loss=0.1528, ctc_loss=0.3252, over 4940.00 frames. ], tot_loss[loss=0.4648, simple_loss=0.3888, pruned_loss=0.1898, ctc_loss=0.4031, over 966817.97 frames. ], batch size: 19, lr: 3.58e-02,
2024-10-07 23:15:08,824 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=2854.0, ans=0.36621875000000004
2024-10-07 23:15:28,407 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=21.26 vs. limit=8.57275
2024-10-07 23:15:29,730 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=68.00 vs. limit=8.57275
2024-10-07 23:15:47,185 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=2864.0, ans=0.09259999999999999
2024-10-07 23:15:49,847 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.326e+02 1.648e+02 1.880e+02 2.201e+02 5.941e+02, threshold=3.760e+02, percent-clipped=4.0
2024-10-07 23:15:52,350 INFO [train.py:1153] Epoch 2, batch 1750, loss[loss=0.378, simple_loss=0.3419, pruned_loss=0.1457, ctc_loss=0.3068, over 4959.00 frames. ], tot_loss[loss=0.4641, simple_loss=0.3883, pruned_loss=0.1895, ctc_loss=0.4022, over 967075.83 frames. ], batch size: 19, lr: 3.58e-02,
2024-10-07 23:15:57,113 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.79 vs. limit=9.650500000000001
2024-10-07 23:16:04,757 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module2.whiten, num_groups=1, num_channels=192, metric=3.13 vs. limit=8.5765
2024-10-07 23:16:05,364 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=2870.6666666666665, ans=0.08852499999999999
2024-10-07 23:16:19,527 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=10.37 vs. limit=9.6555
2024-10-07 23:16:56,339 INFO [train.py:1153] Epoch 2, batch 1800, loss[loss=0.3972, simple_loss=0.3654, pruned_loss=0.1473, ctc_loss=0.3357, over 4842.00 frames. ], tot_loss[loss=0.4651, simple_loss=0.3896, pruned_loss=0.1899, ctc_loss=0.4022, over 967791.06 frames. ], batch size: 23, lr: 3.57e-02,
2024-10-07 23:16:56,900 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.63 vs. limit=3.4326
2024-10-07 23:17:22,526 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.22 vs. limit=9.668
2024-10-07 23:17:24,596 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=2890.6666666666665, ans=0.3645
2024-10-07 23:17:50,603 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=2897.3333333333335, ans=0.08702499999999996
2024-10-07 23:17:57,503 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.44 vs. limit=6.448666666666667
2024-10-07 23:17:58,144 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.228e+02 1.662e+02 1.772e+02 1.977e+02 2.580e+02, threshold=3.543e+02, percent-clipped=0.0
2024-10-07 23:17:59,651 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-07 23:18:00,701 INFO [train.py:1153] Epoch 2, batch 1850, loss[loss=0.5096, simple_loss=0.4185, pruned_loss=0.2071, ctc_loss=0.4663, over 4736.00 frames. ], tot_loss[loss=0.4649, simple_loss=0.3898, pruned_loss=0.1896, ctc_loss=0.4019, over 967990.35 frames. ], batch size: 26, lr: 3.57e-02,
2024-10-07 23:18:07,357 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=2900.6666666666665, ans=0.36403125000000003
2024-10-07 23:18:16,623 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.38 vs. limit=9.678
2024-10-07 23:18:24,229 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.66 vs. limit=3.4356
2024-10-07 23:18:24,354 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.08 vs. limit=8.589
2024-10-07 23:18:29,081 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=2907.3333333333335, ans=0.36371875
2024-10-07 23:18:46,952 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=2910.6666666666665, ans=0.24366000000000002
2024-10-07 23:19:04,643 INFO [train.py:1153] Epoch 2, batch 1900, loss[loss=0.4401, simple_loss=0.3784, pruned_loss=0.1733, ctc_loss=0.388, over 4788.00 frames. ], tot_loss[loss=0.4631, simple_loss=0.3885, pruned_loss=0.1887, ctc_loss=0.4007, over 967833.03 frames. ], batch size: 29, lr: 3.56e-02,
2024-10-07 23:19:11,359 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.40 vs. limit=8.594
2024-10-07 23:19:25,011 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=2920.6666666666665, ans=0.36309375
2024-10-07 23:19:29,171 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.07 vs. limit=9.693
2024-10-07 23:19:54,816 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=20.49 vs. limit=9.698
2024-10-07 23:19:56,886 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2930.6666666666665, ans=0.362625
2024-10-07 23:20:05,601 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.336e+02 1.656e+02 1.831e+02 2.080e+02 4.359e+02, threshold=3.662e+02, percent-clipped=2.0
2024-10-07 23:20:08,178 INFO [train.py:1153] Epoch 2, batch 1950, loss[loss=0.4805, simple_loss=0.4063, pruned_loss=0.1932, ctc_loss=0.4209, over 4866.00 frames. ], tot_loss[loss=0.4645, simple_loss=0.3893, pruned_loss=0.1895, ctc_loss=0.4015, over 966712.50 frames. ], batch size: 20, lr: 3.55e-02,
2024-10-07 23:20:46,130 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=2944.0, ans=0.362
2024-10-07 23:20:52,731 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=2944.0, ans=0.362
2024-10-07 23:21:03,023 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=2947.3333333333335, ans=0.36184375
2024-10-07 23:21:08,687 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=10.85 vs. limit=9.7105
2024-10-07 23:21:11,922 INFO [train.py:1153] Epoch 2, batch 2000, loss[loss=0.3866, simple_loss=0.3472, pruned_loss=0.1455, ctc_loss=0.3376, over 4959.00 frames. ], tot_loss[loss=0.4641, simple_loss=0.3889, pruned_loss=0.1894, ctc_loss=0.4012, over 966453.06 frames. ], batch size: 19, lr: 3.55e-02,
2024-10-07 23:21:12,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=2950.6666666666665, ans=0.3616875
2024-10-07 23:21:12,799 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.85 vs. limit=5.737666666666667
2024-10-07 23:21:29,795 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.min_positive, batch_count=2954.0, ans=0.0815375
2024-10-07 23:21:46,500 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2957.3333333333335, ans=0.361375
2024-10-07 23:21:51,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=2960.6666666666665, ans=0.36121875000000003
2024-10-07 23:21:57,758 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=2960.6666666666665, ans=0.36121875000000003
2024-10-07 23:21:58,293 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.75 vs. limit=9.7205
2024-10-07 23:22:00,703 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.06 vs. limit=5.740166666666667
2024-10-07 23:22:02,068 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.76 vs. limit=9.722999999999999
2024-10-07 23:22:12,878 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.201e+02 1.691e+02 1.810e+02 2.051e+02 3.940e+02, threshold=3.619e+02, percent-clipped=1.0
2024-10-07 23:22:15,364 INFO [train.py:1153] Epoch 2, batch 2050, loss[loss=0.42, simple_loss=0.3661, pruned_loss=0.1629, ctc_loss=0.3702, over 4914.00 frames. ], tot_loss[loss=0.4623, simple_loss=0.3878, pruned_loss=0.1885, ctc_loss=0.3996, over 966856.73 frames. ], batch size: 19, lr: 3.54e-02,
2024-10-07 23:22:22,444 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten.whitening_limit, batch_count=2967.3333333333335, ans=9.7255
2024-10-07 23:22:29,561 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2970.6666666666665, ans=0.27029333333333333
2024-10-07 23:22:32,152 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=2970.6666666666665, ans=0.36075
2024-10-07 23:22:33,949 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.04 vs. limit=5.742666666666667
2024-10-07 23:22:38,783 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.46 vs. limit=3.4455999999999998
2024-10-07 23:22:43,578 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=2974.0, ans=0.088475
2024-10-07 23:22:45,993 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=2974.0, ans=0.033085
2024-10-07 23:23:13,619 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=2980.6666666666665, ans=0.36028125
2024-10-07 23:23:19,755 INFO [train.py:1153] Epoch 2, batch 2100, loss[loss=0.481, simple_loss=0.4019, pruned_loss=0.1972, ctc_loss=0.4141, over 4842.00 frames. ], tot_loss[loss=0.4598, simple_loss=0.3867, pruned_loss=0.187, ctc_loss=0.3971, over 967073.81 frames. ], batch size: 21, lr: 3.54e-02,
2024-10-07 23:23:30,089 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=10.29 vs. limit=9.738
2024-10-07 23:24:20,555 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.251e+02 1.736e+02 1.934e+02 2.264e+02 5.554e+02, threshold=3.869e+02, percent-clipped=2.0
2024-10-07 23:24:22,995 INFO [train.py:1153] Epoch 2, batch 2150, loss[loss=0.4141, simple_loss=0.371, pruned_loss=0.1572, ctc_loss=0.3572, over 4869.00 frames. ], tot_loss[loss=0.4574, simple_loss=0.3855, pruned_loss=0.186, ctc_loss=0.3931, over 967927.80 frames. ], batch size: 20, lr: 3.53e-02,
2024-10-07 23:24:24,493 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=3000.6666666666665, ans=0.032485
2024-10-07 23:24:37,083 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=3004.0, ans=0.3591875
2024-10-07 23:24:44,006 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.08 vs. limit=5.751
2024-10-07 23:24:45,970 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=3004.0, ans=0.79486
2024-10-07 23:24:46,699 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1.whitening_limit, batch_count=3004.0, ans=5.751
2024-10-07 23:24:58,785 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=3007.3333333333335, ans=0.09899494936611666
2024-10-07 23:25:00,049 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=3010.6666666666665, ans=0.03226
2024-10-07 23:25:26,801 INFO [train.py:1153] Epoch 2, batch 2200, loss[loss=0.428, simple_loss=0.3627, pruned_loss=0.1765, ctc_loss=0.3511, over 4746.00 frames. ], tot_loss[loss=0.4559, simple_loss=0.3844, pruned_loss=0.1855, ctc_loss=0.3914, over 967667.89 frames. ], batch size: 26, lr: 3.52e-02,
2024-10-07 23:25:49,364 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.68 vs. limit=5.755166666666667
2024-10-07 23:25:54,201 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=10.88 vs. limit=9.768
2024-10-07 23:26:07,583 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=3027.3333333333335, ans=0.7940433333333333
2024-10-07 23:26:09,156 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.20 vs. limit=9.7705
2024-10-07 23:26:13,483 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.27 vs. limit=9.7705
2024-10-07 23:26:27,544 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.204e+02 1.624e+02 1.758e+02 1.967e+02 2.961e+02, threshold=3.516e+02, percent-clipped=0.0
2024-10-07 23:26:30,266 INFO [train.py:1153] Epoch 2, batch 2250, loss[loss=0.4466, simple_loss=0.3879, pruned_loss=0.1745, ctc_loss=0.3904, over 4876.00 frames. ], tot_loss[loss=0.4549, simple_loss=0.3843, pruned_loss=0.1847, ctc_loss=0.3901, over 967733.55 frames. ], batch size: 22, lr: 3.52e-02,
2024-10-07 23:26:33,272 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=3034.0, ans=9.775500000000001
2024-10-07 23:26:34,171 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=3034.0, ans=0.086225
2024-10-07 23:26:34,222 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=3034.0, ans=0.35778125
2024-10-07 23:26:44,493 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=3037.3333333333335, ans=0.0861
2024-10-07 23:27:06,391 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=3040.6666666666665, ans=0.35746875
2024-10-07 23:27:09,059 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=3044.0, ans=0.35731250000000003
2024-10-07 23:27:24,300 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=3047.3333333333335, ans=0.08095416666666667
2024-10-07 23:27:25,587 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=3047.3333333333335, ans=0.7804733333333334
2024-10-07 23:27:26,893 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=3047.3333333333335, ans=0.7933433333333334
2024-10-07 23:27:34,387 INFO [train.py:1153] Epoch 2, batch 2300, loss[loss=0.3903, simple_loss=0.3463, pruned_loss=0.1489, ctc_loss=0.3412, over 4883.00 frames. ], tot_loss[loss=0.452, simple_loss=0.383, pruned_loss=0.1832, ctc_loss=0.3867, over 968263.98 frames. ], batch size: 19, lr: 3.51e-02,
2024-10-07 23:27:38,797 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.16 vs. limit=9.788
2024-10-07 23:27:41,084 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=3050.6666666666665, ans=0.08560000000000001
2024-10-07 23:28:13,177 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=3060.6666666666665, ans=0.08522500000000001
2024-10-07 23:28:15,596 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=3060.6666666666665, ans=0.7928766666666667
2024-10-07 23:28:23,599 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.66 vs. limit=8.64775
2024-10-07 23:28:28,349 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=3064.0, ans=6.915
2024-10-07 23:28:28,426 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=3064.0, ans=0.040425
2024-10-07 23:28:32,179 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=3064.0, ans=0.03105999999999999
2024-10-07 23:28:35,995 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.286e+02 1.604e+02 1.791e+02 2.059e+02 4.059e+02, threshold=3.582e+02, percent-clipped=1.0
2024-10-07 23:28:37,458 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=3067.3333333333335, ans=0.07746249999999999
2024-10-07 23:28:38,543 INFO [train.py:1153] Epoch 2, batch 2350, loss[loss=0.4351, simple_loss=0.377, pruned_loss=0.1747, ctc_loss=0.3594, over 4844.00 frames. ], tot_loss[loss=0.4515, simple_loss=0.3823, pruned_loss=0.1831, ctc_loss=0.3866, over 968291.76 frames. ], batch size: 23, lr: 3.51e-02,
2024-10-07 23:28:45,213 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer_ff3.min_abs, batch_count=3067.3333333333335, ans=0.15336666666666668
2024-10-07 23:28:52,819 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=3070.6666666666665, ans=0.3560625
2024-10-07 23:28:52,827 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.prob, batch_count=3070.6666666666665, ans=0.3560625
2024-10-07 23:28:57,846 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=3070.6666666666665, ans=0.3560625
2024-10-07 23:28:59,127 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=3070.6666666666665, ans=0.3560625
2024-10-07 23:29:15,732 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=3077.3333333333335, ans=0.35575
2024-10-07 23:29:20,682 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=3077.3333333333335, ans=0.07689999999999997
2024-10-07 23:29:21,553 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.26 vs. limit=9.808
2024-10-07 23:29:23,740 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.85 vs. limit=8.654
2024-10-07 23:29:32,154 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=3080.6666666666665, ans=0.08447500000000001
2024-10-07 23:29:37,787 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.54 vs. limit=9.810500000000001
2024-10-07 23:29:42,133 INFO [train.py:1153] Epoch 2, batch 2400, loss[loss=0.4152, simple_loss=0.3611, pruned_loss=0.163, ctc_loss=0.3582, over 4755.00 frames. ], tot_loss[loss=0.4523, simple_loss=0.3827, pruned_loss=0.1837, ctc_loss=0.3864, over 967510.32 frames. ], batch size: 19, lr: 3.50e-02,
2024-10-07 23:29:55,921 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=3087.3333333333335, ans=0.35528125
2024-10-07 23:30:03,722 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten.whitening_limit, batch_count=3087.3333333333335, ans=9.8155
2024-10-07 23:30:11,205 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=4.896e-02
2024-10-07 23:30:14,832 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=5.103e-01
2024-10-07 23:30:15,228 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.16 vs. limit=9.818
2024-10-07 23:30:18,630 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=3094.0, ans=0.35496875
2024-10-07 23:30:25,548 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.70 vs. limit=9.8205
2024-10-07 23:30:30,362 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.42 vs. limit=3.4641
2024-10-07 23:30:42,583 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.412e+02 1.737e+02 1.896e+02 2.148e+02 3.909e+02, threshold=3.792e+02, percent-clipped=1.0
2024-10-07 23:30:45,180 INFO [train.py:1153] Epoch 2, batch 2450, loss[loss=0.4345, simple_loss=0.3793, pruned_loss=0.1728, ctc_loss=0.3599, over 4886.00 frames. ], tot_loss[loss=0.4547, simple_loss=0.3845, pruned_loss=0.1848, ctc_loss=0.388, over 966929.13 frames. ], batch size: 22, lr: 3.50e-02,
2024-10-07 23:30:58,681 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.66 vs. limit=8.664
2024-10-07 23:31:00,766 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=3104.0, ans=0.35450000000000004
2024-10-07 23:31:07,225 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=3104.0, ans=0.7913600000000001
2024-10-07 23:31:08,064 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.97 vs. limit=5.776
2024-10-07 23:31:22,594 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=3110.6666666666665, ans=0.3541875
2024-10-07 23:31:32,797 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=3110.6666666666665, ans=0.3541875
2024-10-07 23:31:36,875 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=10.27 vs. limit=9.8355
2024-10-07 23:31:49,277 INFO [train.py:1153] Epoch 2, batch 2500, loss[loss=0.5056, simple_loss=0.4042, pruned_loss=0.2198, ctc_loss=0.4187, over 4741.00 frames. ], tot_loss[loss=0.4539, simple_loss=0.3842, pruned_loss=0.1845, ctc_loss=0.3863, over 966616.28 frames. ], batch size: 26, lr: 3.49e-02,
2024-10-07 23:32:00,370 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=10.95 vs. limit=9.838000000000001
2024-10-07 23:32:05,660 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=3.94 vs. limit=8.67025
2024-10-07 23:32:12,420 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=3120.6666666666665, ans=0.08297500000000001
2024-10-07 23:32:12,774 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=5.03 vs. limit=5.248266666666667
2024-10-07 23:32:19,952 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=2.883e-01
2024-10-07 23:32:41,429 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=3130.6666666666665, ans=0.02955999999999999
2024-10-07 23:32:42,177 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.70 vs. limit=6.565333333333333
2024-10-07 23:32:46,529 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=1.286e+00
2024-10-07 23:32:50,193 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.303e+02 1.688e+02 1.859e+02 2.096e+02 3.769e+02, threshold=3.719e+02, percent-clipped=0.0
2024-10-07 23:32:50,934 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.50 vs. limit=8.674
2024-10-07 23:32:52,641 INFO [train.py:1153] Epoch 2, batch 2550, loss[loss=0.4464, simple_loss=0.378, pruned_loss=0.18, ctc_loss=0.3872, over 4959.00 frames. ], tot_loss[loss=0.4547, simple_loss=0.3845, pruned_loss=0.1853, ctc_loss=0.3859, over 967028.66 frames. ], batch size: 19, lr: 3.48e-02,
2024-10-07 23:32:59,046 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=1.107e-02
2024-10-07 23:32:59,089 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=3134.0, ans=0.35309375
2024-10-07 23:33:02,713 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=3134.0, ans=0.029484999999999997
2024-10-07 23:33:05,221 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=3137.3333333333335, ans=0.3529375
2024-10-07 23:33:15,363 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=3137.3333333333335, ans=0.08234999999999999
2024-10-07 23:33:24,041 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=3140.6666666666665, ans=6.962916666666667
2024-10-07 23:33:33,826 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.36 vs. limit=5.786
2024-10-07 23:33:34,270 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=3144.0, ans=0.08209999999999999
2024-10-07 23:33:34,760 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.43 vs. limit=9.858
2024-10-07 23:33:46,113 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.34 vs. limit=9.8605
2024-10-07 23:33:47,298 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.60 vs. limit=9.8605
2024-10-07 23:33:55,052 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=10.89 vs. limit=9.863
2024-10-07 23:33:55,622 INFO [train.py:1153] Epoch 2, batch 2600, loss[loss=0.4855, simple_loss=0.407, pruned_loss=0.2041, ctc_loss=0.3896, over 4852.00 frames. ], tot_loss[loss=0.4534, simple_loss=0.3839, pruned_loss=0.1846, ctc_loss=0.3843, over 966506.11 frames. ], batch size: 20, lr: 3.48e-02,
2024-10-07 23:34:00,873 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=3150.6666666666665, ans=0.7897266666666667
2024-10-07 23:34:04,626 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3150.6666666666665, ans=0.2684933333333333
2024-10-07 23:34:05,961 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=3150.6666666666665, ans=0.2684933333333333
2024-10-07 23:34:08,455 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=3154.0, ans=0.10575000000000001
2024-10-07 23:34:25,620 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=11.71 vs. limit=8.684
2024-10-07 23:34:26,380 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=3157.3333333333335, ans=0.352
2024-10-07 23:34:33,843 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=3160.6666666666665, ans=0.35184375
2024-10-07 23:34:52,896 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=3164.0, ans=0.7892600000000001
2024-10-07 23:34:54,493 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.90 vs. limit=9.873000000000001
2024-10-07 23:34:56,501 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.348e+02 1.673e+02 1.882e+02 2.134e+02 3.322e+02, threshold=3.764e+02, percent-clipped=0.0
2024-10-07 23:34:58,433 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.25 vs. limit=5.791833333333333
2024-10-07 23:34:59,058 INFO [train.py:1153] Epoch 2, batch 2650, loss[loss=0.4943, simple_loss=0.3999, pruned_loss=0.2116, ctc_loss=0.4136, over 4824.00 frames. ], tot_loss[loss=0.4537, simple_loss=0.3834, pruned_loss=0.185, ctc_loss=0.3848, over 966162.46 frames. ], batch size: 38, lr: 3.47e-02,
2024-10-07 23:35:09,903 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.06 vs. limit=8.68775
2024-10-07 23:35:34,876 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=3174.0, ans=0.08097499999999999
2024-10-07 23:35:45,373 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.92 vs. limit=8.6915
2024-10-07 23:35:51,655 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.09 vs. limit=9.8855
2024-10-07 23:35:58,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=3180.6666666666665, ans=0.10241666666666671
2024-10-07 23:36:02,486 INFO [train.py:1153] Epoch 2, batch 2700, loss[loss=0.4147, simple_loss=0.3521, pruned_loss=0.1657, ctc_loss=0.3644, over 4865.00 frames. ], tot_loss[loss=0.4523, simple_loss=0.3832, pruned_loss=0.184, ctc_loss=0.3834, over 966280.62 frames. ], batch size: 28, lr: 3.47e-02,
2024-10-07 23:36:21,123 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.02 vs. limit=9.8905
2024-10-07 23:36:34,712 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=3190.6666666666665, ans=0.08035
2024-10-07 23:36:46,792 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.20 vs. limit=9.8955
2024-10-07 23:36:51,201 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=3194.0, ans=0.08003750000000001
2024-10-07 23:36:51,296 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=3194.0, ans=0.7882100000000001
2024-10-07 23:37:00,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer1.prob, batch_count=3197.3333333333335, ans=0.350125
2024-10-07 23:37:02,937 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=3197.3333333333335, ans=0.350125
2024-10-07 23:37:03,886 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.262e+02 1.572e+02 1.790e+02 2.018e+02 3.673e+02, threshold=3.580e+02, percent-clipped=0.0
2024-10-07 23:37:04,678 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.78 vs. limit=9.898
2024-10-07 23:37:06,379 INFO [train.py:1153] Epoch 2, batch 2750, loss[loss=0.4442, simple_loss=0.3875, pruned_loss=0.1765, ctc_loss=0.3697, over 4800.00 frames. ], tot_loss[loss=0.4488, simple_loss=0.3816, pruned_loss=0.1821, ctc_loss=0.3793, over 966928.59 frames. ], batch size: 19, lr: 3.46e-02,
2024-10-07 23:37:14,326 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=3200.6666666666665, ans=0.7879766666666668
2024-10-07 23:37:18,041 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.min_positive, batch_count=3204.0, ans=0.0399875
2024-10-07 23:37:24,297 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=3204.0, ans=0.34981249999999997
2024-10-07 23:37:30,418 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=3207.3333333333335, ans=0.26792666666666665
2024-10-07 23:37:39,628 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=57.32 vs. limit=8.70275
2024-10-07 23:37:50,611 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=3210.6666666666665, ans=0.34950000000000003
2024-10-07 23:37:56,053 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.32 vs. limit=9.910499999999999
2024-10-07 23:37:56,344 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2.whitening_limit, batch_count=3214.0, ans=6.607
2024-10-07 23:37:58,646 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.44 vs. limit=3.4821
2024-10-07 23:38:03,474 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.39 vs. limit=9.910499999999999
2024-10-07 23:38:03,627 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.43 vs. limit=9.910499999999999
2024-10-07 23:38:09,269 INFO [train.py:1153] Epoch 2, batch 2800, loss[loss=0.4976, simple_loss=0.4171, pruned_loss=0.2065, ctc_loss=0.4129, over 4755.00 frames. ], tot_loss[loss=0.4511, simple_loss=0.3826, pruned_loss=0.1836, ctc_loss=0.3812, over 967102.57 frames. ], batch size: 53, lr: 3.46e-02,
2024-10-07 23:38:10,733 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=3217.3333333333335, ans=0.07934999999999999
2024-10-07 23:38:20,936 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.84 vs. limit=5.805166666666667
2024-10-07 23:38:27,721 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.30 vs. limit=6.610333333333333
2024-10-07 23:38:29,073 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=7.06 vs. limit=8.70775
2024-10-07 23:38:33,276 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=3224.0, ans=0.348875
2024-10-07 23:38:33,754 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.90 vs. limit=9.918
2024-10-07 23:38:37,708 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=6.34 vs. limit=8.709
2024-10-07 23:38:39,583 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=3224.0, ans=0.348875
2024-10-07 23:38:39,590 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=3224.0, ans=0.7871600000000001
2024-10-07 23:38:58,697 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=3230.6666666666665, ans=0.07885
2024-10-07 23:39:03,078 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.80 vs. limit=9.923
2024-10-07 23:39:10,027 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.293e+02 1.734e+02 1.897e+02 2.129e+02 3.243e+02, threshold=3.795e+02, percent-clipped=0.0
2024-10-07 23:39:12,665 INFO [train.py:1153] Epoch 2, batch 2850, loss[loss=0.4041, simple_loss=0.3655, pruned_loss=0.157, ctc_loss=0.3221, over 4938.00 frames. ], tot_loss[loss=0.4539, simple_loss=0.3837, pruned_loss=0.1852, ctc_loss=0.3843, over 966836.47 frames. ], batch size: 20, lr: 3.45e-02,
2024-10-07 23:39:13,436 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.90 vs. limit=9.9255
2024-10-07 23:39:17,429 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.48 vs. limit=9.9255
2024-10-07 23:39:47,671 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=3240.6666666666665, ans=0.34809375
2024-10-07 23:39:49,155 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=3240.6666666666665, ans=9.9305
2024-10-07 23:39:58,787 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.35 vs. limit=9.933
2024-10-07 23:40:11,111 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.07 vs. limit=6.623666666666667
2024-10-07 23:40:15,535 INFO [train.py:1153] Epoch 2, batch 2900, loss[loss=0.4061, simple_loss=0.3697, pruned_loss=0.1532, ctc_loss=0.3408, over 4747.00 frames. ], tot_loss[loss=0.4541, simple_loss=0.3839, pruned_loss=0.1853, ctc_loss=0.3845, over 965928.43 frames. ], batch size: 20, lr: 3.45e-02,
2024-10-07 23:40:16,872 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=3250.6666666666665, ans=0.34762499999999996
2024-10-07 23:40:23,589 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.66 vs. limit=5.3002666666666665
2024-10-07 23:40:32,415 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.42 vs. limit=9.9405
2024-10-07 23:40:35,852 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=3254.0, ans=0.07797499999999999
2024-10-07 23:40:42,655 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=10.48 vs. limit=9.943
2024-10-07 23:40:44,275 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.51 vs. limit=9.943
2024-10-07 23:40:53,249 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=4.25 vs. limit=4.6521333333333335
2024-10-07 23:40:57,501 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.min_positive, batch_count=3260.6666666666665, ans=0.07962083333333334
2024-10-07 23:41:08,874 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=3264.0, ans=0.347
2024-10-07 23:41:11,864 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.74 vs. limit=8.724
2024-10-07 23:41:15,951 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.52 vs. limit=8.724
2024-10-07 23:41:16,547 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.298e+02 1.664e+02 1.836e+02 2.021e+02 2.816e+02, threshold=3.671e+02, percent-clipped=0.0
2024-10-07 23:41:18,916 INFO [train.py:1153] Epoch 2, batch 2950, loss[loss=0.4323, simple_loss=0.3753, pruned_loss=0.1724, ctc_loss=0.3616, over 4799.00 frames. ], tot_loss[loss=0.4514, simple_loss=0.3827, pruned_loss=0.1837, ctc_loss=0.3814, over 966518.31 frames. ], batch size: 19, lr: 3.44e-02,
2024-10-07 23:41:29,749 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.10 vs. limit=9.9505
2024-10-07 23:41:42,205 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=3270.6666666666665, ans=0.34668750000000004
2024-10-07 23:41:42,307 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.max_abs, batch_count=3270.6666666666665, ans=7.0441666666666665
2024-10-07 23:41:46,839 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2.whitening_limit, batch_count=3274.0, ans=6.6370000000000005
2024-10-07 23:42:02,972 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten.whitening_limit, batch_count=3277.3333333333335, ans=9.958
2024-10-07 23:42:16,457 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=3280.6666666666665, ans=0.07
2024-10-07 23:42:22,687 INFO [train.py:1153] Epoch 2, batch 3000, loss[loss=0.3634, simple_loss=0.3344, pruned_loss=0.139, ctc_loss=0.2857, over 4834.00 frames. ], tot_loss[loss=0.4505, simple_loss=0.382, pruned_loss=0.1833, ctc_loss=0.381, over 967210.30 frames. ], batch size: 21, lr: 3.43e-02,
2024-10-07 23:42:22,687 INFO [train.py:1176] Computing validation loss
2024-10-07 23:42:27,379 INFO [zipformer.py:1858] name=encoder.encoders.3.encoder.layers.3.self_attn_weights, attn_weights_entropy = tensor([1.4398, 2.3721, 1.8850, 1.9242, 1.5396, 2.2452, 2.4824, 1.4306],
       device='cuda:0')
2024-10-07 23:42:30,100 INFO [train.py:1185] Epoch 2, validation: loss=0.2894, simple_loss=0.331, pruned_loss=0.08822, ctc_loss=0.1784, over 90464.00 frames.
2024-10-07 23:42:30,101 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-07 23:42:37,064 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.53 vs. limit=9.963000000000001
2024-10-07 23:42:41,305 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=3287.3333333333335, ans=0.34590624999999997
2024-10-07 23:42:41,353 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=3287.3333333333335, ans=0.08908333333333329
2024-10-07 23:42:46,153 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=3287.3333333333335, ans=0.07945416666666667
2024-10-07 23:42:54,799 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=3290.6666666666665, ans=0.0766
2024-10-07 23:43:00,024 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=3290.6666666666665, ans=7.056666666666667
2024-10-07 23:43:03,822 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=3290.6666666666665, ans=0.0766
2024-10-07 23:43:08,237 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.26 vs. limit=6.647
2024-10-07 23:43:08,790 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=3294.0, ans=0.07647499999999999
2024-10-07 23:43:13,104 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.26 vs. limit=6.647
2024-10-07 23:43:25,533 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.50 vs. limit=9.972999999999999
2024-10-07 23:43:26,977 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.40 vs. limit=6.648666666666667
2024-10-07 23:43:29,822 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.277e+02 1.649e+02 1.911e+02 2.118e+02 6.503e+02, threshold=3.822e+02, percent-clipped=2.0
2024-10-07 23:43:32,364 INFO [train.py:1153] Epoch 2, batch 3050, loss[loss=0.403, simple_loss=0.3671, pruned_loss=0.1531, ctc_loss=0.3313, over 4749.00 frames. ], tot_loss[loss=0.4494, simple_loss=0.3814, pruned_loss=0.1828, ctc_loss=0.3798, over 966712.08 frames. ], batch size: 19, lr: 3.43e-02,
2024-10-07 23:43:43,217 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.72 vs. limit=9.9755
2024-10-07 23:43:48,898 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=3304.0, ans=7.0649999999999995
2024-10-07 23:43:50,214 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3304.0, ans=0.26696
2024-10-07 23:43:54,070 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3304.0, ans=0.26696
2024-10-07 23:43:56,647 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.min_positive, batch_count=3307.3333333333335, ans=0.21692666666666666
2024-10-07 23:43:57,959 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=3307.3333333333335, ans=0.24961
2024-10-07 23:43:59,621 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=10.11 vs. limit=9.9805
2024-10-07 23:44:01,602 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=3307.3333333333335, ans=0.24961
2024-10-07 23:44:09,750 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.13 vs. limit=9.983
2024-10-07 23:44:11,749 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=3310.6666666666665, ans=0.34481249999999997
2024-10-07 23:44:14,730 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.45 vs. limit=9.983
2024-10-07 23:44:15,475 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=3310.6666666666665, ans=0.08616666666666667
2024-10-07 23:44:24,678 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.22 vs. limit=6.657
2024-10-07 23:44:29,385 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-07 23:44:35,635 INFO [train.py:1153] Epoch 2, batch 3100, loss[loss=0.4772, simple_loss=0.3861, pruned_loss=0.1965, ctc_loss=0.4385, over 4817.00 frames. ], tot_loss[loss=0.4476, simple_loss=0.3808, pruned_loss=0.1816, ctc_loss=0.3779, over 966447.38 frames. ], batch size: 38, lr: 3.42e-02,
2024-10-07 23:44:35,839 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=3317.3333333333335, ans=0.7838933333333333
2024-10-07 23:44:39,158 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.56 vs. limit=9.988
2024-10-07 23:44:44,789 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=3317.3333333333335, ans=0.26682666666666666
2024-10-07 23:45:03,322 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.17 vs. limit=5.8309999999999995
2024-10-07 23:45:06,762 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.67 vs. limit=9.993
2024-10-07 23:45:07,640 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=3324.0, ans=0.3441875
2024-10-07 23:45:24,059 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3327.3333333333335, ans=0.26672666666666667
2024-10-07 23:45:25,886 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.67 vs. limit=5.832666666666666
2024-10-07 23:45:36,651 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.368e+02 1.625e+02 1.812e+02 2.072e+02 3.599e+02, threshold=3.624e+02, percent-clipped=0.0
2024-10-07 23:45:39,063 INFO [train.py:1153] Epoch 2, batch 3150, loss[loss=0.4849, simple_loss=0.3883, pruned_loss=0.2067, ctc_loss=0.4203, over 4796.00 frames. ], tot_loss[loss=0.4466, simple_loss=0.3805, pruned_loss=0.1812, ctc_loss=0.3762, over 966706.57 frames. ], batch size: 40, lr: 3.42e-02,
2024-10-07 23:45:59,849 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.02 vs. limit=10.003
2024-10-07 23:46:09,541 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-07 23:46:33,507 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=3347.3333333333335, ans=0.34309375
2024-10-07 23:46:42,183 INFO [train.py:1153] Epoch 2, batch 3200, loss[loss=0.4314, simple_loss=0.3895, pruned_loss=0.1661, ctc_loss=0.3528, over 4758.00 frames. ], tot_loss[loss=0.4458, simple_loss=0.3805, pruned_loss=0.1808, ctc_loss=0.374, over 967163.60 frames. ], batch size: 20, lr: 3.41e-02,
2024-10-07 23:46:42,804 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.16 vs. limit=10.013
2024-10-07 23:46:47,438 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=3350.6666666666665, ans=0.07435
2024-10-07 23:46:48,603 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=3350.6666666666665, ans=0.3429375
2024-10-07 23:46:48,939 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.66 vs. limit=10.013
2024-10-07 23:47:13,915 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=3357.3333333333335, ans=0.5
2024-10-07 23:47:19,378 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=4.76 vs. limit=5.344266666666666
2024-10-07 23:47:28,189 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=3360.6666666666665, ans=0.03949791666666667
2024-10-07 23:47:28,228 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=3360.6666666666665, ans=0.34246875
2024-10-07 23:47:38,070 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=3364.0, ans=0.3423125
2024-10-07 23:47:42,724 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.245e+02 1.653e+02 1.861e+02 2.090e+02 3.756e+02, threshold=3.723e+02, percent-clipped=1.0
2024-10-07 23:47:45,190 INFO [train.py:1153] Epoch 2, batch 3250, loss[loss=0.5208, simple_loss=0.4245, pruned_loss=0.218, ctc_loss=0.4526, over 4860.00 frames. ], tot_loss[loss=0.4454, simple_loss=0.3805, pruned_loss=0.1807, ctc_loss=0.3721, over 967266.74 frames. ], batch size: 24, lr: 3.41e-02,
2024-10-07 23:47:52,125 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.49 vs. limit=10.025500000000001
2024-10-07 23:48:05,313 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=3370.6666666666665, ans=0.0736
2024-10-07 23:48:05,363 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.prob, batch_count=3370.6666666666665, ans=0.34199999999999997
2024-10-07 23:48:30,961 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=8.36 vs. limit=8.7665
2024-10-07 23:48:46,952 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=3384.0, ans=0.0731
2024-10-07 23:48:48,020 INFO [train.py:1153] Epoch 2, batch 3300, loss[loss=0.4824, simple_loss=0.3988, pruned_loss=0.2007, ctc_loss=0.4117, over 4840.00 frames. ], tot_loss[loss=0.4422, simple_loss=0.3785, pruned_loss=0.1792, ctc_loss=0.3689, over 967749.25 frames. ], batch size: 43, lr: 3.40e-02,
2024-10-07 23:49:03,796 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.45 vs. limit=8.77025
2024-10-07 23:49:07,659 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=15.97 vs. limit=8.77025
2024-10-07 23:49:16,423 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.64 vs. limit=3.5086
2024-10-07 23:49:22,983 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1.whitening_limit, batch_count=3390.6666666666665, ans=5.847666666666667
2024-10-07 23:49:42,602 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=3397.3333333333335, ans=0.34075
2024-10-07 23:49:44,466 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.28 vs. limit=10.048
2024-10-07 23:49:48,882 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.292e+02 1.760e+02 1.965e+02 2.246e+02 3.663e+02, threshold=3.929e+02, percent-clipped=0.0
2024-10-07 23:49:51,316 INFO [train.py:1153] Epoch 2, batch 3350, loss[loss=0.4625, simple_loss=0.3941, pruned_loss=0.1907, ctc_loss=0.3733, over 4799.00 frames. ], tot_loss[loss=0.4455, simple_loss=0.3802, pruned_loss=0.1811, ctc_loss=0.3714, over 967044.17 frames. ], batch size: 40, lr: 3.40e-02,
2024-10-07 23:50:07,186 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.70 vs. limit=10.053
2024-10-07 23:50:10,593 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=3404.0, ans=0.04949747468305833
2024-10-07 23:50:29,613 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=3410.6666666666665, ans=0.340125
2024-10-07 23:50:30,764 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=3410.6666666666665, ans=0.0721
2024-10-07 23:50:40,878 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=3414.0, ans=0.33996875
2024-10-07 23:50:52,330 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=3414.0, ans=0.023184999999999997
2024-10-07 23:50:54,765 INFO [train.py:1153] Epoch 2, batch 3400, loss[loss=0.353, simple_loss=0.3401, pruned_loss=0.1271, ctc_loss=0.2794, over 4959.00 frames. ], tot_loss[loss=0.4436, simple_loss=0.379, pruned_loss=0.18, ctc_loss=0.3706, over 966834.69 frames. ], batch size: 19, lr: 3.39e-02,
2024-10-07 23:51:19,210 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.18 vs. limit=8.784
2024-10-07 23:51:30,035 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3424.0, ans=0.26576
2024-10-07 23:51:31,242 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=3427.3333333333335, ans=0.07147499999999998
2024-10-07 23:51:55,305 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.255e+02 1.701e+02 1.909e+02 2.206e+02 4.237e+02, threshold=3.818e+02, percent-clipped=2.0
2024-10-07 23:51:56,856 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=3434.0, ans=0.33903125
2024-10-07 23:51:57,846 INFO [train.py:1153] Epoch 2, batch 3450, loss[loss=0.4751, simple_loss=0.391, pruned_loss=0.1973, ctc_loss=0.4118, over 4836.00 frames. ], tot_loss[loss=0.4432, simple_loss=0.3785, pruned_loss=0.1799, ctc_loss=0.3707, over 967029.08 frames. ], batch size: 43, lr: 3.39e-02,
2024-10-07 23:52:04,271 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3434.0, ans=0.26566
2024-10-07 23:52:08,674 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.98 vs. limit=5.8585
2024-10-07 23:52:19,452 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=3437.3333333333335, ans=0.33887500000000004
2024-10-07 23:52:23,770 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.05 vs. limit=5.860166666666666
2024-10-07 23:52:25,715 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=3440.6666666666665, ans=0.07097500000000001
2024-10-07 23:53:01,230 INFO [train.py:1153] Epoch 2, batch 3500, loss[loss=0.4269, simple_loss=0.3802, pruned_loss=0.1706, ctc_loss=0.3308, over 4883.00 frames. ], tot_loss[loss=0.4433, simple_loss=0.3781, pruned_loss=0.1801, ctc_loss=0.3709, over 967387.59 frames. ], batch size: 19, lr: 3.38e-02,
2024-10-07 23:53:19,761 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=8.43 vs. limit=8.79525
2024-10-07 23:53:23,101 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.18 vs. limit=10.0905
2024-10-07 23:53:30,363 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=3457.3333333333335, ans=0.7789933333333333
2024-10-07 23:53:38,074 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.min_positive, batch_count=3460.6666666666665, ans=0.07837083333333333
2024-10-07 23:53:43,581 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=7.84 vs. limit=8.79775
2024-10-07 23:53:59,820 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=3464.0, ans=0.067
2024-10-07 23:54:01,760 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.21 vs. limit=10.097999999999999
2024-10-07 23:54:02,493 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.327e+02 1.653e+02 1.854e+02 2.120e+02 4.342e+02, threshold=3.708e+02, percent-clipped=1.0
2024-10-07 23:54:04,869 INFO [train.py:1153] Epoch 2, batch 3550, loss[loss=0.4192, simple_loss=0.3549, pruned_loss=0.1716, ctc_loss=0.3507, over 4795.00 frames. ], tot_loss[loss=0.443, simple_loss=0.3784, pruned_loss=0.1799, ctc_loss=0.3698, over 967440.72 frames. ], batch size: 29, lr: 3.37e-02,
2024-10-07 23:54:05,344 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.95 vs. limit=10.1005
2024-10-07 23:54:10,059 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=3467.3333333333335, ans=0.07832916666666667
2024-10-07 23:54:25,639 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=10.72 vs. limit=10.103
2024-10-07 23:54:26,443 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=3470.6666666666665, ans=0.09899494936611666
2024-10-07 23:54:29,482 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=3474.0, ans=0.06972500000000001
2024-10-07 23:55:02,160 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=17.59 vs. limit=10.1105
2024-10-07 23:55:07,520 INFO [train.py:1153] Epoch 2, batch 3600, loss[loss=0.406, simple_loss=0.373, pruned_loss=0.1552, ctc_loss=0.3218, over 4925.00 frames. ], tot_loss[loss=0.4424, simple_loss=0.3784, pruned_loss=0.1794, ctc_loss=0.3689, over 967477.28 frames. ], batch size: 20, lr: 3.37e-02,
2024-10-07 23:55:08,934 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=3484.0, ans=0.02160999999999999
2024-10-07 23:55:15,908 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.35 vs. limit=10.113
2024-10-07 23:55:32,031 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.95 vs. limit=8.809
2024-10-07 23:55:36,841 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=3490.6666666666665, ans=0.0636666666666667
2024-10-07 23:56:05,452 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.98 vs. limit=10.123000000000001
2024-10-07 23:56:08,471 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.320e+02 1.675e+02 1.816e+02 2.028e+02 3.129e+02, threshold=3.633e+02, percent-clipped=0.0
2024-10-07 23:56:08,985 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.89 vs. limit=10.123000000000001
2024-10-07 23:56:11,058 INFO [train.py:1153] Epoch 2, batch 3650, loss[loss=0.4792, simple_loss=0.4057, pruned_loss=0.1952, ctc_loss=0.406, over 4851.00 frames. ], tot_loss[loss=0.4416, simple_loss=0.3782, pruned_loss=0.1789, ctc_loss=0.3677, over 967979.47 frames. ], batch size: 31, lr: 3.36e-02,
2024-10-07 23:56:12,575 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=3500.6666666666665, ans=0.06872500000000001
2024-10-07 23:56:23,111 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.48 vs. limit=10.128
2024-10-07 23:56:31,462 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=3504.0, ans=0.062
2024-10-07 23:56:44,668 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.62 vs. limit=3.5261
2024-10-07 23:56:47,393 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.91 vs. limit=8.81525
2024-10-07 23:57:14,448 INFO [train.py:1153] Epoch 2, batch 3700, loss[loss=0.4302, simple_loss=0.3694, pruned_loss=0.1707, ctc_loss=0.3744, over 4853.00 frames. ], tot_loss[loss=0.4412, simple_loss=0.3782, pruned_loss=0.1787, ctc_loss=0.367, over 967367.50 frames. ], batch size: 24, lr: 3.36e-02,
2024-10-07 23:57:16,313 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.07 vs. limit=5.879333333333333
2024-10-07 23:57:24,683 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=3517.3333333333335, ans=0.335125
2024-10-07 23:57:27,801 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.78 vs. limit=10.1405
2024-10-07 23:57:41,550 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.61 vs. limit=10.143
2024-10-07 23:57:47,304 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=3524.0, ans=0.3348125
2024-10-07 23:57:48,832 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=10.03 vs. limit=10.143
2024-10-07 23:57:56,949 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.09 vs. limit=8.82275
2024-10-07 23:57:56,991 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.55 vs. limit=10.1455
2024-10-07 23:57:57,498 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=3527.3333333333335, ans=0.33465625
2024-10-07 23:58:03,375 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.54 vs. limit=6.7636666666666665
2024-10-07 23:58:11,437 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=3530.6666666666665, ans=0.09899494936611666
2024-10-07 23:58:15,178 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.225e+02 1.712e+02 1.880e+02 2.054e+02 2.937e+02, threshold=3.759e+02, percent-clipped=0.0
2024-10-07 23:58:16,626 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=3534.0, ans=0.07
2024-10-07 23:58:17,654 INFO [train.py:1153] Epoch 2, batch 3750, loss[loss=0.4003, simple_loss=0.3477, pruned_loss=0.1604, ctc_loss=0.3304, over 4959.00 frames. ], tot_loss[loss=0.4393, simple_loss=0.3774, pruned_loss=0.1778, ctc_loss=0.3643, over 967811.19 frames. ], batch size: 19, lr: 3.35e-02,
2024-10-07 23:58:28,086 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=3534.0, ans=0.25301
2024-10-07 23:58:34,579 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=3537.3333333333335, ans=0.06734999999999997
2024-10-07 23:58:35,851 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=3537.3333333333335, ans=0.7761933333333334
2024-10-07 23:59:04,833 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=3544.0, ans=0.06709999999999999
2024-10-07 23:59:11,579 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=5.34 vs. limit=8.83025
2024-10-07 23:59:13,545 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=3547.3333333333335, ans=0.06697499999999998
2024-10-07 23:59:20,967 INFO [train.py:1153] Epoch 2, batch 3800, loss[loss=0.4449, simple_loss=0.3741, pruned_loss=0.1815, ctc_loss=0.3817, over 4774.00 frames. ], tot_loss[loss=0.4365, simple_loss=0.3756, pruned_loss=0.1765, ctc_loss=0.3611, over 967504.90 frames. ], batch size: 26, lr: 3.35e-02,
2024-10-07 23:59:31,522 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=3550.6666666666665, ans=0.26449333333333336
2024-10-07 23:59:35,559 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.68 vs. limit=10.1655
2024-10-07 23:59:36,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=3554.0, ans=0.33340625
2024-10-07 23:59:39,700 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.80 vs. limit=10.1655
2024-10-07 23:59:41,728 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=3554.0, ans=0.33340625
2024-10-07 23:59:43,462 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.74 vs. limit=10.1655
2024-10-07 23:59:45,621 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=3557.3333333333335, ans=0.33325
2024-10-07 23:59:49,914 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.83 vs. limit=10.168
2024-10-08 00:00:06,309 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.42 vs. limit=6.780333333333333
2024-10-08 00:00:07,602 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=10.69 vs. limit=10.1705
2024-10-08 00:00:08,468 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=3560.6666666666665, ans=0.33309374999999997
2024-10-08 00:00:22,323 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.339e+02 1.655e+02 1.823e+02 2.158e+02 5.213e+02, threshold=3.645e+02, percent-clipped=1.0
2024-10-08 00:00:24,901 INFO [train.py:1153] Epoch 2, batch 3850, loss[loss=0.3984, simple_loss=0.3516, pruned_loss=0.1592, ctc_loss=0.3167, over 4821.00 frames. ], tot_loss[loss=0.4341, simple_loss=0.3742, pruned_loss=0.1753, ctc_loss=0.3582, over 967557.35 frames. ], batch size: 38, lr: 3.34e-02,
2024-10-08 00:00:34,548 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=5.00 vs. limit=8.83775
2024-10-08 00:00:56,687 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3574.0, ans=0.26426
2024-10-08 00:01:02,324 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.52 vs. limit=3.5366
2024-10-08 00:01:02,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=3577.3333333333335, ans=0.7857733333333333
2024-10-08 00:01:10,651 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=3577.3333333333335, ans=0.25366
2024-10-08 00:01:11,981 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3577.3333333333335, ans=0.26422666666666667
2024-10-08 00:01:12,264 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.86 vs. limit=10.183
2024-10-08 00:01:13,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=3577.3333333333335, ans=0.03882083333333333
2024-10-08 00:01:18,564 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3580.6666666666665, ans=0.26419333333333334
2024-10-08 00:01:28,349 INFO [train.py:1153] Epoch 2, batch 3900, loss[loss=0.4268, simple_loss=0.3716, pruned_loss=0.1717, ctc_loss=0.3467, over 4751.00 frames. ], tot_loss[loss=0.4356, simple_loss=0.3751, pruned_loss=0.1763, ctc_loss=0.3587, over 967006.82 frames. ], batch size: 26, lr: 3.34e-02,
2024-10-08 00:01:46,934 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=3587.3333333333335, ans=0.048212499999999964
2024-10-08 00:01:49,367 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=3587.3333333333335, ans=0.33184375
2024-10-08 00:01:51,871 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=3590.6666666666665, ans=0.3316875
2024-10-08 00:02:01,000 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.41 vs. limit=8.8465
2024-10-08 00:02:07,484 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.56 vs. limit=10.1955
2024-10-08 00:02:11,427 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=10.88 vs. limit=10.1955
2024-10-08 00:02:21,124 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=3597.3333333333335, ans=0.7740933333333333
2024-10-08 00:02:22,451 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=3597.3333333333335, ans=0.7740933333333333
2024-10-08 00:02:28,679 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.244e+02 1.682e+02 1.792e+02 1.981e+02 3.316e+02, threshold=3.584e+02, percent-clipped=0.0
2024-10-08 00:02:29,300 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.46 vs. limit=10.198
2024-10-08 00:02:31,067 INFO [train.py:1153] Epoch 2, batch 3950, loss[loss=0.4857, simple_loss=0.408, pruned_loss=0.2046, ctc_loss=0.3853, over 4837.00 frames. ], tot_loss[loss=0.4303, simple_loss=0.3722, pruned_loss=0.1735, ctc_loss=0.3532, over 967330.14 frames. ], batch size: 36, lr: 3.33e-02,
2024-10-08 00:02:31,243 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=3600.6666666666665, ans=7.250416666666666
2024-10-08 00:02:40,082 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=3600.6666666666665, ans=0.5
2024-10-08 00:02:42,542 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3604.0, ans=0.26396
2024-10-08 00:02:49,758 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.81 vs. limit=5.901
2024-10-08 00:02:55,545 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=3607.3333333333335, ans=0.04908333333333331
2024-10-08 00:03:20,924 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=3614.0, ans=0.33059375
2024-10-08 00:03:20,937 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=3614.0, ans=0.04671250000000002
2024-10-08 00:03:21,407 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.23 vs. limit=8.85525
2024-10-08 00:03:25,795 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=3614.0, ans=0.33059375
2024-10-08 00:03:34,502 INFO [train.py:1153] Epoch 2, batch 4000, loss[loss=0.3535, simple_loss=0.3355, pruned_loss=0.1304, ctc_loss=0.2768, over 4815.00 frames. ], tot_loss[loss=0.4311, simple_loss=0.3726, pruned_loss=0.1739, ctc_loss=0.3543, over 967288.92 frames. ], batch size: 19, lr: 3.33e-02,
2024-10-08 00:03:36,710 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.09 vs. limit=10.213000000000001
2024-10-08 00:03:57,284 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=3620.6666666666665, ans=0.33028124999999997
2024-10-08 00:04:23,603 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.59 vs. limit=10.2205
2024-10-08 00:04:30,714 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=3630.6666666666665, ans=0.3298125
2024-10-08 00:04:30,758 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=3630.6666666666665, ans=0.06385000000000002
2024-10-08 00:04:32,610 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.72 vs. limit=10.222999999999999
2024-10-08 00:04:33,302 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 00:04:35,652 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.099e+02 1.664e+02 1.904e+02 2.076e+02 3.997e+02, threshold=3.808e+02, percent-clipped=2.0
2024-10-08 00:04:38,249 INFO [train.py:1153] Epoch 2, batch 4050, loss[loss=0.5255, simple_loss=0.4317, pruned_loss=0.2186, ctc_loss=0.4552, over 4768.00 frames. ], tot_loss[loss=0.4296, simple_loss=0.3718, pruned_loss=0.1733, ctc_loss=0.3521, over 967673.41 frames. ], batch size: 53, lr: 3.32e-02,
2024-10-08 00:04:49,924 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=3637.3333333333335, ans=0.01815999999999998
2024-10-08 00:05:12,284 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=3640.6666666666665, ans=0.32934375
2024-10-08 00:05:12,286 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=3640.6666666666665, ans=0.32934375
2024-10-08 00:05:13,583 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=3640.6666666666665, ans=0.04491666666666666
2024-10-08 00:05:20,064 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=3644.0, ans=0.018009999999999998
2024-10-08 00:05:20,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=6.24 vs. limit=8.8665
2024-10-08 00:05:35,665 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=3647.3333333333335, ans=0.038602083333333335
2024-10-08 00:05:41,698 INFO [train.py:1153] Epoch 2, batch 4100, loss[loss=0.4319, simple_loss=0.3772, pruned_loss=0.1691, ctc_loss=0.3712, over 4868.00 frames. ], tot_loss[loss=0.4296, simple_loss=0.372, pruned_loss=0.1733, ctc_loss=0.3516, over 967077.11 frames. ], batch size: 31, lr: 3.32e-02,
2024-10-08 00:06:03,552 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=3654.0, ans=0.7721100000000001
2024-10-08 00:06:03,625 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=3654.0, ans=0.7721100000000001
2024-10-08 00:06:12,437 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=3657.3333333333335, ans=0.3285625
2024-10-08 00:06:17,854 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.45 vs. limit=3.5486
2024-10-08 00:06:35,603 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.40 vs. limit=10.248000000000001
2024-10-08 00:06:42,833 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.320e+02 1.654e+02 1.851e+02 2.103e+02 3.192e+02, threshold=3.702e+02, percent-clipped=0.0
2024-10-08 00:06:45,336 INFO [train.py:1153] Epoch 2, batch 4150, loss[loss=0.4081, simple_loss=0.3684, pruned_loss=0.1553, ctc_loss=0.343, over 4749.00 frames. ], tot_loss[loss=0.4298, simple_loss=0.3725, pruned_loss=0.1734, ctc_loss=0.3508, over 967122.98 frames. ], batch size: 20, lr: 3.31e-02,
2024-10-08 00:06:56,376 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.64 vs. limit=3.5501
2024-10-08 00:07:07,256 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=3670.6666666666665, ans=0.3279375
2024-10-08 00:07:12,675 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=10.89 vs. limit=10.2555
2024-10-08 00:07:21,694 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.56 vs. limit=10.2555
2024-10-08 00:07:23,904 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=3677.3333333333335, ans=0.043149999999999966
2024-10-08 00:07:40,778 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=3680.6666666666665, ans=0.32746875
2024-10-08 00:07:49,428 INFO [train.py:1153] Epoch 2, batch 4200, loss[loss=0.4542, simple_loss=0.3749, pruned_loss=0.1947, ctc_loss=0.3598, over 4830.00 frames. ], tot_loss[loss=0.4305, simple_loss=0.3731, pruned_loss=0.1738, ctc_loss=0.3504, over 967292.55 frames. ], batch size: 31, lr: 3.31e-02,
2024-10-08 00:07:52,019 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3684.0, ans=0.26316
2024-10-08 00:07:57,077 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=3684.0, ans=0.3273125
2024-10-08 00:08:24,942 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=3690.6666666666665, ans=0.327
2024-10-08 00:08:38,108 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.96 vs. limit=10.2705
2024-10-08 00:08:50,219 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.242e+02 1.627e+02 1.844e+02 1.979e+02 3.659e+02, threshold=3.688e+02, percent-clipped=0.0
2024-10-08 00:08:50,371 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3697.3333333333335, ans=0.26302666666666663
2024-10-08 00:08:52,751 INFO [train.py:1153] Epoch 2, batch 4250, loss[loss=0.3724, simple_loss=0.3506, pruned_loss=0.1388, ctc_loss=0.2916, over 4745.00 frames. ], tot_loss[loss=0.4278, simple_loss=0.3717, pruned_loss=0.1724, ctc_loss=0.3476, over 967191.80 frames. ], batch size: 19, lr: 3.30e-02,
2024-10-08 00:08:58,470 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.41 vs. limit=8.88775
2024-10-08 00:09:17,473 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.22 vs. limit=6.853666666666667
2024-10-08 00:09:24,896 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=3707.3333333333335, ans=0.32621875
2024-10-08 00:09:28,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=3707.3333333333335, ans=0.32621875
2024-10-08 00:09:39,197 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.49 vs. limit=6.855333333333333
2024-10-08 00:09:45,183 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=3714.0, ans=0.7700100000000001
2024-10-08 00:09:50,744 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.91 vs. limit=10.285499999999999
2024-10-08 00:09:55,935 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.13 vs. limit=10.288
2024-10-08 00:09:56,640 INFO [train.py:1153] Epoch 2, batch 4300, loss[loss=0.4538, simple_loss=0.3845, pruned_loss=0.1855, ctc_loss=0.3801, over 4839.00 frames. ], tot_loss[loss=0.4278, simple_loss=0.3712, pruned_loss=0.1727, ctc_loss=0.3474, over 967435.15 frames. ], batch size: 21, lr: 3.30e-02,
2024-10-08 00:09:58,124 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=3717.3333333333335, ans=0.32575
2024-10-08 00:10:10,683 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3720.6666666666665, ans=0.2627933333333333
2024-10-08 00:10:11,235 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.38 vs. limit=8.89525
2024-10-08 00:10:13,243 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=3720.6666666666665, ans=0.03491666666666671
2024-10-08 00:10:20,136 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=7.23 vs. limit=8.89525
2024-10-08 00:10:21,191 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.86 vs. limit=10.293
2024-10-08 00:10:30,113 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.38 vs. limit=10.293
2024-10-08 00:10:30,822 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=3724.0, ans=0.26276
2024-10-08 00:10:36,322 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.03 vs. limit=8.89775
2024-10-08 00:10:39,597 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.min_positive, batch_count=3727.3333333333335, ans=0.07670416666666667
2024-10-08 00:10:40,135 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.15 vs. limit=5.9318333333333335
2024-10-08 00:10:50,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3730.6666666666665, ans=0.26269333333333333
2024-10-08 00:10:50,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3730.6666666666665, ans=0.26269333333333333
2024-10-08 00:10:57,297 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.249e+02 1.669e+02 1.932e+02 2.245e+02 5.703e+02, threshold=3.864e+02, percent-clipped=5.0
2024-10-08 00:10:59,760 INFO [train.py:1153] Epoch 2, batch 4350, loss[loss=0.3724, simple_loss=0.3363, pruned_loss=0.1445, ctc_loss=0.2985, over 4840.00 frames. ], tot_loss[loss=0.4265, simple_loss=0.3708, pruned_loss=0.172, ctc_loss=0.3455, over 966282.73 frames. ], batch size: 21, lr: 3.29e-02,
2024-10-08 00:11:06,327 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=3734.0, ans=0.07666250000000001
2024-10-08 00:11:13,700 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=3737.3333333333335, ans=0.3248125
2024-10-08 00:11:20,195 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=3737.3333333333335, ans=0.3248125
2024-10-08 00:11:23,940 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 00:11:37,831 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=3744.0, ans=0.059599999999999986
2024-10-08 00:11:40,321 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=3744.0, ans=0.3245
2024-10-08 00:11:55,456 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=3747.3333333333335, ans=0.05947499999999997
2024-10-08 00:11:56,844 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=3747.3333333333335, ans=0.05947499999999997
2024-10-08 00:12:03,042 INFO [train.py:1153] Epoch 2, batch 4400, loss[loss=0.4072, simple_loss=0.3678, pruned_loss=0.157, ctc_loss=0.3318, over 4748.00 frames. ], tot_loss[loss=0.4264, simple_loss=0.3709, pruned_loss=0.1719, ctc_loss=0.3451, over 965909.51 frames. ], batch size: 26, lr: 3.29e-02,
2024-10-08 00:12:10,738 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=3750.6666666666665, ans=0.039025000000000004
2024-10-08 00:12:10,858 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=1.764e+00
2024-10-08 00:12:11,561 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=13.67 vs. limit=10.313
2024-10-08 00:12:24,921 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.60 vs. limit=5.9385
2024-10-08 00:12:29,552 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=3757.3333333333335, ans=0.7684933333333334
2024-10-08 00:12:33,427 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3757.3333333333335, ans=0.26242666666666664
2024-10-08 00:12:37,561 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.82 vs. limit=10.318
2024-10-08 00:12:44,212 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=3760.6666666666665, ans=10.3205
2024-10-08 00:12:50,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=3760.6666666666665, ans=0.029916666666666702
2024-10-08 00:12:51,335 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=3760.6666666666665, ans=0.058975
2024-10-08 00:12:52,632 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=3764.0, ans=0.3235625
2024-10-08 00:12:56,870 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.90 vs. limit=10.323
2024-10-08 00:12:58,129 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=6.89 vs. limit=8.9115
2024-10-08 00:13:03,930 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.401e+02 1.703e+02 1.870e+02 2.138e+02 2.869e+02, threshold=3.740e+02, percent-clipped=1.0
2024-10-08 00:13:06,576 INFO [train.py:1153] Epoch 2, batch 4450, loss[loss=0.4254, simple_loss=0.382, pruned_loss=0.1701, ctc_loss=0.3218, over 4883.00 frames. ], tot_loss[loss=0.4278, simple_loss=0.3715, pruned_loss=0.1728, ctc_loss=0.3459, over 966227.33 frames. ], batch size: 19, lr: 3.28e-02,
2024-10-08 00:13:35,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=3774.0, ans=0.7679100000000001
2024-10-08 00:13:42,258 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=3774.0, ans=0.32309375
2024-10-08 00:13:42,751 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.36 vs. limit=10.3305
2024-10-08 00:13:47,706 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.max_abs, batch_count=3777.3333333333335, ans=7.360833333333334
2024-10-08 00:13:54,625 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.40 vs. limit=10.333
2024-10-08 00:13:56,454 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=3780.6666666666665, ans=0.01493499999999999
2024-10-08 00:14:04,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.40 vs. limit=10.3355
2024-10-08 00:14:10,077 INFO [train.py:1153] Epoch 2, batch 4500, loss[loss=0.4322, simple_loss=0.3709, pruned_loss=0.1754, ctc_loss=0.3571, over 4845.00 frames. ], tot_loss[loss=0.4241, simple_loss=0.37, pruned_loss=0.1708, ctc_loss=0.3418, over 966157.38 frames. ], batch size: 28, lr: 3.28e-02,
2024-10-08 00:14:25,313 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=3787.3333333333335, ans=0.32246874999999997
2024-10-08 00:14:34,363 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=3790.6666666666665, ans=0.05785000000000001
2024-10-08 00:14:38,726 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=12.75 vs. limit=6.895333333333333
2024-10-08 00:14:50,042 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.54 vs. limit=10.3455
2024-10-08 00:15:09,563 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=3797.3333333333335, ans=0.7670933333333334
2024-10-08 00:15:10,592 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.127e+02 1.668e+02 1.804e+02 1.974e+02 3.920e+02, threshold=3.608e+02, percent-clipped=1.0
2024-10-08 00:15:10,792 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=3797.3333333333335, ans=0.322
2024-10-08 00:15:12,993 INFO [train.py:1153] Epoch 2, batch 4550, loss[loss=0.315, simple_loss=0.3004, pruned_loss=0.1159, ctc_loss=0.2445, over 4848.00 frames. ], tot_loss[loss=0.4231, simple_loss=0.3695, pruned_loss=0.1702, ctc_loss=0.3403, over 965944.90 frames. ], batch size: 20, lr: 3.27e-02,
2024-10-08 00:15:28,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=3804.0, ans=0.3216875
2024-10-08 00:15:40,011 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=3807.3333333333335, ans=0.014334999999999987
2024-10-08 00:16:04,729 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.11 vs. limit=6.907
2024-10-08 00:16:13,399 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.26 vs. limit=10.3605
2024-10-08 00:16:16,606 INFO [train.py:1153] Epoch 2, batch 4600, loss[loss=0.481, simple_loss=0.406, pruned_loss=0.1991, ctc_loss=0.3942, over 4759.00 frames. ], tot_loss[loss=0.4224, simple_loss=0.3695, pruned_loss=0.1698, ctc_loss=0.3395, over 966274.07 frames. ], batch size: 45, lr: 3.27e-02,
2024-10-08 00:16:26,944 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=3817.3333333333335, ans=0.26182666666666665
2024-10-08 00:16:33,284 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=3820.6666666666665, ans=0.09899494936611666
2024-10-08 00:17:12,939 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=3830.6666666666665, ans=0.3204375
2024-10-08 00:17:17,786 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.246e+02 1.727e+02 1.912e+02 2.126e+02 3.165e+02, threshold=3.825e+02, percent-clipped=0.0
2024-10-08 00:17:20,335 INFO [train.py:1153] Epoch 2, batch 4650, loss[loss=0.4644, simple_loss=0.3982, pruned_loss=0.1907, ctc_loss=0.3726, over 4819.00 frames. ], tot_loss[loss=0.4236, simple_loss=0.37, pruned_loss=0.1706, ctc_loss=0.3397, over 965720.76 frames. ], batch size: 36, lr: 3.26e-02,
2024-10-08 00:17:44,929 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=7.90 vs. limit=8.94025
2024-10-08 00:17:53,274 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer1.prob, batch_count=3840.6666666666665, ans=0.31996875
2024-10-08 00:18:04,721 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3844.0, ans=0.26156
2024-10-08 00:18:23,664 INFO [train.py:1153] Epoch 2, batch 4700, loss[loss=0.3473, simple_loss=0.3184, pruned_loss=0.1345, ctc_loss=0.2683, over 4940.00 frames. ], tot_loss[loss=0.4224, simple_loss=0.3692, pruned_loss=0.17, ctc_loss=0.3387, over 965802.64 frames. ], batch size: 19, lr: 3.26e-02,
2024-10-08 00:18:28,238 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.40 vs. limit=8.943999999999999
2024-10-08 00:18:29,304 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.16 vs. limit=10.388
2024-10-08 00:18:34,249 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=3850.6666666666665, ans=0.3195
2024-10-08 00:18:39,256 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=3854.0, ans=0.05547499999999997
2024-10-08 00:18:50,877 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.96 vs. limit=10.393
2024-10-08 00:18:58,064 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=3857.3333333333335, ans=0.03302499999999997
2024-10-08 00:19:01,918 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=3860.6666666666665, ans=0.31903125
2024-10-08 00:19:04,937 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.25 vs. limit=8.94775
2024-10-08 00:19:21,772 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.07 vs. limit=10.398
2024-10-08 00:19:24,641 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.276e+02 1.712e+02 1.876e+02 2.156e+02 3.734e+02, threshold=3.753e+02, percent-clipped=0.0
2024-10-08 00:19:27,633 INFO [train.py:1153] Epoch 2, batch 4750, loss[loss=0.4553, simple_loss=0.3896, pruned_loss=0.1866, ctc_loss=0.3696, over 4725.00 frames. ], tot_loss[loss=0.4244, simple_loss=0.3702, pruned_loss=0.1711, ctc_loss=0.3408, over 965609.32 frames. ], batch size: 45, lr: 3.25e-02,
2024-10-08 00:19:40,372 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.63 vs. limit=3.5806
2024-10-08 00:19:52,191 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=3874.0, ans=0.76441
2024-10-08 00:19:54,779 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=3874.0, ans=0.31840625
2024-10-08 00:19:56,071 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=3874.0, ans=0.76441
2024-10-08 00:20:29,239 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.95 vs. limit=5.971
2024-10-08 00:20:29,859 INFO [train.py:1153] Epoch 2, batch 4800, loss[loss=0.3837, simple_loss=0.3416, pruned_loss=0.1521, ctc_loss=0.3038, over 4888.00 frames. ], tot_loss[loss=0.4202, simple_loss=0.3678, pruned_loss=0.1689, ctc_loss=0.3368, over 966010.38 frames. ], batch size: 22, lr: 3.25e-02,
2024-10-08 00:20:30,737 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=9.99 vs. limit=10.413
2024-10-08 00:20:32,851 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.26 vs. limit=10.413
2024-10-08 00:20:32,936 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.44 vs. limit=10.413
2024-10-08 00:20:41,579 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=3887.3333333333335, ans=0.31778125
2024-10-08 00:20:47,157 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=2.80 vs. limit=8.95775
2024-10-08 00:20:50,511 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=3887.3333333333335, ans=0.31778125
2024-10-08 00:20:58,828 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.93 vs. limit=10.418
2024-10-08 00:21:07,228 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=3894.0, ans=0.76371
2024-10-08 00:21:11,307 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.35 vs. limit=10.4205
2024-10-08 00:21:13,942 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.76 vs. limit=10.4205
2024-10-08 00:21:31,248 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.377e+02 1.742e+02 1.945e+02 2.061e+02 3.371e+02, threshold=3.889e+02, percent-clipped=0.0
2024-10-08 00:21:33,848 INFO [train.py:1153] Epoch 2, batch 4850, loss[loss=0.4459, simple_loss=0.3704, pruned_loss=0.1846, ctc_loss=0.3804, over 4854.00 frames. ], tot_loss[loss=0.4202, simple_loss=0.3686, pruned_loss=0.1687, ctc_loss=0.3363, over 966638.87 frames. ], batch size: 28, lr: 3.24e-02,
2024-10-08 00:21:45,710 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=3904.0, ans=0.317
2024-10-08 00:21:57,134 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=3904.0, ans=0.76336
2024-10-08 00:22:02,207 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=3907.3333333333335, ans=0.31684375
2024-10-08 00:22:17,579 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=3910.6666666666665, ans=0.7631266666666667
2024-10-08 00:22:37,998 INFO [train.py:1153] Epoch 2, batch 4900, loss[loss=0.3911, simple_loss=0.3536, pruned_loss=0.1544, ctc_loss=0.2998, over 4846.00 frames. ], tot_loss[loss=0.4222, simple_loss=0.3696, pruned_loss=0.1698, ctc_loss=0.3381, over 967160.19 frames. ], batch size: 21, lr: 3.24e-02,
2024-10-08 00:22:44,522 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=3917.3333333333335, ans=0.31637499999999996
2024-10-08 00:22:57,410 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.58 vs. limit=10.4405
2024-10-08 00:22:58,612 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.48 vs. limit=10.4405
2024-10-08 00:23:08,148 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=13.65 vs. limit=10.443
2024-10-08 00:23:29,059 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=3930.6666666666665, ans=0.31575
2024-10-08 00:23:30,847 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=6.68 vs. limit=8.974
2024-10-08 00:23:39,141 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.311e+02 1.831e+02 1.985e+02 2.205e+02 3.679e+02, threshold=3.971e+02, percent-clipped=0.0
2024-10-08 00:23:39,394 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=3930.6666666666665, ans=0.25896
2024-10-08 00:23:41,459 INFO [train.py:1153] Epoch 2, batch 4950, loss[loss=0.4918, simple_loss=0.3931, pruned_loss=0.2139, ctc_loss=0.4068, over 4772.00 frames. ], tot_loss[loss=0.425, simple_loss=0.3711, pruned_loss=0.1715, ctc_loss=0.3396, over 966954.64 frames. ], batch size: 53, lr: 3.23e-02,
2024-10-08 00:24:07,423 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=3940.6666666666665, ans=0.011334999999999998
2024-10-08 00:24:18,100 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.79 vs. limit=10.4555
2024-10-08 00:24:23,616 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=3.68 vs. limit=5.5776
2024-10-08 00:24:35,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=3947.3333333333335, ans=0.7618433333333334
2024-10-08 00:24:35,797 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.45 vs. limit=10.4605
2024-10-08 00:24:45,578 INFO [train.py:1153] Epoch 2, batch 5000, loss[loss=0.3882, simple_loss=0.3653, pruned_loss=0.1451, ctc_loss=0.3026, over 4801.00 frames. ], tot_loss[loss=0.423, simple_loss=0.3704, pruned_loss=0.1702, ctc_loss=0.3378, over 967809.36 frames. ], batch size: 29, lr: 3.23e-02,
2024-10-08 00:25:04,565 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.15 vs. limit=6.977
2024-10-08 00:25:12,809 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.min_positive, batch_count=3957.3333333333335, ans=0.03763333333333334
2024-10-08 00:25:19,295 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=3957.3333333333335, ans=0.3145
2024-10-08 00:25:26,208 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.39 vs. limit=5.584266666666666
2024-10-08 00:25:27,416 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.96 vs. limit=10.4705
2024-10-08 00:25:37,027 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=3964.0, ans=7.4775
2024-10-08 00:25:44,591 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=3964.0, ans=0.3141875
2024-10-08 00:25:46,973 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.337e+02 1.633e+02 1.796e+02 2.055e+02 3.249e+02, threshold=3.592e+02, percent-clipped=0.0
2024-10-08 00:25:48,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.67 vs. limit=10.4755
2024-10-08 00:25:49,549 INFO [train.py:1153] Epoch 2, batch 5050, loss[loss=0.3899, simple_loss=0.369, pruned_loss=0.1483, ctc_loss=0.2855, over 4851.00 frames. ], tot_loss[loss=0.4217, simple_loss=0.3693, pruned_loss=0.1699, ctc_loss=0.336, over 968659.83 frames. ], batch size: 19, lr: 3.22e-02,
2024-10-08 00:26:31,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=3977.3333333333335, ans=0.31356249999999997
2024-10-08 00:26:31,737 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.71 vs. limit=3.5966
2024-10-08 00:26:33,572 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=3977.3333333333335, ans=0.7607933333333333
2024-10-08 00:26:37,371 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=3977.3333333333335, ans=0.7607933333333333
2024-10-08 00:26:52,514 INFO [train.py:1153] Epoch 2, batch 5100, loss[loss=0.3221, simple_loss=0.3175, pruned_loss=0.1136, ctc_loss=0.2487, over 4817.00 frames. ], tot_loss[loss=0.4228, simple_loss=0.3698, pruned_loss=0.1706, ctc_loss=0.3364, over 967844.55 frames. ], batch size: 19, lr: 3.22e-02,
2024-10-08 00:27:23,309 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3990.6666666666665, ans=0.26009333333333334
2024-10-08 00:27:27,952 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=3.09 vs. limit=8.9965
2024-10-08 00:27:32,443 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=3994.0, ans=0.76021
2024-10-08 00:27:33,760 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=3994.0, ans=0.31278125
2024-10-08 00:27:52,600 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/checkpoint-12000.pt
2024-10-08 00:27:54,628 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.279e+02 1.650e+02 1.809e+02 2.069e+02 3.965e+02, threshold=3.618e+02, percent-clipped=1.0
2024-10-08 00:27:56,798 INFO [train.py:1153] Epoch 2, batch 5150, loss[loss=0.4768, simple_loss=0.3951, pruned_loss=0.2044, ctc_loss=0.374, over 4824.00 frames. ], tot_loss[loss=0.4209, simple_loss=0.3689, pruned_loss=0.1695, ctc_loss=0.3344, over 968029.23 frames. ], batch size: 36, lr: 3.21e-02,
2024-10-08 00:28:14,124 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=4004.0, ans=0.0
2024-10-08 00:28:35,289 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=4010.6666666666665, ans=0.04995555555555556
2024-10-08 00:28:42,952 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=4010.6666666666665, ans=0.04995555555555556
2024-10-08 00:28:43,185 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.53 vs. limit=10.508
2024-10-08 00:28:53,688 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.67 vs. limit=7.007
2024-10-08 00:28:57,366 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=11.33 vs. limit=9.00525
2024-10-08 00:28:59,012 INFO [train.py:1153] Epoch 2, batch 5200, loss[loss=0.4361, simple_loss=0.3639, pruned_loss=0.1845, ctc_loss=0.3481, over 4774.00 frames. ], tot_loss[loss=0.4196, simple_loss=0.3682, pruned_loss=0.169, ctc_loss=0.3327, over 967653.16 frames. ], batch size: 29, lr: 3.21e-02,
2024-10-08 00:29:00,794 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.51 vs. limit=10.513
2024-10-08 00:29:05,446 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=4017.3333333333335, ans=0.0
2024-10-08 00:29:11,952 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=4020.6666666666665, ans=0.26031
2024-10-08 00:29:12,463 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.96 vs. limit=10.5155
2024-10-08 00:29:33,572 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=4024.0, ans=0.311375
2024-10-08 00:29:34,776 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=4024.0, ans=0.311375
2024-10-08 00:29:36,057 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=4027.3333333333335, ans=0.04949747468305833
2024-10-08 00:30:00,087 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.206e+02 1.656e+02 1.936e+02 2.163e+02 4.435e+02, threshold=3.873e+02, percent-clipped=4.0
2024-10-08 00:30:01,921 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.61 vs. limit=3.6051
2024-10-08 00:30:02,658 INFO [train.py:1153] Epoch 2, batch 5250, loss[loss=0.3644, simple_loss=0.3404, pruned_loss=0.1369, ctc_loss=0.2868, over 4868.00 frames. ], tot_loss[loss=0.4165, simple_loss=0.3667, pruned_loss=0.1672, ctc_loss=0.3298, over 967732.45 frames. ], batch size: 20, lr: 3.20e-02,
2024-10-08 00:30:25,574 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=4037.3333333333335, ans=0.31074999999999997
2024-10-08 00:30:30,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=4040.6666666666665, ans=0.31059375
2024-10-08 00:30:31,767 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=4040.6666666666665, ans=0.26061
2024-10-08 00:30:58,780 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=4047.3333333333335, ans=0.025
2024-10-08 00:31:01,199 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=4047.3333333333335, ans=0.7583433333333334
2024-10-08 00:31:06,167 INFO [train.py:1153] Epoch 2, batch 5300, loss[loss=0.4199, simple_loss=0.3781, pruned_loss=0.1665, ctc_loss=0.3214, over 4823.00 frames. ], tot_loss[loss=0.4171, simple_loss=0.3672, pruned_loss=0.1674, ctc_loss=0.3304, over 967938.02 frames. ], batch size: 38, lr: 3.20e-02,
2024-10-08 00:31:19,003 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=4054.0, ans=0.30996875
2024-10-08 00:31:19,007 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=4054.0, ans=0.30996875
2024-10-08 00:31:23,428 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.33 vs. limit=9.02025
2024-10-08 00:31:24,019 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4054.0, ans=0.25945999999999997
2024-10-08 00:31:31,540 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=4057.3333333333335, ans=0.7905733333333334
2024-10-08 00:31:34,602 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.85 vs. limit=6.014333333333333
2024-10-08 00:31:34,831 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.05 vs. limit=9.0215
2024-10-08 00:31:38,455 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.54 vs. limit=10.543
2024-10-08 00:31:41,604 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=4057.3333333333335, ans=0.3098125
2024-10-08 00:31:45,268 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=4060.6666666666665, ans=0.30965624999999997
2024-10-08 00:31:48,235 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.23 vs. limit=10.5455
2024-10-08 00:31:50,826 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.79 vs. limit=5.624266666666666
2024-10-08 00:31:52,862 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=4060.6666666666665, ans=0.0
2024-10-08 00:32:06,800 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.457e+02 1.686e+02 1.848e+02 2.091e+02 3.136e+02, threshold=3.696e+02, percent-clipped=0.0
2024-10-08 00:32:09,275 INFO [train.py:1153] Epoch 2, batch 5350, loss[loss=0.4466, simple_loss=0.3762, pruned_loss=0.1907, ctc_loss=0.3392, over 4978.00 frames. ], tot_loss[loss=0.4185, simple_loss=0.3679, pruned_loss=0.1683, ctc_loss=0.3307, over 967334.95 frames. ], batch size: 19, lr: 3.19e-02,
2024-10-08 00:32:14,409 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=4067.3333333333335, ans=0.30934375000000003
2024-10-08 00:32:23,211 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=4070.6666666666665, ans=0.3091875
2024-10-08 00:32:23,726 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.59 vs. limit=9.0265
2024-10-08 00:32:30,816 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=4070.6666666666665, ans=0.7907066666666667
2024-10-08 00:32:37,178 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=4074.0, ans=0.009983913043478261
2024-10-08 00:32:40,896 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=1.218e+00
2024-10-08 00:32:41,095 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.53 vs. limit=3.6111
2024-10-08 00:32:45,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=4077.3333333333335, ans=0.025
2024-10-08 00:32:48,671 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.56 vs. limit=10.558
2024-10-08 00:33:05,132 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=39.69 vs. limit=9.03025
2024-10-08 00:33:07,796 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.97 vs. limit=10.560500000000001
2024-10-08 00:33:12,198 INFO [train.py:1153] Epoch 2, batch 5400, loss[loss=0.3968, simple_loss=0.3578, pruned_loss=0.155, ctc_loss=0.3143, over 4782.00 frames. ], tot_loss[loss=0.4196, simple_loss=0.3682, pruned_loss=0.169, ctc_loss=0.3328, over 966509.97 frames. ], batch size: 49, lr: 3.19e-02,
2024-10-08 00:33:22,031 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.44 vs. limit=10.563
2024-10-08 00:33:40,645 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=4090.6666666666665, ans=0.30825
2024-10-08 00:33:41,898 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4090.6666666666665, ans=0.25909333333333334
2024-10-08 00:33:50,504 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=4094.0, ans=0.0
2024-10-08 00:33:50,591 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=4094.0, ans=0.30809375
2024-10-08 00:34:05,360 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.60 vs. limit=10.573
2024-10-08 00:34:08,232 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=4097.333333333333, ans=0.2590266666666667
2024-10-08 00:34:09,504 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=4097.333333333333, ans=0.3079375
2024-10-08 00:34:09,929 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.37 vs. limit=10.573
2024-10-08 00:34:13,056 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.138e+02 1.671e+02 1.838e+02 1.992e+02 3.085e+02, threshold=3.675e+02, percent-clipped=0.0
2024-10-08 00:34:15,718 INFO [train.py:1153] Epoch 2, batch 5450, loss[loss=0.3852, simple_loss=0.3554, pruned_loss=0.1493, ctc_loss=0.2912, over 4940.00 frames. ], tot_loss[loss=0.4166, simple_loss=0.3671, pruned_loss=0.1672, ctc_loss=0.3291, over 967059.66 frames. ], batch size: 19, lr: 3.18e-02,
2024-10-08 00:34:50,951 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.78 vs. limit=10.5805
2024-10-08 00:34:53,825 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.14 vs. limit=7.0553333333333335
2024-10-08 00:35:08,064 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=4114.0, ans=0.30715625
2024-10-08 00:35:08,789 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.52 vs. limit=3.6170999999999998
2024-10-08 00:35:19,458 INFO [train.py:1153] Epoch 2, batch 5500, loss[loss=0.4801, simple_loss=0.3918, pruned_loss=0.2032, ctc_loss=0.4046, over 4791.00 frames. ], tot_loss[loss=0.4157, simple_loss=0.3665, pruned_loss=0.1668, ctc_loss=0.3283, over 967515.17 frames. ], batch size: 49, lr: 3.18e-02,
2024-10-08 00:35:47,710 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=4124.0, ans=0.3066875
2024-10-08 00:36:20,732 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.190e+02 1.669e+02 1.823e+02 2.027e+02 3.877e+02, threshold=3.645e+02, percent-clipped=1.0
2024-10-08 00:36:23,088 INFO [train.py:1153] Epoch 2, batch 5550, loss[loss=0.3995, simple_loss=0.3513, pruned_loss=0.1633, ctc_loss=0.3031, over 4800.00 frames. ], tot_loss[loss=0.4131, simple_loss=0.3646, pruned_loss=0.1656, ctc_loss=0.326, over 967285.15 frames. ], batch size: 19, lr: 3.17e-02,
2024-10-08 00:36:30,208 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1.whitening_limit, batch_count=4134.0, ans=6.0335
2024-10-08 00:36:45,214 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=11.45 vs. limit=9.0515
2024-10-08 00:36:50,091 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4140.666666666667, ans=0.25859333333333334
2024-10-08 00:36:52,468 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=4140.666666666667, ans=0.04941388888888889
2024-10-08 00:37:02,707 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=4144.0, ans=0.30574999999999997
2024-10-08 00:37:09,181 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=4144.0, ans=0.30574999999999997
2024-10-08 00:37:19,154 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=4147.333333333333, ans=0.025
2024-10-08 00:37:24,599 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=3.70 vs. limit=9.05525
2024-10-08 00:37:26,588 INFO [train.py:1153] Epoch 2, batch 5600, loss[loss=0.4313, simple_loss=0.3727, pruned_loss=0.1753, ctc_loss=0.3484, over 4864.00 frames. ], tot_loss[loss=0.4139, simple_loss=0.3649, pruned_loss=0.1662, ctc_loss=0.3264, over 967222.51 frames. ], batch size: 28, lr: 3.17e-02,
2024-10-08 00:37:30,403 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=4150.666666666667, ans=0.30543750000000003
2024-10-08 00:37:33,390 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.97 vs. limit=6.0376666666666665
2024-10-08 00:37:57,415 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=4157.333333333333, ans=0.305125
2024-10-08 00:38:10,040 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 00:38:18,909 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=4164.0, ans=0.3048125
2024-10-08 00:38:27,666 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.297e+02 1.655e+02 1.834e+02 2.042e+02 3.987e+02, threshold=3.668e+02, percent-clipped=1.0
2024-10-08 00:38:30,229 INFO [train.py:1153] Epoch 2, batch 5650, loss[loss=0.4898, simple_loss=0.4032, pruned_loss=0.2082, ctc_loss=0.4003, over 4747.00 frames. ], tot_loss[loss=0.4112, simple_loss=0.3634, pruned_loss=0.1647, ctc_loss=0.3239, over 967197.69 frames. ], batch size: 45, lr: 3.16e-02,
2024-10-08 00:38:31,753 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=4167.333333333333, ans=0.0
2024-10-08 00:38:39,210 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=4167.333333333333, ans=0.025
2024-10-08 00:38:43,621 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.69 vs. limit=3.6256
2024-10-08 00:38:45,056 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.01 vs. limit=6.042666666666667
2024-10-08 00:38:50,457 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=4170.666666666667, ans=0.3045
2024-10-08 00:38:54,396 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=4174.0, ans=0.0739125
2024-10-08 00:38:54,969 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.26 vs. limit=10.6305
2024-10-08 00:39:02,058 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4174.0, ans=0.25826
2024-10-08 00:39:03,747 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.44 vs. limit=9.06525
2024-10-08 00:39:10,102 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.08 vs. limit=9.0665
2024-10-08 00:39:19,137 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.54 vs. limit=10.633
2024-10-08 00:39:22,042 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.33 vs. limit=7.090333333333334
2024-10-08 00:39:22,419 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.min_abs, batch_count=4180.666666666667, ans=0.26271
2024-10-08 00:39:33,741 INFO [train.py:1153] Epoch 2, batch 5700, loss[loss=0.4949, simple_loss=0.4216, pruned_loss=0.2067, ctc_loss=0.3865, over 4891.00 frames. ], tot_loss[loss=0.411, simple_loss=0.3634, pruned_loss=0.1645, ctc_loss=0.3239, over 966631.55 frames. ], batch size: 22, lr: 3.16e-02,
2024-10-08 00:39:40,838 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.29 vs. limit=10.638
2024-10-08 00:40:07,113 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=4190.666666666667, ans=0.3035625
2024-10-08 00:40:16,629 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2.whitening_limit, batch_count=4194.0, ans=7.0969999999999995
2024-10-08 00:40:30,625 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.27 vs. limit=6.049333333333333
2024-10-08 00:40:35,383 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.304e+02 1.708e+02 1.841e+02 2.086e+02 3.497e+02, threshold=3.682e+02, percent-clipped=0.0
2024-10-08 00:40:37,824 INFO [train.py:1153] Epoch 2, batch 5750, loss[loss=0.5005, simple_loss=0.4075, pruned_loss=0.2171, ctc_loss=0.3984, over 4836.00 frames. ], tot_loss[loss=0.4147, simple_loss=0.3659, pruned_loss=0.1663, ctc_loss=0.3272, over 966899.95 frames. ], batch size: 43, lr: 3.16e-02,
2024-10-08 00:40:37,975 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=4200.666666666667, ans=0.03687291666666667
2024-10-08 00:40:39,200 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=4200.666666666667, ans=0.30309375
2024-10-08 00:40:45,533 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=4200.666666666667, ans=0.0
2024-10-08 00:40:59,061 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.70 vs. limit=7.102
2024-10-08 00:41:07,064 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=4207.333333333333, ans=0.7527433333333333
2024-10-08 00:41:13,720 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.58 vs. limit=9.07775
2024-10-08 00:41:16,635 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module2.whiten, num_groups=1, num_channels=192, metric=3.19 vs. limit=9.079
2024-10-08 00:41:21,588 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.91 vs. limit=10.658000000000001
2024-10-08 00:41:25,226 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=10.59 vs. limit=10.658000000000001
2024-10-08 00:41:31,298 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4214.0, ans=0.25786
2024-10-08 00:41:41,150 INFO [train.py:1153] Epoch 2, batch 5800, loss[loss=0.5384, simple_loss=0.4456, pruned_loss=0.2287, ctc_loss=0.4344, over 4834.00 frames. ], tot_loss[loss=0.4155, simple_loss=0.3663, pruned_loss=0.1668, ctc_loss=0.3277, over 966214.63 frames. ], batch size: 43, lr: 3.15e-02,
2024-10-08 00:41:45,046 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=4217.333333333333, ans=0.3023125
2024-10-08 00:41:45,512 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=12.14 vs. limit=9.0815
2024-10-08 00:41:48,173 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.36 vs. limit=10.663
2024-10-08 00:41:48,918 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=4217.333333333333, ans=0.3023125
2024-10-08 00:41:49,248 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.01 vs. limit=9.0815
2024-10-08 00:41:55,741 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.90 vs. limit=10.6655
2024-10-08 00:42:13,639 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.61 vs. limit=3.6336
2024-10-08 00:42:15,490 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer1.prob, batch_count=4224.0, ans=0.302
2024-10-08 00:42:15,917 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=12.89 vs. limit=9.084
2024-10-08 00:42:19,388 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=4227.333333333333, ans=0.04905277777777778
2024-10-08 00:42:21,974 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=4227.333333333333, ans=0.30184374999999997
2024-10-08 00:42:31,379 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.37 vs. limit=9.086500000000001
2024-10-08 00:42:31,438 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.03 vs. limit=10.673
2024-10-08 00:42:35,466 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.72 vs. limit=3.6346
2024-10-08 00:42:42,586 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.306e+02 1.603e+02 1.842e+02 2.053e+02 4.312e+02, threshold=3.684e+02, percent-clipped=1.0
2024-10-08 00:42:45,112 INFO [train.py:1153] Epoch 2, batch 5850, loss[loss=0.4991, simple_loss=0.4079, pruned_loss=0.2156, ctc_loss=0.398, over 4736.00 frames. ], tot_loss[loss=0.4128, simple_loss=0.365, pruned_loss=0.1653, ctc_loss=0.3248, over 966622.28 frames. ], batch size: 45, lr: 3.15e-02,
2024-10-08 00:42:45,122 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=4234.0, ans=0.30153125000000003
2024-10-08 00:42:49,681 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.68 vs. limit=10.6755
2024-10-08 00:43:06,081 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.40 vs. limit=10.677999999999999
2024-10-08 00:43:14,941 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.55 vs. limit=10.6805
2024-10-08 00:43:20,995 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=4240.666666666667, ans=0.0
2024-10-08 00:43:35,282 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=4247.333333333333, ans=0.30090625
2024-10-08 00:43:45,335 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=4247.333333333333, ans=0.30090625
2024-10-08 00:43:47,059 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.80 vs. limit=10.6855
2024-10-08 00:43:49,149 INFO [train.py:1153] Epoch 2, batch 5900, loss[loss=0.4641, simple_loss=0.4014, pruned_loss=0.1894, ctc_loss=0.3701, over 4803.00 frames. ], tot_loss[loss=0.4141, simple_loss=0.366, pruned_loss=0.1658, ctc_loss=0.3261, over 966764.37 frames. ], batch size: 34, lr: 3.14e-02,
2024-10-08 00:44:11,665 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.82 vs. limit=7.127
2024-10-08 00:44:12,584 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.20 vs. limit=6.0634999999999994
2024-10-08 00:44:17,807 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.61 vs. limit=3.6386
2024-10-08 00:44:27,299 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=4260.666666666667, ans=0.7508766666666666
2024-10-08 00:44:39,201 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.72 vs. limit=10.698
2024-10-08 00:44:45,128 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=4264.0, ans=0.300125
2024-10-08 00:44:48,221 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.35 vs. limit=6.066
2024-10-08 00:44:50,116 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.203e+02 1.664e+02 1.856e+02 2.028e+02 5.230e+02, threshold=3.712e+02, percent-clipped=1.0
2024-10-08 00:44:52,566 INFO [train.py:1153] Epoch 2, batch 5950, loss[loss=0.4562, simple_loss=0.3874, pruned_loss=0.1883, ctc_loss=0.3708, over 4792.00 frames. ], tot_loss[loss=0.412, simple_loss=0.3653, pruned_loss=0.1645, ctc_loss=0.3239, over 966255.73 frames. ], batch size: 34, lr: 3.14e-02,
2024-10-08 00:44:58,989 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=4267.333333333333, ans=0.26401
2024-10-08 00:45:05,726 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.39 vs. limit=9.1015
2024-10-08 00:45:06,501 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=4270.666666666667, ans=0.29981250000000004
2024-10-08 00:45:14,291 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=4270.666666666667, ans=0.7505266666666667
2024-10-08 00:45:18,596 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=10.89 vs. limit=10.7055
2024-10-08 00:45:19,405 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=4274.0, ans=0.0
2024-10-08 00:45:27,377 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.78 vs. limit=10.7055
2024-10-08 00:45:52,577 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4280.666666666667, ans=0.25719333333333333
2024-10-08 00:45:55,151 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=4284.0, ans=0.009938260869565218
2024-10-08 00:45:56,204 INFO [train.py:1153] Epoch 2, batch 6000, loss[loss=0.4464, simple_loss=0.3757, pruned_loss=0.1859, ctc_loss=0.3634, over 4767.00 frames. ], tot_loss[loss=0.4116, simple_loss=0.3648, pruned_loss=0.1645, ctc_loss=0.3234, over 966842.22 frames. ], batch size: 49, lr: 3.13e-02,
2024-10-08 00:45:56,204 INFO [train.py:1176] Computing validation loss
2024-10-08 00:46:03,683 INFO [train.py:1185] Epoch 2, validation: loss=0.2619, simple_loss=0.3161, pruned_loss=0.07412, ctc_loss=0.1487, over 90464.00 frames.
2024-10-08 00:46:03,684 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 00:46:29,731 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.45 vs. limit=9.109
2024-10-08 00:46:34,754 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.39 vs. limit=9.109
2024-10-08 00:46:47,274 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=4294.0, ans=0.7497100000000001
2024-10-08 00:47:04,700 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.236e+02 1.704e+02 1.881e+02 2.033e+02 3.048e+02, threshold=3.761e+02, percent-clipped=0.0
2024-10-08 00:47:07,310 INFO [train.py:1153] Epoch 2, batch 6050, loss[loss=0.3485, simple_loss=0.3422, pruned_loss=0.1262, ctc_loss=0.2559, over 4817.00 frames. ], tot_loss[loss=0.4103, simple_loss=0.3641, pruned_loss=0.1638, ctc_loss=0.3219, over 966741.63 frames. ], batch size: 19, lr: 3.13e-02,
2024-10-08 00:47:40,645 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=4307.333333333333, ans=0.29809375
2024-10-08 00:47:47,244 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.54 vs. limit=9.1165
2024-10-08 00:47:48,015 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.8.prob, batch_count=4310.666666666667, ans=0.29793749999999997
2024-10-08 00:47:54,407 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=4310.666666666667, ans=0.29793749999999997
2024-10-08 00:48:10,685 INFO [train.py:1153] Epoch 2, batch 6100, loss[loss=0.4634, simple_loss=0.3953, pruned_loss=0.1919, ctc_loss=0.3693, over 4829.00 frames. ], tot_loss[loss=0.4119, simple_loss=0.3652, pruned_loss=0.1648, ctc_loss=0.3229, over 966344.25 frames. ], batch size: 34, lr: 3.12e-02,
2024-10-08 00:48:15,711 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=4317.333333333333, ans=0.29762500000000003
2024-10-08 00:48:24,514 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=4320.666666666667, ans=0.26481
2024-10-08 00:48:52,268 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=4327.333333333333, ans=7.704583333333333
2024-10-08 00:48:58,640 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=4327.333333333333, ans=0.29715625
2024-10-08 00:49:11,256 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.379e+02 1.673e+02 1.845e+02 2.097e+02 4.273e+02, threshold=3.690e+02, percent-clipped=2.0
2024-10-08 00:49:13,669 INFO [train.py:1153] Epoch 2, batch 6150, loss[loss=0.4474, simple_loss=0.3864, pruned_loss=0.1829, ctc_loss=0.3568, over 4819.00 frames. ], tot_loss[loss=0.4096, simple_loss=0.3637, pruned_loss=0.1636, ctc_loss=0.3205, over 966514.96 frames. ], batch size: 43, lr: 3.12e-02,
2024-10-08 00:49:20,261 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=4334.0, ans=0.04860833333333334
2024-10-08 00:49:27,549 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=4337.333333333333, ans=0.2966875
2024-10-08 00:49:30,587 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.93 vs. limit=9.1265
2024-10-08 00:49:33,153 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.23 vs. limit=10.753
2024-10-08 00:49:45,134 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=4340.666666666667, ans=0.25659333333333334
2024-10-08 00:49:50,154 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=4344.0, ans=0.04856666666666667
2024-10-08 00:49:52,701 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=4344.0, ans=0.7479600000000001
2024-10-08 00:50:05,372 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=4347.333333333333, ans=0.26521
2024-10-08 00:50:06,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=4347.333333333333, ans=0.0
2024-10-08 00:50:13,277 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.09 vs. limit=5.738933333333334
2024-10-08 00:50:15,819 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.57 vs. limit=9.1315
2024-10-08 00:50:16,377 INFO [train.py:1153] Epoch 2, batch 6200, loss[loss=0.4114, simple_loss=0.3718, pruned_loss=0.1585, ctc_loss=0.3351, over 4782.00 frames. ], tot_loss[loss=0.4096, simple_loss=0.3641, pruned_loss=0.1634, ctc_loss=0.3205, over 966628.53 frames. ], batch size: 29, lr: 3.11e-02,
2024-10-08 00:50:31,732 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=4354.0, ans=0.07
2024-10-08 00:50:51,854 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=4357.333333333333, ans=0.7474933333333333
2024-10-08 00:50:54,830 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.79 vs. limit=5.744266666666666
2024-10-08 00:51:00,923 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=4360.666666666667, ans=0.7473766666666667
2024-10-08 00:51:11,095 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=4364.0, ans=0.25636
2024-10-08 00:51:14,766 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4364.0, ans=0.25636
2024-10-08 00:51:17,169 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.343e+02 1.637e+02 1.777e+02 1.934e+02 3.166e+02, threshold=3.554e+02, percent-clipped=0.0
2024-10-08 00:51:18,119 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.51 vs. limit=10.773
2024-10-08 00:51:19,683 INFO [train.py:1153] Epoch 2, batch 6250, loss[loss=0.4326, simple_loss=0.3834, pruned_loss=0.1717, ctc_loss=0.3463, over 4753.00 frames. ], tot_loss[loss=0.4069, simple_loss=0.3627, pruned_loss=0.162, ctc_loss=0.3179, over 966807.41 frames. ], batch size: 26, lr: 3.11e-02,
2024-10-08 00:51:49,858 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.76 vs. limit=7.186999999999999
2024-10-08 00:51:50,945 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.24 vs. limit=10.7805
2024-10-08 00:51:51,558 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4374.0, ans=0.25626
2024-10-08 00:51:54,115 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4374.0, ans=0.25626
2024-10-08 00:51:59,185 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=4377.333333333333, ans=0.29481250000000003
2024-10-08 00:52:04,536 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.36 vs. limit=6.094333333333333
2024-10-08 00:52:04,543 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=6.56 vs. limit=5.750933333333333
2024-10-08 00:52:08,327 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.68 vs. limit=10.783
2024-10-08 00:52:15,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=4380.666666666667, ans=0.025
2024-10-08 00:52:17,225 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten.whitening_limit, batch_count=4380.666666666667, ans=9.14275
2024-10-08 00:52:21,315 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1.whitening_limit, batch_count=4380.666666666667, ans=6.095166666666667
2024-10-08 00:52:22,211 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.99 vs. limit=10.788
2024-10-08 00:52:23,046 INFO [train.py:1153] Epoch 2, batch 6300, loss[loss=0.3008, simple_loss=0.2845, pruned_loss=0.1114, ctc_loss=0.2356, over 4978.00 frames. ], tot_loss[loss=0.4044, simple_loss=0.3609, pruned_loss=0.1608, ctc_loss=0.3159, over 966675.69 frames. ], batch size: 19, lr: 3.11e-02,
2024-10-08 00:52:33,365 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=4384.0, ans=0.74656
2024-10-08 00:52:37,300 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=4387.333333333333, ans=0.048386111111111116
2024-10-08 00:52:38,100 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=16.46 vs. limit=10.7905
2024-10-08 00:53:01,280 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=4394.0, ans=0.29403124999999997
2024-10-08 00:53:03,725 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=4394.0, ans=0.048358333333333337
2024-10-08 00:53:04,047 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=4.92 vs. limit=6.0985
2024-10-08 00:53:12,608 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=4397.333333333333, ans=0.04834444444444445
2024-10-08 00:53:13,245 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.68 vs. limit=3.6596
2024-10-08 00:53:16,298 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4397.333333333333, ans=0.2560266666666667
2024-10-08 00:53:17,026 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=3.68 vs. limit=5.758933333333333
2024-10-08 00:53:23,942 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.348e+02 1.692e+02 1.869e+02 2.011e+02 3.700e+02, threshold=3.739e+02, percent-clipped=1.0
2024-10-08 00:53:25,492 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.13 vs. limit=10.8005
2024-10-08 00:53:26,325 INFO [train.py:1153] Epoch 2, batch 6350, loss[loss=0.4237, simple_loss=0.3634, pruned_loss=0.1743, ctc_loss=0.3386, over 4816.00 frames. ], tot_loss[loss=0.4044, simple_loss=0.3611, pruned_loss=0.1608, ctc_loss=0.3155, over 966257.29 frames. ], batch size: 36, lr: 3.10e-02,
2024-10-08 00:53:33,610 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.29 vs. limit=7.200333333333333
2024-10-08 00:53:35,406 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4400.666666666667, ans=0.2559933333333333
2024-10-08 00:53:36,663 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=4400.666666666667, ans=0.04833055555555556
2024-10-08 00:53:38,515 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.53 vs. limit=10.803
2024-10-08 00:53:42,864 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 00:53:42,878 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer1.prob, batch_count=4404.0, ans=0.2935625
2024-10-08 00:53:44,592 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.66 vs. limit=10.803
2024-10-08 00:53:50,967 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.39 vs. limit=10.8055
2024-10-08 00:54:21,337 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=4414.0, ans=0.00991
2024-10-08 00:54:29,999 INFO [train.py:1153] Epoch 2, batch 6400, loss[loss=0.3636, simple_loss=0.3435, pruned_loss=0.1326, ctc_loss=0.296, over 4870.00 frames. ], tot_loss[loss=0.4018, simple_loss=0.3595, pruned_loss=0.1594, ctc_loss=0.3132, over 965976.39 frames. ], batch size: 23, lr: 3.10e-02,
2024-10-08 00:54:32,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=4417.333333333333, ans=0.29293749999999996
2024-10-08 00:54:49,688 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.39 vs. limit=7.210333333333334
2024-10-08 00:54:59,497 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4424.0, ans=0.25576
2024-10-08 00:55:00,673 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=4424.0, ans=0.009907826086956522
2024-10-08 00:55:15,993 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4427.333333333333, ans=0.25572666666666666
2024-10-08 00:55:31,048 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.293e+02 1.704e+02 1.868e+02 2.039e+02 3.594e+02, threshold=3.736e+02, percent-clipped=0.0
2024-10-08 00:55:33,595 INFO [train.py:1153] Epoch 2, batch 6450, loss[loss=0.4043, simple_loss=0.361, pruned_loss=0.1631, ctc_loss=0.3034, over 4752.00 frames. ], tot_loss[loss=0.4016, simple_loss=0.3596, pruned_loss=0.1593, ctc_loss=0.3127, over 965310.76 frames. ], batch size: 26, lr: 3.09e-02,
2024-10-08 00:55:33,727 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer1.prob, batch_count=4434.0, ans=0.29215625
2024-10-08 00:55:47,099 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.73 vs. limit=9.164
2024-10-08 00:55:54,401 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=4437.333333333333, ans=0.29200000000000004
2024-10-08 00:55:54,917 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=2.67 vs. limit=9.164
2024-10-08 00:56:09,252 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.42 vs. limit=10.8305
2024-10-08 00:56:11,278 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=4444.0, ans=0.0
2024-10-08 00:56:15,047 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=4444.0, ans=0.0
2024-10-08 00:56:17,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=4444.0, ans=0.2916875
2024-10-08 00:56:24,329 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.37 vs. limit=10.8355
2024-10-08 00:56:36,724 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=4450.666666666667, ans=0.291375
2024-10-08 00:56:37,839 INFO [train.py:1153] Epoch 2, batch 6500, loss[loss=0.3413, simple_loss=0.336, pruned_loss=0.1239, ctc_loss=0.2473, over 4715.00 frames. ], tot_loss[loss=0.3999, simple_loss=0.3589, pruned_loss=0.1583, ctc_loss=0.3107, over 964908.23 frames. ], batch size: 26, lr: 3.09e-02,
2024-10-08 00:56:41,753 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=4450.666666666667, ans=0.291375
2024-10-08 00:57:16,402 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=4460.666666666667, ans=0.29090625000000003
2024-10-08 00:57:22,764 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=4460.666666666667, ans=0.2553933333333333
2024-10-08 00:57:22,839 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=4460.666666666667, ans=0.29090625000000003
2024-10-08 00:57:25,184 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=4460.666666666667, ans=7.787916666666667
2024-10-08 00:57:39,381 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.273e+02 1.646e+02 1.787e+02 1.964e+02 2.647e+02, threshold=3.574e+02, percent-clipped=0.0
2024-10-08 00:57:41,866 INFO [train.py:1153] Epoch 2, batch 6550, loss[loss=0.3534, simple_loss=0.3324, pruned_loss=0.1331, ctc_loss=0.2705, over 4978.00 frames. ], tot_loss[loss=0.3994, simple_loss=0.3589, pruned_loss=0.1579, ctc_loss=0.3099, over 964547.30 frames. ], batch size: 19, lr: 3.08e-02,
2024-10-08 00:57:51,661 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.36 vs. limit=9.17525
2024-10-08 00:58:00,100 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=4470.666666666667, ans=0.03602916666666667
2024-10-08 00:58:19,988 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.23 vs. limit=10.858
2024-10-08 00:58:29,243 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.52 vs. limit=10.858
2024-10-08 00:58:35,077 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=4480.666666666667, ans=0.28996875
2024-10-08 00:58:36,324 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=4480.666666666667, ans=0.28996875
2024-10-08 00:58:36,380 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=4480.666666666667, ans=0.2551933333333333
2024-10-08 00:58:39,460 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=6.62 vs. limit=9.180250000000001
2024-10-08 00:58:40,640 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.13 vs. limit=10.8605
2024-10-08 00:58:46,566 INFO [train.py:1153] Epoch 2, batch 6600, loss[loss=0.3857, simple_loss=0.3387, pruned_loss=0.1556, ctc_loss=0.3037, over 4852.00 frames. ], tot_loss[loss=0.3963, simple_loss=0.3576, pruned_loss=0.156, ctc_loss=0.3074, over 965213.75 frames. ], batch size: 23, lr: 3.08e-02,
2024-10-08 00:58:49,265 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.min_abs, batch_count=4484.0, ans=0.26726
2024-10-08 00:58:59,141 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.48 vs. limit=7.243666666666666
2024-10-08 00:59:06,095 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4487.333333333333, ans=0.25512666666666667
2024-10-08 00:59:07,419 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=4487.333333333333, ans=0.28965625
2024-10-08 00:59:19,507 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=12.75 vs. limit=9.184
2024-10-08 00:59:25,920 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.31 vs. limit=10.8705
2024-10-08 00:59:29,485 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=4494.0, ans=0.28934375
2024-10-08 00:59:48,705 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.198e+02 1.718e+02 1.901e+02 2.147e+02 2.771e+02, threshold=3.803e+02, percent-clipped=0.0
2024-10-08 00:59:51,239 INFO [train.py:1153] Epoch 2, batch 6650, loss[loss=0.2966, simple_loss=0.3016, pruned_loss=0.1035, ctc_loss=0.2116, over 4746.00 frames. ], tot_loss[loss=0.3926, simple_loss=0.3555, pruned_loss=0.1542, ctc_loss=0.3036, over 967021.79 frames. ], batch size: 20, lr: 3.07e-02,
2024-10-08 00:59:52,686 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=4500.666666666667, ans=0.009891159420289855
2024-10-08 01:00:11,942 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=4504.0, ans=0.288875
2024-10-08 01:00:30,836 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.53 vs. limit=10.883
2024-10-08 01:00:41,728 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=4514.0, ans=0.28840625
2024-10-08 01:00:52,311 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=4514.0, ans=0.025
2024-10-08 01:00:56,037 INFO [train.py:1153] Epoch 2, batch 6700, loss[loss=0.3626, simple_loss=0.3394, pruned_loss=0.1397, ctc_loss=0.2655, over 4929.00 frames. ], tot_loss[loss=0.3883, simple_loss=0.353, pruned_loss=0.1519, ctc_loss=0.2995, over 969157.59 frames. ], batch size: 20, lr: 3.07e-02,
2024-10-08 01:01:05,874 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=4517.333333333333, ans=10.888
2024-10-08 01:01:10,740 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.82 vs. limit=3.6781
2024-10-08 01:01:22,076 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=4524.0, ans=0.025
2024-10-08 01:01:35,793 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.07 vs. limit=6.131833333333333
2024-10-08 01:01:36,467 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4527.333333333333, ans=0.25472666666666666
2024-10-08 01:01:48,152 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=4530.666666666667, ans=0.287625
2024-10-08 01:01:54,661 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.max_abs, batch_count=4530.666666666667, ans=7.831666666666667
2024-10-08 01:01:58,496 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.235e+02 1.586e+02 1.758e+02 1.918e+02 2.752e+02, threshold=3.516e+02, percent-clipped=0.0
2024-10-08 01:02:00,988 INFO [train.py:1153] Epoch 2, batch 6750, loss[loss=0.3722, simple_loss=0.3421, pruned_loss=0.1422, ctc_loss=0.2951, over 4908.00 frames. ], tot_loss[loss=0.3837, simple_loss=0.3498, pruned_loss=0.1498, ctc_loss=0.295, over 972224.50 frames. ], batch size: 19, lr: 3.07e-02,
2024-10-08 01:02:06,447 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=4534.0, ans=0.00988391304347826
2024-10-08 01:02:37,908 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=4540.666666666667, ans=0.28715625
2024-10-08 01:02:48,902 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.30 vs. limit=10.908
2024-10-08 01:02:51,398 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.19 vs. limit=10.908
2024-10-08 01:02:58,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=4547.333333333333, ans=0.7408433333333334
2024-10-08 01:03:02,026 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.25 vs. limit=10.910499999999999
2024-10-08 01:03:06,525 INFO [train.py:1153] Epoch 2, batch 6800, loss[loss=0.3814, simple_loss=0.3455, pruned_loss=0.1468, ctc_loss=0.3093, over 4912.00 frames. ], tot_loss[loss=0.3792, simple_loss=0.3468, pruned_loss=0.1476, ctc_loss=0.2909, over 974552.59 frames. ], batch size: 19, lr: 3.06e-02,
2024-10-08 01:03:11,134 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.68 vs. limit=10.913
2024-10-08 01:03:13,894 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=10.89 vs. limit=10.913
2024-10-08 01:03:17,350 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 01:03:32,669 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.33 vs. limit=10.918
2024-10-08 01:03:37,723 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=14.57 vs. limit=9.209
2024-10-08 01:03:39,642 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=4557.333333333333, ans=0.286375
2024-10-08 01:03:39,785 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=4557.333333333333, ans=0.26836
2024-10-08 01:03:55,571 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.prob, batch_count=4560.666666666667, ans=0.28621874999999997
2024-10-08 01:04:08,953 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=4564.0, ans=0.74026
2024-10-08 01:04:10,084 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.132e+02 1.507e+02 1.716e+02 1.907e+02 3.150e+02, threshold=3.432e+02, percent-clipped=0.0
2024-10-08 01:04:10,999 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.40 vs. limit=9.211500000000001
2024-10-08 01:04:12,726 INFO [train.py:1153] Epoch 2, batch 6850, loss[loss=0.3447, simple_loss=0.3428, pruned_loss=0.121, ctc_loss=0.2614, over 4978.00 frames. ], tot_loss[loss=0.3747, simple_loss=0.3439, pruned_loss=0.1454, ctc_loss=0.2868, over 978900.94 frames. ], batch size: 19, lr: 3.06e-02,
2024-10-08 01:04:14,176 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/epoch-2.pt
2024-10-08 01:04:44,472 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=4568.0, ans=0.04763333333333333
2024-10-08 01:04:45,244 INFO [train.py:1153] Epoch 3, batch 0, loss[loss=0.3689, simple_loss=0.3554, pruned_loss=0.1343, ctc_loss=0.2846, over 4853.00 frames. ], tot_loss[loss=0.3689, simple_loss=0.3554, pruned_loss=0.1343, ctc_loss=0.2846, over 4853.00 frames. ], batch size: 19, lr: 2.91e-02,
2024-10-08 01:04:45,246 INFO [train.py:1176] Computing validation loss
2024-10-08 01:04:50,999 INFO [train.py:1185] Epoch 3, validation: loss=0.2652, simple_loss=0.3178, pruned_loss=0.07617, ctc_loss=0.1507, over 90464.00 frames.
2024-10-08 01:04:51,000 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 01:04:53,301 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=4568.0, ans=0.285875
2024-10-08 01:04:55,649 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.17 vs. limit=5.8271999999999995
2024-10-08 01:04:59,203 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.72 vs. limit=7.284
2024-10-08 01:05:07,988 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=4571.333333333333, ans=0.28571875
2024-10-08 01:05:21,427 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=4574.666666666667, ans=0.009875072463768116
2024-10-08 01:05:32,865 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=4578.0, ans=0.28540625
2024-10-08 01:05:33,518 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.87 vs. limit=3.6867
2024-10-08 01:05:45,624 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=4581.333333333333, ans=0.025
2024-10-08 01:05:51,957 INFO [train.py:1153] Epoch 3, batch 50, loss[loss=0.4051, simple_loss=0.3606, pruned_loss=0.1651, ctc_loss=0.2981, over 4910.00 frames. ], tot_loss[loss=0.4148, simple_loss=0.3659, pruned_loss=0.1664, ctc_loss=0.3268, over 217751.89 frames. ], batch size: 19, lr: 2.90e-02,
2024-10-08 01:05:54,678 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=4584.666666666667, ans=0.28509375000000003
2024-10-08 01:06:05,070 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=4588.0, ans=0.04755
2024-10-08 01:06:06,305 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=4588.0, ans=0.07
2024-10-08 01:06:08,961 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=4588.0, ans=0.2849375
2024-10-08 01:06:10,371 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=4588.0, ans=0.2849375
2024-10-08 01:06:34,279 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=4594.666666666667, ans=0.25405333333333335
2024-10-08 01:06:42,329 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.71 vs. limit=3.6897
2024-10-08 01:06:50,779 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.248e+02 1.671e+02 1.822e+02 2.103e+02 3.738e+02, threshold=3.644e+02, percent-clipped=1.0
2024-10-08 01:06:55,748 INFO [train.py:1153] Epoch 3, batch 100, loss[loss=0.4186, simple_loss=0.375, pruned_loss=0.1634, ctc_loss=0.3386, over 4748.00 frames. ], tot_loss[loss=0.4114, simple_loss=0.364, pruned_loss=0.1644, ctc_loss=0.3249, over 383381.49 frames. ], batch size: 19, lr: 2.90e-02,
2024-10-08 01:07:02,249 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=4601.333333333333, ans=0.04749444444444445
2024-10-08 01:07:06,544 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.21 vs. limit=9.2255
2024-10-08 01:07:30,812 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.33 vs. limit=10.956
2024-10-08 01:07:44,744 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=12.35 vs. limit=7.305666666666666
2024-10-08 01:07:49,659 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.41 vs. limit=9.2305
2024-10-08 01:07:51,276 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=9.10 vs. limit=9.2305
2024-10-08 01:07:55,754 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=4614.666666666667, ans=0.2836875
2024-10-08 01:07:59,364 INFO [train.py:1153] Epoch 3, batch 150, loss[loss=0.3391, simple_loss=0.3238, pruned_loss=0.1277, ctc_loss=0.2471, over 4910.00 frames. ], tot_loss[loss=0.404, simple_loss=0.3604, pruned_loss=0.1602, ctc_loss=0.3182, over 513301.12 frames. ], batch size: 19, lr: 2.89e-02,
2024-10-08 01:08:10,301 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=5.06 vs. limit=9.23175
2024-10-08 01:08:14,862 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4621.333333333333, ans=0.25378666666666666
2024-10-08 01:08:25,580 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.12 vs. limit=9.23425
2024-10-08 01:08:30,627 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.38 vs. limit=5.849866666666667
2024-10-08 01:08:42,701 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=4628.0, ans=0.2830625
2024-10-08 01:08:45,642 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.77 vs. limit=3.6942
2024-10-08 01:08:57,052 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.57 vs. limit=10.9735
2024-10-08 01:08:57,778 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.259e+02 1.618e+02 1.834e+02 2.128e+02 3.876e+02, threshold=3.667e+02, percent-clipped=1.0
2024-10-08 01:09:02,849 INFO [train.py:1153] Epoch 3, batch 200, loss[loss=0.4496, simple_loss=0.3823, pruned_loss=0.184, ctc_loss=0.3724, over 4732.00 frames. ], tot_loss[loss=0.3976, simple_loss=0.3572, pruned_loss=0.1567, ctc_loss=0.3113, over 613779.36 frames. ], batch size: 45, lr: 2.89e-02,
2024-10-08 01:09:04,214 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=4634.666666666667, ans=0.28275
2024-10-08 01:09:21,285 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.38 vs. limit=10.9785
2024-10-08 01:09:25,904 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=4638.0, ans=0.03550625
2024-10-08 01:10:01,150 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=4648.0, ans=0.009859130434782608
2024-10-08 01:10:05,828 INFO [train.py:1153] Epoch 3, batch 250, loss[loss=0.4254, simple_loss=0.3863, pruned_loss=0.1648, ctc_loss=0.3372, over 4819.00 frames. ], tot_loss[loss=0.4006, simple_loss=0.3591, pruned_loss=0.1583, ctc_loss=0.3137, over 692437.34 frames. ], batch size: 38, lr: 2.89e-02,
2024-10-08 01:10:13,470 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=4651.333333333333, ans=0.00985840579710145
2024-10-08 01:10:13,512 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=4651.333333333333, ans=0.25348666666666664
2024-10-08 01:10:14,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=4651.333333333333, ans=0.25348666666666664
2024-10-08 01:10:37,663 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=4658.0, ans=0.28165625
2024-10-08 01:10:41,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=4658.0, ans=0.0708875
2024-10-08 01:11:04,531 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.245e+02 1.635e+02 1.890e+02 2.142e+02 2.785e+02, threshold=3.780e+02, percent-clipped=0.0
2024-10-08 01:11:09,574 INFO [train.py:1153] Epoch 3, batch 300, loss[loss=0.4342, simple_loss=0.3781, pruned_loss=0.1773, ctc_loss=0.3394, over 4751.00 frames. ], tot_loss[loss=0.4004, simple_loss=0.3592, pruned_loss=0.158, ctc_loss=0.3139, over 752839.27 frames. ], batch size: 32, lr: 2.88e-02,
2024-10-08 01:11:10,330 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten.whitening_limit, batch_count=4668.0, ans=9.2505
2024-10-08 01:11:19,825 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=4668.0, ans=0.73662
2024-10-08 01:11:21,190 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=4671.333333333333, ans=0.28103125
2024-10-08 01:11:30,062 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=4671.333333333333, ans=0.0
2024-10-08 01:11:35,039 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=4674.666666666667, ans=0.04949747468305833
2024-10-08 01:11:40,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.71 vs. limit=3.7012
2024-10-08 01:12:04,550 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.27 vs. limit=9.2555
2024-10-08 01:12:05,470 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4681.333333333333, ans=0.25318666666666667
2024-10-08 01:12:12,857 INFO [train.py:1153] Epoch 3, batch 350, loss[loss=0.3174, simple_loss=0.3126, pruned_loss=0.1144, ctc_loss=0.2337, over 4883.00 frames. ], tot_loss[loss=0.3973, simple_loss=0.3575, pruned_loss=0.1565, ctc_loss=0.3106, over 800287.03 frames. ], batch size: 19, lr: 2.88e-02,
2024-10-08 01:12:25,295 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.27 vs. limit=6.172
2024-10-08 01:12:28,852 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.65 vs. limit=3.7032
2024-10-08 01:13:10,955 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.188e+02 1.740e+02 1.822e+02 2.011e+02 4.338e+02, threshold=3.643e+02, percent-clipped=1.0
2024-10-08 01:13:13,607 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=4698.0, ans=0.04709166666666667
2024-10-08 01:13:16,105 INFO [train.py:1153] Epoch 3, batch 400, loss[loss=0.3872, simple_loss=0.3565, pruned_loss=0.1492, ctc_loss=0.2984, over 4891.00 frames. ], tot_loss[loss=0.3963, simple_loss=0.3566, pruned_loss=0.1562, ctc_loss=0.3089, over 836908.40 frames. ], batch size: 22, lr: 2.87e-02,
2024-10-08 01:13:27,431 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.59 vs. limit=11.026
2024-10-08 01:13:42,044 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.33 vs. limit=9.2655
2024-10-08 01:13:48,962 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=4708.0, ans=0.73522
2024-10-08 01:13:49,554 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.34 vs. limit=9.2655
2024-10-08 01:13:49,718 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=3.43 vs. limit=5.8832
2024-10-08 01:14:02,620 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=4711.333333333333, ans=0.27915625
2024-10-08 01:14:05,167 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=4714.666666666667, ans=0.025
2024-10-08 01:14:14,179 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=4714.666666666667, ans=0.7349866666666667
2024-10-08 01:14:18,901 INFO [train.py:1153] Epoch 3, batch 450, loss[loss=0.3408, simple_loss=0.3326, pruned_loss=0.1229, ctc_loss=0.2584, over 4838.00 frames. ], tot_loss[loss=0.3934, simple_loss=0.3552, pruned_loss=0.1545, ctc_loss=0.3065, over 865371.94 frames. ], batch size: 23, lr: 2.87e-02,
2024-10-08 01:14:24,013 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=4718.0, ans=0.025
2024-10-08 01:14:32,252 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.10 vs. limit=9.2705
2024-10-08 01:14:33,250 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.36 vs. limit=11.041
2024-10-08 01:14:53,591 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.54 vs. limit=11.0435
2024-10-08 01:15:03,973 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.22 vs. limit=9.273
2024-10-08 01:15:05,275 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.57 vs. limit=11.046
2024-10-08 01:15:15,341 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=6.72 vs. limit=9.27425
2024-10-08 01:15:17,523 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.340e+02 1.612e+02 1.744e+02 1.901e+02 2.942e+02, threshold=3.487e+02, percent-clipped=0.0
2024-10-08 01:15:22,399 INFO [train.py:1153] Epoch 3, batch 500, loss[loss=0.3908, simple_loss=0.3607, pruned_loss=0.1532, ctc_loss=0.2862, over 4784.00 frames. ], tot_loss[loss=0.3922, simple_loss=0.3551, pruned_loss=0.1538, ctc_loss=0.3042, over 887930.17 frames. ], batch size: 34, lr: 2.87e-02,
2024-10-08 01:15:22,597 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=4734.666666666667, ans=0.7342866666666666
2024-10-08 01:15:26,938 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.63 vs. limit=3.7102
2024-10-08 01:15:38,203 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.51 vs. limit=6.1845
2024-10-08 01:15:47,341 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.56 vs. limit=9.278
2024-10-08 01:15:57,437 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.00 vs. limit=3.7112
2024-10-08 01:16:09,420 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=4744.666666666667, ans=0.27759375
2024-10-08 01:16:10,699 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=4744.666666666667, ans=0.009838115942028986
2024-10-08 01:16:18,693 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.24 vs. limit=9.2805
2024-10-08 01:16:25,873 INFO [train.py:1153] Epoch 3, batch 550, loss[loss=0.3998, simple_loss=0.3525, pruned_loss=0.1575, ctc_loss=0.3302, over 4811.00 frames. ], tot_loss[loss=0.3918, simple_loss=0.3547, pruned_loss=0.1537, ctc_loss=0.3036, over 905469.28 frames. ], batch size: 40, lr: 2.86e-02,
2024-10-08 01:16:32,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.93 vs. limit=11.0635
2024-10-08 01:16:42,598 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=4754.666666666667, ans=0.009835942028985507
2024-10-08 01:16:55,058 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4758.0, ans=0.25242
2024-10-08 01:17:04,009 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=4761.333333333333, ans=0.2768125
2024-10-08 01:17:23,777 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.228e+02 1.700e+02 1.944e+02 2.235e+02 5.391e+02, threshold=3.888e+02, percent-clipped=1.0
2024-10-08 01:17:28,950 INFO [train.py:1153] Epoch 3, batch 600, loss[loss=0.3953, simple_loss=0.3548, pruned_loss=0.1555, ctc_loss=0.3121, over 4834.00 frames. ], tot_loss[loss=0.3889, simple_loss=0.3536, pruned_loss=0.152, ctc_loss=0.3009, over 919397.50 frames. ], batch size: 38, lr: 2.86e-02,
2024-10-08 01:17:40,482 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=4771.333333333333, ans=0.00983231884057971
2024-10-08 01:17:45,525 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4771.333333333333, ans=0.25228666666666666
2024-10-08 01:17:49,438 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=4771.333333333333, ans=0.7330033333333333
2024-10-08 01:17:58,690 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.74 vs. limit=9.2905
2024-10-08 01:18:18,731 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=4781.333333333333, ans=0.009830144927536233
2024-10-08 01:18:18,787 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=4781.333333333333, ans=0.04674444444444445
2024-10-08 01:18:20,046 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=4781.333333333333, ans=0.04674444444444445
2024-10-08 01:18:32,524 INFO [train.py:1153] Epoch 3, batch 650, loss[loss=0.3817, simple_loss=0.3524, pruned_loss=0.1479, ctc_loss=0.2878, over 4830.00 frames. ], tot_loss[loss=0.3862, simple_loss=0.3527, pruned_loss=0.1504, ctc_loss=0.2974, over 930122.99 frames. ], batch size: 21, lr: 2.85e-02,
2024-10-08 01:18:47,773 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=4788.0, ans=0.2755625
2024-10-08 01:19:09,454 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=4794.666666666667, ans=0.27525
2024-10-08 01:19:28,229 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4798.0, ans=0.25202
2024-10-08 01:19:30,781 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.331e+02 1.659e+02 1.794e+02 1.996e+02 5.289e+02, threshold=3.588e+02, percent-clipped=2.0
2024-10-08 01:19:34,674 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=4801.333333333333, ans=0.2749375
2024-10-08 01:19:35,753 INFO [train.py:1153] Epoch 3, batch 700, loss[loss=0.3387, simple_loss=0.3235, pruned_loss=0.1262, ctc_loss=0.2538, over 4744.00 frames. ], tot_loss[loss=0.3883, simple_loss=0.3543, pruned_loss=0.1514, ctc_loss=0.2989, over 937973.14 frames. ], batch size: 19, lr: 2.85e-02,
2024-10-08 01:19:44,940 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=10.13 vs. limit=9.3005
2024-10-08 01:19:45,869 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=4801.333333333333, ans=0.2749375
2024-10-08 01:19:45,885 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=4801.333333333333, ans=0.7980133333333334
2024-10-08 01:19:49,608 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=4804.666666666667, ans=0.27478125
2024-10-08 01:19:55,981 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=4804.666666666667, ans=0.7318366666666667
2024-10-08 01:19:59,703 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=4808.0, ans=0.274625
2024-10-08 01:20:03,401 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4808.0, ans=0.25192
2024-10-08 01:20:23,204 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=6.06 vs. limit=9.30425
2024-10-08 01:20:23,916 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=4811.333333333333, ans=0.7316033333333334
2024-10-08 01:20:29,009 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=4814.666666666667, ans=0.7314866666666666
2024-10-08 01:20:32,825 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=4814.666666666667, ans=0.27431249999999996
2024-10-08 01:20:39,178 INFO [train.py:1153] Epoch 3, batch 750, loss[loss=0.3648, simple_loss=0.3419, pruned_loss=0.1369, ctc_loss=0.2846, over 4881.00 frames. ], tot_loss[loss=0.3848, simple_loss=0.3524, pruned_loss=0.1495, ctc_loss=0.2957, over 944846.41 frames. ], batch size: 22, lr: 2.85e-02,
2024-10-08 01:20:39,337 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=4818.0, ans=0.025
2024-10-08 01:20:48,224 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=4818.0, ans=0.25182
2024-10-08 01:21:12,324 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=4824.666666666667, ans=0.04656388888888889
2024-10-08 01:21:37,189 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.187e+02 1.605e+02 1.854e+02 2.015e+02 2.982e+02, threshold=3.709e+02, percent-clipped=0.0
2024-10-08 01:21:41,166 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=4834.666666666667, ans=0.07
2024-10-08 01:21:42,249 INFO [train.py:1153] Epoch 3, batch 800, loss[loss=0.3447, simple_loss=0.3289, pruned_loss=0.1249, ctc_loss=0.2766, over 4854.00 frames. ], tot_loss[loss=0.3834, simple_loss=0.3511, pruned_loss=0.1489, ctc_loss=0.295, over 949610.04 frames. ], batch size: 19, lr: 2.84e-02,
2024-10-08 01:22:23,159 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=4844.666666666667, ans=0.27290625
2024-10-08 01:22:31,407 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.92 vs. limit=11.1335
2024-10-08 01:22:33,794 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.39 vs. limit=11.136
2024-10-08 01:22:45,774 INFO [train.py:1153] Epoch 3, batch 850, loss[loss=0.4008, simple_loss=0.3566, pruned_loss=0.1572, ctc_loss=0.3264, over 4784.00 frames. ], tot_loss[loss=0.3829, simple_loss=0.3505, pruned_loss=0.1487, ctc_loss=0.2946, over 953994.92 frames. ], batch size: 29, lr: 2.84e-02,
2024-10-08 01:22:48,571 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=4851.333333333333, ans=0.7302033333333333
2024-10-08 01:23:02,845 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.14 vs. limit=11.141
2024-10-08 01:23:05,433 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.68 vs. limit=9.3205
2024-10-08 01:23:12,367 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=4858.0, ans=0.27228125000000003
2024-10-08 01:23:43,841 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.166e+02 1.646e+02 1.858e+02 2.050e+02 4.594e+02, threshold=3.717e+02, percent-clipped=1.0
2024-10-08 01:23:48,917 INFO [train.py:1153] Epoch 3, batch 900, loss[loss=0.392, simple_loss=0.3623, pruned_loss=0.1503, ctc_loss=0.3029, over 4853.00 frames. ], tot_loss[loss=0.3849, simple_loss=0.3513, pruned_loss=0.1498, ctc_loss=0.2967, over 956834.94 frames. ], batch size: 19, lr: 2.83e-02,
2024-10-08 01:23:55,504 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=4868.0, ans=0.04638333333333333
2024-10-08 01:24:00,257 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten.whitening_limit, batch_count=4868.0, ans=9.3255
2024-10-08 01:24:01,159 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.24 vs. limit=11.151
2024-10-08 01:24:06,161 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=4871.333333333333, ans=0.025
2024-10-08 01:24:16,521 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.71 vs. limit=11.156
2024-10-08 01:24:32,730 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn1.whiten.whitening_limit, batch_count=4878.0, ans=11.1585
2024-10-08 01:24:33,718 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.33 vs. limit=5.9512
2024-10-08 01:24:37,381 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.42 vs. limit=11.1585
2024-10-08 01:24:38,325 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.prob, batch_count=4881.333333333333, ans=0.2711875
2024-10-08 01:24:44,503 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=4881.333333333333, ans=0.025
2024-10-08 01:24:50,854 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=4884.666666666667, ans=0.025
2024-10-08 01:24:51,834 INFO [train.py:1153] Epoch 3, batch 950, loss[loss=0.3358, simple_loss=0.3217, pruned_loss=0.1234, ctc_loss=0.2579, over 4814.00 frames. ], tot_loss[loss=0.3862, simple_loss=0.3523, pruned_loss=0.1504, ctc_loss=0.2985, over 958831.21 frames. ], batch size: 19, lr: 2.83e-02,
2024-10-08 01:25:06,595 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=4888.0, ans=0.270875
2024-10-08 01:25:21,215 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.39 vs. limit=11.1685
2024-10-08 01:25:30,306 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=4894.666666666667, ans=0.04627222222222222
2024-10-08 01:25:36,674 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=4894.666666666667, ans=0.27056250000000004
2024-10-08 01:25:49,038 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.273e+02 1.623e+02 1.829e+02 2.022e+02 3.635e+02, threshold=3.657e+02, percent-clipped=0.0
2024-10-08 01:25:54,250 INFO [train.py:1153] Epoch 3, batch 1000, loss[loss=0.3568, simple_loss=0.3511, pruned_loss=0.1286, ctc_loss=0.2633, over 4933.00 frames. ], tot_loss[loss=0.3892, simple_loss=0.3542, pruned_loss=0.1519, ctc_loss=0.3013, over 960663.23 frames. ], batch size: 20, lr: 2.83e-02,
2024-10-08 01:26:19,614 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=4908.0, ans=0.04621666666666667
2024-10-08 01:26:22,703 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.05 vs. limit=6.227
2024-10-08 01:26:25,977 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=4908.0, ans=0.04621666666666667
2024-10-08 01:26:27,643 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.93 vs. limit=11.181000000000001
2024-10-08 01:26:37,881 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.42 vs. limit=11.1835
2024-10-08 01:26:47,551 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=4914.666666666667, ans=0.269625
2024-10-08 01:26:51,399 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=8.361e-02
2024-10-08 01:26:57,396 INFO [train.py:1153] Epoch 3, batch 1050, loss[loss=0.4182, simple_loss=0.375, pruned_loss=0.1648, ctc_loss=0.3293, over 4808.00 frames. ], tot_loss[loss=0.3873, simple_loss=0.3532, pruned_loss=0.1509, ctc_loss=0.2992, over 962743.06 frames. ], batch size: 25, lr: 2.82e-02,
2024-10-08 01:26:58,908 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=4918.0, ans=0.046175
2024-10-08 01:27:07,153 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.68 vs. limit=3.7377000000000002
2024-10-08 01:27:36,907 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=4928.0, ans=0.269
2024-10-08 01:27:47,283 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.64 vs. limit=7.4656666666666665
2024-10-08 01:27:48,127 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4931.333333333333, ans=0.25068666666666667
2024-10-08 01:27:49,378 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 01:27:55,673 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.169e+02 1.620e+02 1.781e+02 1.957e+02 3.103e+02, threshold=3.562e+02, percent-clipped=0.0
2024-10-08 01:27:59,561 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=4934.666666666667, ans=0.26868749999999997
2024-10-08 01:28:00,600 INFO [train.py:1153] Epoch 3, batch 1100, loss[loss=0.353, simple_loss=0.3317, pruned_loss=0.1324, ctc_loss=0.274, over 4853.00 frames. ], tot_loss[loss=0.3869, simple_loss=0.3534, pruned_loss=0.1506, ctc_loss=0.2983, over 964075.80 frames. ], batch size: 20, lr: 2.82e-02,
2024-10-08 01:28:02,419 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.82 vs. limit=11.201
2024-10-08 01:28:03,811 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=6.89 vs. limit=9.3505
2024-10-08 01:28:06,185 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.98 vs. limit=11.201
2024-10-08 01:28:13,881 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.14 vs. limit=9.35175
2024-10-08 01:28:21,429 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=9.53 vs. limit=9.35175
2024-10-08 01:28:23,604 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=4938.0, ans=0.7271700000000001
2024-10-08 01:28:31,621 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys.whitening_limit, batch_count=4941.333333333333, ans=3.7412
2024-10-08 01:28:32,933 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.91 vs. limit=9.353
2024-10-08 01:28:41,838 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=18.07 vs. limit=11.2085
2024-10-08 01:28:51,412 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=4948.0, ans=0.2680625
2024-10-08 01:28:56,681 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.56 vs. limit=9.3555
2024-10-08 01:28:59,355 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.65 vs. limit=11.211
2024-10-08 01:29:01,796 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.54 vs. limit=11.211
2024-10-08 01:29:03,829 INFO [train.py:1153] Epoch 3, batch 1150, loss[loss=0.4251, simple_loss=0.3882, pruned_loss=0.1659, ctc_loss=0.3258, over 4849.00 frames. ], tot_loss[loss=0.3871, simple_loss=0.353, pruned_loss=0.1509, ctc_loss=0.2985, over 964446.24 frames. ], batch size: 20, lr: 2.81e-02,
2024-10-08 01:29:17,298 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.75 vs. limit=3.7432
2024-10-08 01:29:17,388 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.27 vs. limit=11.216000000000001
2024-10-08 01:29:24,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.75 vs. limit=3.7432
2024-10-08 01:29:44,782 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.96 vs. limit=9.3605
2024-10-08 01:29:46,662 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=4961.333333333333, ans=0.04599444444444445
2024-10-08 01:29:51,001 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.88 vs. limit=7.480666666666666
2024-10-08 01:30:00,636 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=1.036e-02
2024-10-08 01:30:01,541 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.386e+02 1.654e+02 1.827e+02 2.020e+02 2.728e+02, threshold=3.654e+02, percent-clipped=0.0
2024-10-08 01:30:02,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=4964.666666666667, ans=0.0
2024-10-08 01:30:06,695 INFO [train.py:1153] Epoch 3, batch 1200, loss[loss=0.3851, simple_loss=0.3425, pruned_loss=0.1543, ctc_loss=0.2974, over 4816.00 frames. ], tot_loss[loss=0.3871, simple_loss=0.3531, pruned_loss=0.1508, ctc_loss=0.2985, over 964407.43 frames. ], batch size: 25, lr: 2.81e-02,
2024-10-08 01:30:13,327 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 01:30:41,278 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=4974.666666666667, ans=0.2668125
2024-10-08 01:30:42,397 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4974.666666666667, ans=0.2502533333333333
2024-10-08 01:31:07,421 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=4981.333333333333, ans=0.04591111111111112
2024-10-08 01:31:09,725 INFO [train.py:1153] Epoch 3, batch 1250, loss[loss=0.432, simple_loss=0.3792, pruned_loss=0.1742, ctc_loss=0.3407, over 4796.00 frames. ], tot_loss[loss=0.3867, simple_loss=0.353, pruned_loss=0.1506, ctc_loss=0.2982, over 964358.53 frames. ], batch size: 32, lr: 2.81e-02,
2024-10-08 01:31:18,049 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.60 vs. limit=9.369250000000001
2024-10-08 01:31:25,678 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.54 vs. limit=11.241
2024-10-08 01:31:29,919 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=4988.0, ans=0.27482
2024-10-08 01:31:31,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module2.whiten, num_groups=1, num_channels=192, metric=3.43 vs. limit=9.3705
2024-10-08 01:31:52,597 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=4994.666666666667, ans=0.035
2024-10-08 01:32:07,640 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.260e+02 1.643e+02 1.840e+02 2.072e+02 4.254e+02, threshold=3.679e+02, percent-clipped=2.0
2024-10-08 01:32:10,155 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4998.0, ans=0.25002
2024-10-08 01:32:10,812 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.97 vs. limit=11.2485
2024-10-08 01:32:12,582 INFO [train.py:1153] Epoch 3, batch 1300, loss[loss=0.413, simple_loss=0.3727, pruned_loss=0.1641, ctc_loss=0.313, over 4821.00 frames. ], tot_loss[loss=0.3847, simple_loss=0.3515, pruned_loss=0.1498, ctc_loss=0.2957, over 965497.62 frames. ], batch size: 43, lr: 2.80e-02,
2024-10-08 01:32:13,768 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=5001.333333333333, ans=0.26556250000000003
2024-10-08 01:32:23,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=5004.666666666667, ans=0.26540625
2024-10-08 01:32:23,938 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=5004.666666666667, ans=0.04581388888888889
2024-10-08 01:32:54,350 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.66 vs. limit=9.37925
2024-10-08 01:32:58,968 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=5011.333333333333, ans=8.132083333333334
2024-10-08 01:32:59,289 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.65 vs. limit=11.2585
2024-10-08 01:33:01,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=5014.666666666667, ans=0.7244866666666667
2024-10-08 01:33:15,178 INFO [train.py:1153] Epoch 3, batch 1350, loss[loss=0.4049, simple_loss=0.3777, pruned_loss=0.1561, ctc_loss=0.2999, over 4859.00 frames. ], tot_loss[loss=0.383, simple_loss=0.351, pruned_loss=0.1487, ctc_loss=0.2938, over 966383.31 frames. ], batch size: 21, lr: 2.80e-02,
2024-10-08 01:33:30,472 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=5021.333333333333, ans=0.04574444444444445
2024-10-08 01:33:40,870 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.19 vs. limit=11.2685
2024-10-08 01:33:54,441 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=5028.0, ans=0.068575
2024-10-08 01:34:03,978 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.98 vs. limit=9.3855
2024-10-08 01:34:05,314 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.64 vs. limit=11.2735
2024-10-08 01:34:11,969 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.01 vs. limit=11.2735
2024-10-08 01:34:13,413 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.278e+02 1.609e+02 1.793e+02 2.001e+02 3.406e+02, threshold=3.586e+02, percent-clipped=0.0
2024-10-08 01:34:18,338 INFO [train.py:1153] Epoch 3, batch 1400, loss[loss=0.3229, simple_loss=0.3197, pruned_loss=0.1174, ctc_loss=0.2279, over 4940.00 frames. ], tot_loss[loss=0.3839, simple_loss=0.3512, pruned_loss=0.1494, ctc_loss=0.2948, over 966619.67 frames. ], batch size: 19, lr: 2.80e-02,
2024-10-08 01:34:37,701 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.07 vs. limit=9.38925
2024-10-08 01:34:41,379 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=11.44 vs. limit=9.38925
2024-10-08 01:34:41,462 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.03 vs. limit=9.38925
2024-10-08 01:35:09,940 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.91 vs. limit=3.7572
2024-10-08 01:35:20,604 INFO [train.py:1153] Epoch 3, batch 1450, loss[loss=0.424, simple_loss=0.3603, pruned_loss=0.1768, ctc_loss=0.3351, over 4790.00 frames. ], tot_loss[loss=0.3849, simple_loss=0.3515, pruned_loss=0.15, ctc_loss=0.2957, over 966514.54 frames. ], batch size: 34, lr: 2.79e-02,
2024-10-08 01:35:23,191 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=5051.333333333333, ans=0.26321875
2024-10-08 01:35:38,693 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.68 vs. limit=11.291
2024-10-08 01:35:40,966 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.71 vs. limit=11.291
2024-10-08 01:35:44,303 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5058.0, ans=0.24942
2024-10-08 01:35:52,891 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=5058.0, ans=0.26290625
2024-10-08 01:36:10,273 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=5064.666666666667, ans=0.26259374999999996
2024-10-08 01:36:17,911 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.272e+02 1.609e+02 1.755e+02 2.005e+02 3.342e+02, threshold=3.511e+02, percent-clipped=0.0
2024-10-08 01:36:18,764 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=8.35 vs. limit=9.39925
2024-10-08 01:36:22,920 INFO [train.py:1153] Epoch 3, batch 1500, loss[loss=0.3976, simple_loss=0.3626, pruned_loss=0.1545, ctc_loss=0.3092, over 4725.00 frames. ], tot_loss[loss=0.385, simple_loss=0.3518, pruned_loss=0.1499, ctc_loss=0.2962, over 966200.56 frames. ], batch size: 26, lr: 2.79e-02,
2024-10-08 01:36:23,688 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.66 vs. limit=11.301
2024-10-08 01:36:48,008 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=5074.666666666667, ans=0.7223866666666667
2024-10-08 01:36:53,105 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=5074.666666666667, ans=0.262125
2024-10-08 01:36:55,741 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=5074.666666666667, ans=0.262125
2024-10-08 01:37:18,576 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 01:37:19,711 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=5081.333333333333, ans=0.04549444444444445
2024-10-08 01:37:19,723 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=5081.333333333333, ans=0.04549444444444445
2024-10-08 01:37:23,382 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=5081.333333333333, ans=0.2618125
2024-10-08 01:37:23,424 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=5081.333333333333, ans=0.009764927536231885
2024-10-08 01:37:25,755 INFO [train.py:1153] Epoch 3, batch 1550, loss[loss=0.4622, simple_loss=0.3912, pruned_loss=0.1939, ctc_loss=0.3634, over 4843.00 frames. ], tot_loss[loss=0.3864, simple_loss=0.3525, pruned_loss=0.1505, ctc_loss=0.298, over 966131.15 frames. ], batch size: 31, lr: 2.79e-02,
2024-10-08 01:37:26,292 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.21 vs. limit=6.271166666666667
2024-10-08 01:37:32,712 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.70 vs. limit=11.3135
2024-10-08 01:37:55,753 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.04 vs. limit=11.3185
2024-10-08 01:38:06,851 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.90 vs. limit=11.321
2024-10-08 01:38:15,314 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=5098.0, ans=0.72157
2024-10-08 01:38:21,093 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.65 vs. limit=3.7647
2024-10-08 01:38:22,959 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=5098.0, ans=0.26103125
2024-10-08 01:38:23,916 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.239e+02 1.670e+02 1.841e+02 2.051e+02 3.353e+02, threshold=3.682e+02, percent-clipped=0.0
2024-10-08 01:38:28,845 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=13.06 vs. limit=11.326
2024-10-08 01:38:29,144 INFO [train.py:1153] Epoch 3, batch 1600, loss[loss=0.3308, simple_loss=0.3163, pruned_loss=0.1208, ctc_loss=0.2597, over 4813.00 frames. ], tot_loss[loss=0.3844, simple_loss=0.3517, pruned_loss=0.1494, ctc_loss=0.2958, over 966293.26 frames. ], batch size: 25, lr: 2.78e-02,
2024-10-08 01:38:38,962 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn1.whiten.whitening_limit, batch_count=5101.333333333333, ans=11.326
2024-10-08 01:38:43,938 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.57 vs. limit=11.3285
2024-10-08 01:38:45,343 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.45 vs. limit=7.552333333333333
2024-10-08 01:38:48,412 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=5104.666666666667, ans=0.009759855072463769
2024-10-08 01:39:02,301 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=5108.0, ans=0.26056250000000003
2024-10-08 01:39:07,256 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=5111.333333333333, ans=0.26040625
2024-10-08 01:39:11,132 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=5111.333333333333, ans=0.7211033333333334
2024-10-08 01:39:32,582 INFO [train.py:1153] Epoch 3, batch 1650, loss[loss=0.352, simple_loss=0.3331, pruned_loss=0.1304, ctc_loss=0.2753, over 4769.00 frames. ], tot_loss[loss=0.3841, simple_loss=0.3516, pruned_loss=0.1492, ctc_loss=0.2954, over 966869.12 frames. ], batch size: 29, lr: 2.78e-02,
2024-10-08 01:39:41,659 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5118.0, ans=0.24881999999999999
2024-10-08 01:39:54,591 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.14 vs. limit=9.4205
2024-10-08 01:39:57,506 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.14 vs. limit=6.281166666666667
2024-10-08 01:39:58,189 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=5124.666666666667, ans=0.009755507246376811
2024-10-08 01:40:00,860 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten.whitening_limit, batch_count=5124.666666666667, ans=9.42175
2024-10-08 01:40:27,455 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=5131.333333333333, ans=0.04528611111111112
2024-10-08 01:40:31,264 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.250e+02 1.624e+02 1.765e+02 2.009e+02 3.803e+02, threshold=3.530e+02, percent-clipped=1.0
2024-10-08 01:40:36,390 INFO [train.py:1153] Epoch 3, batch 1700, loss[loss=0.3874, simple_loss=0.3528, pruned_loss=0.1536, ctc_loss=0.287, over 4940.00 frames. ], tot_loss[loss=0.3828, simple_loss=0.3507, pruned_loss=0.1485, ctc_loss=0.2948, over 967050.32 frames. ], batch size: 19, lr: 2.77e-02,
2024-10-08 01:40:36,616 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=5134.666666666667, ans=0.2593125
2024-10-08 01:40:50,468 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=5138.0, ans=0.72017
2024-10-08 01:41:08,294 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=5141.333333333333, ans=0.07
2024-10-08 01:41:13,737 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.73 vs. limit=9.42925
2024-10-08 01:41:22,012 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=5144.666666666667, ans=0.04523055555555556
2024-10-08 01:41:38,474 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=5151.333333333333, ans=0.009749710144927537
2024-10-08 01:41:39,563 INFO [train.py:1153] Epoch 3, batch 1750, loss[loss=0.3719, simple_loss=0.34, pruned_loss=0.1448, ctc_loss=0.2854, over 4959.00 frames. ], tot_loss[loss=0.3804, simple_loss=0.3491, pruned_loss=0.1473, ctc_loss=0.2927, over 967082.60 frames. ], batch size: 19, lr: 2.77e-02,
2024-10-08 01:42:01,014 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.42 vs. limit=7.577333333333334
2024-10-08 01:42:21,662 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5161.333333333333, ans=0.24838666666666667
2024-10-08 01:42:26,650 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5161.333333333333, ans=0.24838666666666667
2024-10-08 01:42:37,796 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.270e+02 1.619e+02 1.830e+02 2.059e+02 3.958e+02, threshold=3.661e+02, percent-clipped=1.0
2024-10-08 01:42:42,768 INFO [train.py:1153] Epoch 3, batch 1800, loss[loss=0.4099, simple_loss=0.3808, pruned_loss=0.1576, ctc_loss=0.3093, over 4882.00 frames. ], tot_loss[loss=0.3811, simple_loss=0.35, pruned_loss=0.1476, ctc_loss=0.2925, over 967932.67 frames. ], batch size: 23, lr: 2.77e-02,
2024-10-08 01:42:52,546 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.77 vs. limit=11.376
2024-10-08 01:42:59,256 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=5171.333333333333, ans=0.25759375
2024-10-08 01:43:11,937 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=5174.666666666667, ans=0.2574375
2024-10-08 01:43:17,812 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.43 vs. limit=9.4405
2024-10-08 01:43:18,350 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=5174.666666666667, ans=0.04510555555555556
2024-10-08 01:43:28,142 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.50 vs. limit=6.0712
2024-10-08 01:43:45,932 INFO [train.py:1153] Epoch 3, batch 1850, loss[loss=0.3768, simple_loss=0.3547, pruned_loss=0.1427, ctc_loss=0.2839, over 4733.00 frames. ], tot_loss[loss=0.3799, simple_loss=0.3495, pruned_loss=0.1469, ctc_loss=0.2912, over 968082.61 frames. ], batch size: 26, lr: 2.76e-02,
2024-10-08 01:43:50,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=5184.666666666667, ans=0.025
2024-10-08 01:44:17,020 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=5191.333333333333, ans=0.025
2024-10-08 01:44:22,677 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.34 vs. limit=6.298666666666667
2024-10-08 01:44:35,780 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 01:44:43,065 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.124e+02 1.658e+02 1.767e+02 2.038e+02 4.166e+02, threshold=3.534e+02, percent-clipped=1.0
2024-10-08 01:44:43,536 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.79 vs. limit=11.3985
2024-10-08 01:44:48,028 INFO [train.py:1153] Epoch 3, batch 1900, loss[loss=0.4468, simple_loss=0.3936, pruned_loss=0.1784, ctc_loss=0.3581, over 4781.00 frames. ], tot_loss[loss=0.3828, simple_loss=0.3514, pruned_loss=0.1483, ctc_loss=0.2941, over 967979.38 frames. ], batch size: 29, lr: 2.76e-02,
2024-10-08 01:45:31,284 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.90 vs. limit=3.7817
2024-10-08 01:45:34,082 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.15 vs. limit=3.7817
2024-10-08 01:45:40,935 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=5214.666666666667, ans=0.7174866666666667
2024-10-08 01:45:50,799 INFO [train.py:1153] Epoch 3, batch 1950, loss[loss=0.3983, simple_loss=0.3635, pruned_loss=0.1549, ctc_loss=0.3083, over 4860.00 frames. ], tot_loss[loss=0.3822, simple_loss=0.3508, pruned_loss=0.148, ctc_loss=0.2942, over 966832.86 frames. ], batch size: 20, lr: 2.76e-02,
2024-10-08 01:45:50,964 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=5218.0, ans=0.7173700000000001
2024-10-08 01:45:53,724 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=10.93 vs. limit=11.413499999999999
2024-10-08 01:45:55,011 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.74 vs. limit=11.413499999999999
2024-10-08 01:45:57,170 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=5218.0, ans=0.025
2024-10-08 01:46:16,251 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.79 vs. limit=11.4185
2024-10-08 01:46:25,624 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5224.666666666667, ans=0.24775333333333333
2024-10-08 01:46:31,921 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=5228.0, ans=0.25493750000000004
2024-10-08 01:46:36,988 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=5228.0, ans=0.80228
2024-10-08 01:46:48,317 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.290e+02 1.802e+02 1.954e+02 2.168e+02 2.972e+02, threshold=3.909e+02, percent-clipped=0.0
2024-10-08 01:46:52,848 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.14 vs. limit=9.463000000000001
2024-10-08 01:46:53,513 INFO [train.py:1153] Epoch 3, batch 2000, loss[loss=0.3598, simple_loss=0.3394, pruned_loss=0.1362, ctc_loss=0.2693, over 4959.00 frames. ], tot_loss[loss=0.384, simple_loss=0.3515, pruned_loss=0.1491, ctc_loss=0.2955, over 966651.70 frames. ], batch size: 19, lr: 2.75e-02,
2024-10-08 01:46:56,819 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten.whitening_limit, batch_count=5234.666666666667, ans=9.463000000000001
2024-10-08 01:47:15,960 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=5238.0, ans=0.009730869565217392
2024-10-08 01:47:25,036 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=5241.333333333333, ans=0.009730144927536231
2024-10-08 01:47:34,031 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=5244.666666666667, ans=0.25415625
2024-10-08 01:47:34,246 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.70 vs. limit=9.46675
2024-10-08 01:47:36,692 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 01:47:56,396 INFO [train.py:1153] Epoch 3, batch 2050, loss[loss=0.3545, simple_loss=0.3213, pruned_loss=0.1402, ctc_loss=0.2684, over 4911.00 frames. ], tot_loss[loss=0.3831, simple_loss=0.3511, pruned_loss=0.1485, ctc_loss=0.2948, over 967015.70 frames. ], batch size: 19, lr: 2.75e-02,
2024-10-08 01:48:09,224 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=5254.666666666667, ans=0.7160866666666668
2024-10-08 01:48:18,253 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=5254.666666666667, ans=0.009727246376811594
2024-10-08 01:48:33,344 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=5261.333333333333, ans=0.7158533333333333
2024-10-08 01:48:42,518 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.80 vs. limit=3.7892
2024-10-08 01:48:54,559 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.283e+02 1.575e+02 1.764e+02 1.974e+02 3.475e+02, threshold=3.529e+02, percent-clipped=0.0
2024-10-08 01:48:59,655 INFO [train.py:1153] Epoch 3, batch 2100, loss[loss=0.3484, simple_loss=0.325, pruned_loss=0.1299, ctc_loss=0.28, over 4868.00 frames. ], tot_loss[loss=0.3806, simple_loss=0.3499, pruned_loss=0.1473, ctc_loss=0.2918, over 967194.34 frames. ], batch size: 21, lr: 2.75e-02,
2024-10-08 01:49:01,635 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.26 vs. limit=9.4755
2024-10-08 01:49:17,446 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.80 vs. limit=11.4535
2024-10-08 01:49:27,626 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=512, metric=3.01 vs. limit=9.478
2024-10-08 01:49:29,573 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=5274.666666666667, ans=0.25275000000000003
2024-10-08 01:49:35,101 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.67 vs. limit=11.456
2024-10-08 01:49:42,054 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=5278.0, ans=0.009722173913043478
2024-10-08 01:49:47,073 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5278.0, ans=0.24722
2024-10-08 01:49:50,106 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.55 vs. limit=11.461
2024-10-08 01:50:02,003 INFO [train.py:1153] Epoch 3, batch 2150, loss[loss=0.35, simple_loss=0.3351, pruned_loss=0.1288, ctc_loss=0.2683, over 4858.00 frames. ], tot_loss[loss=0.3786, simple_loss=0.3487, pruned_loss=0.1463, ctc_loss=0.2898, over 967926.42 frames. ], batch size: 20, lr: 2.74e-02,
2024-10-08 01:50:09,570 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=5284.666666666667, ans=0.04464722222222223
2024-10-08 01:50:12,175 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=5284.666666666667, ans=0.25228125
2024-10-08 01:50:21,552 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.61 vs. limit=11.466000000000001
2024-10-08 01:50:28,745 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=5291.333333333333, ans=0.04461944444444445
2024-10-08 01:50:29,878 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5291.333333333333, ans=0.24708666666666668
2024-10-08 01:50:34,778 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=5291.333333333333, ans=0.7148033333333333
2024-10-08 01:50:36,045 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=5291.333333333333, ans=0.25196874999999996
2024-10-08 01:50:41,762 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=16.38 vs. limit=9.4855
2024-10-08 01:50:59,967 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.039e+02 1.579e+02 1.763e+02 2.075e+02 3.074e+02, threshold=3.527e+02, percent-clipped=0.0
2024-10-08 01:51:03,001 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.91 vs. limit=3.7946999999999997
2024-10-08 01:51:05,024 INFO [train.py:1153] Epoch 3, batch 2200, loss[loss=0.3782, simple_loss=0.3339, pruned_loss=0.1507, ctc_loss=0.3027, over 4734.00 frames. ], tot_loss[loss=0.3791, simple_loss=0.3491, pruned_loss=0.1464, ctc_loss=0.2903, over 967621.86 frames. ], batch size: 26, lr: 2.74e-02,
2024-10-08 01:51:19,599 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.99 vs. limit=6.3261666666666665
2024-10-08 01:51:25,549 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=5304.666666666667, ans=0.2469533333333333
2024-10-08 01:51:38,675 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.40 vs. limit=11.481
2024-10-08 01:51:46,387 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.44 vs. limit=9.49175
2024-10-08 01:51:58,687 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=5314.666666666667, ans=0.04452222222222223
2024-10-08 01:52:02,543 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=5314.666666666667, ans=0.09899494936611666
2024-10-08 01:52:05,484 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.49 vs. limit=9.493
2024-10-08 01:52:08,672 INFO [train.py:1153] Epoch 3, batch 2250, loss[loss=0.3459, simple_loss=0.3379, pruned_loss=0.1274, ctc_loss=0.2475, over 4871.00 frames. ], tot_loss[loss=0.3787, simple_loss=0.349, pruned_loss=0.1462, ctc_loss=0.2901, over 967584.85 frames. ], batch size: 22, lr: 2.73e-02,
2024-10-08 01:52:12,554 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=5318.0, ans=0.25071875
2024-10-08 01:52:26,863 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5321.333333333333, ans=0.24678666666666665
2024-10-08 01:52:28,747 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=3.04 vs. limit=6.128533333333333
2024-10-08 01:52:43,921 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=5324.666666666667, ans=0.035
2024-10-08 01:52:51,706 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5328.0, ans=0.24672
2024-10-08 01:52:58,593 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.85 vs. limit=11.4985
2024-10-08 01:53:03,078 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=5331.333333333333, ans=0.7134033333333334
2024-10-08 01:53:04,304 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=5331.333333333333, ans=0.25009375
2024-10-08 01:53:05,596 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/checkpoint-16000.pt
2024-10-08 01:53:07,617 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.231e+02 1.601e+02 1.780e+02 2.008e+02 4.365e+02, threshold=3.559e+02, percent-clipped=2.0
2024-10-08 01:53:12,094 INFO [train.py:1153] Epoch 3, batch 2300, loss[loss=0.3387, simple_loss=0.3294, pruned_loss=0.1237, ctc_loss=0.2519, over 4883.00 frames. ], tot_loss[loss=0.3768, simple_loss=0.3481, pruned_loss=0.1452, ctc_loss=0.288, over 968262.63 frames. ], batch size: 19, lr: 2.73e-02,
2024-10-08 01:53:41,363 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=7.96 vs. limit=9.503
2024-10-08 01:53:43,048 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.79 vs. limit=3.8012
2024-10-08 01:53:49,665 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 01:53:54,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=5344.666666666667, ans=0.7129366666666667
2024-10-08 01:53:56,388 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.19 vs. limit=9.504249999999999
2024-10-08 01:54:14,432 INFO [train.py:1153] Epoch 3, batch 2350, loss[loss=0.3364, simple_loss=0.3291, pruned_loss=0.1229, ctc_loss=0.2447, over 4888.00 frames. ], tot_loss[loss=0.3768, simple_loss=0.3482, pruned_loss=0.1451, ctc_loss=0.2878, over 968426.88 frames. ], batch size: 23, lr: 2.73e-02,
2024-10-08 01:54:15,166 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.14 vs. limit=9.50675
2024-10-08 01:54:23,272 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=5351.333333333333, ans=0.24915625000000002
2024-10-08 01:54:28,774 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.71 vs. limit=9.508
2024-10-08 01:54:43,005 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5358.0, ans=0.24642
2024-10-08 01:54:52,369 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.22 vs. limit=9.5105
2024-10-08 01:55:05,759 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=5364.666666666667, ans=0.24853124999999998
2024-10-08 01:55:11,810 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.249e+02 1.599e+02 1.816e+02 2.000e+02 3.358e+02, threshold=3.633e+02, percent-clipped=0.0
2024-10-08 01:55:16,972 INFO [train.py:1153] Epoch 3, batch 2400, loss[loss=0.3659, simple_loss=0.3423, pruned_loss=0.1385, ctc_loss=0.2811, over 4755.00 frames. ], tot_loss[loss=0.3765, simple_loss=0.348, pruned_loss=0.145, ctc_loss=0.2877, over 967756.35 frames. ], batch size: 19, lr: 2.72e-02,
2024-10-08 01:55:21,278 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.02 vs. limit=11.526
2024-10-08 01:55:25,761 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=5368.0, ans=0.248375
2024-10-08 01:55:28,710 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=8.33 vs. limit=9.51425
2024-10-08 01:55:44,450 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=5374.666666666667, ans=0.04427222222222223
2024-10-08 01:55:45,797 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=5374.666666666667, ans=0.04949747468305833
2024-10-08 01:55:47,129 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 01:56:19,375 INFO [train.py:1153] Epoch 3, batch 2450, loss[loss=0.379, simple_loss=0.3483, pruned_loss=0.1429, ctc_loss=0.3095, over 4884.00 frames. ], tot_loss[loss=0.3781, simple_loss=0.3489, pruned_loss=0.1457, ctc_loss=0.2894, over 966894.84 frames. ], batch size: 22, lr: 2.72e-02,
2024-10-08 01:56:30,060 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.41 vs. limit=6.153866666666667
2024-10-08 01:56:38,321 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=5388.0, ans=0.009698260869565217
2024-10-08 01:56:39,577 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=5388.0, ans=0.04421666666666667
2024-10-08 01:56:44,665 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=5391.333333333333, ans=0.24728125
2024-10-08 01:56:46,001 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5391.333333333333, ans=0.24608666666666668
2024-10-08 01:56:47,408 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.52 vs. limit=11.5435
2024-10-08 01:56:57,742 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=11.53 vs. limit=11.546
2024-10-08 01:56:58,808 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.39 vs. limit=7.697333333333333
2024-10-08 01:56:58,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=6.31 vs. limit=6.157866666666667
2024-10-08 01:57:12,118 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=5398.0, ans=0.24696875000000001
2024-10-08 01:57:15,536 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=8.30 vs. limit=9.52425
2024-10-08 01:57:17,175 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.190e+02 1.609e+02 1.771e+02 1.926e+02 2.897e+02, threshold=3.542e+02, percent-clipped=0.0
2024-10-08 01:57:22,219 INFO [train.py:1153] Epoch 3, batch 2500, loss[loss=0.3666, simple_loss=0.3305, pruned_loss=0.1427, ctc_loss=0.2933, over 4725.00 frames. ], tot_loss[loss=0.3775, simple_loss=0.3485, pruned_loss=0.1455, ctc_loss=0.2887, over 966508.97 frames. ], batch size: 26, lr: 2.72e-02,
2024-10-08 01:57:27,377 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=5401.333333333333, ans=0.2468125
2024-10-08 01:57:53,533 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=5408.0, ans=0.2465
2024-10-08 01:58:25,114 INFO [train.py:1153] Epoch 3, batch 2550, loss[loss=0.3265, simple_loss=0.3128, pruned_loss=0.1197, ctc_loss=0.2518, over 4959.00 frames. ], tot_loss[loss=0.3791, simple_loss=0.3493, pruned_loss=0.1464, ctc_loss=0.2903, over 967015.16 frames. ], batch size: 19, lr: 2.71e-02,
2024-10-08 01:58:30,712 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.39 vs. limit=6.167199999999999
2024-10-08 01:58:33,255 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.31 vs. limit=6.3545
2024-10-08 01:58:33,853 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=5418.0, ans=0.04409166666666667
2024-10-08 01:58:43,025 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.52 vs. limit=9.533
2024-10-08 01:58:50,167 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=5424.666666666667, ans=0.24571874999999999
2024-10-08 01:58:52,548 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5424.666666666667, ans=0.24575333333333332
2024-10-08 01:59:03,650 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=5428.0, ans=0.24556250000000002
2024-10-08 01:59:03,655 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5428.0, ans=0.24572
2024-10-08 01:59:04,229 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=11.76 vs. limit=11.571
2024-10-08 01:59:18,070 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.86 vs. limit=3.8147
2024-10-08 01:59:22,338 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.227e+02 1.665e+02 1.877e+02 2.071e+02 4.611e+02, threshold=3.754e+02, percent-clipped=2.0
2024-10-08 01:59:27,331 INFO [train.py:1153] Epoch 3, batch 2600, loss[loss=0.3415, simple_loss=0.3248, pruned_loss=0.1273, ctc_loss=0.2592, over 4861.00 frames. ], tot_loss[loss=0.3785, simple_loss=0.3489, pruned_loss=0.1462, ctc_loss=0.2894, over 966572.97 frames. ], batch size: 20, lr: 2.71e-02,
2024-10-08 01:59:59,501 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.96 vs. limit=9.5405
2024-10-08 02:00:15,558 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.33 vs. limit=9.54175
2024-10-08 02:00:16,484 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=5448.0, ans=0.04396666666666667
2024-10-08 02:00:19,371 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.06 vs. limit=6.362
2024-10-08 02:00:30,152 INFO [train.py:1153] Epoch 3, batch 2650, loss[loss=0.4219, simple_loss=0.3712, pruned_loss=0.1689, ctc_loss=0.3369, over 4824.00 frames. ], tot_loss[loss=0.3805, simple_loss=0.3503, pruned_loss=0.1471, ctc_loss=0.2917, over 966183.60 frames. ], batch size: 38, lr: 2.71e-02,
2024-10-08 02:00:40,033 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.03 vs. limit=6.180533333333333
2024-10-08 02:00:49,592 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=5454.666666666667, ans=0.025
2024-10-08 02:00:53,171 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=5454.666666666667, ans=0.2443125
2024-10-08 02:01:15,437 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=5461.333333333333, ans=0.244
2024-10-08 02:01:28,043 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.194e+02 1.621e+02 1.830e+02 2.004e+02 4.396e+02, threshold=3.660e+02, percent-clipped=1.0
2024-10-08 02:01:32,911 INFO [train.py:1153] Epoch 3, batch 2700, loss[loss=0.3784, simple_loss=0.3661, pruned_loss=0.1378, ctc_loss=0.2874, over 4859.00 frames. ], tot_loss[loss=0.3797, simple_loss=0.3496, pruned_loss=0.1467, ctc_loss=0.291, over 966413.52 frames. ], batch size: 28, lr: 2.70e-02,
2024-10-08 02:01:42,423 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.70 vs. limit=11.600999999999999
2024-10-08 02:01:57,020 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 02:01:57,689 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.88 vs. limit=11.606
2024-10-08 02:02:02,272 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5474.666666666667, ans=0.24525333333333332
2024-10-08 02:02:03,492 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=5474.666666666667, ans=0.243375
2024-10-08 02:02:04,708 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=5474.666666666667, ans=0.243375
2024-10-08 02:02:11,231 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.63 vs. limit=11.6085
2024-10-08 02:02:12,239 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=5478.0, ans=0.04384166666666667
2024-10-08 02:02:16,456 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=7.88 vs. limit=9.55425
2024-10-08 02:02:29,177 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.84 vs. limit=11.611
2024-10-08 02:02:34,091 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten.whitening_limit, batch_count=5481.333333333333, ans=9.5555
2024-10-08 02:02:35,978 INFO [train.py:1153] Epoch 3, batch 2750, loss[loss=0.3931, simple_loss=0.3536, pruned_loss=0.159, ctc_loss=0.2864, over 4802.00 frames. ], tot_loss[loss=0.3767, simple_loss=0.3481, pruned_loss=0.1451, ctc_loss=0.2876, over 967104.01 frames. ], batch size: 19, lr: 2.70e-02,
2024-10-08 02:02:45,297 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.10 vs. limit=9.556750000000001
2024-10-08 02:02:45,665 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.74 vs. limit=3.8227
2024-10-08 02:02:51,258 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=5488.0, ans=0.009676521739130434
2024-10-08 02:02:58,859 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=5488.0, ans=0.24275000000000002
2024-10-08 02:02:58,865 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=5488.0, ans=0.24275000000000002
2024-10-08 02:03:01,418 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5491.333333333333, ans=0.24508666666666667
2024-10-08 02:03:09,124 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=5491.333333333333, ans=0.24259375
2024-10-08 02:03:12,800 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5494.666666666667, ans=0.24505333333333332
2024-10-08 02:03:16,585 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=5494.666666666667, ans=0.06565833333333335
2024-10-08 02:03:20,334 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=5494.666666666667, ans=0.7076866666666667
2024-10-08 02:03:24,627 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.49 vs. limit=9.560500000000001
2024-10-08 02:03:33,887 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.253e+02 1.622e+02 1.739e+02 1.961e+02 2.415e+02, threshold=3.479e+02, percent-clipped=0.0
2024-10-08 02:03:37,854 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.max_abs, batch_count=5501.333333333333, ans=8.438333333333333
2024-10-08 02:03:38,962 INFO [train.py:1153] Epoch 3, batch 2800, loss[loss=0.4133, simple_loss=0.3667, pruned_loss=0.1635, ctc_loss=0.3319, over 4771.00 frames. ], tot_loss[loss=0.3756, simple_loss=0.3469, pruned_loss=0.1448, ctc_loss=0.2869, over 967090.38 frames. ], batch size: 53, lr: 2.70e-02,
2024-10-08 02:04:30,263 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.26 vs. limit=11.636
2024-10-08 02:04:39,927 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=5518.0, ans=0.03275625
2024-10-08 02:04:41,064 INFO [train.py:1153] Epoch 3, batch 2850, loss[loss=0.2959, simple_loss=0.3167, pruned_loss=0.09821, ctc_loss=0.1967, over 4934.00 frames. ], tot_loss[loss=0.378, simple_loss=0.3485, pruned_loss=0.1459, ctc_loss=0.2893, over 967037.05 frames. ], batch size: 20, lr: 2.69e-02,
2024-10-08 02:04:42,489 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=5518.0, ans=0.28277
2024-10-08 02:04:45,220 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.30 vs. limit=9.56925
2024-10-08 02:04:49,209 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.54 vs. limit=9.56925
2024-10-08 02:04:53,554 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=5521.333333333333, ans=0.7067533333333333
2024-10-08 02:04:56,053 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=5521.333333333333, ans=0.24478666666666665
2024-10-08 02:04:57,274 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=5521.333333333333, ans=0.009669275362318841
2024-10-08 02:04:57,901 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=7.96 vs. limit=9.5705
2024-10-08 02:04:59,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=5521.333333333333, ans=0.2411875
2024-10-08 02:05:04,477 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=5524.666666666667, ans=0.24103124999999997
2024-10-08 02:05:14,458 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=5524.666666666667, ans=0.025
2024-10-08 02:05:24,431 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=5528.0, ans=0.043633333333333336
2024-10-08 02:05:37,907 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.118e+02 1.576e+02 1.735e+02 1.906e+02 2.913e+02, threshold=3.471e+02, percent-clipped=0.0
2024-10-08 02:05:39,274 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=5531.333333333333, ans=0.0
2024-10-08 02:05:42,925 INFO [train.py:1153] Epoch 3, batch 2900, loss[loss=0.3247, simple_loss=0.3137, pruned_loss=0.1217, ctc_loss=0.2304, over 4752.00 frames. ], tot_loss[loss=0.3773, simple_loss=0.3483, pruned_loss=0.1456, ctc_loss=0.2881, over 966250.42 frames. ], batch size: 20, lr: 2.69e-02,
2024-10-08 02:05:48,025 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer_ff2.min_abs, batch_count=5534.666666666667, ans=0.1
2024-10-08 02:05:58,227 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.min_positive, batch_count=5538.0, ans=0.0653875
2024-10-08 02:06:07,031 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 02:06:12,870 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=7.57 vs. limit=9.578
2024-10-08 02:06:13,456 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=5541.333333333333, ans=0.7060533333333334
2024-10-08 02:06:29,614 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5544.666666666667, ans=0.24455333333333332
2024-10-08 02:06:29,916 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.67 vs. limit=7.772333333333334
2024-10-08 02:06:36,301 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten.whitening_limit, batch_count=5548.0, ans=11.661
2024-10-08 02:06:45,582 INFO [train.py:1153] Epoch 3, batch 2950, loss[loss=0.3257, simple_loss=0.3242, pruned_loss=0.1156, ctc_loss=0.2398, over 4799.00 frames. ], tot_loss[loss=0.3761, simple_loss=0.3476, pruned_loss=0.145, ctc_loss=0.2868, over 966736.13 frames. ], batch size: 19, lr: 2.69e-02,
2024-10-08 02:07:22,538 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=5561.333333333333, ans=0.24438666666666667
2024-10-08 02:07:30,299 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.73 vs. limit=9.5855
2024-10-08 02:07:41,688 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.10 vs. limit=11.6735
2024-10-08 02:07:43,561 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.203e+02 1.578e+02 1.814e+02 1.994e+02 3.779e+02, threshold=3.628e+02, percent-clipped=1.0
2024-10-08 02:07:47,496 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=5568.0, ans=0.239
2024-10-08 02:07:48,050 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.48 vs. limit=11.676
2024-10-08 02:07:48,372 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.61 vs. limit=7.784
2024-10-08 02:07:48,641 INFO [train.py:1153] Epoch 3, batch 3000, loss[loss=0.3493, simple_loss=0.3344, pruned_loss=0.1291, ctc_loss=0.2651, over 4852.00 frames. ], tot_loss[loss=0.3754, simple_loss=0.3468, pruned_loss=0.1447, ctc_loss=0.2865, over 967399.23 frames. ], batch size: 21, lr: 2.68e-02,
2024-10-08 02:07:48,641 INFO [train.py:1176] Computing validation loss
2024-10-08 02:07:56,118 INFO [train.py:1185] Epoch 3, validation: loss=0.2413, simple_loss=0.3026, pruned_loss=0.0646, ctc_loss=0.1269, over 90464.00 frames.
2024-10-08 02:07:56,118 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 02:08:13,632 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=5571.333333333333, ans=0.28357
2024-10-08 02:08:13,655 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=5571.333333333333, ans=8.482083333333334
2024-10-08 02:08:25,286 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=13.14 vs. limit=7.787333333333334
2024-10-08 02:08:43,930 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=5578.0, ans=0.23853124999999997
2024-10-08 02:08:47,655 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=5581.333333333333, ans=0.7046533333333334
2024-10-08 02:08:58,257 INFO [train.py:1153] Epoch 3, batch 3050, loss[loss=0.3178, simple_loss=0.3169, pruned_loss=0.1121, ctc_loss=0.2365, over 4751.00 frames. ], tot_loss[loss=0.3756, simple_loss=0.3467, pruned_loss=0.1449, ctc_loss=0.2869, over 966865.61 frames. ], batch size: 19, lr: 2.68e-02,
2024-10-08 02:09:00,918 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=5584.666666666667, ans=0.07
2024-10-08 02:09:05,762 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=5584.666666666667, ans=0.025
2024-10-08 02:09:07,590 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=11.69 vs. limit=11.688500000000001
2024-10-08 02:09:08,300 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=5584.666666666667, ans=0.043397222222222226
2024-10-08 02:09:09,505 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=5588.0, ans=8.4925
2024-10-08 02:09:13,192 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=5588.0, ans=0.2380625
2024-10-08 02:09:14,949 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.79 vs. limit=3.8382
2024-10-08 02:09:18,174 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=5588.0, ans=0.2380625
2024-10-08 02:09:19,412 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=5588.0, ans=0.2380625
2024-10-08 02:09:25,010 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.19 vs. limit=9.59675
2024-10-08 02:09:36,755 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=5594.666666666667, ans=0.23775000000000002
2024-10-08 02:09:37,104 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=6.09 vs. limit=9.597999999999999
2024-10-08 02:09:49,117 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=5598.0, ans=0.70407
2024-10-08 02:09:51,621 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=5598.0, ans=0.0
2024-10-08 02:09:54,460 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=5598.0, ans=0.04334166666666667
2024-10-08 02:09:55,399 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.141e+02 1.637e+02 1.819e+02 1.968e+02 3.323e+02, threshold=3.638e+02, percent-clipped=0.0
2024-10-08 02:10:00,360 INFO [train.py:1153] Epoch 3, batch 3100, loss[loss=0.3525, simple_loss=0.3277, pruned_loss=0.1331, ctc_loss=0.2779, over 4835.00 frames. ], tot_loss[loss=0.3746, simple_loss=0.3462, pruned_loss=0.1444, ctc_loss=0.2855, over 966586.56 frames. ], batch size: 38, lr: 2.68e-02,
2024-10-08 02:10:23,759 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module1.whiten, num_groups=1, num_channels=192, metric=2.79 vs. limit=9.60175
2024-10-08 02:10:29,385 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=5608.0, ans=0.23712499999999997
2024-10-08 02:10:58,403 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=5614.666666666667, ans=0.2438533333333333
2024-10-08 02:11:02,124 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=5618.0, ans=0.009648260869565217
2024-10-08 02:11:03,184 INFO [train.py:1153] Epoch 3, batch 3150, loss[loss=0.4819, simple_loss=0.4125, pruned_loss=0.1984, ctc_loss=0.3863, over 4808.00 frames. ], tot_loss[loss=0.3714, simple_loss=0.3444, pruned_loss=0.1426, ctc_loss=0.2828, over 966855.46 frames. ], batch size: 40, lr: 2.67e-02,
2024-10-08 02:11:19,874 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 02:11:21,085 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=5621.333333333333, ans=0.28432
2024-10-08 02:11:21,905 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.07 vs. limit=6.405333333333333
2024-10-08 02:11:22,299 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=5621.333333333333, ans=0.2365
2024-10-08 02:11:29,787 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5624.666666666667, ans=0.24375333333333332
2024-10-08 02:11:45,996 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=5628.0, ans=0.2361875
2024-10-08 02:11:59,775 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=5631.333333333333, ans=0.23603125000000003
2024-10-08 02:12:00,803 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.188e+02 1.532e+02 1.731e+02 1.908e+02 3.287e+02, threshold=3.463e+02, percent-clipped=0.0
2024-10-08 02:12:05,864 INFO [train.py:1153] Epoch 3, batch 3200, loss[loss=0.3935, simple_loss=0.3705, pruned_loss=0.1522, ctc_loss=0.2803, over 4752.00 frames. ], tot_loss[loss=0.37, simple_loss=0.3437, pruned_loss=0.1418, ctc_loss=0.2817, over 967342.17 frames. ], batch size: 20, lr: 2.67e-02,
2024-10-08 02:12:08,352 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=5634.666666666667, ans=0.24365333333333333
2024-10-08 02:12:19,606 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=5638.0, ans=0.025
2024-10-08 02:12:22,185 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=5638.0, ans=0.24362
2024-10-08 02:12:25,159 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.80 vs. limit=11.7285
2024-10-08 02:12:44,722 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=5644.666666666667, ans=0.009642463768115943
2024-10-08 02:13:08,441 INFO [train.py:1153] Epoch 3, batch 3250, loss[loss=0.3807, simple_loss=0.3426, pruned_loss=0.1514, ctc_loss=0.2904, over 4864.00 frames. ], tot_loss[loss=0.3704, simple_loss=0.3439, pruned_loss=0.1421, ctc_loss=0.2817, over 967433.32 frames. ], batch size: 24, lr: 2.67e-02,
2024-10-08 02:13:14,929 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=5651.333333333333, ans=0.03233958333333334
2024-10-08 02:13:25,208 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=5654.666666666667, ans=0.23493750000000002
2024-10-08 02:13:30,539 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.44 vs. limit=6.261866666666666
2024-10-08 02:13:47,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten.whitening_limit, batch_count=5661.333333333333, ans=9.623000000000001
2024-10-08 02:13:48,783 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5661.333333333333, ans=0.24338666666666667
2024-10-08 02:13:51,429 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=5661.333333333333, ans=0.0
2024-10-08 02:14:06,315 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.148e+02 1.566e+02 1.762e+02 1.958e+02 2.481e+02, threshold=3.525e+02, percent-clipped=0.0
2024-10-08 02:14:07,763 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=5664.666666666667, ans=0.23446875
2024-10-08 02:14:09,462 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.71 vs. limit=9.62425
2024-10-08 02:14:11,326 INFO [train.py:1153] Epoch 3, batch 3300, loss[loss=0.4008, simple_loss=0.3593, pruned_loss=0.159, ctc_loss=0.3106, over 4828.00 frames. ], tot_loss[loss=0.3688, simple_loss=0.3428, pruned_loss=0.1415, ctc_loss=0.2795, over 967978.69 frames. ], batch size: 43, lr: 2.66e-02,
2024-10-08 02:14:11,490 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=5668.0, ans=0.23431249999999998
2024-10-08 02:14:33,312 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.06 vs. limit=3.8507
2024-10-08 02:14:36,462 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=1.304e-02
2024-10-08 02:14:38,761 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=5674.666666666667, ans=0.043022222222222226
2024-10-08 02:14:42,814 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.51 vs. limit=9.628
2024-10-08 02:14:58,971 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5678.0, ans=0.24322
2024-10-08 02:15:08,893 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=5681.333333333333, ans=0.2336875
2024-10-08 02:15:13,707 INFO [train.py:1153] Epoch 3, batch 3350, loss[loss=0.4011, simple_loss=0.3617, pruned_loss=0.1613, ctc_loss=0.2949, over 4792.00 frames. ], tot_loss[loss=0.3728, simple_loss=0.3449, pruned_loss=0.1438, ctc_loss=0.2826, over 967075.10 frames. ], batch size: 40, lr: 2.66e-02,
2024-10-08 02:15:14,131 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.31 vs. limit=7.842333333333333
2024-10-08 02:15:25,000 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=5688.0, ans=0.233375
2024-10-08 02:15:34,284 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=6.86 vs. limit=9.633
2024-10-08 02:15:40,337 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.48 vs. limit=6.276533333333333
2024-10-08 02:15:48,610 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5691.333333333333, ans=0.24308666666666667
2024-10-08 02:15:48,708 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=5691.333333333333, ans=8.557083333333333
2024-10-08 02:15:51,841 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.49 vs. limit=11.771
2024-10-08 02:16:00,128 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5694.666666666667, ans=0.24305333333333332
2024-10-08 02:16:11,227 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.278e+02 1.690e+02 1.884e+02 2.020e+02 3.230e+02, threshold=3.768e+02, percent-clipped=0.0
2024-10-08 02:16:11,368 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=5698.0, ans=0.23290624999999998
2024-10-08 02:16:16,191 INFO [train.py:1153] Epoch 3, batch 3400, loss[loss=0.2943, simple_loss=0.2873, pruned_loss=0.1075, ctc_loss=0.216, over 4959.00 frames. ], tot_loss[loss=0.3746, simple_loss=0.3457, pruned_loss=0.1447, ctc_loss=0.285, over 966800.52 frames. ], batch size: 19, lr: 2.66e-02,
2024-10-08 02:16:32,537 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=5704.666666666667, ans=0.042897222222222225
2024-10-08 02:16:42,815 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=4.06 vs. limit=9.6405
2024-10-08 02:17:02,258 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=5711.333333333333, ans=0.23228125
2024-10-08 02:17:02,389 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=5711.333333333333, ans=0.28567
2024-10-08 02:17:18,209 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.40 vs. limit=11.788499999999999
2024-10-08 02:17:18,771 INFO [train.py:1153] Epoch 3, batch 3450, loss[loss=0.3863, simple_loss=0.3566, pruned_loss=0.1491, ctc_loss=0.2944, over 4841.00 frames. ], tot_loss[loss=0.3755, simple_loss=0.3466, pruned_loss=0.1448, ctc_loss=0.2867, over 966952.54 frames. ], batch size: 43, lr: 2.65e-02,
2024-10-08 02:17:19,445 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.08 vs. limit=11.788499999999999
2024-10-08 02:17:20,238 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=5718.0, ans=0.23196875
2024-10-08 02:17:23,146 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=5718.0, ans=11.788499999999999
2024-10-08 02:17:29,017 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=5718.0, ans=0.025
2024-10-08 02:17:37,834 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=5721.333333333333, ans=0.23181249999999998
2024-10-08 02:17:58,420 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.08 vs. limit=11.796
2024-10-08 02:18:05,525 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=5728.0, ans=0.69952
2024-10-08 02:18:07,980 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=5731.333333333333, ans=0.6994033333333334
2024-10-08 02:18:11,609 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=5731.333333333333, ans=0.042786111111111115
2024-10-08 02:18:13,454 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.42 vs. limit=11.7985
2024-10-08 02:18:16,681 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.074e+02 1.602e+02 1.788e+02 2.017e+02 5.342e+02, threshold=3.576e+02, percent-clipped=2.0
2024-10-08 02:18:16,883 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=5731.333333333333, ans=0.0
2024-10-08 02:18:19,321 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=5731.333333333333, ans=0.03208958333333334
2024-10-08 02:18:21,801 INFO [train.py:1153] Epoch 3, batch 3500, loss[loss=0.3858, simple_loss=0.3679, pruned_loss=0.1431, ctc_loss=0.2939, over 4883.00 frames. ], tot_loss[loss=0.3754, simple_loss=0.347, pruned_loss=0.1445, ctc_loss=0.2871, over 967293.16 frames. ], batch size: 19, lr: 2.65e-02,
2024-10-08 02:18:28,132 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=5734.666666666667, ans=0.2311875
2024-10-08 02:18:31,839 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=5734.666666666667, ans=0.0
2024-10-08 02:18:36,050 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.71 vs. limit=9.65175
2024-10-08 02:18:53,164 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=5741.333333333333, ans=0.042744444444444446
2024-10-08 02:18:53,250 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=5741.333333333333, ans=0.042744444444444446
2024-10-08 02:18:58,521 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.18 vs. limit=9.654250000000001
2024-10-08 02:19:03,728 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.74 vs. limit=9.654250000000001
2024-10-08 02:19:13,472 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=5748.0, ans=0.0
2024-10-08 02:19:15,888 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=5748.0, ans=0.00962
2024-10-08 02:19:24,538 INFO [train.py:1153] Epoch 3, batch 3550, loss[loss=0.4308, simple_loss=0.3654, pruned_loss=0.1792, ctc_loss=0.3448, over 4778.00 frames. ], tot_loss[loss=0.3719, simple_loss=0.3452, pruned_loss=0.1426, ctc_loss=0.2839, over 967149.41 frames. ], batch size: 29, lr: 2.65e-02,
2024-10-08 02:19:41,090 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=5754.666666666667, ans=0.23025
2024-10-08 02:19:57,218 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=5758.0, ans=0.042675000000000005
2024-10-08 02:20:03,521 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=5761.333333333333, ans=0.0
2024-10-08 02:20:05,950 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5761.333333333333, ans=0.24238666666666667
2024-10-08 02:20:15,987 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=5764.666666666667, ans=0.025
2024-10-08 02:20:22,186 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.243e+02 1.580e+02 1.761e+02 1.951e+02 2.821e+02, threshold=3.522e+02, percent-clipped=0.0
2024-10-08 02:20:27,128 INFO [train.py:1153] Epoch 3, batch 3600, loss[loss=0.342, simple_loss=0.3405, pruned_loss=0.1233, ctc_loss=0.2423, over 4929.00 frames. ], tot_loss[loss=0.3706, simple_loss=0.3444, pruned_loss=0.142, ctc_loss=0.2824, over 967229.23 frames. ], batch size: 20, lr: 2.64e-02,
2024-10-08 02:20:38,828 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=5771.333333333333, ans=0.6980033333333333
2024-10-08 02:20:51,514 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=5774.666666666667, ans=0.22931249999999997
2024-10-08 02:20:52,608 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=5774.666666666667, ans=0.14658433333333334
2024-10-08 02:21:15,202 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=5778.0, ans=0.0638875
2024-10-08 02:21:16,501 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=5781.333333333333, ans=0.22899999999999998
2024-10-08 02:21:19,790 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.27 vs. limit=11.836
2024-10-08 02:21:25,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=5781.333333333333, ans=0.07
2024-10-08 02:21:30,017 INFO [train.py:1153] Epoch 3, batch 3650, loss[loss=0.3318, simple_loss=0.3252, pruned_loss=0.123, ctc_loss=0.231, over 4854.00 frames. ], tot_loss[loss=0.3703, simple_loss=0.3442, pruned_loss=0.1419, ctc_loss=0.2818, over 967729.50 frames. ], batch size: 31, lr: 2.64e-02,
2024-10-08 02:21:44,996 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=5788.0, ans=0.025
2024-10-08 02:21:46,361 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5788.0, ans=0.24212
2024-10-08 02:21:47,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=5788.0, ans=0.2286875
2024-10-08 02:22:10,154 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=5794.666666666667, ans=0.24205333333333331
2024-10-08 02:22:15,269 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=5794.666666666667, ans=0.009609855072463768
2024-10-08 02:22:27,756 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.183e+02 1.633e+02 1.806e+02 2.111e+02 3.227e+02, threshold=3.611e+02, percent-clipped=0.0
2024-10-08 02:22:32,698 INFO [train.py:1153] Epoch 3, batch 3700, loss[loss=0.3753, simple_loss=0.3467, pruned_loss=0.1447, ctc_loss=0.2861, over 4862.00 frames. ], tot_loss[loss=0.3688, simple_loss=0.3437, pruned_loss=0.1409, ctc_loss=0.2802, over 967099.11 frames. ], batch size: 24, lr: 2.64e-02,
2024-10-08 02:22:32,824 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=5801.333333333333, ans=0.2280625
2024-10-08 02:22:47,408 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.57 vs. limit=9.67675
2024-10-08 02:22:50,563 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=5804.666666666667, ans=0.22790624999999998
2024-10-08 02:22:57,036 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=5808.0, ans=0.22775
2024-10-08 02:23:19,722 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=5811.333333333333, ans=0.22759374999999998
2024-10-08 02:23:24,462 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=5814.666666666667, ans=0.22743750000000001
2024-10-08 02:23:27,045 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=5814.666666666667, ans=0.04243888888888889
2024-10-08 02:23:35,495 INFO [train.py:1153] Epoch 3, batch 3750, loss[loss=0.3657, simple_loss=0.3441, pruned_loss=0.1383, ctc_loss=0.2763, over 4959.00 frames. ], tot_loss[loss=0.3714, simple_loss=0.3452, pruned_loss=0.1422, ctc_loss=0.2834, over 967529.72 frames. ], batch size: 19, lr: 2.63e-02,
2024-10-08 02:23:36,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=5818.0, ans=0.69637
2024-10-08 02:23:49,444 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=5821.333333333333, ans=0.22712500000000002
2024-10-08 02:24:03,098 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=5824.666666666667, ans=0.09899494936611666
2024-10-08 02:24:08,320 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.40 vs. limit=9.68425
2024-10-08 02:24:15,897 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.63 vs. limit=9.685500000000001
2024-10-08 02:24:33,072 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.192e+02 1.591e+02 1.757e+02 2.019e+02 4.648e+02, threshold=3.514e+02, percent-clipped=1.0
2024-10-08 02:24:37,979 INFO [train.py:1153] Epoch 3, batch 3800, loss[loss=0.4153, simple_loss=0.3601, pruned_loss=0.1679, ctc_loss=0.3372, over 4773.00 frames. ], tot_loss[loss=0.3684, simple_loss=0.3435, pruned_loss=0.1406, ctc_loss=0.2802, over 967356.79 frames. ], batch size: 26, lr: 2.63e-02,
2024-10-08 02:24:40,573 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=5834.666666666667, ans=0.22649999999999998
2024-10-08 02:24:46,900 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=5834.666666666667, ans=0.22649999999999998
2024-10-08 02:24:46,950 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=5834.666666666667, ans=0.22649999999999998
2024-10-08 02:24:54,443 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=5838.0, ans=0.22634375
2024-10-08 02:25:33,866 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=5848.0, ans=0.225875
2024-10-08 02:25:39,983 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=5851.333333333333, ans=0.28776999999999997
2024-10-08 02:25:41,084 INFO [train.py:1153] Epoch 3, batch 3850, loss[loss=0.4088, simple_loss=0.3706, pruned_loss=0.1608, ctc_loss=0.3137, over 4825.00 frames. ], tot_loss[loss=0.3668, simple_loss=0.3426, pruned_loss=0.1398, ctc_loss=0.2788, over 967355.20 frames. ], batch size: 38, lr: 2.63e-02,
2024-10-08 02:25:46,380 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=5851.333333333333, ans=0.06342916666666668
2024-10-08 02:26:11,606 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=5858.0, ans=0.009596086956521739
2024-10-08 02:26:26,763 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=5861.333333333333, ans=0.22525
2024-10-08 02:26:32,416 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.60 vs. limit=7.932333333333334
2024-10-08 02:26:39,149 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.277e+02 1.543e+02 1.720e+02 1.923e+02 3.055e+02, threshold=3.439e+02, percent-clipped=0.0
2024-10-08 02:26:44,083 INFO [train.py:1153] Epoch 3, batch 3900, loss[loss=0.3512, simple_loss=0.3564, pruned_loss=0.1226, ctc_loss=0.2523, over 4700.00 frames. ], tot_loss[loss=0.3667, simple_loss=0.3425, pruned_loss=0.1396, ctc_loss=0.2791, over 966869.22 frames. ], batch size: 26, lr: 2.63e-02,
2024-10-08 02:26:51,133 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.18 vs. limit=11.901
2024-10-08 02:27:05,695 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.71 vs. limit=9.70175
2024-10-08 02:27:24,366 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=5878.0, ans=0.025
2024-10-08 02:27:41,978 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=5881.333333333333, ans=0.22431250000000003
2024-10-08 02:27:42,595 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=5881.333333333333, ans=11.911
2024-10-08 02:27:46,845 INFO [train.py:1153] Epoch 3, batch 3950, loss[loss=0.409, simple_loss=0.3669, pruned_loss=0.1599, ctc_loss=0.3281, over 4831.00 frames. ], tot_loss[loss=0.365, simple_loss=0.3416, pruned_loss=0.1386, ctc_loss=0.2779, over 967039.88 frames. ], batch size: 36, lr: 2.62e-02,
2024-10-08 02:27:48,834 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten.whitening_limit, batch_count=5884.666666666667, ans=11.913499999999999
2024-10-08 02:27:51,054 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten.whitening_limit, batch_count=5884.666666666667, ans=11.913499999999999
2024-10-08 02:28:02,283 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=7.031e-02
2024-10-08 02:28:25,834 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.22 vs. limit=6.4736666666666665
2024-10-08 02:28:28,196 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.62 vs. limit=9.7105
2024-10-08 02:28:31,885 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.80 vs. limit=3.8842
2024-10-08 02:28:44,886 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.180e+02 1.610e+02 1.816e+02 1.929e+02 3.434e+02, threshold=3.633e+02, percent-clipped=0.0
2024-10-08 02:28:49,894 INFO [train.py:1153] Epoch 3, batch 4000, loss[loss=0.3539, simple_loss=0.3475, pruned_loss=0.1298, ctc_loss=0.2517, over 4818.00 frames. ], tot_loss[loss=0.3646, simple_loss=0.3412, pruned_loss=0.1385, ctc_loss=0.2776, over 967180.30 frames. ], batch size: 19, lr: 2.62e-02,
2024-10-08 02:28:52,651 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=5901.333333333333, ans=0.223375
2024-10-08 02:28:56,792 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.66 vs. limit=9.713000000000001
2024-10-08 02:29:04,378 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.70 vs. limit=3.8857
2024-10-08 02:29:11,905 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.77 vs. limit=6.361866666666667
2024-10-08 02:29:23,398 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=19.20 vs. limit=9.7155
2024-10-08 02:29:24,184 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=5908.0, ans=0.009585217391304348
2024-10-08 02:29:35,797 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.45 vs. limit=11.9335
2024-10-08 02:29:36,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=5911.333333333333, ans=0.6931033333333334
2024-10-08 02:29:43,074 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=5914.666666666667, ans=0.042022222222222225
2024-10-08 02:29:49,416 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=5914.666666666667, ans=0.22275
2024-10-08 02:29:52,892 INFO [train.py:1153] Epoch 3, batch 4050, loss[loss=0.4195, simple_loss=0.3703, pruned_loss=0.1681, ctc_loss=0.3312, over 4778.00 frames. ], tot_loss[loss=0.3665, simple_loss=0.3425, pruned_loss=0.1395, ctc_loss=0.2785, over 967493.81 frames. ], batch size: 53, lr: 2.62e-02,
2024-10-08 02:29:56,580 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.prob, batch_count=5918.0, ans=0.22259374999999998
2024-10-08 02:30:10,489 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=5921.333333333333, ans=0.24078666666666668
2024-10-08 02:30:20,318 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=5924.666666666667, ans=0.6926366666666667
2024-10-08 02:30:24,370 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.35 vs. limit=9.72175
2024-10-08 02:30:45,402 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=2.58 vs. limit=9.72425
2024-10-08 02:30:50,147 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.092e+02 1.577e+02 1.738e+02 1.979e+02 2.991e+02, threshold=3.475e+02, percent-clipped=0.0
2024-10-08 02:30:51,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.53 vs. limit=11.9485
2024-10-08 02:30:55,086 INFO [train.py:1153] Epoch 3, batch 4100, loss[loss=0.3805, simple_loss=0.351, pruned_loss=0.1482, ctc_loss=0.2842, over 4855.00 frames. ], tot_loss[loss=0.3669, simple_loss=0.3428, pruned_loss=0.1397, ctc_loss=0.279, over 966874.33 frames. ], batch size: 31, lr: 2.61e-02,
2024-10-08 02:30:56,872 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=2.85 vs. limit=9.7255
2024-10-08 02:31:02,978 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=5934.666666666667, ans=0.22181250000000002
2024-10-08 02:31:25,791 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.65 vs. limit=3.8912
2024-10-08 02:31:27,987 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=5941.333333333333, ans=0.22149999999999997
2024-10-08 02:31:36,599 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=5944.666666666667, ans=0.22134375
2024-10-08 02:31:57,618 INFO [train.py:1153] Epoch 3, batch 4150, loss[loss=0.3573, simple_loss=0.347, pruned_loss=0.1299, ctc_loss=0.27, over 4758.00 frames. ], tot_loss[loss=0.3687, simple_loss=0.344, pruned_loss=0.1407, ctc_loss=0.2803, over 967037.12 frames. ], batch size: 20, lr: 2.61e-02,
2024-10-08 02:32:27,794 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=5958.0, ans=0.69147
2024-10-08 02:32:29,013 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=5958.0, ans=0.009574347826086956
2024-10-08 02:32:35,297 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=5961.333333333333, ans=0.28942
2024-10-08 02:32:40,469 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=1.831e-01
2024-10-08 02:32:40,475 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=5961.333333333333, ans=0.2205625
2024-10-08 02:32:47,305 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.86 vs. limit=7.982333333333333
2024-10-08 02:32:55,134 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.264e+02 1.632e+02 1.875e+02 2.035e+02 3.546e+02, threshold=3.750e+02, percent-clipped=1.0
2024-10-08 02:32:55,300 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=5964.666666666667, ans=0.2403533333333333
2024-10-08 02:33:00,103 INFO [train.py:1153] Epoch 3, batch 4200, loss[loss=0.3742, simple_loss=0.3396, pruned_loss=0.1432, ctc_loss=0.3059, over 4840.00 frames. ], tot_loss[loss=0.3682, simple_loss=0.3439, pruned_loss=0.1404, ctc_loss=0.2793, over 967257.29 frames. ], batch size: 31, lr: 2.61e-02,
2024-10-08 02:33:07,904 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=5968.0, ans=0.07
2024-10-08 02:33:09,036 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=5968.0, ans=0.24031999999999998
2024-10-08 02:33:23,469 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.31 vs. limit=9.73925
2024-10-08 02:33:35,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=5974.666666666667, ans=0.009570724637681159
2024-10-08 02:33:46,179 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.53 vs. limit=11.9835
2024-10-08 02:33:48,173 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer_ff2.min_abs, batch_count=5978.0, ans=0.1
2024-10-08 02:33:56,137 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.68 vs. limit=6.495333333333333
2024-10-08 02:34:03,028 INFO [train.py:1153] Epoch 3, batch 4250, loss[loss=0.3049, simple_loss=0.3069, pruned_loss=0.1062, ctc_loss=0.2263, over 4751.00 frames. ], tot_loss[loss=0.3643, simple_loss=0.3417, pruned_loss=0.1382, ctc_loss=0.2761, over 967186.42 frames. ], batch size: 19, lr: 2.60e-02,
2024-10-08 02:34:19,612 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=5988.0, ans=0.21931250000000002
2024-10-08 02:34:31,538 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=11.91 vs. limit=11.993500000000001
2024-10-08 02:34:40,068 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.26 vs. limit=9.748000000000001
2024-10-08 02:34:43,387 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=5994.666666666667, ans=0.009566376811594203
2024-10-08 02:35:00,797 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.206e+02 1.592e+02 1.749e+02 1.971e+02 3.063e+02, threshold=3.498e+02, percent-clipped=0.0
2024-10-08 02:35:05,851 INFO [train.py:1153] Epoch 3, batch 4300, loss[loss=0.3525, simple_loss=0.3247, pruned_loss=0.1325, ctc_loss=0.2885, over 4830.00 frames. ], tot_loss[loss=0.3651, simple_loss=0.3421, pruned_loss=0.1387, ctc_loss=0.2766, over 967467.95 frames. ], batch size: 21, lr: 2.60e-02,
2024-10-08 02:35:12,141 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=6001.333333333333, ans=0.21868749999999998
2024-10-08 02:35:12,211 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.min_abs, batch_count=6001.333333333333, ans=0.29002
2024-10-08 02:35:14,683 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=6001.333333333333, ans=0.21868749999999998
2024-10-08 02:35:20,791 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=6004.666666666667, ans=0.21853125
2024-10-08 02:35:32,970 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=6008.0, ans=0.21837499999999999
2024-10-08 02:35:44,106 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=6011.333333333333, ans=0.041619444444444445
2024-10-08 02:35:44,526 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.48 vs. limit=12.0085
2024-10-08 02:35:48,033 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.65 vs. limit=9.754249999999999
2024-10-08 02:35:49,696 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=3.08 vs. limit=6.404533333333333
2024-10-08 02:35:57,748 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6014.666666666667, ans=0.2398533333333333
2024-10-08 02:36:02,799 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=6014.666666666667, ans=0.2180625
2024-10-08 02:36:05,682 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.58 vs. limit=12.011
2024-10-08 02:36:07,655 INFO [train.py:1153] Epoch 3, batch 4350, loss[loss=0.3492, simple_loss=0.3426, pruned_loss=0.1261, ctc_loss=0.2595, over 4832.00 frames. ], tot_loss[loss=0.3669, simple_loss=0.3433, pruned_loss=0.1398, ctc_loss=0.2776, over 966343.99 frames. ], batch size: 21, lr: 2.60e-02,
2024-10-08 02:36:41,450 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=6024.666666666667, ans=0.025
2024-10-08 02:36:49,551 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.94 vs. limit=9.7605
2024-10-08 02:36:56,226 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=6031.333333333333, ans=0.21728124999999998
2024-10-08 02:37:04,728 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.247e+02 1.609e+02 1.787e+02 1.965e+02 3.869e+02, threshold=3.574e+02, percent-clipped=1.0
2024-10-08 02:37:05,326 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.48 vs. limit=9.76175
2024-10-08 02:37:09,796 INFO [train.py:1153] Epoch 3, batch 4400, loss[loss=0.35, simple_loss=0.3305, pruned_loss=0.1331, ctc_loss=0.2584, over 4733.00 frames. ], tot_loss[loss=0.3668, simple_loss=0.3432, pruned_loss=0.1397, ctc_loss=0.2773, over 965986.74 frames. ], batch size: 26, lr: 2.59e-02,
2024-10-08 02:37:12,440 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=6034.666666666667, ans=0.6887866666666667
2024-10-08 02:37:23,698 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6038.0, ans=0.23962
2024-10-08 02:37:44,735 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=6041.333333333333, ans=0.21681250000000002
2024-10-08 02:37:48,636 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=6044.666666666667, ans=0.6884366666666667
2024-10-08 02:38:06,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=6048.0, ans=0.21650000000000003
2024-10-08 02:38:12,191 INFO [train.py:1153] Epoch 3, batch 4450, loss[loss=0.3441, simple_loss=0.3381, pruned_loss=0.1249, ctc_loss=0.2508, over 4883.00 frames. ], tot_loss[loss=0.3677, simple_loss=0.3436, pruned_loss=0.1401, ctc_loss=0.2787, over 966268.77 frames. ], batch size: 19, lr: 2.59e-02,
2024-10-08 02:38:17,344 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=6051.333333333333, ans=0.025
2024-10-08 02:38:25,263 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.33 vs. limit=12.041
2024-10-08 02:38:27,905 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.80 vs. limit=8.027333333333333
2024-10-08 02:38:40,973 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=6058.0, ans=0.025
2024-10-08 02:38:50,389 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=16.71 vs. limit=8.030666666666667
2024-10-08 02:39:04,549 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6064.666666666667, ans=0.2393533333333333
2024-10-08 02:39:09,494 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.266e+02 1.584e+02 1.720e+02 1.862e+02 4.194e+02, threshold=3.440e+02, percent-clipped=1.0
2024-10-08 02:39:14,421 INFO [train.py:1153] Epoch 3, batch 4500, loss[loss=0.4467, simple_loss=0.3831, pruned_loss=0.1876, ctc_loss=0.3379, over 4836.00 frames. ], tot_loss[loss=0.3679, simple_loss=0.3439, pruned_loss=0.1402, ctc_loss=0.279, over 966294.40 frames. ], batch size: 28, lr: 2.59e-02,
2024-10-08 02:39:21,034 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=3.672e-01
2024-10-08 02:39:29,005 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=3.05 vs. limit=6.428533333333333
2024-10-08 02:39:42,085 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=6074.666666666667, ans=0.6873866666666667
2024-10-08 02:39:48,317 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=6074.666666666667, ans=0.009548985507246376
2024-10-08 02:40:03,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=6081.333333333333, ans=0.2149375
2024-10-08 02:40:06,099 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.89 vs. limit=6.520333333333333
2024-10-08 02:40:13,240 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=10.78 vs. limit=9.7805
2024-10-08 02:40:16,551 INFO [train.py:1153] Epoch 3, batch 4550, loss[loss=0.308, simple_loss=0.3017, pruned_loss=0.111, ctc_loss=0.2305, over 4856.00 frames. ], tot_loss[loss=0.3669, simple_loss=0.3431, pruned_loss=0.1398, ctc_loss=0.2776, over 966131.79 frames. ], batch size: 20, lr: 2.58e-02,
2024-10-08 02:40:18,280 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.77 vs. limit=3.9127
2024-10-08 02:40:22,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=6084.666666666667, ans=0.21478124999999998
2024-10-08 02:40:30,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=6088.0, ans=0.214625
2024-10-08 02:40:52,337 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.23 vs. limit=8.045666666666666
2024-10-08 02:41:13,834 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.198e+02 1.530e+02 1.691e+02 1.866e+02 4.004e+02, threshold=3.383e+02, percent-clipped=1.0
2024-10-08 02:41:14,068 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6098.0, ans=0.23902
2024-10-08 02:41:18,750 INFO [train.py:1153] Epoch 3, batch 4600, loss[loss=0.4398, simple_loss=0.3761, pruned_loss=0.1835, ctc_loss=0.341, over 4744.00 frames. ], tot_loss[loss=0.3662, simple_loss=0.3425, pruned_loss=0.1396, ctc_loss=0.2768, over 966414.35 frames. ], batch size: 45, lr: 2.58e-02,
2024-10-08 02:41:23,948 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=6101.333333333333, ans=0.041244444444444445
2024-10-08 02:41:33,101 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.52 vs. limit=12.0785
2024-10-08 02:41:43,644 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=6108.0, ans=0.21368749999999997
2024-10-08 02:41:52,261 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=6108.0, ans=0.041216666666666665
2024-10-08 02:41:52,322 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=6108.0, ans=0.23892
2024-10-08 02:42:04,627 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=6111.333333333333, ans=0.21353125
2024-10-08 02:42:08,157 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.81 vs. limit=8.057333333333334
2024-10-08 02:42:08,630 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=6114.666666666667, ans=0.6859866666666667
2024-10-08 02:42:09,832 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=6114.666666666667, ans=0.21337499999999998
2024-10-08 02:42:09,849 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=6114.666666666667, ans=0.6859866666666667
2024-10-08 02:42:12,092 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6114.666666666667, ans=0.2388533333333333
2024-10-08 02:42:20,528 INFO [train.py:1153] Epoch 3, batch 4650, loss[loss=0.3755, simple_loss=0.3511, pruned_loss=0.1412, ctc_loss=0.2938, over 4833.00 frames. ], tot_loss[loss=0.3648, simple_loss=0.342, pruned_loss=0.1388, ctc_loss=0.2748, over 965625.55 frames. ], batch size: 36, lr: 2.58e-02,
2024-10-08 02:42:30,795 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.09 vs. limit=12.0885
2024-10-08 02:42:35,917 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=16.08 vs. limit=12.091000000000001
2024-10-08 02:42:46,581 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.81 vs. limit=9.79675
2024-10-08 02:43:05,221 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.61 vs. limit=6.4512
2024-10-08 02:43:17,041 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.163e+02 1.560e+02 1.727e+02 1.912e+02 3.100e+02, threshold=3.454e+02, percent-clipped=0.0
2024-10-08 02:43:21,887 INFO [train.py:1153] Epoch 3, batch 4700, loss[loss=0.3381, simple_loss=0.3289, pruned_loss=0.1248, ctc_loss=0.2445, over 4940.00 frames. ], tot_loss[loss=0.3627, simple_loss=0.3409, pruned_loss=0.1376, ctc_loss=0.2733, over 965504.98 frames. ], batch size: 19, lr: 2.58e-02,
2024-10-08 02:43:26,494 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.38 vs. limit=8.067333333333334
2024-10-08 02:43:38,247 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=6138.0, ans=0.041091666666666665
2024-10-08 02:43:40,209 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.98 vs. limit=8.068999999999999
2024-10-08 02:43:42,470 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.65 vs. limit=12.1035
2024-10-08 02:43:44,443 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=6138.0, ans=0.041091666666666665
2024-10-08 02:43:54,260 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=6141.333333333333, ans=0.212125
2024-10-08 02:44:07,288 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.77 vs. limit=12.1085
2024-10-08 02:44:08,506 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.17 vs. limit=6.5361666666666665
2024-10-08 02:44:21,851 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=6148.0, ans=0.81148
2024-10-08 02:44:24,137 INFO [train.py:1153] Epoch 3, batch 4750, loss[loss=0.3626, simple_loss=0.3482, pruned_loss=0.1341, ctc_loss=0.2723, over 4734.00 frames. ], tot_loss[loss=0.364, simple_loss=0.3415, pruned_loss=0.1384, ctc_loss=0.2743, over 965342.80 frames. ], batch size: 45, lr: 2.57e-02,
2024-10-08 02:44:54,268 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=6158.0, ans=0.21134375
2024-10-08 02:45:07,413 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.92 vs. limit=12.121
2024-10-08 02:45:10,572 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=6161.333333333333, ans=0.21118749999999997
2024-10-08 02:45:19,205 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=6164.666666666667, ans=0.21103125
2024-10-08 02:45:21,410 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.241e+02 1.606e+02 1.811e+02 1.989e+02 2.979e+02, threshold=3.623e+02, percent-clipped=0.0
2024-10-08 02:45:26,393 INFO [train.py:1153] Epoch 3, batch 4800, loss[loss=0.3237, simple_loss=0.3145, pruned_loss=0.1195, ctc_loss=0.2349, over 4903.00 frames. ], tot_loss[loss=0.3635, simple_loss=0.3414, pruned_loss=0.138, ctc_loss=0.2739, over 965755.66 frames. ], batch size: 22, lr: 2.57e-02,
2024-10-08 02:45:33,325 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.11 vs. limit=3.9252000000000002
2024-10-08 02:45:37,916 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=6171.333333333333, ans=0.0
2024-10-08 02:45:40,432 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=6171.333333333333, ans=0.23828666666666667
2024-10-08 02:45:46,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=6171.333333333333, ans=0.025
2024-10-08 02:45:48,009 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=6171.333333333333, ans=0.23828666666666667
2024-10-08 02:45:49,274 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=6171.333333333333, ans=0.21071875
2024-10-08 02:46:07,861 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=6178.0, ans=0.21040625000000002
2024-10-08 02:46:11,658 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=6178.0, ans=0.21040625000000002
2024-10-08 02:46:16,792 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=6181.333333333333, ans=0.21025
2024-10-08 02:46:20,669 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=6181.333333333333, ans=0.21025
2024-10-08 02:46:29,188 INFO [train.py:1153] Epoch 3, batch 4850, loss[loss=0.3962, simple_loss=0.3559, pruned_loss=0.1577, ctc_loss=0.3029, over 4847.00 frames. ], tot_loss[loss=0.3612, simple_loss=0.3401, pruned_loss=0.1367, ctc_loss=0.2721, over 966401.69 frames. ], batch size: 28, lr: 2.57e-02,
2024-10-08 02:46:53,052 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=6191.333333333333, ans=0.040869444444444444
2024-10-08 02:46:58,684 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.15 vs. limit=12.1435
2024-10-08 02:47:04,081 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=6191.333333333333, ans=0.0
2024-10-08 02:47:12,731 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.max_abs, batch_count=6194.666666666667, ans=8.871666666666666
2024-10-08 02:47:24,398 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.65 vs. limit=12.1485
2024-10-08 02:47:26,408 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.089e+02 1.490e+02 1.685e+02 1.909e+02 2.579e+02, threshold=3.371e+02, percent-clipped=0.0
2024-10-08 02:47:28,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=6198.0, ans=0.025
2024-10-08 02:47:31,188 INFO [train.py:1153] Epoch 3, batch 4900, loss[loss=0.372, simple_loss=0.3423, pruned_loss=0.1447, ctc_loss=0.2808, over 4848.00 frames. ], tot_loss[loss=0.3628, simple_loss=0.3409, pruned_loss=0.1377, ctc_loss=0.2734, over 967082.33 frames. ], batch size: 21, lr: 2.56e-02,
2024-10-08 02:47:52,186 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=6204.666666666667, ans=0.009520724637681159
2024-10-08 02:47:52,828 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.99 vs. limit=9.82675
2024-10-08 02:47:53,913 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.10 vs. limit=3.9307
2024-10-08 02:48:04,205 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=6208.0, ans=0.00952
2024-10-08 02:48:09,528 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.04 vs. limit=3.9317
2024-10-08 02:48:32,307 INFO [train.py:1153] Epoch 3, batch 4950, loss[loss=0.4255, simple_loss=0.3737, pruned_loss=0.1703, ctc_loss=0.3418, over 4783.00 frames. ], tot_loss[loss=0.3647, simple_loss=0.3416, pruned_loss=0.1389, ctc_loss=0.2754, over 966807.76 frames. ], batch size: 53, lr: 2.56e-02,
2024-10-08 02:48:48,748 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6221.333333333333, ans=0.23778666666666667
2024-10-08 02:48:49,861 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6221.333333333333, ans=0.23778666666666667
2024-10-08 02:48:49,879 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=6221.333333333333, ans=0.040744444444444444
2024-10-08 02:49:16,167 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=2.311e-02
2024-10-08 02:49:29,662 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.129e+02 1.627e+02 1.781e+02 1.914e+02 3.212e+02, threshold=3.562e+02, percent-clipped=0.0
2024-10-08 02:49:33,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=6234.666666666667, ans=0.025
2024-10-08 02:49:34,254 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.68 vs. limit=6.558666666666667
2024-10-08 02:49:34,806 INFO [train.py:1153] Epoch 3, batch 5000, loss[loss=0.3041, simple_loss=0.3094, pruned_loss=0.1068, ctc_loss=0.2131, over 4751.00 frames. ], tot_loss[loss=0.3634, simple_loss=0.3409, pruned_loss=0.1381, ctc_loss=0.2741, over 967640.14 frames. ], batch size: 29, lr: 2.56e-02,
2024-10-08 02:49:46,383 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.87 vs. limit=6.4952000000000005
2024-10-08 02:49:52,412 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=6238.0, ans=0.20759375000000002
2024-10-08 02:49:56,472 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.58 vs. limit=9.83925
2024-10-08 02:50:06,750 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.07 vs. limit=9.8405
2024-10-08 02:50:14,224 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.20 vs. limit=9.841750000000001
2024-10-08 02:50:21,196 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=6244.666666666667, ans=0.07
2024-10-08 02:50:23,761 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=6248.0, ans=0.207125
2024-10-08 02:50:36,270 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6251.333333333333, ans=0.23748666666666668
2024-10-08 02:50:37,352 INFO [train.py:1153] Epoch 3, batch 5050, loss[loss=0.3315, simple_loss=0.3366, pruned_loss=0.1184, ctc_loss=0.2235, over 4852.00 frames. ], tot_loss[loss=0.3608, simple_loss=0.3396, pruned_loss=0.1367, ctc_loss=0.2716, over 968572.55 frames. ], batch size: 19, lr: 2.56e-02,
2024-10-08 02:50:38,692 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=6251.333333333333, ans=0.009510579710144927
2024-10-08 02:50:43,677 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=6251.333333333333, ans=0.009510579710144927
2024-10-08 02:50:46,238 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=6251.333333333333, ans=0.29377
2024-10-08 02:50:49,940 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=6254.666666666667, ans=0.025
2024-10-08 02:50:53,627 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=6254.666666666667, ans=0.025
2024-10-08 02:51:05,846 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=6258.0, ans=0.040591666666666665
2024-10-08 02:51:12,052 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.min_positive, batch_count=6258.0, ans=0.18741999999999998
2024-10-08 02:51:13,462 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=6261.333333333333, ans=0.20650000000000002
2024-10-08 02:51:22,230 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=6261.333333333333, ans=0.20650000000000002
2024-10-08 02:51:34,719 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.204e+02 1.501e+02 1.651e+02 1.834e+02 2.531e+02, threshold=3.303e+02, percent-clipped=0.0
2024-10-08 02:51:39,199 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.03 vs. limit=12.201
2024-10-08 02:51:39,767 INFO [train.py:1153] Epoch 3, batch 5100, loss[loss=0.3691, simple_loss=0.3504, pruned_loss=0.1388, ctc_loss=0.2757, over 4814.00 frames. ], tot_loss[loss=0.3629, simple_loss=0.341, pruned_loss=0.1378, ctc_loss=0.2732, over 967940.39 frames. ], batch size: 19, lr: 2.55e-02,
2024-10-08 02:51:50,225 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=6268.0, ans=0.009506956521739131
2024-10-08 02:51:57,482 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=6271.333333333333, ans=0.20603125
2024-10-08 02:51:57,992 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.72 vs. limit=6.567833333333333
2024-10-08 02:52:20,261 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=6278.0, ans=0.81278
2024-10-08 02:52:24,056 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=6278.0, ans=0.20571875
2024-10-08 02:52:26,448 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=6278.0, ans=0.20571875
2024-10-08 02:52:33,752 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=6281.333333333333, ans=0.20556249999999998
2024-10-08 02:52:34,993 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=6281.333333333333, ans=0.040494444444444444
2024-10-08 02:52:36,271 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=6281.333333333333, ans=0.20556249999999998
2024-10-08 02:52:40,032 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=6281.333333333333, ans=0.20556249999999998
2024-10-08 02:52:42,262 INFO [train.py:1153] Epoch 3, batch 5150, loss[loss=0.3594, simple_loss=0.3302, pruned_loss=0.1381, ctc_loss=0.2806, over 4833.00 frames. ], tot_loss[loss=0.363, simple_loss=0.3407, pruned_loss=0.1379, ctc_loss=0.2738, over 967998.85 frames. ], batch size: 36, lr: 2.55e-02,
2024-10-08 02:52:53,694 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=6288.0, ans=0.20525
2024-10-08 02:52:57,435 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=6288.0, ans=0.009502608695652174
2024-10-08 02:53:11,266 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 02:53:16,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.96 vs. limit=9.85925
2024-10-08 02:53:21,174 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6294.666666666667, ans=0.23705333333333334
2024-10-08 02:53:21,749 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.74 vs. limit=3.9442
2024-10-08 02:53:39,536 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.228e+02 1.577e+02 1.723e+02 1.921e+02 2.967e+02, threshold=3.447e+02, percent-clipped=0.0
2024-10-08 02:53:42,653 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.34 vs. limit=12.2235
2024-10-08 02:53:44,529 INFO [train.py:1153] Epoch 3, batch 5200, loss[loss=0.3683, simple_loss=0.3369, pruned_loss=0.1452, ctc_loss=0.2732, over 4770.00 frames. ], tot_loss[loss=0.3636, simple_loss=0.3408, pruned_loss=0.1383, ctc_loss=0.2746, over 967581.87 frames. ], batch size: 29, lr: 2.55e-02,
2024-10-08 02:53:56,334 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.66 vs. limit=12.2285
2024-10-08 02:54:07,110 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6304.666666666667, ans=0.23695333333333332
2024-10-08 02:54:12,233 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=6308.0, ans=0.0
2024-10-08 02:54:13,973 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=13.20 vs. limit=9.8655
2024-10-08 02:54:28,096 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=3.07 vs. limit=6.524533333333333
2024-10-08 02:54:47,208 INFO [train.py:1153] Epoch 3, batch 5250, loss[loss=0.3112, simple_loss=0.3246, pruned_loss=0.1037, ctc_loss=0.2259, over 4860.00 frames. ], tot_loss[loss=0.362, simple_loss=0.3401, pruned_loss=0.1372, ctc_loss=0.2737, over 967743.81 frames. ], batch size: 20, lr: 2.54e-02,
2024-10-08 02:54:56,499 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.51 vs. limit=6.5794999999999995
2024-10-08 02:55:21,843 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.11 vs. limit=12.243500000000001
2024-10-08 02:55:38,560 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=6331.333333333333, ans=0.04028611111111111
2024-10-08 02:55:44,891 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.088e+02 1.539e+02 1.733e+02 1.912e+02 2.505e+02, threshold=3.466e+02, percent-clipped=0.0
2024-10-08 02:55:49,690 INFO [train.py:1153] Epoch 3, batch 5300, loss[loss=0.3906, simple_loss=0.3579, pruned_loss=0.1471, ctc_loss=0.3227, over 4846.00 frames. ], tot_loss[loss=0.3619, simple_loss=0.3402, pruned_loss=0.1371, ctc_loss=0.2736, over 967904.97 frames. ], batch size: 38, lr: 2.54e-02,
2024-10-08 02:55:51,003 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6334.666666666667, ans=0.23665333333333333
2024-10-08 02:55:54,702 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=6334.666666666667, ans=0.0
2024-10-08 02:56:04,724 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=6338.0, ans=0.20290625
2024-10-08 02:56:21,880 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=7.44 vs. limit=6.585333333333333
2024-10-08 02:56:28,351 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6344.666666666667, ans=0.23655333333333334
2024-10-08 02:56:33,860 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.47 vs. limit=8.172333333333334
2024-10-08 02:56:34,508 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=6344.666666666667, ans=0.009490289855072463
2024-10-08 02:56:43,273 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=6348.0, ans=0.2024375
2024-10-08 02:56:43,623 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.00 vs. limit=12.261
2024-10-08 02:56:51,723 INFO [train.py:1153] Epoch 3, batch 5350, loss[loss=0.3137, simple_loss=0.3043, pruned_loss=0.1137, ctc_loss=0.2391, over 4978.00 frames. ], tot_loss[loss=0.3626, simple_loss=0.3406, pruned_loss=0.1375, ctc_loss=0.2738, over 967291.11 frames. ], batch size: 19, lr: 2.54e-02,
2024-10-08 02:57:01,887 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=6351.333333333333, ans=0.20228125000000002
2024-10-08 02:57:25,003 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=6358.0, ans=0.20196874999999997
2024-10-08 02:57:48,461 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.171e+02 1.588e+02 1.737e+02 1.940e+02 3.321e+02, threshold=3.474e+02, percent-clipped=0.0
2024-10-08 02:57:53,473 INFO [train.py:1153] Epoch 3, batch 5400, loss[loss=0.4146, simple_loss=0.368, pruned_loss=0.1647, ctc_loss=0.3297, over 4796.00 frames. ], tot_loss[loss=0.3641, simple_loss=0.3414, pruned_loss=0.1383, ctc_loss=0.2756, over 966660.44 frames. ], batch size: 49, lr: 2.53e-02,
2024-10-08 02:57:56,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=6368.0, ans=0.2015
2024-10-08 02:57:59,771 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6368.0, ans=0.23631999999999997
2024-10-08 02:58:07,276 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=6371.333333333333, ans=0.040119444444444444
2024-10-08 02:58:22,789 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=2.69 vs. limit=9.8905
2024-10-08 02:58:24,731 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=6374.666666666667, ans=0.6768866666666666
2024-10-08 02:58:30,163 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.41 vs. limit=9.89175
2024-10-08 02:58:32,266 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 02:58:37,443 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.47 vs. limit=9.89175
2024-10-08 02:58:39,659 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6378.0, ans=0.23621999999999999
2024-10-08 02:58:55,549 INFO [train.py:1153] Epoch 3, batch 5450, loss[loss=0.408, simple_loss=0.3751, pruned_loss=0.1583, ctc_loss=0.3103, over 4940.00 frames. ], tot_loss[loss=0.3613, simple_loss=0.3401, pruned_loss=0.1367, ctc_loss=0.2728, over 967274.00 frames. ], batch size: 19, lr: 2.53e-02,
2024-10-08 02:58:58,228 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=6384.666666666667, ans=0.20071875
2024-10-08 02:59:11,762 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.min_positive, batch_count=6388.0, ans=0.060075
2024-10-08 02:59:16,865 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=6388.0, ans=0.20056249999999998
2024-10-08 02:59:30,689 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=6391.333333333333, ans=0.20040625
2024-10-08 02:59:32,482 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.39 vs. limit=12.296
2024-10-08 02:59:38,208 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.max_abs, batch_count=6394.666666666667, ans=8.996666666666666
2024-10-08 02:59:52,989 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.163e+02 1.483e+02 1.701e+02 1.926e+02 4.059e+02, threshold=3.402e+02, percent-clipped=1.0
2024-10-08 02:59:54,363 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=6398.0, ans=0.6760700000000001
2024-10-08 02:59:54,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten.whitening_limit, batch_count=6398.0, ans=12.2985
2024-10-08 02:59:57,862 INFO [train.py:1153] Epoch 3, batch 5500, loss[loss=0.4081, simple_loss=0.3508, pruned_loss=0.1658, ctc_loss=0.3344, over 4782.00 frames. ], tot_loss[loss=0.3601, simple_loss=0.3392, pruned_loss=0.1361, ctc_loss=0.272, over 967661.73 frames. ], batch size: 49, lr: 2.53e-02,
2024-10-08 03:00:00,917 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=5.24 vs. limit=9.900500000000001
2024-10-08 03:00:07,725 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=6401.333333333333, ans=0.1999375
2024-10-08 03:00:21,125 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.58 vs. limit=12.3035
2024-10-08 03:00:23,371 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.09 vs. limit=6.602
2024-10-08 03:00:26,680 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=6408.0, ans=0.199625
2024-10-08 03:00:41,303 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=6411.333333333333, ans=0.19946874999999997
2024-10-08 03:00:43,345 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.95 vs. limit=8.205666666666666
2024-10-08 03:00:45,581 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.24 vs. limit=12.3085
2024-10-08 03:00:59,643 INFO [train.py:1153] Epoch 3, batch 5550, loss[loss=0.332, simple_loss=0.3216, pruned_loss=0.1246, ctc_loss=0.233, over 4799.00 frames. ], tot_loss[loss=0.359, simple_loss=0.3385, pruned_loss=0.1356, ctc_loss=0.271, over 967338.35 frames. ], batch size: 19, lr: 2.53e-02,
2024-10-08 03:01:01,065 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6418.0, ans=0.23581999999999997
2024-10-08 03:01:07,138 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=6418.0, ans=0.19915624999999998
2024-10-08 03:01:20,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=6421.333333333333, ans=0.0
2024-10-08 03:01:31,977 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=6424.666666666667, ans=0.009472898550724638
2024-10-08 03:01:36,554 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=9.03 vs. limit=9.910499999999999
2024-10-08 03:01:38,247 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=6428.0, ans=0.19868750000000002
2024-10-08 03:01:50,528 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=6431.333333333333, ans=0.6749033333333334
2024-10-08 03:01:54,277 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=5.378e-03
2024-10-08 03:01:56,420 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.200e+02 1.565e+02 1.706e+02 1.946e+02 3.310e+02, threshold=3.413e+02, percent-clipped=0.0
2024-10-08 03:02:01,439 INFO [train.py:1153] Epoch 3, batch 5600, loss[loss=0.281, simple_loss=0.2967, pruned_loss=0.0933, ctc_loss=0.197, over 4851.00 frames. ], tot_loss[loss=0.3593, simple_loss=0.3391, pruned_loss=0.1355, ctc_loss=0.2709, over 967370.76 frames. ], batch size: 28, lr: 2.52e-02,
2024-10-08 03:02:07,223 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=13.00 vs. limit=12.326
2024-10-08 03:02:16,741 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=6438.0, ans=0.23562
2024-10-08 03:02:39,053 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=6444.666666666667, ans=0.19790625
2024-10-08 03:02:44,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=6444.666666666667, ans=0.19790625
2024-10-08 03:02:46,522 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=6444.666666666667, ans=0.19790625
2024-10-08 03:02:55,133 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6448.0, ans=0.23552
2024-10-08 03:03:00,241 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 03:03:03,559 INFO [train.py:1153] Epoch 3, batch 5650, loss[loss=0.429, simple_loss=0.3703, pruned_loss=0.1751, ctc_loss=0.3438, over 4764.00 frames. ], tot_loss[loss=0.3593, simple_loss=0.3393, pruned_loss=0.1354, ctc_loss=0.271, over 967382.17 frames. ], batch size: 45, lr: 2.52e-02,
2024-10-08 03:03:11,219 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=6451.333333333333, ans=0.19759375
2024-10-08 03:03:17,391 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6454.666666666667, ans=0.23545333333333332
2024-10-08 03:03:18,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=6454.666666666667, ans=0.1974375
2024-10-08 03:03:38,378 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.24 vs. limit=9.92175
2024-10-08 03:03:38,892 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module1.whiten, num_groups=1, num_channels=192, metric=3.11 vs. limit=9.92175
2024-10-08 03:03:43,127 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=6461.333333333333, ans=0.197125
2024-10-08 03:03:50,692 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.min_abs, batch_count=6461.333333333333, ans=0.29692
2024-10-08 03:04:00,456 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.141e+02 1.628e+02 1.801e+02 1.947e+02 2.995e+02, threshold=3.602e+02, percent-clipped=0.0
2024-10-08 03:04:05,356 INFO [train.py:1153] Epoch 3, batch 5700, loss[loss=0.3383, simple_loss=0.3275, pruned_loss=0.1236, ctc_loss=0.2546, over 4894.00 frames. ], tot_loss[loss=0.3595, simple_loss=0.3393, pruned_loss=0.1356, ctc_loss=0.2709, over 966743.36 frames. ], batch size: 22, lr: 2.52e-02,
2024-10-08 03:04:16,987 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.27 vs. limit=3.9707
2024-10-08 03:04:26,513 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=6471.333333333333, ans=0.19665624999999998
2024-10-08 03:04:29,520 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.97 vs. limit=3.9712
2024-10-08 03:04:50,267 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=6478.0, ans=0.02975625
2024-10-08 03:05:02,038 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.09 vs. limit=3.9722
2024-10-08 03:05:04,609 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.35 vs. limit=12.361
2024-10-08 03:05:07,651 INFO [train.py:1153] Epoch 3, batch 5750, loss[loss=0.4301, simple_loss=0.3622, pruned_loss=0.1802, ctc_loss=0.344, over 4813.00 frames. ], tot_loss[loss=0.3586, simple_loss=0.3387, pruned_loss=0.1353, ctc_loss=0.2697, over 967027.33 frames. ], batch size: 43, lr: 2.51e-02,
2024-10-08 03:05:41,703 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=13.30 vs. limit=9.93425
2024-10-08 03:05:49,793 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.min_abs, batch_count=6494.666666666667, ans=0.29742
2024-10-08 03:05:56,254 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.36 vs. limit=12.3735
2024-10-08 03:06:04,297 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.257e+02 1.550e+02 1.757e+02 1.956e+02 3.104e+02, threshold=3.514e+02, percent-clipped=0.0
2024-10-08 03:06:07,005 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=6498.0, ans=0.19540625
2024-10-08 03:06:09,291 INFO [train.py:1153] Epoch 3, batch 5800, loss[loss=0.4028, simple_loss=0.3613, pruned_loss=0.1609, ctc_loss=0.3062, over 4831.00 frames. ], tot_loss[loss=0.3596, simple_loss=0.3388, pruned_loss=0.136, ctc_loss=0.2706, over 966374.15 frames. ], batch size: 43, lr: 2.51e-02,
2024-10-08 03:06:09,464 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=6501.333333333333, ans=0.0
2024-10-08 03:06:14,308 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=6501.333333333333, ans=0.03957777777777778
2024-10-08 03:06:22,036 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten.whitening_limit, batch_count=6504.666666666667, ans=9.93925
2024-10-08 03:06:40,028 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=5.09 vs. limit=5.3016000000000005
2024-10-08 03:06:42,904 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=6508.0, ans=0.19493749999999999
2024-10-08 03:06:47,080 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.16 vs. limit=9.941749999999999
2024-10-08 03:07:03,807 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=6514.666666666667, ans=0.194625
2024-10-08 03:07:11,055 INFO [train.py:1153] Epoch 3, batch 5850, loss[loss=0.416, simple_loss=0.3679, pruned_loss=0.1656, ctc_loss=0.3322, over 4768.00 frames. ], tot_loss[loss=0.3604, simple_loss=0.3395, pruned_loss=0.1365, ctc_loss=0.2708, over 966613.31 frames. ], batch size: 45, lr: 2.51e-02,
2024-10-08 03:07:14,033 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.88 vs. limit=3.9777
2024-10-08 03:07:18,449 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=6518.0, ans=0.81518
2024-10-08 03:07:45,666 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=6524.666666666667, ans=0.19415624999999997
2024-10-08 03:07:47,014 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=1.412e+00
2024-10-08 03:07:53,038 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=6528.0, ans=0.194
2024-10-08 03:07:55,521 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6528.0, ans=0.23471999999999998
2024-10-08 03:08:04,676 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.31 vs. limit=9.94925
2024-10-08 03:08:07,927 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.080e+02 1.519e+02 1.695e+02 1.835e+02 2.256e+02, threshold=3.391e+02, percent-clipped=0.0
2024-10-08 03:08:12,902 INFO [train.py:1153] Epoch 3, batch 5900, loss[loss=0.3418, simple_loss=0.3262, pruned_loss=0.1281, ctc_loss=0.253, over 4794.00 frames. ], tot_loss[loss=0.3584, simple_loss=0.3385, pruned_loss=0.1354, ctc_loss=0.2688, over 966628.14 frames. ], batch size: 34, lr: 2.51e-02,
2024-10-08 03:08:17,901 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=6534.666666666667, ans=0.029579166666666667
2024-10-08 03:08:41,993 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.00 vs. limit=9.953
2024-10-08 03:08:46,618 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=6541.333333333333, ans=9.088333333333333
2024-10-08 03:09:02,603 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=6548.0, ans=0.19306250000000003
2024-10-08 03:09:03,833 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=6548.0, ans=0.19306250000000003
2024-10-08 03:09:09,982 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=6548.0, ans=0.03938333333333334
2024-10-08 03:09:14,705 INFO [train.py:1153] Epoch 3, batch 5950, loss[loss=0.3823, simple_loss=0.3502, pruned_loss=0.1496, ctc_loss=0.2879, over 4803.00 frames. ], tot_loss[loss=0.3588, simple_loss=0.339, pruned_loss=0.1356, ctc_loss=0.269, over 965992.88 frames. ], batch size: 34, lr: 2.50e-02,
2024-10-08 03:09:15,293 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.08 vs. limit=6.620533333333333
2024-10-08 03:09:31,001 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=6554.666666666667, ans=0.009444637681159421
2024-10-08 03:09:31,434 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.92 vs. limit=9.958
2024-10-08 03:09:32,685 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=10.65 vs. limit=9.958
2024-10-08 03:09:37,345 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6554.666666666667, ans=0.23445333333333332
2024-10-08 03:09:47,812 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten.whitening_limit, batch_count=6558.0, ans=9.95925
2024-10-08 03:09:48,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=6558.0, ans=0.19259375
2024-10-08 03:09:49,717 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=6558.0, ans=0.025
2024-10-08 03:09:50,982 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=6561.333333333333, ans=0.19243749999999998
2024-10-08 03:09:52,599 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.34 vs. limit=6.624533333333334
2024-10-08 03:10:04,991 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.05 vs. limit=3.9847
2024-10-08 03:10:08,171 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=6564.666666666667, ans=0.19228125000000001
2024-10-08 03:10:11,110 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.28 vs. limit=12.4235
2024-10-08 03:10:11,723 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.249e+02 1.548e+02 1.745e+02 1.978e+02 3.665e+02, threshold=3.490e+02, percent-clipped=1.0
2024-10-08 03:10:16,709 INFO [train.py:1153] Epoch 3, batch 6000, loss[loss=0.4396, simple_loss=0.3755, pruned_loss=0.1812, ctc_loss=0.3535, over 4774.00 frames. ], tot_loss[loss=0.359, simple_loss=0.3392, pruned_loss=0.1355, ctc_loss=0.2697, over 966493.57 frames. ], batch size: 49, lr: 2.50e-02,
2024-10-08 03:10:16,709 INFO [train.py:1176] Computing validation loss
2024-10-08 03:10:24,072 INFO [train.py:1185] Epoch 3, validation: loss=0.2262, simple_loss=0.2941, pruned_loss=0.05663, ctc_loss=0.1124, over 90464.00 frames.
2024-10-08 03:10:24,072 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 03:10:29,202 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=6568.0, ans=0.192125
2024-10-08 03:10:33,266 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.29 vs. limit=9.963000000000001
2024-10-08 03:10:34,012 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=6568.0, ans=0.192125
2024-10-08 03:10:36,064 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.27 vs. limit=9.96425
2024-10-08 03:10:47,948 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=6574.666666666667, ans=0.03927222222222222
2024-10-08 03:10:50,746 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.25 vs. limit=9.9655
2024-10-08 03:11:12,705 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=6581.333333333333, ans=0.0
2024-10-08 03:11:14,595 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.72 vs. limit=9.968
2024-10-08 03:11:25,477 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=12.45 vs. limit=12.438500000000001
2024-10-08 03:11:26,209 INFO [train.py:1153] Epoch 3, batch 6050, loss[loss=0.2782, simple_loss=0.2939, pruned_loss=0.09348, ctc_loss=0.1888, over 4815.00 frames. ], tot_loss[loss=0.3571, simple_loss=0.3384, pruned_loss=0.1343, ctc_loss=0.2677, over 966472.46 frames. ], batch size: 19, lr: 2.50e-02,
2024-10-08 03:11:31,894 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.38 vs. limit=12.438500000000001
2024-10-08 03:11:33,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=6584.666666666667, ans=0.19134374999999998
2024-10-08 03:11:36,416 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.01 vs. limit=9.96925
2024-10-08 03:11:55,264 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.17 vs. limit=3.9887
2024-10-08 03:12:05,906 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=6594.666666666667, ans=0.6691866666666667
2024-10-08 03:12:14,730 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=6598.0, ans=0.039175
2024-10-08 03:12:18,327 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=6598.0, ans=0.039175
2024-10-08 03:12:23,284 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.069e+02 1.567e+02 1.755e+02 1.882e+02 4.957e+02, threshold=3.509e+02, percent-clipped=0.0
2024-10-08 03:12:28,274 INFO [train.py:1153] Epoch 3, batch 6100, loss[loss=0.436, simple_loss=0.3846, pruned_loss=0.1798, ctc_loss=0.3194, over 4799.00 frames. ], tot_loss[loss=0.3568, simple_loss=0.3381, pruned_loss=0.1343, ctc_loss=0.2673, over 966082.57 frames. ], batch size: 34, lr: 2.50e-02,
2024-10-08 03:12:37,050 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=6601.333333333333, ans=0.03916111111111111
2024-10-08 03:12:47,988 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=6604.666666666667, ans=0.05872083333333334
2024-10-08 03:13:18,904 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=6614.666666666667, ans=0.18993749999999998
2024-10-08 03:13:18,905 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=6614.666666666667, ans=0.18993749999999998
2024-10-08 03:13:24,246 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.53 vs. limit=12.461
2024-10-08 03:13:24,396 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.63 vs. limit=12.461
2024-10-08 03:13:30,016 INFO [train.py:1153] Epoch 3, batch 6150, loss[loss=0.3737, simple_loss=0.3398, pruned_loss=0.1431, ctc_loss=0.3038, over 4845.00 frames. ], tot_loss[loss=0.3551, simple_loss=0.3367, pruned_loss=0.1336, ctc_loss=0.2662, over 966240.37 frames. ], batch size: 43, lr: 2.49e-02,
2024-10-08 03:13:51,372 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=6621.333333333333, ans=0.189625
2024-10-08 03:14:09,053 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=512, metric=3.28 vs. limit=9.9855
2024-10-08 03:14:18,551 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=6631.333333333333, ans=0.18915625000000003
2024-10-08 03:14:27,046 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.124e+02 1.531e+02 1.730e+02 1.886e+02 4.137e+02, threshold=3.460e+02, percent-clipped=2.0
2024-10-08 03:14:32,409 INFO [train.py:1153] Epoch 3, batch 6200, loss[loss=0.4416, simple_loss=0.3927, pruned_loss=0.1744, ctc_loss=0.3548, over 4780.00 frames. ], tot_loss[loss=0.3549, simple_loss=0.3366, pruned_loss=0.1335, ctc_loss=0.2654, over 966522.05 frames. ], batch size: 29, lr: 2.49e-02,
2024-10-08 03:14:32,597 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=6634.666666666667, ans=0.189
2024-10-08 03:14:44,786 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=6638.0, ans=0.66767
2024-10-08 03:14:59,772 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=6641.333333333333, ans=0.1886875
2024-10-08 03:15:06,441 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.07 vs. limit=12.481
2024-10-08 03:15:21,020 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=6648.0, ans=0.18837500000000001
2024-10-08 03:15:29,716 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=6648.0, ans=0.18837500000000001
2024-10-08 03:15:32,667 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=6.88 vs. limit=9.993
2024-10-08 03:15:33,483 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6651.333333333333, ans=0.23348666666666668
2024-10-08 03:15:34,553 INFO [train.py:1153] Epoch 3, batch 6250, loss[loss=0.2997, simple_loss=0.3129, pruned_loss=0.0985, ctc_loss=0.2237, over 4724.00 frames. ], tot_loss[loss=0.3528, simple_loss=0.3359, pruned_loss=0.1321, ctc_loss=0.2635, over 966807.08 frames. ], batch size: 26, lr: 2.49e-02,
2024-10-08 03:15:48,433 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=6654.666666666667, ans=0.18806250000000002
2024-10-08 03:15:54,720 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 03:16:22,242 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.50 vs. limit=9.998000000000001
2024-10-08 03:16:25,562 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=6664.666666666667, ans=0.18759375
2024-10-08 03:16:30,526 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/checkpoint-20000.pt
2024-10-08 03:16:32,443 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.122e+02 1.517e+02 1.693e+02 1.892e+02 2.572e+02, threshold=3.385e+02, percent-clipped=0.0
2024-10-08 03:16:32,580 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=6664.666666666667, ans=0.18759375
2024-10-08 03:16:35,810 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=6668.0, ans=0.03888333333333334
2024-10-08 03:16:36,878 INFO [train.py:1153] Epoch 3, batch 6300, loss[loss=0.3444, simple_loss=0.3351, pruned_loss=0.1262, ctc_loss=0.2533, over 4978.00 frames. ], tot_loss[loss=0.3544, simple_loss=0.337, pruned_loss=0.133, ctc_loss=0.2647, over 966443.54 frames. ], batch size: 19, lr: 2.48e-02,
2024-10-08 03:16:49,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6671.333333333333, ans=0.23328666666666667
2024-10-08 03:17:06,782 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.54 vs. limit=10.003
2024-10-08 03:17:07,674 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 03:17:17,371 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=6678.0, ans=0.18696875000000002
2024-10-08 03:17:18,616 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=6678.0, ans=0.66627
2024-10-08 03:17:38,428 INFO [train.py:1153] Epoch 3, batch 6350, loss[loss=0.4059, simple_loss=0.3599, pruned_loss=0.1641, ctc_loss=0.3091, over 4817.00 frames. ], tot_loss[loss=0.3514, simple_loss=0.3346, pruned_loss=0.1317, ctc_loss=0.2619, over 966125.21 frames. ], batch size: 36, lr: 2.48e-02,
2024-10-08 03:17:39,861 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6684.666666666667, ans=0.23315333333333332
2024-10-08 03:17:46,057 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6684.666666666667, ans=0.23315333333333332
2024-10-08 03:17:54,880 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.97 vs. limit=12.516
2024-10-08 03:17:59,679 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=6688.0, ans=0.1865
2024-10-08 03:18:03,212 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=6691.333333333333, ans=0.23308666666666666
2024-10-08 03:18:06,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=6691.333333333333, ans=0.035
2024-10-08 03:18:08,210 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6691.333333333333, ans=0.23308666666666666
2024-10-08 03:18:10,966 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=6691.333333333333, ans=0.03878611111111112
2024-10-08 03:18:13,279 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=6691.333333333333, ans=0.18634374999999997
2024-10-08 03:18:18,517 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.82 vs. limit=4.0042
2024-10-08 03:18:20,637 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=6694.666666666667, ans=0.8169466666666667
2024-10-08 03:18:29,643 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.40 vs. limit=12.5235
2024-10-08 03:18:35,193 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.332e+02 1.494e+02 1.681e+02 1.937e+02 2.457e+02, threshold=3.363e+02, percent-clipped=0.0
2024-10-08 03:18:40,193 INFO [train.py:1153] Epoch 3, batch 6400, loss[loss=0.3184, simple_loss=0.3223, pruned_loss=0.1122, ctc_loss=0.2255, over 4861.00 frames. ], tot_loss[loss=0.3508, simple_loss=0.3346, pruned_loss=0.1312, ctc_loss=0.2615, over 965953.90 frames. ], batch size: 23, lr: 2.48e-02,
2024-10-08 03:18:47,616 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=6701.333333333333, ans=0.185875
2024-10-08 03:18:47,730 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=6701.333333333333, ans=0.0
2024-10-08 03:18:51,437 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=6704.666666666667, ans=0.09899494936611666
2024-10-08 03:18:51,761 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.64 vs. limit=12.528500000000001
2024-10-08 03:18:54,343 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.94 vs. limit=10.01425
2024-10-08 03:19:00,302 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.37 vs. limit=10.01425
2024-10-08 03:19:01,815 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.43 vs. limit=6.681866666666666
2024-10-08 03:19:18,373 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6711.333333333333, ans=0.2328866666666667
2024-10-08 03:19:34,298 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=6714.666666666667, ans=0.07
2024-10-08 03:19:35,000 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.86 vs. limit=10.018
2024-10-08 03:19:40,826 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=12.83 vs. limit=10.01925
2024-10-08 03:19:41,434 INFO [train.py:1153] Epoch 3, batch 6450, loss[loss=0.3571, simple_loss=0.3477, pruned_loss=0.1328, ctc_loss=0.2519, over 4716.00 frames. ], tot_loss[loss=0.3491, simple_loss=0.3337, pruned_loss=0.1303, ctc_loss=0.26, over 965338.34 frames. ], batch size: 26, lr: 2.48e-02,
2024-10-08 03:19:42,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=6718.0, ans=0.00940913043478261
2024-10-08 03:19:48,874 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=6718.0, ans=0.0
2024-10-08 03:19:57,977 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.40 vs. limit=12.541
2024-10-08 03:20:01,171 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=6721.333333333333, ans=0.18493749999999998
2024-10-08 03:20:06,159 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6724.666666666667, ans=0.2327533333333333
2024-10-08 03:20:09,357 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.70 vs. limit=12.5435
2024-10-08 03:20:18,518 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=6728.0, ans=0.81728
2024-10-08 03:20:20,942 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6728.0, ans=0.23271999999999998
2024-10-08 03:20:38,240 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.219e+02 1.551e+02 1.733e+02 1.894e+02 2.809e+02, threshold=3.466e+02, percent-clipped=0.0
2024-10-08 03:20:38,845 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.57 vs. limit=12.5485
2024-10-08 03:20:43,257 INFO [train.py:1153] Epoch 3, batch 6500, loss[loss=0.3347, simple_loss=0.3237, pruned_loss=0.1236, ctc_loss=0.2458, over 4747.00 frames. ], tot_loss[loss=0.3463, simple_loss=0.3322, pruned_loss=0.1288, ctc_loss=0.2572, over 964952.77 frames. ], batch size: 26, lr: 2.47e-02,
2024-10-08 03:20:44,546 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=6734.666666666667, ans=0.03860555555555556
2024-10-08 03:20:45,756 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=6734.666666666667, ans=0.1843125
2024-10-08 03:20:53,311 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=6734.666666666667, ans=0.1843125
2024-10-08 03:21:36,553 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=6748.0, ans=0.23252
2024-10-08 03:21:45,054 INFO [train.py:1153] Epoch 3, batch 6550, loss[loss=0.2976, simple_loss=0.2995, pruned_loss=0.1065, ctc_loss=0.2069, over 4978.00 frames. ], tot_loss[loss=0.3433, simple_loss=0.3305, pruned_loss=0.1271, ctc_loss=0.2545, over 964727.79 frames. ], batch size: 19, lr: 2.47e-02,
2024-10-08 03:21:56,343 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=6754.666666666667, ans=0.183375
2024-10-08 03:22:00,127 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6754.666666666667, ans=0.23245333333333332
2024-10-08 03:22:07,815 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.20 vs. limit=12.565999999999999
2024-10-08 03:22:09,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6758.0, ans=0.23242
2024-10-08 03:22:14,778 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=6758.0, ans=0.03850833333333334
2024-10-08 03:22:21,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6761.333333333333, ans=0.23238666666666669
2024-10-08 03:22:34,760 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=6764.666666666667, ans=0.025
2024-10-08 03:22:41,013 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=6764.666666666667, ans=0.6632366666666667
2024-10-08 03:22:41,968 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.266e+02 1.564e+02 1.698e+02 1.916e+02 2.642e+02, threshold=3.396e+02, percent-clipped=0.0
2024-10-08 03:22:47,078 INFO [train.py:1153] Epoch 3, batch 6600, loss[loss=0.3309, simple_loss=0.3133, pruned_loss=0.1253, ctc_loss=0.2448, over 4850.00 frames. ], tot_loss[loss=0.3428, simple_loss=0.3303, pruned_loss=0.1269, ctc_loss=0.2537, over 965312.97 frames. ], batch size: 23, lr: 2.47e-02,
2024-10-08 03:23:00,076 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.18 vs. limit=12.5785
2024-10-08 03:23:00,419 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1.whitening_limit, batch_count=6771.333333333333, ans=6.692833333333333
2024-10-08 03:23:09,799 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=6771.333333333333, ans=0.028839583333333335
2024-10-08 03:23:10,579 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.77 vs. limit=6.692833333333333
2024-10-08 03:23:16,056 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6774.666666666667, ans=0.2322533333333333
2024-10-08 03:23:18,461 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=6774.666666666667, ans=0.03843888888888889
2024-10-08 03:23:50,052 INFO [train.py:1153] Epoch 3, batch 6650, loss[loss=0.2946, simple_loss=0.311, pruned_loss=0.09757, ctc_loss=0.2076, over 4744.00 frames. ], tot_loss[loss=0.3391, simple_loss=0.3279, pruned_loss=0.1251, ctc_loss=0.2502, over 966984.39 frames. ], batch size: 20, lr: 2.47e-02,
2024-10-08 03:24:08,257 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.41 vs. limit=6.715199999999999
2024-10-08 03:24:19,085 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=6791.333333333333, ans=0.03836944444444445
2024-10-08 03:24:44,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten.whitening_limit, batch_count=6798.0, ans=12.5985
2024-10-08 03:24:46,896 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=6798.0, ans=0.03834166666666667
2024-10-08 03:24:46,900 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=6798.0, ans=0.0
2024-10-08 03:24:47,920 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.182e+02 1.587e+02 1.777e+02 1.957e+02 2.914e+02, threshold=3.554e+02, percent-clipped=0.0
2024-10-08 03:24:50,631 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=6798.0, ans=0.18134375000000003
2024-10-08 03:24:50,657 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6798.0, ans=0.23202
2024-10-08 03:24:52,940 INFO [train.py:1153] Epoch 3, batch 6700, loss[loss=0.3203, simple_loss=0.3212, pruned_loss=0.1129, ctc_loss=0.234, over 4935.00 frames. ], tot_loss[loss=0.3345, simple_loss=0.3254, pruned_loss=0.1227, ctc_loss=0.2455, over 969139.28 frames. ], batch size: 20, lr: 2.46e-02,
2024-10-08 03:24:54,873 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=6.98 vs. limit=6.720533333333333
2024-10-08 03:25:04,537 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=6804.666666666667, ans=0.0
2024-10-08 03:25:11,422 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.07 vs. limit=12.6035
2024-10-08 03:25:28,780 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6808.0, ans=0.23192
2024-10-08 03:25:54,068 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=8.35 vs. limit=10.0555
2024-10-08 03:25:56,802 INFO [train.py:1153] Epoch 3, batch 6750, loss[loss=0.2886, simple_loss=0.2931, pruned_loss=0.1017, ctc_loss=0.2018, over 4907.00 frames. ], tot_loss[loss=0.3296, simple_loss=0.3222, pruned_loss=0.1204, ctc_loss=0.2407, over 972216.00 frames. ], batch size: 19, lr: 2.46e-02,
2024-10-08 03:26:04,937 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.84 vs. limit=12.6135
2024-10-08 03:26:09,697 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=6821.333333333333, ans=0.18025000000000002
2024-10-08 03:26:25,180 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.76 vs. limit=12.618500000000001
2024-10-08 03:26:26,338 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.99 vs. limit=8.412333333333333
2024-10-08 03:26:30,998 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=6824.666666666667, ans=0.6611366666666667
2024-10-08 03:26:32,238 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=6824.666666666667, ans=0.03823055555555556
2024-10-08 03:26:44,906 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6828.0, ans=0.23171999999999998
2024-10-08 03:26:54,998 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.314e+02 1.501e+02 1.680e+02 1.865e+02 3.499e+02, threshold=3.360e+02, percent-clipped=0.0
2024-10-08 03:27:00,198 INFO [train.py:1153] Epoch 3, batch 6800, loss[loss=0.3155, simple_loss=0.3132, pruned_loss=0.1142, ctc_loss=0.2236, over 4911.00 frames. ], tot_loss[loss=0.3284, simple_loss=0.3211, pruned_loss=0.1201, ctc_loss=0.2388, over 974537.90 frames. ], batch size: 19, lr: 2.46e-02,
2024-10-08 03:27:00,795 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=66.49 vs. limit=10.063
2024-10-08 03:27:37,951 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=12.75 vs. limit=12.6335
2024-10-08 03:27:42,667 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=6844.666666666667, ans=0.0
2024-10-08 03:27:46,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=6844.666666666667, ans=0.00938159420289855
2024-10-08 03:27:49,090 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.max_abs, batch_count=6844.666666666667, ans=9.277916666666666
2024-10-08 03:27:58,166 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=6848.0, ans=0.0
2024-10-08 03:28:01,195 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=4.08 vs. limit=10.068
2024-10-08 03:28:02,005 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 03:28:02,096 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=6848.0, ans=0.179
2024-10-08 03:28:04,441 INFO [train.py:1153] Epoch 3, batch 6850, loss[loss=0.2655, simple_loss=0.2846, pruned_loss=0.08671, ctc_loss=0.1821, over 4978.00 frames. ], tot_loss[loss=0.3272, simple_loss=0.3204, pruned_loss=0.1195, ctc_loss=0.2374, over 978887.83 frames. ], batch size: 19, lr: 2.45e-02,
2024-10-08 03:28:05,853 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/epoch-3.pt
2024-10-08 03:28:34,012 INFO [train.py:1153] Epoch 4, batch 0, loss[loss=0.2941, simple_loss=0.3066, pruned_loss=0.09764, ctc_loss=0.2159, over 4852.00 frames. ], tot_loss[loss=0.2941, simple_loss=0.3066, pruned_loss=0.09764, ctc_loss=0.2159, over 4852.00 frames. ], batch size: 19, lr: 2.30e-02,
2024-10-08 03:28:34,013 INFO [train.py:1176] Computing validation loss
2024-10-08 03:28:39,840 INFO [train.py:1185] Epoch 4, validation: loss=0.2259, simple_loss=0.293, pruned_loss=0.05726, ctc_loss=0.1105, over 90464.00 frames.
2024-10-08 03:28:39,841 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 03:29:00,279 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=16.03 vs. limit=12.6415
2024-10-08 03:29:01,098 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=6858.666666666667, ans=0.09899494936611666
2024-10-08 03:29:05,989 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=6858.666666666667, ans=0.1785
2024-10-08 03:29:18,679 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten.whitening_limit, batch_count=6862.0, ans=10.07325
2024-10-08 03:29:31,700 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.116e+02 1.433e+02 1.559e+02 1.817e+02 2.834e+02, threshold=3.118e+02, percent-clipped=0.0
2024-10-08 03:29:34,623 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=22.19 vs. limit=12.649000000000001
2024-10-08 03:29:39,057 INFO [train.py:1153] Epoch 4, batch 50, loss[loss=0.3, simple_loss=0.2992, pruned_loss=0.1073, ctc_loss=0.2152, over 4908.00 frames. ], tot_loss[loss=0.359, simple_loss=0.3394, pruned_loss=0.1349, ctc_loss=0.2718, over 217764.35 frames. ], batch size: 19, lr: 2.29e-02,
2024-10-08 03:29:47,330 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=7.88 vs. limit=10.07575
2024-10-08 03:29:51,629 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=6872.0, ans=0.0
2024-10-08 03:29:59,845 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.55 vs. limit=12.654
2024-10-08 03:30:11,650 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=6875.333333333333, ans=0.17771874999999998
2024-10-08 03:30:16,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=6878.666666666667, ans=0.23121333333333333
2024-10-08 03:30:39,159 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=6882.0, ans=0.0569875
2024-10-08 03:30:41,468 INFO [train.py:1153] Epoch 4, batch 100, loss[loss=0.2839, simple_loss=0.3078, pruned_loss=0.09189, ctc_loss=0.1905, over 4752.00 frames. ], tot_loss[loss=0.3586, simple_loss=0.3384, pruned_loss=0.1351, ctc_loss=0.2716, over 383461.72 frames. ], batch size: 19, lr: 2.29e-02,
2024-10-08 03:30:42,059 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten.whitening_limit, batch_count=6885.333333333333, ans=10.082
2024-10-08 03:30:45,292 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=6885.333333333333, ans=0.028483333333333336
2024-10-08 03:30:49,001 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=6885.333333333333, ans=0.23114666666666667
2024-10-08 03:30:54,050 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=6888.666666666667, ans=0.17709375
2024-10-08 03:31:04,407 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.90 vs. limit=4.0333000000000006
2024-10-08 03:31:05,341 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=6892.0, ans=0.0
2024-10-08 03:31:07,103 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.59 vs. limit=6.723
2024-10-08 03:31:30,598 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=6898.666666666667, ans=0.17662499999999998
2024-10-08 03:31:36,784 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.828e+01 1.545e+02 1.710e+02 1.908e+02 3.911e+02, threshold=3.420e+02, percent-clipped=1.0
2024-10-08 03:31:39,563 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=6898.666666666667, ans=0.17662499999999998
2024-10-08 03:31:44,474 INFO [train.py:1153] Epoch 4, batch 150, loss[loss=0.2798, simple_loss=0.305, pruned_loss=0.08905, ctc_loss=0.1915, over 4910.00 frames. ], tot_loss[loss=0.3513, simple_loss=0.3349, pruned_loss=0.1311, ctc_loss=0.2637, over 513379.36 frames. ], batch size: 19, lr: 2.29e-02,
2024-10-08 03:31:58,347 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=6905.333333333333, ans=0.17631249999999998
2024-10-08 03:32:07,040 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=6905.333333333333, ans=0.2309466666666667
2024-10-08 03:32:37,929 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=6915.333333333333, ans=0.17584375000000002
2024-10-08 03:32:38,066 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 03:32:41,724 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=6915.333333333333, ans=0.17584375000000002
2024-10-08 03:32:42,867 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=6915.333333333333, ans=0.0
2024-10-08 03:32:46,412 INFO [train.py:1153] Epoch 4, batch 200, loss[loss=0.3577, simple_loss=0.337, pruned_loss=0.1332, ctc_loss=0.28, over 4743.00 frames. ], tot_loss[loss=0.3482, simple_loss=0.3332, pruned_loss=0.1295, ctc_loss=0.2601, over 613758.04 frames. ], batch size: 45, lr: 2.29e-02,
2024-10-08 03:33:09,042 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=6922.0, ans=0.17553125000000003
2024-10-08 03:33:14,542 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=15.98 vs. limit=10.097
2024-10-08 03:33:21,560 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=6925.333333333333, ans=0.028358333333333336
2024-10-08 03:33:41,352 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.062e+02 1.466e+02 1.635e+02 1.831e+02 4.583e+02, threshold=3.270e+02, percent-clipped=1.0
2024-10-08 03:33:48,810 INFO [train.py:1153] Epoch 4, batch 250, loss[loss=0.4779, simple_loss=0.4167, pruned_loss=0.1957, ctc_loss=0.3694, over 4833.00 frames. ], tot_loss[loss=0.3473, simple_loss=0.3328, pruned_loss=0.129, ctc_loss=0.2593, over 692511.38 frames. ], batch size: 38, lr: 2.28e-02,
2024-10-08 03:33:55,052 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=6935.333333333333, ans=0.17490624999999999
2024-10-08 03:34:02,217 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=6938.666666666667, ans=0.17475000000000002
2024-10-08 03:34:23,696 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=2.59 vs. limit=10.10325
2024-10-08 03:34:27,308 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6945.333333333333, ans=0.23054666666666668
2024-10-08 03:34:34,829 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=6945.333333333333, ans=0.6569133333333333
2024-10-08 03:34:40,158 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.05 vs. limit=10.10575
2024-10-08 03:34:41,028 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=3.576e-01
2024-10-08 03:34:50,703 INFO [train.py:1153] Epoch 4, batch 300, loss[loss=0.3387, simple_loss=0.3221, pruned_loss=0.1244, ctc_loss=0.2662, over 4762.00 frames. ], tot_loss[loss=0.3444, simple_loss=0.3317, pruned_loss=0.1272, ctc_loss=0.2568, over 752887.33 frames. ], batch size: 32, lr: 2.28e-02,
2024-10-08 03:35:14,033 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.38 vs. limit=8.477666666666666
2024-10-08 03:35:21,386 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.62 vs. limit=6.7396666666666665
2024-10-08 03:35:31,761 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=6962.0, ans=0.23038
2024-10-08 03:35:42,187 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.06 vs. limit=6.741333333333333
2024-10-08 03:35:44,100 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=6965.333333333333, ans=0.00935536231884058
2024-10-08 03:35:44,174 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=6965.333333333333, ans=0.1735
2024-10-08 03:35:45,116 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.156e+02 1.503e+02 1.661e+02 1.907e+02 2.702e+02, threshold=3.322e+02, percent-clipped=0.0
2024-10-08 03:35:52,138 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.21 vs. limit=12.7265
2024-10-08 03:35:52,489 INFO [train.py:1153] Epoch 4, batch 350, loss[loss=0.3262, simple_loss=0.3261, pruned_loss=0.1184, ctc_loss=0.2238, over 4883.00 frames. ], tot_loss[loss=0.3443, simple_loss=0.3313, pruned_loss=0.1274, ctc_loss=0.2565, over 800322.46 frames. ], batch size: 19, lr: 2.28e-02,
2024-10-08 03:35:54,963 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=6968.666666666667, ans=0.13553983333333336
2024-10-08 03:35:55,123 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=6968.666666666667, ans=0.00935463768115942
2024-10-08 03:36:16,861 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.98 vs. limit=4.0463000000000005
2024-10-08 03:36:20,012 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=6975.333333333333, ans=0.025
2024-10-08 03:36:20,115 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=2.406e-02
2024-10-08 03:36:20,433 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.17 vs. limit=10.11575
2024-10-08 03:36:54,469 INFO [train.py:1153] Epoch 4, batch 400, loss[loss=0.289, simple_loss=0.3076, pruned_loss=0.0947, ctc_loss=0.2027, over 4862.00 frames. ], tot_loss[loss=0.3429, simple_loss=0.3305, pruned_loss=0.1266, ctc_loss=0.2551, over 837047.37 frames. ], batch size: 22, lr: 2.28e-02,
2024-10-08 03:36:57,023 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=6985.333333333333, ans=0.037561111111111115
2024-10-08 03:37:14,271 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=6988.666666666667, ans=0.17240624999999998
2024-10-08 03:37:24,105 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=6992.0, ans=0.0
2024-10-08 03:37:44,532 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.20 vs. limit=6.749666666666666
2024-10-08 03:37:48,606 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.092e+02 1.479e+02 1.657e+02 1.867e+02 3.412e+02, threshold=3.313e+02, percent-clipped=1.0
2024-10-08 03:37:53,685 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=6998.666666666667, ans=0.17193750000000002
2024-10-08 03:37:56,039 INFO [train.py:1153] Epoch 4, batch 450, loss[loss=0.3308, simple_loss=0.3204, pruned_loss=0.1215, ctc_loss=0.2455, over 4866.00 frames. ], tot_loss[loss=0.3421, simple_loss=0.33, pruned_loss=0.1261, ctc_loss=0.2546, over 865684.31 frames. ], batch size: 23, lr: 2.27e-02,
2024-10-08 03:37:57,505 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer2.prob, batch_count=7002.0, ans=0.17178125
2024-10-08 03:38:11,166 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=7005.333333333333, ans=0.04949747468305833
2024-10-08 03:38:13,602 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=7005.333333333333, ans=0.2299466666666667
2024-10-08 03:38:19,119 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.39 vs. limit=10.126999999999999
2024-10-08 03:38:28,587 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=7008.666666666667, ans=0.17146875
2024-10-08 03:38:32,802 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.80 vs. limit=12.759
2024-10-08 03:38:35,278 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.33 vs. limit=12.759
2024-10-08 03:38:36,524 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.93 vs. limit=12.759
2024-10-08 03:38:40,169 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.02 vs. limit=4.0518
2024-10-08 03:38:46,716 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.85 vs. limit=6.753833333333333
2024-10-08 03:38:51,372 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.53 vs. limit=12.7615
2024-10-08 03:38:57,711 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.71 vs. limit=6.754666666666667
2024-10-08 03:38:58,299 INFO [train.py:1153] Epoch 4, batch 500, loss[loss=0.3151, simple_loss=0.3097, pruned_loss=0.1136, ctc_loss=0.2333, over 4799.00 frames. ], tot_loss[loss=0.3393, simple_loss=0.3285, pruned_loss=0.1247, ctc_loss=0.2518, over 888223.35 frames. ], batch size: 34, lr: 2.27e-02,
2024-10-08 03:39:04,615 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7018.666666666667, ans=0.22981333333333331
2024-10-08 03:39:24,843 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.48 vs. limit=8.512666666666666
2024-10-08 03:39:28,639 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.04 vs. limit=12.769
2024-10-08 03:39:37,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=7028.666666666667, ans=0.037380555555555556
2024-10-08 03:39:49,030 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=7032.0, ans=0.65388
2024-10-08 03:39:50,270 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=7032.0, ans=0.09899494936611666
2024-10-08 03:39:51,582 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=7032.0, ans=0.025
2024-10-08 03:39:52,519 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.053e+02 1.497e+02 1.650e+02 1.793e+02 2.723e+02, threshold=3.300e+02, percent-clipped=0.0
2024-10-08 03:39:59,935 INFO [train.py:1153] Epoch 4, batch 550, loss[loss=0.4018, simple_loss=0.363, pruned_loss=0.1589, ctc_loss=0.3071, over 4803.00 frames. ], tot_loss[loss=0.3414, simple_loss=0.3298, pruned_loss=0.1258, ctc_loss=0.2535, over 905650.63 frames. ], batch size: 40, lr: 2.27e-02,
2024-10-08 03:40:07,463 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=7035.333333333333, ans=0.17021874999999997
2024-10-08 03:40:19,707 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=7038.666666666667, ans=0.009339420289855073
2024-10-08 03:40:36,931 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=7045.333333333333, ans=0.16975
2024-10-08 03:40:38,330 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=7045.333333333333, ans=0.009337971014492755
2024-10-08 03:40:51,987 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=7048.666666666667, ans=0.025
2024-10-08 03:41:01,776 INFO [train.py:1153] Epoch 4, batch 600, loss[loss=0.3783, simple_loss=0.3497, pruned_loss=0.1438, ctc_loss=0.2983, over 4826.00 frames. ], tot_loss[loss=0.3393, simple_loss=0.3287, pruned_loss=0.1247, ctc_loss=0.2514, over 919433.57 frames. ], batch size: 38, lr: 2.27e-02,
2024-10-08 03:41:06,964 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=7052.0, ans=0.07
2024-10-08 03:41:13,842 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.05 vs. limit=8.527666666666667
2024-10-08 03:41:25,339 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=7058.666666666667, ans=0.2294133333333333
2024-10-08 03:41:26,550 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=7058.666666666667, ans=0.2294133333333333
2024-10-08 03:41:27,011 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.53 vs. limit=10.147
2024-10-08 03:41:27,824 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=7058.666666666667, ans=0.037255555555555556
2024-10-08 03:41:29,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.76 vs. limit=4.0588
2024-10-08 03:41:45,018 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=7062.0, ans=0.30593000000000004
2024-10-08 03:41:55,387 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 03:41:56,212 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.173e+02 1.535e+02 1.723e+02 1.937e+02 4.277e+02, threshold=3.446e+02, percent-clipped=1.0
2024-10-08 03:42:03,502 INFO [train.py:1153] Epoch 4, batch 650, loss[loss=0.3216, simple_loss=0.3238, pruned_loss=0.1133, ctc_loss=0.232, over 4834.00 frames. ], tot_loss[loss=0.3361, simple_loss=0.327, pruned_loss=0.123, ctc_loss=0.2481, over 930187.25 frames. ], batch size: 21, lr: 2.26e-02,
2024-10-08 03:42:14,723 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=7072.0, ans=0.16849999999999998
2024-10-08 03:42:27,307 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=7075.333333333333, ans=0.09899494936611666
2024-10-08 03:42:33,182 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.43 vs. limit=12.8065
2024-10-08 03:42:54,788 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=7082.0, ans=0.16803125000000002
2024-10-08 03:42:57,693 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.31 vs. limit=12.811499999999999
2024-10-08 03:42:58,969 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.14 vs. limit=6.7705
2024-10-08 03:43:05,719 INFO [train.py:1153] Epoch 4, batch 700, loss[loss=0.2859, simple_loss=0.2938, pruned_loss=0.09934, ctc_loss=0.1984, over 4751.00 frames. ], tot_loss[loss=0.3364, simple_loss=0.3268, pruned_loss=0.1233, ctc_loss=0.2485, over 938143.18 frames. ], batch size: 19, lr: 2.26e-02,
2024-10-08 03:43:13,304 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=9.610e-01
2024-10-08 03:43:15,800 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=7085.333333333333, ans=0.167875
2024-10-08 03:43:16,930 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=7088.666666666667, ans=0.16771875000000003
2024-10-08 03:43:18,457 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.07 vs. limit=12.8165
2024-10-08 03:43:29,690 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.64 vs. limit=6.8368
2024-10-08 03:43:30,468 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=7092.0, ans=0.1675625
2024-10-08 03:43:31,504 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=7092.0, ans=0.1675625
2024-10-08 03:43:35,406 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=7092.0, ans=0.009327826086956523
2024-10-08 03:43:37,810 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=7092.0, ans=0.30638
2024-10-08 03:44:00,005 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.114e+02 1.475e+02 1.633e+02 1.887e+02 5.403e+02, threshold=3.266e+02, percent-clipped=1.0
2024-10-08 03:44:07,515 INFO [train.py:1153] Epoch 4, batch 750, loss[loss=0.3192, simple_loss=0.3238, pruned_loss=0.1123, ctc_loss=0.2251, over 4884.00 frames. ], tot_loss[loss=0.3353, simple_loss=0.3267, pruned_loss=0.1225, ctc_loss=0.2471, over 945003.39 frames. ], batch size: 22, lr: 2.26e-02,
2024-10-08 03:45:02,583 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.19 vs. limit=10.16825
2024-10-08 03:45:06,980 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=7115.333333333333, ans=0.025
2024-10-08 03:45:09,234 INFO [train.py:1153] Epoch 4, batch 800, loss[loss=0.2946, simple_loss=0.3023, pruned_loss=0.1001, ctc_loss=0.2164, over 4855.00 frames. ], tot_loss[loss=0.3335, simple_loss=0.3258, pruned_loss=0.1215, ctc_loss=0.2455, over 949810.43 frames. ], batch size: 19, lr: 2.26e-02,
2024-10-08 03:45:11,818 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7118.666666666667, ans=0.2288133333333333
2024-10-08 03:45:59,714 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=7132.0, ans=0.1656875
2024-10-08 03:46:04,221 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.078e+02 1.436e+02 1.590e+02 1.813e+02 3.873e+02, threshold=3.180e+02, percent-clipped=1.0
2024-10-08 03:46:04,364 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7132.0, ans=0.22868
2024-10-08 03:46:05,615 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=7132.0, ans=0.025
2024-10-08 03:46:11,047 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=2.88 vs. limit=10.17575
2024-10-08 03:46:11,569 INFO [train.py:1153] Epoch 4, batch 850, loss[loss=0.3016, simple_loss=0.3065, pruned_loss=0.1039, ctc_loss=0.222, over 4773.00 frames. ], tot_loss[loss=0.3337, simple_loss=0.326, pruned_loss=0.1215, ctc_loss=0.2457, over 954002.51 frames. ], batch size: 29, lr: 2.25e-02,
2024-10-08 03:46:23,953 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.63 vs. limit=12.854
2024-10-08 03:46:58,679 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=7148.666666666667, ans=0.16490624999999998
2024-10-08 03:46:58,766 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=7148.666666666667, ans=0.16490624999999998
2024-10-08 03:47:01,067 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=7148.666666666667, ans=0.16490624999999998
2024-10-08 03:47:12,057 INFO [train.py:1153] Epoch 4, batch 900, loss[loss=0.2668, simple_loss=0.2862, pruned_loss=0.08777, ctc_loss=0.1797, over 4852.00 frames. ], tot_loss[loss=0.3331, simple_loss=0.3256, pruned_loss=0.1213, ctc_loss=0.2453, over 956970.46 frames. ], batch size: 19, lr: 2.25e-02,
2024-10-08 03:48:06,278 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.158e+02 1.472e+02 1.614e+02 1.803e+02 2.766e+02, threshold=3.228e+02, percent-clipped=0.0
2024-10-08 03:48:13,588 INFO [train.py:1153] Epoch 4, batch 950, loss[loss=0.3309, simple_loss=0.3296, pruned_loss=0.1195, ctc_loss=0.2331, over 4819.00 frames. ], tot_loss[loss=0.3339, simple_loss=0.3261, pruned_loss=0.1216, ctc_loss=0.2463, over 958840.31 frames. ], batch size: 19, lr: 2.25e-02,
2024-10-08 03:48:19,143 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.07 vs. limit=8.584333333333333
2024-10-08 03:48:45,393 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=7175.333333333333, ans=0.03676944444444445
2024-10-08 03:49:03,129 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.27 vs. limit=10.193249999999999
2024-10-08 03:49:14,806 INFO [train.py:1153] Epoch 4, batch 1000, loss[loss=0.241, simple_loss=0.2638, pruned_loss=0.07421, ctc_loss=0.1745, over 4931.00 frames. ], tot_loss[loss=0.335, simple_loss=0.3261, pruned_loss=0.1223, ctc_loss=0.248, over 960696.18 frames. ], batch size: 20, lr: 2.25e-02,
2024-10-08 03:49:28,683 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=7188.666666666667, ans=0.16303125000000002
2024-10-08 03:49:35,177 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.14 vs. limit=4.0783000000000005
2024-10-08 03:49:54,445 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=7195.333333333333, ans=0.16271875000000002
2024-10-08 03:49:56,916 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=7195.333333333333, ans=0.07
2024-10-08 03:50:05,836 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.68 vs. limit=12.899000000000001
2024-10-08 03:50:07,189 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.85 vs. limit=10.1995
2024-10-08 03:50:09,277 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.127e+02 1.462e+02 1.606e+02 1.840e+02 2.707e+02, threshold=3.212e+02, percent-clipped=0.0
2024-10-08 03:50:10,599 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=7198.666666666667, ans=0.036672222222222224
2024-10-08 03:50:16,566 INFO [train.py:1153] Epoch 4, batch 1050, loss[loss=0.3858, simple_loss=0.3439, pruned_loss=0.1551, ctc_loss=0.294, over 4796.00 frames. ], tot_loss[loss=0.3334, simple_loss=0.3253, pruned_loss=0.1215, ctc_loss=0.2462, over 962859.70 frames. ], batch size: 25, lr: 2.25e-02,
2024-10-08 03:50:27,666 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=7205.333333333333, ans=0.16225
2024-10-08 03:50:29,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.25 vs. limit=10.202
2024-10-08 03:50:38,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=7205.333333333333, ans=0.16225
2024-10-08 03:50:40,991 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=7208.666666666667, ans=0.036630555555555555
2024-10-08 03:50:43,595 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=7208.666666666667, ans=0.036630555555555555
2024-10-08 03:50:54,769 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=7212.0, ans=0.036616666666666665
2024-10-08 03:50:59,590 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=7212.0, ans=0.009301739130434783
2024-10-08 03:51:08,161 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=7215.333333333333, ans=0.009301014492753624
2024-10-08 03:51:17,836 INFO [train.py:1153] Epoch 4, batch 1100, loss[loss=0.335, simple_loss=0.3289, pruned_loss=0.1205, ctc_loss=0.25, over 4860.00 frames. ], tot_loss[loss=0.3334, simple_loss=0.3252, pruned_loss=0.1215, ctc_loss=0.2464, over 964083.16 frames. ], batch size: 20, lr: 2.24e-02,
2024-10-08 03:51:35,730 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.62 vs. limit=10.20825
2024-10-08 03:51:44,985 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=7225.333333333333, ans=0.16131250000000003
2024-10-08 03:51:45,457 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.48 vs. limit=12.919
2024-10-08 03:51:58,547 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=7228.666666666667, ans=0.009298115942028985
2024-10-08 03:52:07,288 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=7232.0, ans=0.025
2024-10-08 03:52:11,838 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.002e+02 1.435e+02 1.620e+02 1.833e+02 2.600e+02, threshold=3.241e+02, percent-clipped=0.0
2024-10-08 03:52:19,281 INFO [train.py:1153] Epoch 4, batch 1150, loss[loss=0.3014, simple_loss=0.3021, pruned_loss=0.1076, ctc_loss=0.2137, over 4863.00 frames. ], tot_loss[loss=0.3343, simple_loss=0.3258, pruned_loss=0.1219, ctc_loss=0.2474, over 964369.01 frames. ], batch size: 20, lr: 2.24e-02,
2024-10-08 03:52:23,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=7235.333333333333, ans=0.6467633333333334
2024-10-08 03:52:23,623 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.89 vs. limit=10.21325
2024-10-08 03:52:24,917 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=12.73 vs. limit=12.9265
2024-10-08 03:52:35,614 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 03:52:52,626 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=7242.0, ans=0.0
2024-10-08 03:53:03,930 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.88 vs. limit=12.934000000000001
2024-10-08 03:53:20,936 INFO [train.py:1153] Epoch 4, batch 1200, loss[loss=0.3461, simple_loss=0.3368, pruned_loss=0.125, ctc_loss=0.2637, over 4793.00 frames. ], tot_loss[loss=0.3351, simple_loss=0.3265, pruned_loss=0.1223, ctc_loss=0.2478, over 964319.14 frames. ], batch size: 25, lr: 2.24e-02,
2024-10-08 03:53:29,897 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=7252.0, ans=0.1600625
2024-10-08 03:53:31,055 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=7252.0, ans=9.532499999999999
2024-10-08 03:53:32,312 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=7255.333333333333, ans=0.15990625000000003
2024-10-08 03:53:45,678 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=7258.666666666667, ans=0.027316666666666666
2024-10-08 03:54:07,548 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=7262.0, ans=0.15959374999999998
2024-10-08 03:54:08,154 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=6.23 vs. limit=10.22325
2024-10-08 03:54:14,968 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.189e+02 1.487e+02 1.670e+02 1.813e+02 2.658e+02, threshold=3.341e+02, percent-clipped=0.0
2024-10-08 03:54:17,569 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=7265.333333333333, ans=0.07
2024-10-08 03:54:21,259 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=7268.666666666667, ans=0.15928124999999999
2024-10-08 03:54:22,325 INFO [train.py:1153] Epoch 4, batch 1250, loss[loss=0.3931, simple_loss=0.3576, pruned_loss=0.155, ctc_loss=0.2962, over 4758.00 frames. ], tot_loss[loss=0.3343, simple_loss=0.3258, pruned_loss=0.122, ctc_loss=0.2472, over 964346.90 frames. ], batch size: 32, lr: 2.24e-02,
2024-10-08 03:54:34,272 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.80 vs. limit=12.954
2024-10-08 03:54:44,713 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=7272.0, ans=0.15912500000000002
2024-10-08 03:54:51,971 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=7275.333333333333, ans=0.15896875
2024-10-08 03:54:58,189 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=7278.666666666667, ans=0.6452466666666667
2024-10-08 03:55:18,039 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=7282.0, ans=0.15865625
2024-10-08 03:55:23,954 INFO [train.py:1153] Epoch 4, batch 1300, loss[loss=0.3577, simple_loss=0.3293, pruned_loss=0.14, ctc_loss=0.2654, over 4823.00 frames. ], tot_loss[loss=0.3322, simple_loss=0.3245, pruned_loss=0.1209, ctc_loss=0.2452, over 965666.90 frames. ], batch size: 43, lr: 2.23e-02,
2024-10-08 03:55:29,517 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.90 vs. limit=12.964
2024-10-08 03:55:43,919 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=7288.666666666667, ans=0.15834375
2024-10-08 03:55:45,421 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.67 vs. limit=12.9665
2024-10-08 03:55:53,857 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=7292.0, ans=0.15818749999999998
2024-10-08 03:55:58,771 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=7292.0, ans=0.054425
2024-10-08 03:56:12,921 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=1.925e+00
2024-10-08 03:56:13,342 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.44 vs. limit=10.237
2024-10-08 03:56:14,059 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=7298.666666666667, ans=0.036255555555555555
2024-10-08 03:56:18,946 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.119e+02 1.441e+02 1.629e+02 1.801e+02 2.932e+02, threshold=3.258e+02, percent-clipped=0.0
2024-10-08 03:56:23,836 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=7298.666666666667, ans=0.6445466666666667
2024-10-08 03:56:25,982 INFO [train.py:1153] Epoch 4, batch 1350, loss[loss=0.291, simple_loss=0.2875, pruned_loss=0.1017, ctc_loss=0.2279, over 4855.00 frames. ], tot_loss[loss=0.3315, simple_loss=0.3242, pruned_loss=0.1205, ctc_loss=0.2443, over 966349.53 frames. ], batch size: 21, lr: 2.23e-02,
2024-10-08 03:56:45,449 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=7305.333333333333, ans=0.1575625
2024-10-08 03:56:52,997 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.53 vs. limit=10.24075
2024-10-08 03:57:25,153 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.26 vs. limit=10.24325
2024-10-08 03:57:26,928 INFO [train.py:1153] Epoch 4, batch 1400, loss[loss=0.2689, simple_loss=0.2853, pruned_loss=0.08903, ctc_loss=0.1864, over 4940.00 frames. ], tot_loss[loss=0.3324, simple_loss=0.325, pruned_loss=0.121, ctc_loss=0.2447, over 966568.61 frames. ], batch size: 19, lr: 2.23e-02,
2024-10-08 03:57:35,248 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=5.48 vs. limit=5.463733333333334
2024-10-08 03:57:49,465 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=2.66 vs. limit=10.245750000000001
2024-10-08 03:57:50,192 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=7325.333333333333, ans=0.009277101449275363
2024-10-08 03:58:03,430 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7328.666666666667, ans=0.22671333333333332
2024-10-08 03:58:04,720 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=7328.666666666667, ans=0.15646875
2024-10-08 03:58:13,790 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.00 vs. limit=4.0992999999999995
2024-10-08 03:58:20,316 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.090e+02 1.433e+02 1.608e+02 1.722e+02 2.411e+02, threshold=3.217e+02, percent-clipped=0.0
2024-10-08 03:58:20,898 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.60 vs. limit=10.2495
2024-10-08 03:58:24,590 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.22 vs. limit=10.2495
2024-10-08 03:58:27,506 INFO [train.py:1153] Epoch 4, batch 1450, loss[loss=0.3962, simple_loss=0.3662, pruned_loss=0.1547, ctc_loss=0.2918, over 4795.00 frames. ], tot_loss[loss=0.3356, simple_loss=0.3268, pruned_loss=0.1228, ctc_loss=0.2471, over 966449.11 frames. ], batch size: 34, lr: 2.23e-02,
2024-10-08 03:58:31,309 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=7335.333333333333, ans=0.6432633333333334
2024-10-08 03:58:46,446 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.43 vs. limit=10.251999999999999
2024-10-08 03:58:55,860 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=7342.0, ans=0.15584375
2024-10-08 03:59:03,593 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.65 vs. limit=13.009
2024-10-08 03:59:12,993 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=7345.333333333333, ans=0.009272753623188405
2024-10-08 03:59:28,731 INFO [train.py:1153] Epoch 4, batch 1500, loss[loss=0.3131, simple_loss=0.3097, pruned_loss=0.1137, ctc_loss=0.2227, over 4725.00 frames. ], tot_loss[loss=0.3353, simple_loss=0.3264, pruned_loss=0.1226, ctc_loss=0.2475, over 966192.21 frames. ], batch size: 26, lr: 2.22e-02,
2024-10-08 03:59:49,564 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=7355.333333333333, ans=0.035
2024-10-08 03:59:50,764 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=7355.333333333333, ans=0.8235533333333334
2024-10-08 04:00:18,813 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=7365.333333333333, ans=0.6422133333333334
2024-10-08 04:00:22,264 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.093e+02 1.504e+02 1.657e+02 1.871e+02 2.525e+02, threshold=3.313e+02, percent-clipped=0.0
2024-10-08 04:00:29,524 INFO [train.py:1153] Epoch 4, batch 1550, loss[loss=0.2974, simple_loss=0.286, pruned_loss=0.1082, ctc_loss=0.2311, over 4872.00 frames. ], tot_loss[loss=0.336, simple_loss=0.3267, pruned_loss=0.123, ctc_loss=0.2482, over 966190.01 frames. ], batch size: 31, lr: 2.22e-02,
2024-10-08 04:00:45,434 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=7372.0, ans=0.03595
2024-10-08 04:00:50,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.16 vs. limit=13.029
2024-10-08 04:01:05,106 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=7378.666666666667, ans=0.154125
2024-10-08 04:01:10,688 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.00 vs. limit=4.1068
2024-10-08 04:01:20,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=7382.0, ans=0.15396875
2024-10-08 04:01:21,690 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=5.71 vs. limit=10.26825
2024-10-08 04:01:27,648 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.17 vs. limit=13.0365
2024-10-08 04:01:30,659 INFO [train.py:1153] Epoch 4, batch 1600, loss[loss=0.3221, simple_loss=0.3276, pruned_loss=0.1114, ctc_loss=0.2347, over 4825.00 frames. ], tot_loss[loss=0.3342, simple_loss=0.3261, pruned_loss=0.1219, ctc_loss=0.2464, over 966281.81 frames. ], batch size: 25, lr: 2.22e-02,
2024-10-08 04:01:35,637 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=7385.333333333333, ans=0.03589444444444445
2024-10-08 04:01:46,693 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=7388.666666666667, ans=0.15365625
2024-10-08 04:02:04,683 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.97 vs. limit=8.696
2024-10-08 04:02:05,107 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 04:02:10,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=7395.333333333333, ans=0.15334375
2024-10-08 04:02:24,777 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.064e+02 1.411e+02 1.574e+02 1.760e+02 1.170e+03, threshold=3.149e+02, percent-clipped=1.0
2024-10-08 04:02:31,994 INFO [train.py:1153] Epoch 4, batch 1650, loss[loss=0.2967, simple_loss=0.303, pruned_loss=0.1007, ctc_loss=0.223, over 4774.00 frames. ], tot_loss[loss=0.3342, simple_loss=0.3261, pruned_loss=0.1219, ctc_loss=0.2465, over 966607.59 frames. ], batch size: 29, lr: 2.22e-02,
2024-10-08 04:02:52,288 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.74 vs. limit=13.054
2024-10-08 04:03:11,468 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7412.0, ans=0.22588
2024-10-08 04:03:23,731 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=7415.333333333333, ans=0.6404633333333334
2024-10-08 04:03:32,409 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=7418.666666666667, ans=0.31128
2024-10-08 04:03:33,518 INFO [train.py:1153] Epoch 4, batch 1700, loss[loss=0.3084, simple_loss=0.3182, pruned_loss=0.108, ctc_loss=0.2067, over 4940.00 frames. ], tot_loss[loss=0.332, simple_loss=0.325, pruned_loss=0.1207, ctc_loss=0.2442, over 966818.46 frames. ], batch size: 19, lr: 2.22e-02,
2024-10-08 04:03:36,094 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.prob, batch_count=7418.666666666667, ans=0.15225
2024-10-08 04:03:36,140 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=7418.666666666667, ans=0.035755555555555554
2024-10-08 04:03:38,690 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7418.666666666667, ans=0.2258133333333333
2024-10-08 04:03:42,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=7418.666666666667, ans=0.009256811594202899
2024-10-08 04:04:08,225 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=7425.333333333333, ans=0.1519375
2024-10-08 04:04:16,016 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.61 vs. limit=10.28575
2024-10-08 04:04:27,699 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.069e+02 1.468e+02 1.621e+02 1.783e+02 3.307e+02, threshold=3.242e+02, percent-clipped=1.0
2024-10-08 04:04:35,193 INFO [train.py:1153] Epoch 4, batch 1750, loss[loss=0.2975, simple_loss=0.3064, pruned_loss=0.1033, ctc_loss=0.2055, over 4959.00 frames. ], tot_loss[loss=0.3313, simple_loss=0.3251, pruned_loss=0.1202, ctc_loss=0.2431, over 966900.45 frames. ], batch size: 19, lr: 2.21e-02,
2024-10-08 04:04:59,089 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=7442.0, ans=0.07
2024-10-08 04:05:13,076 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.96 vs. limit=10.292
2024-10-08 04:05:37,510 INFO [train.py:1153] Epoch 4, batch 1800, loss[loss=0.3361, simple_loss=0.3267, pruned_loss=0.1259, ctc_loss=0.2344, over 4873.00 frames. ], tot_loss[loss=0.3332, simple_loss=0.3264, pruned_loss=0.121, ctc_loss=0.2448, over 967780.13 frames. ], batch size: 23, lr: 2.21e-02,
2024-10-08 04:05:40,212 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=7452.0, ans=0.6391800000000001
2024-10-08 04:05:43,825 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=7452.0, ans=0.009249565217391304
2024-10-08 04:06:06,706 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=7458.666666666667, ans=0.15037499999999998
2024-10-08 04:06:31,597 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.110e+02 1.420e+02 1.594e+02 1.739e+02 2.508e+02, threshold=3.189e+02, percent-clipped=0.0
2024-10-08 04:06:33,291 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.57 vs. limit=13.099
2024-10-08 04:06:35,365 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7465.333333333333, ans=0.22534666666666667
2024-10-08 04:06:36,963 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.74 vs. limit=6.866333333333333
2024-10-08 04:06:38,825 INFO [train.py:1153] Epoch 4, batch 1850, loss[loss=0.315, simple_loss=0.3052, pruned_loss=0.1177, ctc_loss=0.2234, over 4736.00 frames. ], tot_loss[loss=0.3317, simple_loss=0.3256, pruned_loss=0.1203, ctc_loss=0.2431, over 967924.76 frames. ], batch size: 26, lr: 2.21e-02,
2024-10-08 04:06:46,932 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.58 vs. limit=6.867166666666667
2024-10-08 04:06:47,548 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=7468.666666666667, ans=0.14990625000000002
2024-10-08 04:06:48,818 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=7468.666666666667, ans=0.04949747468305833
2024-10-08 04:06:48,922 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7468.666666666667, ans=0.2253133333333333
2024-10-08 04:07:18,259 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7478.666666666667, ans=0.22521333333333332
2024-10-08 04:07:18,649 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.16 vs. limit=4.1218
2024-10-08 04:07:25,740 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=7478.666666666667, ans=0.00924376811594203
2024-10-08 04:07:27,388 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.11 vs. limit=13.1115
2024-10-08 04:07:33,612 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.50 vs. limit=13.1115
2024-10-08 04:07:40,260 INFO [train.py:1153] Epoch 4, batch 1900, loss[loss=0.3676, simple_loss=0.3414, pruned_loss=0.1431, ctc_loss=0.2689, over 4784.00 frames. ], tot_loss[loss=0.3331, simple_loss=0.3263, pruned_loss=0.1212, ctc_loss=0.2438, over 967832.65 frames. ], batch size: 29, lr: 2.21e-02,
2024-10-08 04:07:49,009 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=7485.333333333333, ans=0.03547777777777778
2024-10-08 04:07:50,534 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.51 vs. limit=10.307
2024-10-08 04:07:56,685 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.08 vs. limit=13.1165
2024-10-08 04:07:59,198 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.77 vs. limit=10.308250000000001
2024-10-08 04:08:04,930 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=7492.0, ans=0.00924086956521739
2024-10-08 04:08:09,801 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 04:08:17,580 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=26.09 vs. limit=10.31075
2024-10-08 04:08:20,809 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=7495.333333333333, ans=0.14865625
2024-10-08 04:08:23,260 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=7495.333333333333, ans=0.14865625
2024-10-08 04:08:34,102 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.023e+02 1.536e+02 1.702e+02 1.851e+02 2.806e+02, threshold=3.403e+02, percent-clipped=0.0
2024-10-08 04:08:41,508 INFO [train.py:1153] Epoch 4, batch 1950, loss[loss=0.3455, simple_loss=0.3308, pruned_loss=0.1317, ctc_loss=0.2419, over 4862.00 frames. ], tot_loss[loss=0.3347, simple_loss=0.3269, pruned_loss=0.122, ctc_loss=0.246, over 966738.15 frames. ], batch size: 20, lr: 2.20e-02,
2024-10-08 04:08:47,240 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=3.42 vs. limit=7.0008
2024-10-08 04:08:47,836 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=7502.0, ans=0.14834375
2024-10-08 04:09:22,041 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=7512.0, ans=0.14787499999999998
2024-10-08 04:09:42,761 INFO [train.py:1153] Epoch 4, batch 2000, loss[loss=0.3185, simple_loss=0.3396, pruned_loss=0.1072, ctc_loss=0.2076, over 4959.00 frames. ], tot_loss[loss=0.3351, simple_loss=0.3269, pruned_loss=0.1223, ctc_loss=0.2466, over 966583.32 frames. ], batch size: 19, lr: 2.20e-02,
2024-10-08 04:09:51,501 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=7518.666666666667, ans=0.2248133333333333
2024-10-08 04:09:56,400 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=7522.0, ans=0.14740625000000002
2024-10-08 04:10:07,093 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.46 vs. limit=10.322
2024-10-08 04:10:11,762 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=7525.333333333333, ans=13.144
2024-10-08 04:10:28,480 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=7528.666666666667, ans=0.009232898550724638
2024-10-08 04:10:31,595 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=10.29 vs. limit=10.3245
2024-10-08 04:10:37,112 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.103e+02 1.430e+02 1.606e+02 1.776e+02 2.336e+02, threshold=3.212e+02, percent-clipped=0.0
2024-10-08 04:10:38,520 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=7532.0, ans=0.1469375
2024-10-08 04:10:39,663 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=7532.0, ans=0.1469375
2024-10-08 04:10:44,363 INFO [train.py:1153] Epoch 4, batch 2050, loss[loss=0.3421, simple_loss=0.3302, pruned_loss=0.1246, ctc_loss=0.2619, over 4910.00 frames. ], tot_loss[loss=0.334, simple_loss=0.326, pruned_loss=0.1219, ctc_loss=0.2456, over 966862.03 frames. ], batch size: 19, lr: 2.20e-02,
2024-10-08 04:10:48,162 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=7535.333333333333, ans=0.009231449275362319
2024-10-08 04:10:52,885 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=7535.333333333333, ans=0.07
2024-10-08 04:11:00,349 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=7538.666666666667, ans=0.0
2024-10-08 04:11:07,707 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=7542.0, ans=0.03524166666666667
2024-10-08 04:11:10,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=7542.0, ans=0.03524166666666667
2024-10-08 04:11:19,766 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=7545.333333333333, ans=0.03522777777777778
2024-10-08 04:11:25,843 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=7545.333333333333, ans=0.1463125
2024-10-08 04:11:25,895 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=7545.333333333333, ans=0.09899494936611666
2024-10-08 04:11:34,546 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=7548.666666666667, ans=0.8254866666666667
2024-10-08 04:11:44,484 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=7552.0, ans=0.14600000000000002
2024-10-08 04:11:45,533 INFO [train.py:1153] Epoch 4, batch 2100, loss[loss=0.3164, simple_loss=0.3074, pruned_loss=0.1159, ctc_loss=0.234, over 4842.00 frames. ], tot_loss[loss=0.3315, simple_loss=0.3246, pruned_loss=0.1205, ctc_loss=0.2437, over 967040.29 frames. ], batch size: 21, lr: 2.20e-02,
2024-10-08 04:11:54,331 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=7552.0, ans=0.009227826086956522
2024-10-08 04:11:54,740 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.11 vs. limit=4.1328
2024-10-08 04:12:04,150 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=7555.333333333333, ans=0.03518611111111111
2024-10-08 04:12:17,866 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.30 vs. limit=10.3345
2024-10-08 04:12:33,539 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=7565.333333333333, ans=0.22434666666666667
2024-10-08 04:12:37,275 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=7565.333333333333, ans=0.14537499999999998
2024-10-08 04:12:39,407 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.153e+02 1.469e+02 1.633e+02 1.851e+02 3.748e+02, threshold=3.267e+02, percent-clipped=1.0
2024-10-08 04:12:46,864 INFO [train.py:1153] Epoch 4, batch 2150, loss[loss=0.349, simple_loss=0.3374, pruned_loss=0.1297, ctc_loss=0.2529, over 4863.00 frames. ], tot_loss[loss=0.3298, simple_loss=0.3234, pruned_loss=0.1198, ctc_loss=0.2418, over 967876.40 frames. ], batch size: 20, lr: 2.20e-02,
2024-10-08 04:13:03,501 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.62 vs. limit=13.179
2024-10-08 04:13:05,439 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7572.0, ans=0.22427999999999998
2024-10-08 04:13:12,161 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.36 vs. limit=13.1815
2024-10-08 04:13:17,589 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7575.333333333333, ans=0.22424666666666665
2024-10-08 04:13:34,946 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.77 vs. limit=13.186499999999999
2024-10-08 04:13:47,881 INFO [train.py:1153] Epoch 4, batch 2200, loss[loss=0.3084, simple_loss=0.2991, pruned_loss=0.1111, ctc_loss=0.2386, over 4746.00 frames. ], tot_loss[loss=0.3296, simple_loss=0.3235, pruned_loss=0.1195, ctc_loss=0.2417, over 967551.32 frames. ], batch size: 26, lr: 2.19e-02,
2024-10-08 04:13:49,731 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.19 vs. limit=4.1378
2024-10-08 04:14:01,038 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=3.75 vs. limit=7.035466666666666
2024-10-08 04:14:13,853 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=7592.0, ans=0.03503333333333333
2024-10-08 04:14:20,610 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.79 vs. limit=10.347
2024-10-08 04:14:22,560 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=7592.0, ans=0.144125
2024-10-08 04:14:26,152 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=7595.333333333333, ans=0.14396874999999998
2024-10-08 04:14:42,517 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.005e+02 1.416e+02 1.588e+02 1.738e+02 2.815e+02, threshold=3.176e+02, percent-clipped=0.0
2024-10-08 04:14:49,475 INFO [train.py:1153] Epoch 4, batch 2250, loss[loss=0.3109, simple_loss=0.3102, pruned_loss=0.11, ctc_loss=0.2292, over 4869.00 frames. ], tot_loss[loss=0.3304, simple_loss=0.3241, pruned_loss=0.12, ctc_loss=0.2418, over 967613.06 frames. ], batch size: 22, lr: 2.19e-02,
2024-10-08 04:14:55,517 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=7602.0, ans=0.22398
2024-10-08 04:14:56,743 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=7602.0, ans=0.14365624999999999
2024-10-08 04:15:06,448 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=7605.333333333333, ans=0.07
2024-10-08 04:15:11,081 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=7605.333333333333, ans=0.14350000000000002
2024-10-08 04:15:11,184 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=7605.333333333333, ans=0.14350000000000002
2024-10-08 04:15:16,030 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=7608.666666666667, ans=0.14334375
2024-10-08 04:15:22,670 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.69 vs. limit=13.2065
2024-10-08 04:15:26,410 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=48.49 vs. limit=10.3545
2024-10-08 04:15:27,172 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=7612.0, ans=0.14318750000000002
2024-10-08 04:15:32,531 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.44 vs. limit=8.806000000000001
2024-10-08 04:15:35,692 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=7612.0, ans=0.07
2024-10-08 04:15:50,248 INFO [train.py:1153] Epoch 4, batch 2300, loss[loss=0.3078, simple_loss=0.3135, pruned_loss=0.109, ctc_loss=0.2103, over 4883.00 frames. ], tot_loss[loss=0.3282, simple_loss=0.323, pruned_loss=0.1188, ctc_loss=0.2394, over 968207.48 frames. ], batch size: 19, lr: 2.19e-02,
2024-10-08 04:15:52,873 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=7618.666666666667, ans=0.14287499999999997
2024-10-08 04:15:55,435 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=7618.666666666667, ans=0.025
2024-10-08 04:15:59,084 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=7618.666666666667, ans=0.035
2024-10-08 04:16:04,026 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=7622.0, ans=0.63323
2024-10-08 04:16:16,467 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=7625.333333333333, ans=0.14256249999999998
2024-10-08 04:16:20,053 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=7625.333333333333, ans=0.6331133333333334
2024-10-08 04:16:24,900 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=7625.333333333333, ans=0.03489444444444445
2024-10-08 04:16:35,783 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=7628.666666666667, ans=0.14240625
2024-10-08 04:16:44,134 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.012e+02 1.398e+02 1.562e+02 1.718e+02 2.349e+02, threshold=3.124e+02, percent-clipped=0.0
2024-10-08 04:16:50,554 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=7635.333333333333, ans=0.009209710144927536
2024-10-08 04:16:51,611 INFO [train.py:1153] Epoch 4, batch 2350, loss[loss=0.3014, simple_loss=0.3025, pruned_loss=0.1059, ctc_loss=0.2214, over 4859.00 frames. ], tot_loss[loss=0.328, simple_loss=0.323, pruned_loss=0.1187, ctc_loss=0.2389, over 968307.41 frames. ], batch size: 23, lr: 2.19e-02,
2024-10-08 04:16:54,206 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=7635.333333333333, ans=0.03485277777777778
2024-10-08 04:16:56,718 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7635.333333333333, ans=0.22364666666666666
2024-10-08 04:17:08,147 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.18 vs. limit=13.229
2024-10-08 04:17:13,948 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7638.666666666667, ans=0.22361333333333333
2024-10-08 04:17:39,874 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=7648.666666666667, ans=0.6322966666666667
2024-10-08 04:17:44,704 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=6.389e-02
2024-10-08 04:17:48,357 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=7648.666666666667, ans=0.0
2024-10-08 04:17:53,045 INFO [train.py:1153] Epoch 4, batch 2400, loss[loss=0.343, simple_loss=0.3229, pruned_loss=0.1337, ctc_loss=0.2389, over 4750.00 frames. ], tot_loss[loss=0.3294, simple_loss=0.3237, pruned_loss=0.1195, ctc_loss=0.2404, over 967582.97 frames. ], batch size: 19, lr: 2.19e-02,
2024-10-08 04:17:58,095 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=7652.0, ans=0.009206086956521739
2024-10-08 04:17:58,154 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=7652.0, ans=9.782499999999999
2024-10-08 04:18:00,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7652.0, ans=0.22348
2024-10-08 04:18:05,400 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=7655.333333333333, ans=0.00920536231884058
2024-10-08 04:18:11,460 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=7655.333333333333, ans=0.14115624999999998
2024-10-08 04:18:16,109 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=7658.666666666667, ans=0.14100000000000001
2024-10-08 04:18:30,941 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=7662.0, ans=0.07
2024-10-08 04:18:46,693 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.062e+02 1.446e+02 1.593e+02 1.751e+02 2.606e+02, threshold=3.186e+02, percent-clipped=0.0
2024-10-08 04:18:53,260 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.80 vs. limit=13.2515
2024-10-08 04:18:53,935 INFO [train.py:1153] Epoch 4, batch 2450, loss[loss=0.3247, simple_loss=0.3277, pruned_loss=0.1128, ctc_loss=0.2401, over 4877.00 frames. ], tot_loss[loss=0.3312, simple_loss=0.3254, pruned_loss=0.1201, ctc_loss=0.2419, over 966796.70 frames. ], batch size: 22, lr: 2.18e-02,
2024-10-08 04:18:55,312 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=7668.666666666667, ans=0.14053125
2024-10-08 04:18:56,039 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.02 vs. limit=4.1503
2024-10-08 04:18:58,887 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=7668.666666666667, ans=0.14053125
2024-10-08 04:19:13,658 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=7672.0, ans=0.63148
2024-10-08 04:19:14,113 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.73 vs. limit=10.376999999999999
2024-10-08 04:19:14,984 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=7672.0, ans=0.14037500000000003
2024-10-08 04:19:16,149 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=7672.0, ans=0.82672
2024-10-08 04:19:34,500 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=7678.666666666667, ans=0.03467222222222222
2024-10-08 04:19:36,303 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=7.40 vs. limit=10.3795
2024-10-08 04:19:50,730 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=7.689e-03
2024-10-08 04:19:55,319 INFO [train.py:1153] Epoch 4, batch 2500, loss[loss=0.3055, simple_loss=0.3037, pruned_loss=0.1104, ctc_loss=0.2161, over 4728.00 frames. ], tot_loss[loss=0.3297, simple_loss=0.3246, pruned_loss=0.1193, ctc_loss=0.2403, over 966471.94 frames. ], batch size: 26, lr: 2.18e-02,
2024-10-08 04:20:08,609 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.23 vs. limit=13.2665
2024-10-08 04:20:08,984 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=7688.666666666667, ans=0.009198115942028986
2024-10-08 04:20:17,511 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=7688.666666666667, ans=0.13959375000000002
2024-10-08 04:20:27,156 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=7692.0, ans=0.03461666666666667
2024-10-08 04:20:49,233 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.899e+01 1.435e+02 1.648e+02 1.809e+02 2.334e+02, threshold=3.295e+02, percent-clipped=0.0
2024-10-08 04:20:49,376 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.max_abs, batch_count=7698.666666666667, ans=9.811666666666667
2024-10-08 04:20:54,246 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=7698.666666666667, ans=0.025
2024-10-08 04:20:56,583 INFO [train.py:1153] Epoch 4, batch 2550, loss[loss=0.3364, simple_loss=0.3286, pruned_loss=0.1221, ctc_loss=0.2498, over 4959.00 frames. ], tot_loss[loss=0.3311, simple_loss=0.3251, pruned_loss=0.1201, ctc_loss=0.2422, over 966913.72 frames. ], batch size: 19, lr: 2.18e-02,
2024-10-08 04:21:19,336 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.16 vs. limit=10.3895
2024-10-08 04:21:25,431 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=14.94 vs. limit=10.39075
2024-10-08 04:21:26,161 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=7708.666666666667, ans=0.31563
2024-10-08 04:21:33,072 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.04 vs. limit=8.856
2024-10-08 04:21:50,146 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=3.35 vs. limit=7.086133333333333
2024-10-08 04:21:57,953 INFO [train.py:1153] Epoch 4, batch 2600, loss[loss=0.2901, simple_loss=0.3003, pruned_loss=0.09667, ctc_loss=0.2162, over 4863.00 frames. ], tot_loss[loss=0.3305, simple_loss=0.3245, pruned_loss=0.1199, ctc_loss=0.2415, over 966399.94 frames. ], batch size: 20, lr: 2.18e-02,
2024-10-08 04:22:01,864 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=7718.666666666667, ans=0.8271866666666666
2024-10-08 04:22:02,488 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.09 vs. limit=13.289
2024-10-08 04:22:09,183 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=7722.0, ans=0.13803125
2024-10-08 04:22:09,820 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.08 vs. limit=4.1583000000000006
2024-10-08 04:22:09,834 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.36 vs. limit=13.2915
2024-10-08 04:22:32,103 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=7725.333333333333, ans=0.13787500000000003
2024-10-08 04:22:42,079 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=7728.666666666667, ans=0.31593
2024-10-08 04:22:51,828 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.082e+02 1.401e+02 1.561e+02 1.724e+02 2.567e+02, threshold=3.122e+02, percent-clipped=0.0
2024-10-08 04:23:00,186 INFO [train.py:1153] Epoch 4, batch 2650, loss[loss=0.3973, simple_loss=0.3597, pruned_loss=0.1557, ctc_loss=0.3087, over 4828.00 frames. ], tot_loss[loss=0.3318, simple_loss=0.3249, pruned_loss=0.1207, ctc_loss=0.2433, over 966153.63 frames. ], batch size: 38, lr: 2.17e-02,
2024-10-08 04:23:00,308 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=7735.333333333333, ans=0.13740625
2024-10-08 04:23:00,356 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=7735.333333333333, ans=0.03443611111111111
2024-10-08 04:23:22,017 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.84 vs. limit=13.304
2024-10-08 04:23:30,179 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.59 vs. limit=13.3065
2024-10-08 04:23:35,114 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.81 vs. limit=6.936333333333334
2024-10-08 04:23:49,027 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=7748.666666666667, ans=0.03438055555555555
2024-10-08 04:23:51,408 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer1.prob, batch_count=7748.666666666667, ans=0.13678125000000002
2024-10-08 04:23:59,830 INFO [train.py:1153] Epoch 4, batch 2700, loss[loss=0.3117, simple_loss=0.3154, pruned_loss=0.107, ctc_loss=0.2353, over 4828.00 frames. ], tot_loss[loss=0.3305, simple_loss=0.3243, pruned_loss=0.1198, ctc_loss=0.2424, over 966241.39 frames. ], batch size: 28, lr: 2.17e-02,
2024-10-08 04:24:27,905 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=7758.666666666667, ans=0.025
2024-10-08 04:24:44,249 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.85 vs. limit=10.41075
2024-10-08 04:24:53,237 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.033e+02 1.380e+02 1.555e+02 1.735e+02 3.660e+02, threshold=3.110e+02, percent-clipped=1.0
2024-10-08 04:24:57,111 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=7765.333333333333, ans=0.136
2024-10-08 04:25:00,261 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.19 vs. limit=13.3265
2024-10-08 04:25:00,715 INFO [train.py:1153] Epoch 4, batch 2750, loss[loss=0.3374, simple_loss=0.3305, pruned_loss=0.1224, ctc_loss=0.2492, over 4799.00 frames. ], tot_loss[loss=0.3275, simple_loss=0.3221, pruned_loss=0.1186, ctc_loss=0.2396, over 966976.71 frames. ], batch size: 19, lr: 2.17e-02,
2024-10-08 04:25:02,089 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7768.666666666667, ans=0.2223133333333333
2024-10-08 04:25:05,124 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.37 vs. limit=10.41325
2024-10-08 04:25:15,433 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=7772.0, ans=0.03428333333333333
2024-10-08 04:25:18,183 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.27 vs. limit=13.329
2024-10-08 04:25:21,458 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.8.prob, batch_count=7772.0, ans=0.13568750000000002
2024-10-08 04:25:58,651 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.59 vs. limit=10.41825
2024-10-08 04:26:01,697 INFO [train.py:1153] Epoch 4, batch 2800, loss[loss=0.3708, simple_loss=0.3516, pruned_loss=0.1381, ctc_loss=0.2845, over 4791.00 frames. ], tot_loss[loss=0.3301, simple_loss=0.3236, pruned_loss=0.1198, ctc_loss=0.2426, over 967125.62 frames. ], batch size: 53, lr: 2.17e-02,
2024-10-08 04:26:09,151 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=7785.333333333333, ans=0.13506249999999997
2024-10-08 04:26:15,276 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=7788.666666666667, ans=0.13490625
2024-10-08 04:26:30,444 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.17 vs. limit=10.422
2024-10-08 04:26:42,489 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.85 vs. limit=13.346499999999999
2024-10-08 04:26:45,829 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.52 vs. limit=10.42325
2024-10-08 04:26:50,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.34 vs. limit=6.949666666666667
2024-10-08 04:26:55,258 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.031e+02 1.461e+02 1.663e+02 1.865e+02 2.518e+02, threshold=3.327e+02, percent-clipped=0.0
2024-10-08 04:27:02,577 INFO [train.py:1153] Epoch 4, batch 2850, loss[loss=0.2487, simple_loss=0.2836, pruned_loss=0.07358, ctc_loss=0.1664, over 4941.00 frames. ], tot_loss[loss=0.3309, simple_loss=0.3239, pruned_loss=0.1202, ctc_loss=0.2436, over 966916.17 frames. ], batch size: 20, lr: 2.17e-02,
2024-10-08 04:27:13,756 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7805.333333333333, ans=0.22194666666666668
2024-10-08 04:27:35,875 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=7808.666666666667, ans=0.09899494936611666
2024-10-08 04:27:39,822 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.08 vs. limit=8.906
2024-10-08 04:27:41,229 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=11.65 vs. limit=10.4295
2024-10-08 04:27:45,833 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=7812.0, ans=0.04949747468305833
2024-10-08 04:27:46,352 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.82 vs. limit=8.906
2024-10-08 04:28:01,386 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer1.prob, batch_count=7815.333333333333, ans=0.13365624999999998
2024-10-08 04:28:03,664 INFO [train.py:1153] Epoch 4, batch 2900, loss[loss=0.2732, simple_loss=0.3038, pruned_loss=0.08448, ctc_loss=0.1842, over 4737.00 frames. ], tot_loss[loss=0.3332, simple_loss=0.3257, pruned_loss=0.1213, ctc_loss=0.2451, over 966035.40 frames. ], batch size: 20, lr: 2.16e-02,
2024-10-08 04:28:08,655 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=7818.666666666667, ans=0.1335
2024-10-08 04:28:25,342 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.74 vs. limit=10.433250000000001
2024-10-08 04:28:32,493 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=13.71 vs. limit=13.369
2024-10-08 04:28:35,047 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.87 vs. limit=13.369
2024-10-08 04:28:35,896 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=7825.333333333333, ans=0.13318750000000001
2024-10-08 04:28:40,800 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=7828.666666666667, ans=0.03404722222222222
2024-10-08 04:28:50,906 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.29 vs. limit=13.371500000000001
2024-10-08 04:28:53,588 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.82 vs. limit=8.916
2024-10-08 04:28:57,635 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.039e+02 1.424e+02 1.590e+02 1.737e+02 2.995e+02, threshold=3.180e+02, percent-clipped=0.0
2024-10-08 04:29:04,884 INFO [train.py:1153] Epoch 4, batch 2950, loss[loss=0.3008, simple_loss=0.3114, pruned_loss=0.1025, ctc_loss=0.2133, over 4797.00 frames. ], tot_loss[loss=0.3298, simple_loss=0.3238, pruned_loss=0.1196, ctc_loss=0.2417, over 966627.27 frames. ], batch size: 19, lr: 2.16e-02,
2024-10-08 04:29:07,500 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 04:29:31,006 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.50 vs. limit=8.921
2024-10-08 04:29:35,575 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=7842.0, ans=0.13240625
2024-10-08 04:29:42,612 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.23 vs. limit=13.384
2024-10-08 04:29:42,629 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.62 vs. limit=13.384
2024-10-08 04:29:43,491 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.14 vs. limit=13.384
2024-10-08 04:29:50,012 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=5.75 vs. limit=5.569066666666666
2024-10-08 04:29:54,073 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=7848.666666666667, ans=0.13209375
2024-10-08 04:30:06,225 INFO [train.py:1153] Epoch 4, batch 3000, loss[loss=0.3493, simple_loss=0.3477, pruned_loss=0.1237, ctc_loss=0.2583, over 4841.00 frames. ], tot_loss[loss=0.3305, simple_loss=0.3245, pruned_loss=0.1199, ctc_loss=0.2417, over 967160.60 frames. ], batch size: 21, lr: 2.16e-02,
2024-10-08 04:30:06,226 INFO [train.py:1176] Computing validation loss
2024-10-08 04:30:13,677 INFO [train.py:1185] Epoch 4, validation: loss=0.2112, simple_loss=0.2839, pruned_loss=0.04975, ctc_loss=0.09762, over 90464.00 frames.
2024-10-08 04:30:13,678 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 04:30:16,189 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=7852.0, ans=0.13193749999999999
2024-10-08 04:30:44,057 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=7858.666666666667, ans=0.22141333333333332
2024-10-08 04:30:54,551 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=7862.0, ans=0.13146875000000002
2024-10-08 04:31:03,017 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.57 vs. limit=10.4495
2024-10-08 04:31:04,543 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=15.41 vs. limit=13.399000000000001
2024-10-08 04:31:07,410 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.748e+01 1.480e+02 1.610e+02 1.720e+02 2.114e+02, threshold=3.220e+02, percent-clipped=0.0
2024-10-08 04:31:10,370 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.81 vs. limit=10.4495
2024-10-08 04:31:12,176 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=7865.333333333333, ans=0.0
2024-10-08 04:31:13,741 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=12.52 vs. limit=10.45075
2024-10-08 04:31:14,330 INFO [train.py:1153] Epoch 4, batch 3050, loss[loss=0.2923, simple_loss=0.3022, pruned_loss=0.1018, ctc_loss=0.1971, over 4749.00 frames. ], tot_loss[loss=0.3307, simple_loss=0.3246, pruned_loss=0.12, ctc_loss=0.242, over 966580.40 frames. ], batch size: 19, lr: 2.16e-02,
2024-10-08 04:31:18,123 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=7868.666666666667, ans=0.8286866666666667
2024-10-08 04:31:23,640 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.92 vs. limit=6.9671666666666665
2024-10-08 04:31:28,895 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=7872.0, ans=0.0
2024-10-08 04:31:31,281 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=7872.0, ans=0.131
2024-10-08 04:31:36,147 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=7872.0, ans=0.131
2024-10-08 04:31:51,173 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.20 vs. limit=6.969666666666667
2024-10-08 04:31:58,163 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=7878.666666666667, ans=0.1306875
2024-10-08 04:31:58,444 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.61 vs. limit=10.4545
2024-10-08 04:32:09,437 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.32 vs. limit=13.4115
2024-10-08 04:32:15,020 INFO [train.py:1153] Epoch 4, batch 3100, loss[loss=0.3466, simple_loss=0.316, pruned_loss=0.1366, ctc_loss=0.26, over 4816.00 frames. ], tot_loss[loss=0.3295, simple_loss=0.3239, pruned_loss=0.1194, ctc_loss=0.2411, over 966382.63 frames. ], batch size: 38, lr: 2.16e-02,
2024-10-08 04:32:23,880 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.min_positive, batch_count=7885.333333333333, ans=0.17114666666666667
2024-10-08 04:32:28,810 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=7888.666666666667, ans=0.13021875
2024-10-08 04:32:32,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=7888.666666666667, ans=0.03379722222222222
2024-10-08 04:32:43,597 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=7892.0, ans=0.025
2024-10-08 04:32:55,685 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=7895.333333333333, ans=0.0
2024-10-08 04:33:02,450 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.96 vs. limit=13.4215
2024-10-08 04:33:09,103 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.054e+02 1.425e+02 1.550e+02 1.734e+02 3.346e+02, threshold=3.100e+02, percent-clipped=1.0
2024-10-08 04:33:10,551 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7898.666666666667, ans=0.22101333333333334
2024-10-08 04:33:14,182 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=7898.666666666667, ans=0.12974999999999998
2024-10-08 04:33:16,459 INFO [train.py:1153] Epoch 4, batch 3150, loss[loss=0.4316, simple_loss=0.3803, pruned_loss=0.1738, ctc_loss=0.3382, over 4777.00 frames. ], tot_loss[loss=0.3286, simple_loss=0.3233, pruned_loss=0.119, ctc_loss=0.24, over 966720.36 frames. ], batch size: 40, lr: 2.15e-02,
2024-10-08 04:33:19,157 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=7902.0, ans=0.22098
2024-10-08 04:33:20,511 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 04:33:22,826 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=7902.0, ans=0.12959375
2024-10-08 04:33:25,884 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.42 vs. limit=4.1853
2024-10-08 04:33:36,367 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=7905.333333333333, ans=0.025
2024-10-08 04:34:17,748 INFO [train.py:1153] Epoch 4, batch 3200, loss[loss=0.3049, simple_loss=0.3138, pruned_loss=0.1031, ctc_loss=0.2243, over 4743.00 frames. ], tot_loss[loss=0.3281, simple_loss=0.3231, pruned_loss=0.1187, ctc_loss=0.2395, over 967160.04 frames. ], batch size: 20, lr: 2.15e-02,
2024-10-08 04:34:27,700 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=7918.666666666667, ans=0.1288125
2024-10-08 04:34:30,208 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=7922.0, ans=0.0
2024-10-08 04:34:40,507 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.53 vs. limit=13.4415
2024-10-08 04:34:48,505 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=7925.333333333333, ans=0.1285
2024-10-08 04:34:49,135 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.85 vs. limit=10.472
2024-10-08 04:35:01,896 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=7928.666666666667, ans=0.0
2024-10-08 04:35:09,122 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7932.0, ans=0.22068
2024-10-08 04:35:11,507 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.143e+02 1.460e+02 1.601e+02 1.769e+02 2.358e+02, threshold=3.202e+02, percent-clipped=0.0
2024-10-08 04:35:18,798 INFO [train.py:1153] Epoch 4, batch 3250, loss[loss=0.3049, simple_loss=0.3086, pruned_loss=0.1076, ctc_loss=0.2153, over 4858.00 frames. ], tot_loss[loss=0.3293, simple_loss=0.3243, pruned_loss=0.119, ctc_loss=0.2405, over 967218.86 frames. ], batch size: 24, lr: 2.15e-02,
2024-10-08 04:35:32,624 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=7938.666666666667, ans=0.6221466666666666
2024-10-08 04:35:40,593 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.80 vs. limit=6.984666666666667
2024-10-08 04:36:05,055 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.64 vs. limit=7.178133333333333
2024-10-08 04:36:10,809 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.31 vs. limit=13.461500000000001
2024-10-08 04:36:13,571 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=12.61 vs. limit=13.461500000000001
2024-10-08 04:36:20,382 INFO [train.py:1153] Epoch 4, batch 3300, loss[loss=0.3513, simple_loss=0.3446, pruned_loss=0.1273, ctc_loss=0.2584, over 4862.00 frames. ], tot_loss[loss=0.3283, simple_loss=0.3244, pruned_loss=0.1182, ctc_loss=0.2395, over 967776.76 frames. ], batch size: 43, lr: 2.15e-02,
2024-10-08 04:36:20,537 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.min_positive, batch_count=7952.0, ans=0.17048
2024-10-08 04:36:29,178 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=7952.0, ans=0.07
2024-10-08 04:36:49,875 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=7958.666666666667, ans=0.03350555555555555
2024-10-08 04:37:03,950 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.41 vs. limit=10.48575
2024-10-08 04:37:08,257 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=7965.333333333333, ans=0.126625
2024-10-08 04:37:14,200 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.081e+02 1.473e+02 1.600e+02 1.876e+02 2.851e+02, threshold=3.200e+02, percent-clipped=0.0
2024-10-08 04:37:16,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=7965.333333333333, ans=0.126625
2024-10-08 04:37:18,008 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=7965.333333333333, ans=0.126625
2024-10-08 04:37:21,526 INFO [train.py:1153] Epoch 4, batch 3350, loss[loss=0.3399, simple_loss=0.3343, pruned_loss=0.1229, ctc_loss=0.2491, over 4796.00 frames. ], tot_loss[loss=0.3318, simple_loss=0.3257, pruned_loss=0.1204, ctc_loss=0.243, over 966947.08 frames. ], batch size: 40, lr: 2.15e-02,
2024-10-08 04:37:30,202 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=7968.666666666667, ans=0.12646875000000002
2024-10-08 04:37:46,176 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=7975.333333333333, ans=0.12615625000000003
2024-10-08 04:37:47,447 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=7975.333333333333, ans=0.12615625000000003
2024-10-08 04:37:49,093 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=13.60 vs. limit=13.4815
2024-10-08 04:37:59,569 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 04:38:06,960 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=7978.666666666667, ans=0.009135072463768115
2024-10-08 04:38:22,784 INFO [train.py:1153] Epoch 4, batch 3400, loss[loss=0.2197, simple_loss=0.2625, pruned_loss=0.06188, ctc_loss=0.1329, over 4959.00 frames. ], tot_loss[loss=0.3314, simple_loss=0.3251, pruned_loss=0.1203, ctc_loss=0.2425, over 966712.10 frames. ], batch size: 19, lr: 2.14e-02,
2024-10-08 04:38:28,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=7985.333333333333, ans=0.22014666666666666
2024-10-08 04:38:29,531 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=13.32 vs. limit=10.4945
2024-10-08 04:38:46,416 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.04 vs. limit=13.494
2024-10-08 04:38:46,480 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=9.37 vs. limit=10.497
2024-10-08 04:38:47,678 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.46 vs. limit=13.494
2024-10-08 04:38:58,499 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.99 vs. limit=8.997666666666667
2024-10-08 04:39:00,464 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=7995.333333333333, ans=0.03335277777777779
2024-10-08 04:39:05,201 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=7995.333333333333, ans=0.12521875
2024-10-08 04:39:14,972 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/checkpoint-24000.pt
2024-10-08 04:39:16,929 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.303e+01 1.427e+02 1.597e+02 1.805e+02 2.560e+02, threshold=3.193e+02, percent-clipped=0.0
2024-10-08 04:39:20,495 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten.whitening_limit, batch_count=7998.666666666667, ans=13.498999999999999
2024-10-08 04:39:20,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.09 vs. limit=4.1998
2024-10-08 04:39:23,564 INFO [train.py:1153] Epoch 4, batch 3450, loss[loss=0.3994, simple_loss=0.351, pruned_loss=0.1621, ctc_loss=0.3091, over 4855.00 frames. ], tot_loss[loss=0.3307, simple_loss=0.325, pruned_loss=0.1198, ctc_loss=0.242, over 967019.24 frames. ], batch size: 43, lr: 2.14e-02,
2024-10-08 04:39:39,238 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=8005.333333333333, ans=0.125
2024-10-08 04:39:45,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=8005.333333333333, ans=0.0
2024-10-08 04:39:53,405 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=8008.666666666667, ans=0.00912855072463768
2024-10-08 04:39:56,999 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=8008.666666666667, ans=0.21991333333333332
2024-10-08 04:40:09,320 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.59 vs. limit=13.509
2024-10-08 04:40:23,617 INFO [train.py:1153] Epoch 4, batch 3500, loss[loss=0.2945, simple_loss=0.3096, pruned_loss=0.09917, ctc_loss=0.2026, over 4883.00 frames. ], tot_loss[loss=0.3291, simple_loss=0.3237, pruned_loss=0.1191, ctc_loss=0.2407, over 967279.20 frames. ], batch size: 19, lr: 2.14e-02,
2024-10-08 04:40:48,949 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=17.28 vs. limit=13.519
2024-10-08 04:40:56,845 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=8025.333333333333, ans=0.125
2024-10-08 04:41:17,397 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.053e+02 1.402e+02 1.537e+02 1.748e+02 2.870e+02, threshold=3.074e+02, percent-clipped=0.0
2024-10-08 04:41:17,563 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=8032.0, ans=0.0332
2024-10-08 04:41:20,155 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8032.0, ans=0.21968
2024-10-08 04:41:24,822 INFO [train.py:1153] Epoch 4, batch 3550, loss[loss=0.4076, simple_loss=0.3753, pruned_loss=0.156, ctc_loss=0.32, over 4778.00 frames. ], tot_loss[loss=0.3279, simple_loss=0.3229, pruned_loss=0.1186, ctc_loss=0.2393, over 967251.70 frames. ], batch size: 29, lr: 2.14e-02,
2024-10-08 04:41:27,328 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=8035.333333333333, ans=0.03318611111111111
2024-10-08 04:41:31,115 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=8035.333333333333, ans=0.125
2024-10-08 04:41:34,818 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8035.333333333333, ans=0.21964666666666666
2024-10-08 04:41:35,324 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.90 vs. limit=7.0088333333333335
2024-10-08 04:41:43,923 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.41 vs. limit=10.5145
2024-10-08 04:41:45,812 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=8038.666666666667, ans=0.025
2024-10-08 04:41:52,005 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=8042.0, ans=0.125
2024-10-08 04:41:52,489 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.09 vs. limit=13.531500000000001
2024-10-08 04:42:13,849 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=8048.666666666667, ans=0.32073
2024-10-08 04:42:26,203 INFO [train.py:1153] Epoch 4, batch 3600, loss[loss=0.2893, simple_loss=0.304, pruned_loss=0.09604, ctc_loss=0.2063, over 4927.00 frames. ], tot_loss[loss=0.3281, simple_loss=0.3231, pruned_loss=0.1185, ctc_loss=0.2402, over 967421.45 frames. ], batch size: 20, lr: 2.14e-02,
2024-10-08 04:42:26,313 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=8052.0, ans=0.035
2024-10-08 04:42:29,363 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten.whitening_limit, batch_count=8052.0, ans=10.5195
2024-10-08 04:42:31,304 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=8052.0, ans=0.125
2024-10-08 04:42:36,123 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8052.0, ans=0.21948
2024-10-08 04:42:39,007 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten.whitening_limit, batch_count=8055.333333333333, ans=13.5415
2024-10-08 04:42:48,446 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=8055.333333333333, ans=0.033102777777777787
2024-10-08 04:42:52,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=8058.666666666667, ans=0.6179466666666666
2024-10-08 04:43:00,785 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8058.666666666667, ans=0.21941333333333332
2024-10-08 04:43:08,126 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=8062.0, ans=0.125
2024-10-08 04:43:20,152 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.018e+02 1.426e+02 1.633e+02 1.860e+02 2.657e+02, threshold=3.266e+02, percent-clipped=0.0
2024-10-08 04:43:27,412 INFO [train.py:1153] Epoch 4, batch 3650, loss[loss=0.3173, simple_loss=0.3196, pruned_loss=0.1128, ctc_loss=0.2232, over 4844.00 frames. ], tot_loss[loss=0.327, simple_loss=0.3226, pruned_loss=0.118, ctc_loss=0.2388, over 967860.66 frames. ], batch size: 31, lr: 2.13e-02,
2024-10-08 04:43:27,549 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=8068.666666666667, ans=0.125
2024-10-08 04:43:28,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=8068.666666666667, ans=0.125
2024-10-08 04:43:36,190 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.38 vs. limit=10.52575
2024-10-08 04:43:55,356 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=8075.333333333333, ans=0.6173633333333334
2024-10-08 04:44:28,333 INFO [train.py:1153] Epoch 4, batch 3700, loss[loss=0.3753, simple_loss=0.3435, pruned_loss=0.1442, ctc_loss=0.2972, over 4855.00 frames. ], tot_loss[loss=0.3259, simple_loss=0.3218, pruned_loss=0.1175, ctc_loss=0.2377, over 967172.94 frames. ], batch size: 24, lr: 2.13e-02,
2024-10-08 04:44:29,643 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=8085.333333333333, ans=0.125
2024-10-08 04:44:37,141 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 04:44:38,241 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=8085.333333333333, ans=0.8308533333333333
2024-10-08 04:45:01,587 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=8092.0, ans=0.21908
2024-10-08 04:45:05,302 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=8095.333333333333, ans=0.03293611111111111
2024-10-08 04:45:05,631 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.60 vs. limit=13.5715
2024-10-08 04:45:10,071 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=8095.333333333333, ans=0.125
2024-10-08 04:45:10,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=8095.333333333333, ans=0.125
2024-10-08 04:45:22,165 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.015e+01 1.411e+02 1.548e+02 1.700e+02 2.291e+02, threshold=3.096e+02, percent-clipped=0.0
2024-10-08 04:45:24,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=8098.666666666667, ans=0.125
2024-10-08 04:45:29,463 INFO [train.py:1153] Epoch 4, batch 3750, loss[loss=0.2696, simple_loss=0.2813, pruned_loss=0.09407, ctc_loss=0.1742, over 4959.00 frames. ], tot_loss[loss=0.3247, simple_loss=0.321, pruned_loss=0.1168, ctc_loss=0.2371, over 967701.72 frames. ], batch size: 19, lr: 2.13e-02,
2024-10-08 04:46:20,537 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=8115.333333333333, ans=0.125
2024-10-08 04:46:22,456 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=14.76 vs. limit=13.586500000000001
2024-10-08 04:46:28,190 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.34 vs. limit=10.54325
2024-10-08 04:46:30,049 INFO [train.py:1153] Epoch 4, batch 3800, loss[loss=0.3268, simple_loss=0.3235, pruned_loss=0.1166, ctc_loss=0.2425, over 4728.00 frames. ], tot_loss[loss=0.3239, simple_loss=0.3205, pruned_loss=0.1164, ctc_loss=0.2362, over 967518.54 frames. ], batch size: 26, lr: 2.13e-02,
2024-10-08 04:46:30,878 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.71 vs. limit=7.029666666666667
2024-10-08 04:46:35,099 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=8118.666666666667, ans=0.125
2024-10-08 04:46:35,100 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=8118.666666666667, ans=0.12490233333333334
2024-10-08 04:47:04,177 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=4.90 vs. limit=10.547
2024-10-08 04:47:21,136 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.61 vs. limit=13.599
2024-10-08 04:47:24,484 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.088e+02 1.452e+02 1.563e+02 1.751e+02 2.403e+02, threshold=3.126e+02, percent-clipped=0.0
2024-10-08 04:47:31,820 INFO [train.py:1153] Epoch 4, batch 3850, loss[loss=0.3857, simple_loss=0.3688, pruned_loss=0.1467, ctc_loss=0.2732, over 4806.00 frames. ], tot_loss[loss=0.3232, simple_loss=0.3203, pruned_loss=0.1159, ctc_loss=0.2354, over 967434.96 frames. ], batch size: 38, lr: 2.13e-02,
2024-10-08 04:47:35,532 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=8135.333333333333, ans=0.009101014492753624
2024-10-08 04:47:37,327 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.65 vs. limit=10.55075
2024-10-08 04:48:18,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=8145.333333333333, ans=0.125
2024-10-08 04:48:31,959 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.26 vs. limit=13.614
2024-10-08 04:48:32,483 INFO [train.py:1153] Epoch 4, batch 3900, loss[loss=0.2493, simple_loss=0.2751, pruned_loss=0.07828, ctc_loss=0.1672, over 4728.00 frames. ], tot_loss[loss=0.3243, simple_loss=0.3206, pruned_loss=0.1167, ctc_loss=0.2364, over 966924.61 frames. ], batch size: 26, lr: 2.12e-02,
2024-10-08 04:48:38,579 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=8152.0, ans=0.025
2024-10-08 04:49:13,623 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=8162.0, ans=0.009095217391304347
2024-10-08 04:49:23,548 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=8165.333333333333, ans=0.125
2024-10-08 04:49:25,684 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.015e+02 1.441e+02 1.573e+02 1.712e+02 2.772e+02, threshold=3.147e+02, percent-clipped=0.0
2024-10-08 04:49:26,565 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.09 vs. limit=7.041333333333333
2024-10-08 04:49:29,582 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=8165.333333333333, ans=0.6142133333333334
2024-10-08 04:49:32,914 INFO [train.py:1153] Epoch 4, batch 3950, loss[loss=0.3556, simple_loss=0.3388, pruned_loss=0.1326, ctc_loss=0.2679, over 4827.00 frames. ], tot_loss[loss=0.3224, simple_loss=0.3198, pruned_loss=0.1156, ctc_loss=0.2344, over 967008.77 frames. ], batch size: 36, lr: 2.12e-02,
2024-10-08 04:49:35,556 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=8168.666666666667, ans=0.125
2024-10-08 04:49:55,028 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=8172.0, ans=0.125
2024-10-08 04:50:12,193 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 04:50:12,758 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.29 vs. limit=4.2268
2024-10-08 04:50:14,692 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=8178.666666666667, ans=0.125
2024-10-08 04:50:25,095 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=10.20 vs. limit=10.568249999999999
2024-10-08 04:50:33,997 INFO [train.py:1153] Epoch 4, batch 4000, loss[loss=0.2727, simple_loss=0.2921, pruned_loss=0.08871, ctc_loss=0.1894, over 4815.00 frames. ], tot_loss[loss=0.3221, simple_loss=0.3196, pruned_loss=0.1155, ctc_loss=0.2338, over 966962.57 frames. ], batch size: 19, lr: 2.12e-02,
2024-10-08 04:50:59,736 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=8192.0, ans=0.61328
2024-10-08 04:51:12,713 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.10 vs. limit=7.0488333333333335
2024-10-08 04:51:27,790 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.088e+02 1.355e+02 1.541e+02 1.740e+02 2.534e+02, threshold=3.082e+02, percent-clipped=0.0
2024-10-08 04:51:27,996 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=8198.666666666666, ans=0.032505555555555565
2024-10-08 04:51:29,158 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=8198.666666666666, ans=0.6130466666666667
2024-10-08 04:51:29,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=8198.666666666666, ans=0.07
2024-10-08 04:51:34,996 INFO [train.py:1153] Epoch 4, batch 4050, loss[loss=0.3573, simple_loss=0.3432, pruned_loss=0.1326, ctc_loss=0.2651, over 4762.00 frames. ], tot_loss[loss=0.3227, simple_loss=0.32, pruned_loss=0.1159, ctc_loss=0.2342, over 967359.95 frames. ], batch size: 53, lr: 2.12e-02,
2024-10-08 04:52:03,927 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=8208.666666666666, ans=0.009085072463768116
2024-10-08 04:52:08,620 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=8208.666666666666, ans=0.125
2024-10-08 04:52:12,259 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=8212.0, ans=0.125
2024-10-08 04:52:20,322 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=8212.0, ans=0.125
2024-10-08 04:52:20,460 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=8212.0, ans=0.125
2024-10-08 04:52:26,892 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.65 vs. limit=10.58075
2024-10-08 04:52:27,647 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=8215.333333333334, ans=0.125
2024-10-08 04:52:32,437 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=8215.333333333334, ans=0.07
2024-10-08 04:52:33,696 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=8218.666666666666, ans=0.009082898550724638
2024-10-08 04:52:34,697 INFO [train.py:1153] Epoch 4, batch 4100, loss[loss=0.364, simple_loss=0.3386, pruned_loss=0.1381, ctc_loss=0.283, over 4863.00 frames. ], tot_loss[loss=0.3219, simple_loss=0.3193, pruned_loss=0.1156, ctc_loss=0.2334, over 966842.87 frames. ], batch size: 31, lr: 2.12e-02,
2024-10-08 04:52:37,818 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.92 vs. limit=9.109333333333332
2024-10-08 04:52:38,842 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.85 vs. limit=7.054666666666666
2024-10-08 04:52:49,258 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=8222.0, ans=0.04949747468305833
2024-10-08 04:52:51,017 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.21 vs. limit=9.111
2024-10-08 04:52:51,952 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.31 vs. limit=13.6665
2024-10-08 04:53:03,812 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=8225.333333333334, ans=0.125
2024-10-08 04:53:06,280 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=8225.333333333334, ans=0.025
2024-10-08 04:53:07,804 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten.whitening_limit, batch_count=8225.333333333334, ans=10.5845
2024-10-08 04:53:16,360 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.74 vs. limit=10.585749999999999
2024-10-08 04:53:28,010 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.060e+02 1.373e+02 1.519e+02 1.758e+02 3.088e+02, threshold=3.038e+02, percent-clipped=1.0
2024-10-08 04:53:35,461 INFO [train.py:1153] Epoch 4, batch 4150, loss[loss=0.3489, simple_loss=0.3416, pruned_loss=0.1281, ctc_loss=0.2496, over 4737.00 frames. ], tot_loss[loss=0.3224, simple_loss=0.3196, pruned_loss=0.1158, ctc_loss=0.2335, over 967007.94 frames. ], batch size: 20, lr: 2.11e-02,
2024-10-08 04:53:41,670 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=8235.333333333334, ans=0.125
2024-10-08 04:53:47,764 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=8238.666666666666, ans=0.6116466666666667
2024-10-08 04:53:57,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=8238.666666666666, ans=0.21761333333333333
2024-10-08 04:54:02,612 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=8242.0, ans=0.125
2024-10-08 04:54:36,669 INFO [train.py:1153] Epoch 4, batch 4200, loss[loss=0.3919, simple_loss=0.364, pruned_loss=0.1521, ctc_loss=0.2889, over 4845.00 frames. ], tot_loss[loss=0.3223, simple_loss=0.3202, pruned_loss=0.1157, ctc_loss=0.2329, over 967112.54 frames. ], batch size: 31, lr: 2.11e-02,
2024-10-08 04:54:42,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=8252.0, ans=0.025
2024-10-08 04:54:56,323 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=8255.333333333334, ans=0.04949747468305833
2024-10-08 04:54:57,612 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=8255.333333333334, ans=0.125
2024-10-08 04:55:28,246 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=8265.333333333334, ans=0.009072753623188407
2024-10-08 04:55:30,613 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.011e+02 1.377e+02 1.524e+02 1.709e+02 2.122e+02, threshold=3.048e+02, percent-clipped=0.0
2024-10-08 04:55:32,011 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=8265.333333333334, ans=0.025
2024-10-08 04:55:33,123 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=8265.333333333334, ans=0.125
2024-10-08 04:55:36,862 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=8268.666666666666, ans=0.21731333333333333
2024-10-08 04:55:37,946 INFO [train.py:1153] Epoch 4, batch 4250, loss[loss=0.2778, simple_loss=0.295, pruned_loss=0.09096, ctc_loss=0.1966, over 4751.00 frames. ], tot_loss[loss=0.3212, simple_loss=0.3196, pruned_loss=0.1149, ctc_loss=0.2324, over 966969.17 frames. ], batch size: 19, lr: 2.11e-02,
2024-10-08 04:55:42,273 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=12.00 vs. limit=10.60075
2024-10-08 04:55:54,448 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.18 vs. limit=10.602
2024-10-08 04:56:15,328 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.25 vs. limit=4.2418
2024-10-08 04:56:20,033 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=4.86 vs. limit=9.139333333333333
2024-10-08 04:56:21,121 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=11.03 vs. limit=10.6045
2024-10-08 04:56:29,469 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=8282.0, ans=13.711500000000001
2024-10-08 04:56:38,851 INFO [train.py:1153] Epoch 4, batch 4300, loss[loss=0.3365, simple_loss=0.3377, pruned_loss=0.1189, ctc_loss=0.244, over 4841.00 frames. ], tot_loss[loss=0.3211, simple_loss=0.3194, pruned_loss=0.1149, ctc_loss=0.2322, over 967331.43 frames. ], batch size: 21, lr: 2.11e-02,
2024-10-08 04:56:47,506 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=8285.333333333334, ans=0.0
2024-10-08 04:56:51,126 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer_ff3.min_abs, batch_count=8288.666666666666, ans=0.2
2024-10-08 04:57:06,974 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=8292.0, ans=0.025
2024-10-08 04:57:18,145 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.26 vs. limit=7.318133333333334
2024-10-08 04:57:28,880 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.62 vs. limit=13.724
2024-10-08 04:57:31,989 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.752e+01 1.431e+02 1.603e+02 1.767e+02 3.666e+02, threshold=3.206e+02, percent-clipped=1.0
2024-10-08 04:57:39,444 INFO [train.py:1153] Epoch 4, batch 4350, loss[loss=0.3161, simple_loss=0.3194, pruned_loss=0.1152, ctc_loss=0.2056, over 4841.00 frames. ], tot_loss[loss=0.3212, simple_loss=0.32, pruned_loss=0.1148, ctc_loss=0.2323, over 966255.84 frames. ], batch size: 21, lr: 2.11e-02,
2024-10-08 04:57:42,020 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=8302.0, ans=0.032075
2024-10-08 04:57:54,534 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.30 vs. limit=13.729000000000001
2024-10-08 04:58:06,743 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.02 vs. limit=9.154333333333334
2024-10-08 04:58:19,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=8312.0, ans=0.0
2024-10-08 04:58:20,909 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=8312.0, ans=0.035
2024-10-08 04:58:28,071 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=8315.333333333334, ans=0.6089633333333333
2024-10-08 04:58:36,276 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=8315.333333333334, ans=0.125
2024-10-08 04:58:39,721 INFO [train.py:1153] Epoch 4, batch 4400, loss[loss=0.2901, simple_loss=0.2897, pruned_loss=0.09923, ctc_loss=0.23, over 4755.00 frames. ], tot_loss[loss=0.3239, simple_loss=0.322, pruned_loss=0.116, ctc_loss=0.2344, over 965912.00 frames. ], batch size: 26, lr: 2.10e-02,
2024-10-08 04:58:39,926 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=8318.666666666666, ans=0.125
2024-10-08 04:58:49,269 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=8318.666666666666, ans=0.125
2024-10-08 04:59:02,582 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=8325.333333333334, ans=0.03197777777777778
2024-10-08 04:59:15,762 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=8328.666666666666, ans=0.125
2024-10-08 04:59:15,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=8328.666666666666, ans=0.2167133333333333
2024-10-08 04:59:16,257 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.42 vs. limit=10.623249999999999
2024-10-08 04:59:32,904 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.181e+02 1.438e+02 1.608e+02 1.814e+02 2.831e+02, threshold=3.216e+02, percent-clipped=0.0
2024-10-08 04:59:34,329 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=8332.0, ans=0.125
2024-10-08 04:59:40,165 INFO [train.py:1153] Epoch 4, batch 4450, loss[loss=0.2864, simple_loss=0.3022, pruned_loss=0.09733, ctc_loss=0.1899, over 4883.00 frames. ], tot_loss[loss=0.3251, simple_loss=0.3224, pruned_loss=0.1168, ctc_loss=0.2355, over 966257.97 frames. ], batch size: 19, lr: 2.10e-02,
2024-10-08 04:59:40,275 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=8335.333333333334, ans=0.025
2024-10-08 05:00:10,448 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=8342.0, ans=0.00905608695652174
2024-10-08 05:00:15,368 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8345.333333333334, ans=0.21654666666666667
2024-10-08 05:00:19,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=8345.333333333334, ans=0.07
2024-10-08 05:00:25,772 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.24 vs. limit=4.2518
2024-10-08 05:00:33,728 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=8348.666666666666, ans=0.025
2024-10-08 05:00:41,037 INFO [train.py:1153] Epoch 4, batch 4500, loss[loss=0.3549, simple_loss=0.3361, pruned_loss=0.1348, ctc_loss=0.2602, over 4864.00 frames. ], tot_loss[loss=0.324, simple_loss=0.3215, pruned_loss=0.1162, ctc_loss=0.235, over 966250.51 frames. ], batch size: 28, lr: 2.10e-02,
2024-10-08 05:00:41,850 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.40 vs. limit=10.632
2024-10-08 05:00:51,296 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.07 vs. limit=13.764
2024-10-08 05:00:56,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=8355.333333333334, ans=0.125
2024-10-08 05:01:07,207 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.11 vs. limit=4.2538
2024-10-08 05:01:10,439 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=8358.666666666666, ans=0.6074466666666667
2024-10-08 05:01:34,281 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.061e+02 1.416e+02 1.608e+02 1.729e+02 2.130e+02, threshold=3.217e+02, percent-clipped=0.0
2024-10-08 05:01:41,490 INFO [train.py:1153] Epoch 4, batch 4550, loss[loss=0.3677, simple_loss=0.3494, pruned_loss=0.1388, ctc_loss=0.2712, over 4866.00 frames. ], tot_loss[loss=0.3233, simple_loss=0.3214, pruned_loss=0.1158, ctc_loss=0.2341, over 966090.00 frames. ], batch size: 20, lr: 2.10e-02,
2024-10-08 05:01:41,655 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=8368.666666666666, ans=0.125
2024-10-08 05:02:14,606 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=8375.333333333334, ans=0.03176944444444445
2024-10-08 05:02:19,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=8378.666666666666, ans=0.031755555555555565
2024-10-08 05:02:42,443 INFO [train.py:1153] Epoch 4, batch 4600, loss[loss=0.3564, simple_loss=0.3483, pruned_loss=0.1293, ctc_loss=0.2647, over 4764.00 frames. ], tot_loss[loss=0.3221, simple_loss=0.3207, pruned_loss=0.1152, ctc_loss=0.2327, over 966420.07 frames. ], batch size: 45, lr: 2.10e-02,
2024-10-08 05:02:43,938 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=8385.333333333334, ans=0.125
2024-10-08 05:02:53,737 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=8388.666666666666, ans=0.125
2024-10-08 05:02:57,280 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=8388.666666666666, ans=0.21611333333333332
2024-10-08 05:03:04,538 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=8388.666666666666, ans=0.125
2024-10-08 05:03:14,262 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8392.0, ans=0.21608
2024-10-08 05:03:18,408 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten.whitening_limit, batch_count=8395.333333333334, ans=13.796500000000002
2024-10-08 05:03:18,573 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.94 vs. limit=7.098833333333333
2024-10-08 05:03:21,637 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=8395.333333333334, ans=0.6061633333333334
2024-10-08 05:03:25,242 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=8395.333333333334, ans=0.125
2024-10-08 05:03:28,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=8395.333333333334, ans=0.125
2024-10-08 05:03:36,040 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.017e+02 1.404e+02 1.570e+02 1.743e+02 2.719e+02, threshold=3.141e+02, percent-clipped=0.0
2024-10-08 05:03:37,376 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=8398.666666666666, ans=0.6060466666666667
2024-10-08 05:03:43,144 INFO [train.py:1153] Epoch 4, batch 4650, loss[loss=0.3171, simple_loss=0.3226, pruned_loss=0.1071, ctc_loss=0.2434, over 4837.00 frames. ], tot_loss[loss=0.3221, simple_loss=0.3208, pruned_loss=0.1151, ctc_loss=0.233, over 965661.48 frames. ], batch size: 36, lr: 2.09e-02,
2024-10-08 05:03:43,332 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=8402.0, ans=0.125
2024-10-08 05:03:48,143 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=8402.0, ans=0.125
2024-10-08 05:03:58,987 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=8405.333333333334, ans=0.125
2024-10-08 05:04:06,444 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=8408.666666666666, ans=0.025
2024-10-08 05:04:09,181 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=18.89 vs. limit=13.8065
2024-10-08 05:04:12,829 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=8408.666666666666, ans=0.00904159420289855
2024-10-08 05:04:14,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer1.prob, batch_count=8408.666666666666, ans=0.125
2024-10-08 05:04:18,092 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module1.whiten, num_groups=1, num_channels=192, metric=4.06 vs. limit=10.65325
2024-10-08 05:04:28,239 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=8412.0, ans=0.09899494936611666
2024-10-08 05:04:29,375 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=8412.0, ans=0.009040869565217392
2024-10-08 05:04:30,637 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=8415.333333333334, ans=0.125
2024-10-08 05:04:35,843 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.94 vs. limit=13.8115
2024-10-08 05:04:43,564 INFO [train.py:1153] Epoch 4, batch 4700, loss[loss=0.2689, simple_loss=0.2955, pruned_loss=0.08626, ctc_loss=0.1742, over 4940.00 frames. ], tot_loss[loss=0.3199, simple_loss=0.3192, pruned_loss=0.114, ctc_loss=0.2315, over 965599.57 frames. ], batch size: 19, lr: 2.09e-02,
2024-10-08 05:04:53,575 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=8418.666666666666, ans=0.125
2024-10-08 05:04:57,130 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=8422.0, ans=0.05
2024-10-08 05:04:57,137 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=8422.0, ans=0.125
2024-10-08 05:05:23,467 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=8428.666666666666, ans=0.009037246376811595
2024-10-08 05:05:29,638 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=8428.666666666666, ans=0.6049966666666667
2024-10-08 05:05:36,715 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.076e+02 1.395e+02 1.512e+02 1.728e+02 4.652e+02, threshold=3.025e+02, percent-clipped=1.0
2024-10-08 05:05:38,120 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=8432.0, ans=0.05
2024-10-08 05:05:43,832 INFO [train.py:1153] Epoch 4, batch 4750, loss[loss=0.3291, simple_loss=0.3258, pruned_loss=0.1177, ctc_loss=0.2425, over 4734.00 frames. ], tot_loss[loss=0.3191, simple_loss=0.3185, pruned_loss=0.1137, ctc_loss=0.2307, over 965540.25 frames. ], batch size: 45, lr: 2.09e-02,
2024-10-08 05:05:48,725 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=8435.333333333334, ans=0.21564666666666665
2024-10-08 05:05:51,276 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=8435.333333333334, ans=0.6047633333333333
2024-10-08 05:05:53,078 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.01 vs. limit=13.826500000000001
2024-10-08 05:05:54,152 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.93 vs. limit=10.663250000000001
2024-10-08 05:06:16,762 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=8442.0, ans=0.025
2024-10-08 05:06:44,603 INFO [train.py:1153] Epoch 4, batch 4800, loss[loss=0.3511, simple_loss=0.344, pruned_loss=0.1261, ctc_loss=0.2647, over 4876.00 frames. ], tot_loss[loss=0.3209, simple_loss=0.3197, pruned_loss=0.1146, ctc_loss=0.2318, over 965952.03 frames. ], batch size: 22, lr: 2.09e-02,
2024-10-08 05:06:44,707 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=8452.0, ans=0.0
2024-10-08 05:06:55,648 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=8455.333333333334, ans=0.125
2024-10-08 05:07:28,464 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=8462.0, ans=0.125
2024-10-08 05:07:38,185 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.096e+02 1.430e+02 1.554e+02 1.779e+02 3.545e+02, threshold=3.108e+02, percent-clipped=1.0
2024-10-08 05:07:45,436 INFO [train.py:1153] Epoch 4, batch 4850, loss[loss=0.4234, simple_loss=0.3779, pruned_loss=0.1678, ctc_loss=0.333, over 4861.00 frames. ], tot_loss[loss=0.3214, simple_loss=0.3203, pruned_loss=0.1149, ctc_loss=0.2318, over 966767.74 frames. ], batch size: 28, lr: 2.09e-02,
2024-10-08 05:07:54,108 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8468.666666666666, ans=0.21531333333333333
2024-10-08 05:07:55,359 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=8468.666666666666, ans=0.0
2024-10-08 05:07:55,449 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=8468.666666666666, ans=0.6035966666666668
2024-10-08 05:08:02,679 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=8472.0, ans=0.125
2024-10-08 05:08:18,998 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.17 vs. limit=13.8565
2024-10-08 05:08:36,786 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=8482.0, ans=0.031325000000000006
2024-10-08 05:08:46,396 INFO [train.py:1153] Epoch 4, batch 4900, loss[loss=0.3465, simple_loss=0.3364, pruned_loss=0.1294, ctc_loss=0.2442, over 4836.00 frames. ], tot_loss[loss=0.3197, simple_loss=0.3191, pruned_loss=0.1141, ctc_loss=0.2303, over 967276.54 frames. ], batch size: 21, lr: 2.08e-02,
2024-10-08 05:08:52,509 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=8485.333333333334, ans=0.6030133333333334
2024-10-08 05:09:07,852 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=15.24 vs. limit=13.866499999999998
2024-10-08 05:09:20,401 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=8492.0, ans=0.125
2024-10-08 05:09:25,685 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.07 vs. limit=13.871500000000001
2024-10-08 05:09:37,795 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=8498.666666666666, ans=0.125
2024-10-08 05:09:39,754 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.916e+01 1.416e+02 1.596e+02 1.756e+02 2.483e+02, threshold=3.192e+02, percent-clipped=0.0
2024-10-08 05:09:45,815 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=8502.0, ans=0.031241666666666668
2024-10-08 05:09:46,899 INFO [train.py:1153] Epoch 4, batch 4950, loss[loss=0.3467, simple_loss=0.3329, pruned_loss=0.1284, ctc_loss=0.2589, over 4770.00 frames. ], tot_loss[loss=0.3229, simple_loss=0.3208, pruned_loss=0.1159, ctc_loss=0.2332, over 966984.76 frames. ], batch size: 53, lr: 2.08e-02,
2024-10-08 05:10:02,665 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=8505.333333333334, ans=0.125
2024-10-08 05:10:29,318 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=8512.0, ans=0.031200000000000002
2024-10-08 05:10:37,783 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=8515.333333333334, ans=0.125
2024-10-08 05:10:41,516 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=8515.333333333334, ans=0.21484666666666666
2024-10-08 05:10:47,494 INFO [train.py:1153] Epoch 4, batch 5000, loss[loss=0.3208, simple_loss=0.3307, pruned_loss=0.1127, ctc_loss=0.2138, over 4748.00 frames. ], tot_loss[loss=0.3202, simple_loss=0.3192, pruned_loss=0.1144, ctc_loss=0.2312, over 967819.16 frames. ], batch size: 29, lr: 2.08e-02,
2024-10-08 05:11:14,986 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=4.86 vs. limit=10.697000000000001
2024-10-08 05:11:38,055 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.15 vs. limit=13.899000000000001
2024-10-08 05:11:41,428 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.203e+01 1.396e+02 1.583e+02 1.747e+02 3.176e+02, threshold=3.165e+02, percent-clipped=0.0
2024-10-08 05:11:48,703 INFO [train.py:1153] Epoch 4, batch 5050, loss[loss=0.2967, simple_loss=0.3067, pruned_loss=0.1039, ctc_loss=0.1972, over 4852.00 frames. ], tot_loss[loss=0.3186, simple_loss=0.3177, pruned_loss=0.1138, ctc_loss=0.2299, over 968708.17 frames. ], batch size: 19, lr: 2.08e-02,
2024-10-08 05:11:53,705 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=8535.333333333334, ans=0.031102777777777778
2024-10-08 05:11:55,508 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.12 vs. limit=13.9015
2024-10-08 05:12:11,129 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.02 vs. limit=13.904
2024-10-08 05:12:32,768 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=5.724e-01
2024-10-08 05:12:33,843 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=8545.333333333334, ans=0.009011884057971015
2024-10-08 05:12:38,731 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=8548.666666666666, ans=0.125
2024-10-08 05:12:49,608 INFO [train.py:1153] Epoch 4, batch 5100, loss[loss=0.2828, simple_loss=0.3075, pruned_loss=0.09045, ctc_loss=0.1931, over 4816.00 frames. ], tot_loss[loss=0.3229, simple_loss=0.3202, pruned_loss=0.1159, ctc_loss=0.2345, over 967981.24 frames. ], batch size: 19, lr: 2.08e-02,
2024-10-08 05:12:52,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.64 vs. limit=10.707
2024-10-08 05:12:55,864 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=8552.0, ans=0.009010434782608696
2024-10-08 05:13:04,909 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.81 vs. limit=7.138833333333333
2024-10-08 05:13:14,365 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=8558.666666666666, ans=0.125
2024-10-08 05:13:25,708 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.38 vs. limit=10.71075
2024-10-08 05:13:42,324 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=8565.333333333334, ans=0.125
2024-10-08 05:13:43,047 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=13.21 vs. limit=13.924
2024-10-08 05:13:43,307 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.077e+02 1.398e+02 1.575e+02 1.752e+02 2.249e+02, threshold=3.151e+02, percent-clipped=1.0
2024-10-08 05:13:48,747 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.88 vs. limit=10.712
2024-10-08 05:13:50,721 INFO [train.py:1153] Epoch 4, batch 5150, loss[loss=0.29, simple_loss=0.2831, pruned_loss=0.1044, ctc_loss=0.2205, over 4813.00 frames. ], tot_loss[loss=0.3221, simple_loss=0.3199, pruned_loss=0.1154, ctc_loss=0.2338, over 968103.64 frames. ], batch size: 36, lr: 2.08e-02,
2024-10-08 05:14:03,838 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.46 vs. limit=4.2858
2024-10-08 05:14:09,802 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=2.96 vs. limit=10.714500000000001
2024-10-08 05:14:15,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=6.14 vs. limit=7.430133333333334
2024-10-08 05:14:31,975 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten.whitening_limit, batch_count=8578.666666666666, ans=10.716999999999999
2024-10-08 05:14:37,316 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=8578.666666666666, ans=0.025
2024-10-08 05:14:38,622 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=8582.0, ans=0.5996300000000001
2024-10-08 05:14:41,599 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=9.77 vs. limit=10.71825
2024-10-08 05:14:43,006 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.48 vs. limit=10.71825
2024-10-08 05:14:44,817 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=8582.0, ans=0.025
2024-10-08 05:14:49,130 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=8582.0, ans=0.0
2024-10-08 05:14:51,961 INFO [train.py:1153] Epoch 4, batch 5200, loss[loss=0.337, simple_loss=0.3308, pruned_loss=0.1207, ctc_loss=0.255, over 4795.00 frames. ], tot_loss[loss=0.3214, simple_loss=0.3194, pruned_loss=0.1151, ctc_loss=0.2329, over 967692.32 frames. ], batch size: 29, lr: 2.07e-02,
2024-10-08 05:14:52,593 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=13.40 vs. limit=13.939
2024-10-08 05:14:54,380 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=8585.333333333334, ans=0.21414666666666665
2024-10-08 05:15:08,846 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=8588.666666666666, ans=0.125
2024-10-08 05:15:21,059 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=8592.0, ans=0.04949747468305833
2024-10-08 05:15:27,529 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.04 vs. limit=10.72325
2024-10-08 05:15:34,333 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=8595.333333333334, ans=0.09899494936611666
2024-10-08 05:15:45,293 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.536e+01 1.327e+02 1.538e+02 1.709e+02 2.185e+02, threshold=3.077e+02, percent-clipped=0.0
2024-10-08 05:15:52,624 INFO [train.py:1153] Epoch 4, batch 5250, loss[loss=0.2888, simple_loss=0.3064, pruned_loss=0.09685, ctc_loss=0.1935, over 4859.00 frames. ], tot_loss[loss=0.3188, simple_loss=0.3178, pruned_loss=0.1138, ctc_loss=0.2304, over 967721.45 frames. ], batch size: 20, lr: 2.07e-02,
2024-10-08 05:16:01,135 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=8602.0, ans=0.125
2024-10-08 05:16:04,916 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=8605.333333333334, ans=0.09899494936611666
2024-10-08 05:16:13,019 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=16.71 vs. limit=13.954
2024-10-08 05:16:13,577 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=8605.333333333334, ans=0.025
2024-10-08 05:16:30,135 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.45 vs. limit=7.4448
2024-10-08 05:16:30,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=8612.0, ans=0.030783333333333333
2024-10-08 05:16:36,666 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8612.0, ans=0.21388000000000001
2024-10-08 05:16:41,671 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=8615.333333333334, ans=0.05
2024-10-08 05:16:53,000 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.93 vs. limit=13.963999999999999
2024-10-08 05:16:53,624 INFO [train.py:1153] Epoch 4, batch 5300, loss[loss=0.3521, simple_loss=0.3407, pruned_loss=0.1302, ctc_loss=0.2577, over 4838.00 frames. ], tot_loss[loss=0.3174, simple_loss=0.3168, pruned_loss=0.1131, ctc_loss=0.2293, over 967801.02 frames. ], batch size: 38, lr: 2.07e-02,
2024-10-08 05:17:05,433 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.44 vs. limit=10.73325
2024-10-08 05:17:37,294 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=8628.666666666666, ans=0.125
2024-10-08 05:17:37,524 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=8628.666666666666, ans=0.025
2024-10-08 05:17:46,089 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=8632.0, ans=0.125
2024-10-08 05:17:47,051 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.043e+02 1.391e+02 1.519e+02 1.670e+02 2.333e+02, threshold=3.038e+02, percent-clipped=0.0
2024-10-08 05:17:48,508 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=8632.0, ans=0.0307
2024-10-08 05:17:54,441 INFO [train.py:1153] Epoch 4, batch 5350, loss[loss=0.3242, simple_loss=0.3273, pruned_loss=0.113, ctc_loss=0.2374, over 4978.00 frames. ], tot_loss[loss=0.3182, simple_loss=0.3175, pruned_loss=0.1135, ctc_loss=0.2301, over 967204.21 frames. ], batch size: 19, lr: 2.07e-02,
2024-10-08 05:18:09,675 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.66 vs. limit=10.7395
2024-10-08 05:18:41,067 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=8645.333333333334, ans=0.008990144927536232
2024-10-08 05:18:55,350 INFO [train.py:1153] Epoch 4, batch 5400, loss[loss=0.3777, simple_loss=0.3588, pruned_loss=0.1421, ctc_loss=0.2811, over 4784.00 frames. ], tot_loss[loss=0.3219, simple_loss=0.3197, pruned_loss=0.1154, ctc_loss=0.2333, over 966471.34 frames. ], batch size: 49, lr: 2.07e-02,
2024-10-08 05:19:04,109 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=8652.0, ans=0.21348
2024-10-08 05:19:18,036 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.05 vs. limit=13.991500000000002
2024-10-08 05:19:23,679 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=8658.666666666666, ans=0.5969466666666667
2024-10-08 05:19:41,964 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=8662.0, ans=0.008986521739130435
2024-10-08 05:19:49,414 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.763e+01 1.414e+02 1.573e+02 1.762e+02 3.076e+02, threshold=3.146e+02, percent-clipped=1.0
2024-10-08 05:19:55,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=8668.666666666666, ans=0.125
2024-10-08 05:19:56,643 INFO [train.py:1153] Epoch 4, batch 5450, loss[loss=0.2717, simple_loss=0.2927, pruned_loss=0.08794, ctc_loss=0.1871, over 4940.00 frames. ], tot_loss[loss=0.3184, simple_loss=0.318, pruned_loss=0.1136, ctc_loss=0.2293, over 967068.74 frames. ], batch size: 19, lr: 2.06e-02,
2024-10-08 05:20:01,816 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=8668.666666666666, ans=0.125
2024-10-08 05:20:04,689 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=11.47 vs. limit=10.75075
2024-10-08 05:20:09,734 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.96 vs. limit=9.336
2024-10-08 05:20:13,981 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=8672.0, ans=0.125
2024-10-08 05:20:15,860 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.03 vs. limit=4.3008
2024-10-08 05:20:38,714 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=8678.666666666666, ans=0.5962466666666668
2024-10-08 05:20:57,309 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=8685.333333333334, ans=0.030477777777777777
2024-10-08 05:20:58,275 INFO [train.py:1153] Epoch 4, batch 5500, loss[loss=0.3594, simple_loss=0.335, pruned_loss=0.1377, ctc_loss=0.2713, over 4803.00 frames. ], tot_loss[loss=0.319, simple_loss=0.3183, pruned_loss=0.1139, ctc_loss=0.2296, over 967428.19 frames. ], batch size: 49, lr: 2.06e-02,
2024-10-08 05:21:02,483 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.whiten.whitening_limit, batch_count=8685.333333333334, ans=7.4741333333333335
2024-10-08 05:21:05,725 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8685.333333333334, ans=0.21314666666666665
2024-10-08 05:21:11,436 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=9.02 vs. limit=10.75825
2024-10-08 05:21:11,937 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.max_abs, batch_count=8688.666666666666, ans=10.0
2024-10-08 05:21:22,904 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=8692.0, ans=0.21308
2024-10-08 05:21:25,726 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=4.18 vs. limit=7.173
2024-10-08 05:21:51,865 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.053e+02 1.369e+02 1.522e+02 1.710e+02 2.354e+02, threshold=3.044e+02, percent-clipped=0.0
2024-10-08 05:21:52,063 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=8698.666666666666, ans=0.125
2024-10-08 05:21:52,431 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.06 vs. limit=10.762
2024-10-08 05:21:55,745 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=8698.666666666666, ans=0.030422222222222225
2024-10-08 05:21:55,754 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=8698.666666666666, ans=0.5955466666666667
2024-10-08 05:21:59,256 INFO [train.py:1153] Epoch 4, batch 5550, loss[loss=0.2683, simple_loss=0.2853, pruned_loss=0.08811, ctc_loss=0.188, over 4803.00 frames. ], tot_loss[loss=0.318, simple_loss=0.3182, pruned_loss=0.1131, ctc_loss=0.2291, over 967271.40 frames. ], batch size: 19, lr: 2.06e-02,
2024-10-08 05:22:01,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=8702.0, ans=0.030408333333333336
2024-10-08 05:22:18,877 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=8705.333333333334, ans=0.5953133333333334
2024-10-08 05:22:34,658 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=8712.0, ans=0.59508
2024-10-08 05:22:49,128 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=8715.333333333334, ans=0.5949633333333333
2024-10-08 05:22:55,791 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.19 vs. limit=14.0365
2024-10-08 05:22:59,804 INFO [train.py:1153] Epoch 4, batch 5600, loss[loss=0.3218, simple_loss=0.3092, pruned_loss=0.1216, ctc_loss=0.2281, over 4848.00 frames. ], tot_loss[loss=0.3178, simple_loss=0.3175, pruned_loss=0.1131, ctc_loss=0.2295, over 967235.20 frames. ], batch size: 28, lr: 2.06e-02,
2024-10-08 05:23:07,372 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=8718.666666666666, ans=0.030338888888888894
2024-10-08 05:23:28,318 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=8725.333333333334, ans=0.008972753623188405
2024-10-08 05:23:32,042 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=8725.333333333334, ans=0.125
2024-10-08 05:23:53,839 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.007e+02 1.321e+02 1.517e+02 1.700e+02 2.539e+02, threshold=3.034e+02, percent-clipped=0.0
2024-10-08 05:23:59,997 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=8735.333333333334, ans=0.008970579710144928
2024-10-08 05:24:01,035 INFO [train.py:1153] Epoch 4, batch 5650, loss[loss=0.3291, simple_loss=0.3206, pruned_loss=0.121, ctc_loss=0.2389, over 4753.00 frames. ], tot_loss[loss=0.3153, simple_loss=0.316, pruned_loss=0.1119, ctc_loss=0.227, over 967211.39 frames. ], batch size: 45, lr: 2.06e-02,
2024-10-08 05:24:01,659 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=12.36 vs. limit=10.77575
2024-10-08 05:24:25,394 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=8742.0, ans=0.125
2024-10-08 05:24:27,055 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.30 vs. limit=10.77825
2024-10-08 05:24:37,435 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=8745.333333333334, ans=0.125
2024-10-08 05:24:44,792 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=8745.333333333334, ans=0.030227777777777777
2024-10-08 05:24:45,164 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.63 vs. limit=14.059000000000001
2024-10-08 05:25:01,675 INFO [train.py:1153] Epoch 4, batch 5700, loss[loss=0.3417, simple_loss=0.3357, pruned_loss=0.1234, ctc_loss=0.2523, over 4888.00 frames. ], tot_loss[loss=0.3156, simple_loss=0.3163, pruned_loss=0.112, ctc_loss=0.2272, over 966561.90 frames. ], batch size: 22, lr: 2.05e-02,
2024-10-08 05:25:26,858 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.43 vs. limit=10.7845
2024-10-08 05:25:37,214 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=8762.0, ans=0.0
2024-10-08 05:25:53,098 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=8765.333333333334, ans=0.125
2024-10-08 05:25:55,226 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.133e+02 1.398e+02 1.536e+02 1.682e+02 2.374e+02, threshold=3.071e+02, percent-clipped=0.0
2024-10-08 05:26:02,585 INFO [train.py:1153] Epoch 4, batch 5750, loss[loss=0.3483, simple_loss=0.3372, pruned_loss=0.1275, ctc_loss=0.2611, over 4826.00 frames. ], tot_loss[loss=0.3177, simple_loss=0.3176, pruned_loss=0.1131, ctc_loss=0.2291, over 966951.82 frames. ], batch size: 43, lr: 2.05e-02,
2024-10-08 05:26:07,631 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=8768.666666666666, ans=0.125
2024-10-08 05:26:11,590 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.38 vs. limit=10.78825
2024-10-08 05:26:12,507 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=8768.666666666666, ans=0.07
2024-10-08 05:26:17,869 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.07 vs. limit=14.079
2024-10-08 05:26:21,538 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=26.27 vs. limit=9.386
2024-10-08 05:26:27,065 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=8775.333333333334, ans=0.16224666666666665
2024-10-08 05:26:32,019 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.prob, batch_count=8775.333333333334, ans=0.125
2024-10-08 05:27:00,279 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=512, metric=3.54 vs. limit=10.79325
2024-10-08 05:27:03,257 INFO [train.py:1153] Epoch 4, batch 5800, loss[loss=0.3291, simple_loss=0.32, pruned_loss=0.1224, ctc_loss=0.2336, over 4834.00 frames. ], tot_loss[loss=0.3182, simple_loss=0.3178, pruned_loss=0.1133, ctc_loss=0.2298, over 966260.21 frames. ], batch size: 43, lr: 2.05e-02,
2024-10-08 05:27:03,463 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=8785.333333333334, ans=0.025
2024-10-08 05:27:09,493 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=8785.333333333334, ans=0.125
2024-10-08 05:27:26,912 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.99 vs. limit=10.797
2024-10-08 05:27:32,565 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=8792.0, ans=0.030033333333333335
2024-10-08 05:27:33,810 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=8792.0, ans=0.030033333333333335
2024-10-08 05:27:38,659 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=8795.333333333334, ans=0.030019444444444442
2024-10-08 05:27:50,948 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=8798.666666666666, ans=0.125
2024-10-08 05:27:51,014 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=8798.666666666666, ans=0.125
2024-10-08 05:27:57,116 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.030e+02 1.433e+02 1.556e+02 1.681e+02 2.371e+02, threshold=3.112e+02, percent-clipped=0.0
2024-10-08 05:28:04,306 INFO [train.py:1153] Epoch 4, batch 5850, loss[loss=0.3409, simple_loss=0.3136, pruned_loss=0.1304, ctc_loss=0.2683, over 4765.00 frames. ], tot_loss[loss=0.3176, simple_loss=0.3178, pruned_loss=0.1129, ctc_loss=0.2292, over 966586.09 frames. ], batch size: 45, lr: 2.05e-02,
2024-10-08 05:28:17,957 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=8805.333333333334, ans=0.025
2024-10-08 05:28:32,525 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=8808.666666666666, ans=0.125
2024-10-08 05:28:32,880 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.09 vs. limit=7.202166666666667
2024-10-08 05:28:57,157 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=8815.333333333334, ans=0.025
2024-10-08 05:29:05,621 INFO [train.py:1153] Epoch 4, batch 5900, loss[loss=0.3282, simple_loss=0.3106, pruned_loss=0.1234, ctc_loss=0.2475, over 4831.00 frames. ], tot_loss[loss=0.3161, simple_loss=0.3168, pruned_loss=0.1121, ctc_loss=0.2278, over 966724.47 frames. ], batch size: 34, lr: 2.05e-02,
2024-10-08 05:29:35,984 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8825.333333333334, ans=0.21174666666666664
2024-10-08 05:29:40,088 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.33 vs. limit=14.119
2024-10-08 05:29:40,820 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=8828.666666666666, ans=0.21171333333333334
2024-10-08 05:29:41,992 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=8828.666666666666, ans=0.125
2024-10-08 05:29:42,500 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=15.32 vs. limit=10.81075
2024-10-08 05:29:45,692 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=8828.666666666666, ans=0.125
2024-10-08 05:29:55,524 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=8832.0, ans=0.029866666666666666
2024-10-08 05:29:59,027 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.002e+02 1.409e+02 1.606e+02 1.799e+02 3.127e+02, threshold=3.212e+02, percent-clipped=1.0
2024-10-08 05:30:06,333 INFO [train.py:1153] Epoch 4, batch 5950, loss[loss=0.3177, simple_loss=0.3066, pruned_loss=0.1134, ctc_loss=0.2552, over 4807.00 frames. ], tot_loss[loss=0.3151, simple_loss=0.3164, pruned_loss=0.1115, ctc_loss=0.2268, over 966152.48 frames. ], batch size: 34, lr: 2.05e-02,
2024-10-08 05:30:06,474 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=8835.333333333334, ans=0.125
2024-10-08 05:30:22,225 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=8838.666666666666, ans=0.05
2024-10-08 05:30:31,163 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.02 vs. limit=14.131499999999999
2024-10-08 05:30:43,609 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.05 vs. limit=10.817
2024-10-08 05:30:50,240 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=8845.333333333334, ans=0.02981111111111111
2024-10-08 05:30:50,352 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=8845.333333333334, ans=0.125
2024-10-08 05:30:51,489 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=8845.333333333334, ans=0.02981111111111111
2024-10-08 05:30:53,211 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.09 vs. limit=10.817
2024-10-08 05:31:06,148 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=8852.0, ans=0.125
2024-10-08 05:31:07,218 INFO [train.py:1153] Epoch 4, batch 6000, loss[loss=0.3478, simple_loss=0.3403, pruned_loss=0.1242, ctc_loss=0.2674, over 4808.00 frames. ], tot_loss[loss=0.3154, simple_loss=0.3163, pruned_loss=0.1118, ctc_loss=0.227, over 966757.28 frames. ], batch size: 49, lr: 2.04e-02,
2024-10-08 05:31:07,218 INFO [train.py:1176] Computing validation loss
2024-10-08 05:31:14,647 INFO [train.py:1185] Epoch 4, validation: loss=0.2027, simple_loss=0.2781, pruned_loss=0.04552, ctc_loss=0.09071, over 90464.00 frames.
2024-10-08 05:31:14,648 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 05:31:37,758 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 05:31:43,799 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=8858.666666666666, ans=0.025
2024-10-08 05:31:46,201 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8858.666666666666, ans=0.21141333333333334
2024-10-08 05:31:50,478 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.41 vs. limit=9.431000000000001
2024-10-08 05:32:03,684 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=10.12 vs. limit=10.8245
2024-10-08 05:32:04,470 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=8865.333333333334, ans=0.025
2024-10-08 05:32:08,282 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.079e+02 1.348e+02 1.526e+02 1.688e+02 3.466e+02, threshold=3.051e+02, percent-clipped=1.0
2024-10-08 05:32:08,499 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8865.333333333334, ans=0.21134666666666665
2024-10-08 05:32:15,450 INFO [train.py:1153] Epoch 4, batch 6050, loss[loss=0.2807, simple_loss=0.3069, pruned_loss=0.08995, ctc_loss=0.1865, over 4814.00 frames. ], tot_loss[loss=0.3141, simple_loss=0.3159, pruned_loss=0.1111, ctc_loss=0.2255, over 966741.66 frames. ], batch size: 19, lr: 2.04e-02,
2024-10-08 05:32:18,052 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=8868.666666666666, ans=0.008941594202898551
2024-10-08 05:32:31,492 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=8872.0, ans=0.125
2024-10-08 05:32:52,082 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=8878.666666666666, ans=0.04949747468305833
2024-10-08 05:33:00,020 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.91 vs. limit=4.3318
2024-10-08 05:33:00,529 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=8878.666666666666, ans=0.33318000000000003
2024-10-08 05:33:00,948 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.59 vs. limit=10.8295
2024-10-08 05:33:10,728 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.53 vs. limit=7.5527999999999995
2024-10-08 05:33:16,184 INFO [train.py:1153] Epoch 4, batch 6100, loss[loss=0.3594, simple_loss=0.3432, pruned_loss=0.1368, ctc_loss=0.2549, over 4806.00 frames. ], tot_loss[loss=0.3122, simple_loss=0.3146, pruned_loss=0.1102, ctc_loss=0.2235, over 966206.70 frames. ], batch size: 34, lr: 2.04e-02,
2024-10-08 05:33:25,618 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=3.57 vs. limit=7.554133333333334
2024-10-08 05:33:43,152 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 05:34:09,714 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.029e+02 1.352e+02 1.503e+02 1.676e+02 2.563e+02, threshold=3.006e+02, percent-clipped=0.0
2024-10-08 05:34:16,000 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=8902.0, ans=0.0
2024-10-08 05:34:16,982 INFO [train.py:1153] Epoch 4, batch 6150, loss[loss=0.4112, simple_loss=0.3735, pruned_loss=0.1614, ctc_loss=0.3153, over 4838.00 frames. ], tot_loss[loss=0.3132, simple_loss=0.3154, pruned_loss=0.1106, ctc_loss=0.2242, over 966429.92 frames. ], batch size: 43, lr: 2.04e-02,
2024-10-08 05:34:28,231 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=5.151e-03
2024-10-08 05:34:39,773 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.80 vs. limit=14.179
2024-10-08 05:34:45,457 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.37 vs. limit=14.1815
2024-10-08 05:34:52,278 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=8912.0, ans=0.029533333333333335
2024-10-08 05:35:10,287 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.51 vs. limit=14.186499999999999
2024-10-08 05:35:17,933 INFO [train.py:1153] Epoch 4, batch 6200, loss[loss=0.3482, simple_loss=0.3365, pruned_loss=0.1311, ctc_loss=0.2444, over 4786.00 frames. ], tot_loss[loss=0.3143, simple_loss=0.3165, pruned_loss=0.1111, ctc_loss=0.2248, over 966677.19 frames. ], batch size: 29, lr: 2.04e-02,
2024-10-08 05:35:21,805 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=8918.666666666666, ans=0.05
2024-10-08 05:35:22,498 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.33 vs. limit=14.189
2024-10-08 05:35:24,253 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=8918.666666666666, ans=0.02950555555555556
2024-10-08 05:35:29,060 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=8922.0, ans=0.125
2024-10-08 05:35:54,002 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten.whitening_limit, batch_count=8928.666666666666, ans=14.1965
2024-10-08 05:35:54,891 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=8928.666666666666, ans=0.008928550724637682
2024-10-08 05:35:56,537 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.11 vs. limit=4.3393
2024-10-08 05:36:09,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=8932.0, ans=0.008927826086956522
2024-10-08 05:36:11,980 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.895e+01 1.373e+02 1.564e+02 1.691e+02 2.286e+02, threshold=3.128e+02, percent-clipped=0.0
2024-10-08 05:36:19,246 INFO [train.py:1153] Epoch 4, batch 6250, loss[loss=0.3202, simple_loss=0.33, pruned_loss=0.1097, ctc_loss=0.2274, over 4735.00 frames. ], tot_loss[loss=0.3129, simple_loss=0.3156, pruned_loss=0.1104, ctc_loss=0.2232, over 966906.63 frames. ], batch size: 26, lr: 2.04e-02,
2024-10-08 05:36:27,981 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=8935.333333333334, ans=0.008927101449275363
2024-10-08 05:36:33,373 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.01 vs. limit=9.469333333333333
2024-10-08 05:36:35,042 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=6.24 vs. limit=10.852
2024-10-08 05:36:36,648 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=8938.666666666666, ans=0.21061333333333332
2024-10-08 05:36:37,421 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=7.21 vs. limit=7.234666666666667
2024-10-08 05:36:40,240 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=8938.666666666666, ans=0.125
2024-10-08 05:36:40,646 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.77 vs. limit=10.852
2024-10-08 05:36:41,350 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=8938.666666666666, ans=0.008926376811594203
2024-10-08 05:36:43,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=8942.0, ans=0.125
2024-10-08 05:36:50,004 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=8942.0, ans=0.58703
2024-10-08 05:37:20,637 INFO [train.py:1153] Epoch 4, batch 6300, loss[loss=0.294, simple_loss=0.2963, pruned_loss=0.1027, ctc_loss=0.2155, over 4978.00 frames. ], tot_loss[loss=0.3138, simple_loss=0.3161, pruned_loss=0.1109, ctc_loss=0.2244, over 966568.99 frames. ], batch size: 19, lr: 2.03e-02,
2024-10-08 05:37:23,164 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=8952.0, ans=0.029366666666666666
2024-10-08 05:37:35,133 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=15.82 vs. limit=14.2165
2024-10-08 05:38:12,406 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=8965.333333333334, ans=0.125
2024-10-08 05:38:14,587 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.152e+02 1.405e+02 1.555e+02 1.785e+02 3.505e+02, threshold=3.111e+02, percent-clipped=2.0
2024-10-08 05:38:22,019 INFO [train.py:1153] Epoch 4, batch 6350, loss[loss=0.3929, simple_loss=0.3444, pruned_loss=0.1589, ctc_loss=0.3091, over 4794.00 frames. ], tot_loss[loss=0.3128, simple_loss=0.3151, pruned_loss=0.1105, ctc_loss=0.2235, over 966129.58 frames. ], batch size: 36, lr: 2.03e-02,
2024-10-08 05:38:23,444 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=8968.666666666666, ans=0.21031333333333335
2024-10-08 05:38:31,779 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=8968.666666666666, ans=0.33453
2024-10-08 05:38:49,406 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=17.28 vs. limit=14.2315
2024-10-08 05:39:16,697 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=8982.0, ans=0.07
2024-10-08 05:39:22,642 INFO [train.py:1153] Epoch 4, batch 6400, loss[loss=0.3008, simple_loss=0.3245, pruned_loss=0.09976, ctc_loss=0.1937, over 4873.00 frames. ], tot_loss[loss=0.3117, simple_loss=0.3146, pruned_loss=0.11, ctc_loss=0.2218, over 965952.16 frames. ], batch size: 23, lr: 2.03e-02,
2024-10-08 05:39:27,633 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=8985.333333333334, ans=0.029227777777777776
2024-10-08 05:40:00,701 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.max_positive, batch_count=8995.333333333334, ans=0.8399533333333333
2024-10-08 05:40:16,653 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.041e+02 1.360e+02 1.515e+02 1.646e+02 2.833e+02, threshold=3.029e+02, percent-clipped=0.0
2024-10-08 05:40:23,862 INFO [train.py:1153] Epoch 4, batch 6450, loss[loss=0.2993, simple_loss=0.3226, pruned_loss=0.09829, ctc_loss=0.1988, over 4737.00 frames. ], tot_loss[loss=0.3116, simple_loss=0.3151, pruned_loss=0.1098, ctc_loss=0.2211, over 965341.23 frames. ], batch size: 26, lr: 2.03e-02,
2024-10-08 05:40:26,515 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=9002.0, ans=0.05
2024-10-08 05:40:33,884 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9002.0, ans=0.20998
2024-10-08 05:40:35,155 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=9005.333333333334, ans=0.125
2024-10-08 05:40:40,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=15.49 vs. limit=14.254
2024-10-08 05:40:43,886 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=9005.333333333334, ans=0.125
2024-10-08 05:40:53,707 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=9008.666666666666, ans=0.125
2024-10-08 05:41:10,949 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.30 vs. limit=7.253
2024-10-08 05:41:11,958 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=9015.333333333334, ans=0.125
2024-10-08 05:41:17,036 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.13 vs. limit=10.880749999999999
2024-10-08 05:41:21,711 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=9015.333333333334, ans=0.125
2024-10-08 05:41:21,737 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=9015.333333333334, ans=0.008909710144927536
2024-10-08 05:41:25,337 INFO [train.py:1153] Epoch 4, batch 6500, loss[loss=0.2845, simple_loss=0.2977, pruned_loss=0.0993, ctc_loss=0.1819, over 4730.00 frames. ], tot_loss[loss=0.3085, simple_loss=0.3135, pruned_loss=0.108, ctc_loss=0.2185, over 965084.87 frames. ], batch size: 26, lr: 2.03e-02,
2024-10-08 05:41:29,160 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=4.127e-01
2024-10-08 05:41:31,480 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=9018.666666666666, ans=0.008908985507246378
2024-10-08 05:41:53,033 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.09 vs. limit=4.3538
2024-10-08 05:41:57,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=9025.333333333334, ans=0.125
2024-10-08 05:41:59,831 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9025.333333333334, ans=0.20974666666666666
2024-10-08 05:42:08,387 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=9028.666666666666, ans=0.04949747468305833
2024-10-08 05:42:15,711 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=9032.0, ans=0.125
2024-10-08 05:42:19,205 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.127e+02 1.391e+02 1.539e+02 1.688e+02 2.460e+02, threshold=3.079e+02, percent-clipped=0.0
2024-10-08 05:42:20,604 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=9032.0, ans=0.008906086956521739
2024-10-08 05:42:26,696 INFO [train.py:1153] Epoch 4, batch 6550, loss[loss=0.2687, simple_loss=0.289, pruned_loss=0.08801, ctc_loss=0.1808, over 4978.00 frames. ], tot_loss[loss=0.3056, simple_loss=0.3119, pruned_loss=0.1065, ctc_loss=0.2158, over 964638.33 frames. ], batch size: 19, lr: 2.02e-02,
2024-10-08 05:42:30,363 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=9035.333333333334, ans=0.025
2024-10-08 05:42:56,341 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=9042.0, ans=0.5835300000000001
2024-10-08 05:43:03,807 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 05:43:06,241 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=9045.333333333334, ans=0.125
2024-10-08 05:43:28,280 INFO [train.py:1153] Epoch 4, batch 6600, loss[loss=0.2908, simple_loss=0.2966, pruned_loss=0.1023, ctc_loss=0.201, over 4875.00 frames. ], tot_loss[loss=0.3041, simple_loss=0.3109, pruned_loss=0.1056, ctc_loss=0.2149, over 965391.54 frames. ], batch size: 23, lr: 2.02e-02,
2024-10-08 05:43:33,502 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=9052.0, ans=0.02895
2024-10-08 05:43:43,953 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.72 vs. limit=14.2915
2024-10-08 05:44:13,236 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=9062.0, ans=0.028908333333333334
2024-10-08 05:44:15,711 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=9062.0, ans=0.125
2024-10-08 05:44:18,258 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=9065.333333333334, ans=0.5827133333333334
2024-10-08 05:44:19,488 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=9065.333333333334, ans=0.028894444444444445
2024-10-08 05:44:19,695 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.87 vs. limit=4.3598
2024-10-08 05:44:20,650 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=9065.333333333334, ans=0.028894444444444445
2024-10-08 05:44:23,115 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.895e+01 1.357e+02 1.538e+02 1.680e+02 2.395e+02, threshold=3.075e+02, percent-clipped=0.0
2024-10-08 05:44:30,391 INFO [train.py:1153] Epoch 4, batch 6650, loss[loss=0.2135, simple_loss=0.2501, pruned_loss=0.06233, ctc_loss=0.1307, over 4745.00 frames. ], tot_loss[loss=0.3012, simple_loss=0.3093, pruned_loss=0.1042, ctc_loss=0.212, over 967281.90 frames. ], batch size: 20, lr: 2.02e-02,
2024-10-08 05:44:48,902 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=9072.0, ans=0.025
2024-10-08 05:45:01,330 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=9075.333333333334, ans=0.125
2024-10-08 05:45:21,207 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=9082.0, ans=0.025
2024-10-08 05:45:24,199 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.96 vs. limit=14.311499999999999
2024-10-08 05:45:32,316 INFO [train.py:1153] Epoch 4, batch 6700, loss[loss=0.2895, simple_loss=0.3083, pruned_loss=0.09582, ctc_loss=0.1977, over 4937.00 frames. ], tot_loss[loss=0.2985, simple_loss=0.3077, pruned_loss=0.1029, ctc_loss=0.209, over 969364.73 frames. ], batch size: 20, lr: 2.02e-02,
2024-10-08 05:45:56,295 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.max_abs, batch_count=9092.0, ans=10.0
2024-10-08 05:46:11,664 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.81 vs. limit=14.3215
2024-10-08 05:46:27,240 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.902e+01 1.296e+02 1.442e+02 1.575e+02 2.319e+02, threshold=2.885e+02, percent-clipped=0.0
2024-10-08 05:46:34,835 INFO [train.py:1153] Epoch 4, batch 6750, loss[loss=0.2957, simple_loss=0.3042, pruned_loss=0.1005, ctc_loss=0.2153, over 4912.00 frames. ], tot_loss[loss=0.2959, simple_loss=0.3058, pruned_loss=0.1017, ctc_loss=0.206, over 972397.54 frames. ], batch size: 19, lr: 2.02e-02,
2024-10-08 05:46:51,210 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9105.333333333334, ans=0.20894666666666667
2024-10-08 05:47:02,334 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=9108.666666666666, ans=0.5811966666666668
2024-10-08 05:47:15,022 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=9112.0, ans=0.125
2024-10-08 05:47:37,393 INFO [train.py:1153] Epoch 4, batch 6800, loss[loss=0.2512, simple_loss=0.2836, pruned_loss=0.07683, ctc_loss=0.163, over 4909.00 frames. ], tot_loss[loss=0.2948, simple_loss=0.3047, pruned_loss=0.1014, ctc_loss=0.2051, over 974682.50 frames. ], batch size: 19, lr: 2.02e-02,
2024-10-08 05:47:54,321 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=12.12 vs. limit=9.561
2024-10-08 05:47:54,847 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=9122.0, ans=0.035
2024-10-08 05:47:56,153 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=9122.0, ans=0.125
2024-10-08 05:47:57,431 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=9122.0, ans=0.20878
2024-10-08 05:47:57,736 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.90 vs. limit=14.3415
2024-10-08 05:48:19,957 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=9128.666666666666, ans=0.125
2024-10-08 05:48:32,516 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.714e+01 1.250e+02 1.402e+02 1.615e+02 3.015e+02, threshold=2.803e+02, percent-clipped=1.0
2024-10-08 05:48:39,850 INFO [train.py:1153] Epoch 4, batch 6850, loss[loss=0.2505, simple_loss=0.2648, pruned_loss=0.083, ctc_loss=0.1754, over 4978.00 frames. ], tot_loss[loss=0.2912, simple_loss=0.3018, pruned_loss=0.09979, ctc_loss=0.2025, over 979009.11 frames. ], batch size: 19, lr: 2.01e-02,
2024-10-08 05:48:41,252 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/epoch-4.pt
2024-10-08 05:49:08,953 INFO [train.py:1153] Epoch 5, batch 0, loss[loss=0.306, simple_loss=0.307, pruned_loss=0.1086, ctc_loss=0.2195, over 4853.00 frames. ], tot_loss[loss=0.306, simple_loss=0.307, pruned_loss=0.1086, ctc_loss=0.2195, over 4853.00 frames. ], batch size: 19, lr: 1.88e-02,
2024-10-08 05:49:08,953 INFO [train.py:1176] Computing validation loss
2024-10-08 05:49:14,839 INFO [train.py:1185] Epoch 5, validation: loss=0.1987, simple_loss=0.2754, pruned_loss=0.04404, ctc_loss=0.08458, over 90464.00 frames.
2024-10-08 05:49:14,840 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 05:49:26,628 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 05:49:29,960 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=9139.333333333334, ans=0.02858611111111111
2024-10-08 05:49:32,550 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=13.78 vs. limit=14.3545
2024-10-08 05:49:33,821 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.06 vs. limit=14.3545
2024-10-08 05:49:35,020 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.22 vs. limit=14.3545
2024-10-08 05:49:35,586 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=9142.666666666666, ans=0.125
2024-10-08 05:49:51,337 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9146.0, ans=0.20854
2024-10-08 05:49:55,940 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.99 vs. limit=9.573
2024-10-08 05:50:13,379 INFO [train.py:1153] Epoch 5, batch 50, loss[loss=0.277, simple_loss=0.2874, pruned_loss=0.09386, ctc_loss=0.197, over 4915.00 frames. ], tot_loss[loss=0.3188, simple_loss=0.317, pruned_loss=0.1139, ctc_loss=0.2321, over 217819.09 frames. ], batch size: 19, lr: 1.87e-02,
2024-10-08 05:50:35,679 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=9156.0, ans=0.57954
2024-10-08 05:50:52,592 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=9162.666666666666, ans=0.025
2024-10-08 05:51:04,717 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.088e+02 1.379e+02 1.511e+02 1.685e+02 2.353e+02, threshold=3.022e+02, percent-clipped=0.0
2024-10-08 05:51:14,504 INFO [train.py:1153] Epoch 5, batch 100, loss[loss=0.2651, simple_loss=0.2865, pruned_loss=0.08664, ctc_loss=0.1763, over 4749.00 frames. ], tot_loss[loss=0.3215, simple_loss=0.3199, pruned_loss=0.1148, ctc_loss=0.2335, over 383336.03 frames. ], batch size: 19, lr: 1.87e-02,
2024-10-08 05:51:24,455 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=9169.333333333334, ans=0.02846111111111111
2024-10-08 05:51:24,504 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=9169.333333333334, ans=10.0
2024-10-08 05:51:33,786 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=10.04 vs. limit=9.586333333333332
2024-10-08 05:51:34,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.12 vs. limit=14.3795
2024-10-08 05:51:39,177 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=9176.0, ans=0.028433333333333335
2024-10-08 05:51:47,884 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=9176.0, ans=0.028433333333333335
2024-10-08 05:51:54,104 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9179.333333333334, ans=0.20820666666666665
2024-10-08 05:51:55,347 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=9179.333333333334, ans=0.025
2024-10-08 05:51:57,764 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 05:52:16,074 INFO [train.py:1153] Epoch 5, batch 150, loss[loss=0.2737, simple_loss=0.3059, pruned_loss=0.08226, ctc_loss=0.1925, over 4914.00 frames. ], tot_loss[loss=0.3128, simple_loss=0.315, pruned_loss=0.1103, ctc_loss=0.2248, over 513306.44 frames. ], batch size: 19, lr: 1.87e-02,
2024-10-08 05:52:17,455 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9186.0, ans=0.20814
2024-10-08 05:52:30,910 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=9189.333333333334, ans=0.008871884057971015
2024-10-08 05:52:50,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=9192.666666666666, ans=0.05
2024-10-08 05:52:53,131 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=9196.0, ans=0.5781400000000001
2024-10-08 05:53:07,986 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.006e+02 1.276e+02 1.442e+02 1.599e+02 2.529e+02, threshold=2.883e+02, percent-clipped=0.0
2024-10-08 05:53:11,756 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=9199.333333333334, ans=0.008869710144927536
2024-10-08 05:53:17,682 INFO [train.py:1153] Epoch 5, batch 200, loss[loss=0.3308, simple_loss=0.321, pruned_loss=0.1217, ctc_loss=0.2429, over 4707.00 frames. ], tot_loss[loss=0.3081, simple_loss=0.3119, pruned_loss=0.1081, ctc_loss=0.2203, over 613598.53 frames. ], batch size: 45, lr: 1.87e-02,
2024-10-08 05:53:20,645 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.21 vs. limit=10.951
2024-10-08 05:53:22,727 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=9202.666666666666, ans=0.008868985507246376
2024-10-08 05:53:28,856 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=9206.0, ans=0.035
2024-10-08 05:53:28,938 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=9206.0, ans=0.125
2024-10-08 05:54:03,486 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=9212.666666666666, ans=0.125
2024-10-08 05:54:19,218 INFO [train.py:1153] Epoch 5, batch 250, loss[loss=0.339, simple_loss=0.3254, pruned_loss=0.1267, ctc_loss=0.2481, over 4840.00 frames. ], tot_loss[loss=0.3075, simple_loss=0.3119, pruned_loss=0.1076, ctc_loss=0.2195, over 692362.94 frames. ], batch size: 38, lr: 1.87e-02,
2024-10-08 05:54:47,357 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=9226.0, ans=0.05
2024-10-08 05:54:50,329 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.48 vs. limit=14.4195
2024-10-08 05:55:02,076 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=9229.333333333334, ans=0.008863188405797102
2024-10-08 05:55:10,365 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.031e+02 1.334e+02 1.517e+02 1.670e+02 2.982e+02, threshold=3.034e+02, percent-clipped=1.0
2024-10-08 05:55:12,272 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.42 vs. limit=10.962250000000001
2024-10-08 05:55:17,823 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=9232.666666666666, ans=0.028197222222222224
2024-10-08 05:55:20,108 INFO [train.py:1153] Epoch 5, batch 300, loss[loss=0.3799, simple_loss=0.3517, pruned_loss=0.1468, ctc_loss=0.2858, over 4764.00 frames. ], tot_loss[loss=0.307, simple_loss=0.311, pruned_loss=0.1077, ctc_loss=0.2194, over 752866.47 frames. ], batch size: 32, lr: 1.87e-02,
2024-10-08 05:55:45,046 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.10 vs. limit=14.432
2024-10-08 05:56:10,145 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=9249.333333333334, ans=0.05
2024-10-08 05:56:20,931 INFO [train.py:1153] Epoch 5, batch 350, loss[loss=0.2611, simple_loss=0.2791, pruned_loss=0.08604, ctc_loss=0.1774, over 4883.00 frames. ], tot_loss[loss=0.3054, simple_loss=0.3101, pruned_loss=0.1068, ctc_loss=0.2179, over 800200.26 frames. ], batch size: 19, lr: 1.86e-02,
2024-10-08 05:56:22,445 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=9252.666666666666, ans=0.04949747468305833
2024-10-08 05:56:27,298 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=9252.666666666666, ans=0.04949747468305833
2024-10-08 05:56:39,331 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=9256.0, ans=0.125
2024-10-08 05:56:50,282 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=9259.333333333334, ans=0.02808611111111111
2024-10-08 05:57:11,976 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.829e+01 1.305e+02 1.495e+02 1.683e+02 3.815e+02, threshold=2.990e+02, percent-clipped=1.0
2024-10-08 05:57:21,631 INFO [train.py:1153] Epoch 5, batch 400, loss[loss=0.2528, simple_loss=0.2891, pruned_loss=0.07789, ctc_loss=0.1518, over 4882.00 frames. ], tot_loss[loss=0.3033, simple_loss=0.3093, pruned_loss=0.1055, ctc_loss=0.2159, over 836873.80 frames. ], batch size: 22, lr: 1.86e-02,
2024-10-08 05:57:36,676 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=12.07 vs. limit=10.97725
2024-10-08 05:57:45,022 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.85 vs. limit=14.457
2024-10-08 05:58:01,810 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=9279.333333333334, ans=0.5752233333333333
2024-10-08 05:58:10,415 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=9282.666666666666, ans=0.125
2024-10-08 05:58:22,415 INFO [train.py:1153] Epoch 5, batch 450, loss[loss=0.2564, simple_loss=0.2876, pruned_loss=0.08002, ctc_loss=0.163, over 4880.00 frames. ], tot_loss[loss=0.3034, simple_loss=0.3093, pruned_loss=0.1056, ctc_loss=0.2154, over 865468.86 frames. ], batch size: 23, lr: 1.86e-02,
2024-10-08 05:58:43,328 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=9289.333333333334, ans=0.125
2024-10-08 05:58:45,801 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=9292.666666666666, ans=0.125
2024-10-08 05:58:46,978 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=9292.666666666666, ans=0.20707333333333333
2024-10-08 05:59:07,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=9296.0, ans=0.008848695652173912
2024-10-08 05:59:15,216 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.831e+01 1.297e+02 1.474e+02 1.629e+02 3.307e+02, threshold=2.947e+02, percent-clipped=1.0
2024-10-08 05:59:15,365 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=9299.333333333334, ans=0.0
2024-10-08 05:59:18,791 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.20 vs. limit=14.474499999999999
2024-10-08 05:59:24,158 INFO [train.py:1153] Epoch 5, batch 500, loss[loss=0.3064, simple_loss=0.3262, pruned_loss=0.1001, ctc_loss=0.2159, over 4814.00 frames. ], tot_loss[loss=0.3017, simple_loss=0.3086, pruned_loss=0.1046, ctc_loss=0.2139, over 888051.78 frames. ], batch size: 34, lr: 1.86e-02,
2024-10-08 05:59:27,797 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=9302.666666666666, ans=0.125
2024-10-08 05:59:28,107 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.84 vs. limit=4.3954
2024-10-08 05:59:32,988 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.52 vs. limit=9.651333333333334
2024-10-08 05:59:45,393 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=9306.0, ans=0.57429
2024-10-08 05:59:52,624 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=9309.333333333334, ans=0.5741733333333333
2024-10-08 05:59:55,042 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=9309.333333333334, ans=0.5741733333333333
2024-10-08 06:00:01,062 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=9312.666666666666, ans=0.008845072463768117
2024-10-08 06:00:02,236 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=9312.666666666666, ans=0.027863888888888893
2024-10-08 06:00:03,475 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=9312.666666666666, ans=0.33969
2024-10-08 06:00:23,820 INFO [train.py:1153] Epoch 5, batch 550, loss[loss=0.3136, simple_loss=0.3076, pruned_loss=0.1166, ctc_loss=0.2162, over 4804.00 frames. ], tot_loss[loss=0.3005, simple_loss=0.3081, pruned_loss=0.104, ctc_loss=0.2124, over 905589.23 frames. ], batch size: 40, lr: 1.86e-02,
2024-10-08 06:00:27,561 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=9319.333333333334, ans=0.125
2024-10-08 06:01:13,652 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/checkpoint-28000.pt
2024-10-08 06:01:15,659 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.010e+01 1.278e+02 1.459e+02 1.630e+02 3.397e+02, threshold=2.917e+02, percent-clipped=1.0
2024-10-08 06:01:22,965 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.43 vs. limit=10.99975
2024-10-08 06:01:24,849 INFO [train.py:1153] Epoch 5, batch 600, loss[loss=0.2649, simple_loss=0.2856, pruned_loss=0.08715, ctc_loss=0.1747, over 4840.00 frames. ], tot_loss[loss=0.2996, simple_loss=0.308, pruned_loss=0.1033, ctc_loss=0.2115, over 919346.63 frames. ], batch size: 38, lr: 1.86e-02,
2024-10-08 06:01:33,127 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=1.827e-02
2024-10-08 06:01:48,831 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=9342.666666666666, ans=0.125
2024-10-08 06:01:52,460 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=9342.666666666666, ans=0.125
2024-10-08 06:01:55,043 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=3.949e-01
2024-10-08 06:01:56,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.prob, batch_count=9342.666666666666, ans=0.125
2024-10-08 06:02:25,071 INFO [train.py:1153] Epoch 5, batch 650, loss[loss=0.2983, simple_loss=0.3124, pruned_loss=0.1009, ctc_loss=0.2061, over 4836.00 frames. ], tot_loss[loss=0.2971, simple_loss=0.3063, pruned_loss=0.1022, ctc_loss=0.2088, over 930289.24 frames. ], batch size: 21, lr: 1.86e-02,
2024-10-08 06:02:33,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=9352.666666666666, ans=0.125
2024-10-08 06:02:43,245 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=9356.0, ans=0.027683333333333334
2024-10-08 06:02:51,130 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.22 vs. limit=14.5195
2024-10-08 06:02:51,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=9359.333333333334, ans=0.07
2024-10-08 06:02:52,312 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.27 vs. limit=7.339833333333333
2024-10-08 06:02:56,780 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9359.333333333334, ans=0.20640666666666665
2024-10-08 06:02:56,794 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=9359.333333333334, ans=0.125
2024-10-08 06:02:59,798 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.09 vs. limit=14.5195
2024-10-08 06:03:15,772 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.486e+01 1.351e+02 1.449e+02 1.630e+02 3.009e+02, threshold=2.898e+02, percent-clipped=1.0
2024-10-08 06:03:25,393 INFO [train.py:1153] Epoch 5, batch 700, loss[loss=0.2731, simple_loss=0.3012, pruned_loss=0.08566, ctc_loss=0.1842, over 4755.00 frames. ], tot_loss[loss=0.2996, simple_loss=0.3075, pruned_loss=0.1035, ctc_loss=0.2115, over 938192.12 frames. ], batch size: 19, lr: 1.85e-02,
2024-10-08 06:03:29,845 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module2.whiten, num_groups=1, num_channels=192, metric=3.49 vs. limit=11.0135
2024-10-08 06:04:01,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9379.333333333334, ans=0.20620666666666665
2024-10-08 06:04:05,210 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=10.02 vs. limit=9.689666666666668
2024-10-08 06:04:14,341 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9382.666666666666, ans=0.20617333333333332
2024-10-08 06:04:15,957 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=8.48 vs. limit=11.0185
2024-10-08 06:04:26,321 INFO [train.py:1153] Epoch 5, batch 750, loss[loss=0.3169, simple_loss=0.3171, pruned_loss=0.1141, ctc_loss=0.2216, over 4867.00 frames. ], tot_loss[loss=0.2969, simple_loss=0.3061, pruned_loss=0.1021, ctc_loss=0.2088, over 945006.23 frames. ], batch size: 22, lr: 1.85e-02,
2024-10-08 06:04:28,260 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.30 vs. limit=14.5395
2024-10-08 06:04:35,065 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=9386.0, ans=0.027558333333333334
2024-10-08 06:04:38,704 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9389.333333333334, ans=0.20610666666666666
2024-10-08 06:05:04,471 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=9396.0, ans=0.09899494936611666
2024-10-08 06:05:05,682 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=9396.0, ans=0.57114
2024-10-08 06:05:17,737 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.110e+01 1.216e+02 1.396e+02 1.585e+02 3.196e+02, threshold=2.793e+02, percent-clipped=1.0
2024-10-08 06:05:27,369 INFO [train.py:1153] Epoch 5, batch 800, loss[loss=0.2686, simple_loss=0.2894, pruned_loss=0.08654, ctc_loss=0.1869, over 4857.00 frames. ], tot_loss[loss=0.2974, simple_loss=0.3065, pruned_loss=0.1024, ctc_loss=0.209, over 949730.96 frames. ], batch size: 19, lr: 1.85e-02,
2024-10-08 06:05:35,195 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.77 vs. limit=14.552
2024-10-08 06:05:51,133 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=10.60 vs. limit=11.028500000000001
2024-10-08 06:05:59,073 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=9409.333333333334, ans=0.5706733333333334
2024-10-08 06:06:02,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=9412.666666666666, ans=0.125
2024-10-08 06:06:22,232 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=9416.0, ans=0.07
2024-10-08 06:06:28,203 INFO [train.py:1153] Epoch 5, batch 850, loss[loss=0.3291, simple_loss=0.3241, pruned_loss=0.1203, ctc_loss=0.2336, over 4803.00 frames. ], tot_loss[loss=0.2994, simple_loss=0.3079, pruned_loss=0.1033, ctc_loss=0.2106, over 953894.14 frames. ], batch size: 29, lr: 1.85e-02,
2024-10-08 06:06:39,953 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.57 vs. limit=14.567
2024-10-08 06:06:41,099 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=9422.666666666666, ans=14.567
2024-10-08 06:06:48,557 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.50 vs. limit=14.567
2024-10-08 06:06:58,455 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.28 vs. limit=14.5695
2024-10-08 06:07:15,143 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=9429.333333333334, ans=0.025
2024-10-08 06:07:19,842 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.371e+01 1.295e+02 1.470e+02 1.670e+02 2.191e+02, threshold=2.939e+02, percent-clipped=0.0
2024-10-08 06:07:21,142 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.min_positive, batch_count=9432.666666666666, ans=0.025
2024-10-08 06:07:29,794 INFO [train.py:1153] Epoch 5, batch 900, loss[loss=0.2574, simple_loss=0.2822, pruned_loss=0.08218, ctc_loss=0.1707, over 4852.00 frames. ], tot_loss[loss=0.2992, simple_loss=0.308, pruned_loss=0.1032, ctc_loss=0.2101, over 956870.10 frames. ], batch size: 19, lr: 1.85e-02,
2024-10-08 06:07:34,841 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=9436.0, ans=0.125
2024-10-08 06:07:59,183 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=9442.666666666666, ans=0.125
2024-10-08 06:08:04,104 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=9442.666666666666, ans=0.027322222222222227
2024-10-08 06:08:19,088 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.19 vs. limit=14.587
2024-10-08 06:08:30,827 INFO [train.py:1153] Epoch 5, batch 950, loss[loss=0.3122, simple_loss=0.3241, pruned_loss=0.106, ctc_loss=0.2207, over 4819.00 frames. ], tot_loss[loss=0.2993, simple_loss=0.3079, pruned_loss=0.1032, ctc_loss=0.2108, over 958800.08 frames. ], batch size: 19, lr: 1.85e-02,
2024-10-08 06:08:32,713 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.20 vs. limit=11.04475
2024-10-08 06:08:33,449 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=9452.666666666666, ans=0.025
2024-10-08 06:08:39,497 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=9452.666666666666, ans=0.125
2024-10-08 06:08:41,971 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=9456.0, ans=0.125
2024-10-08 06:09:00,070 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=9459.333333333334, ans=0.125
2024-10-08 06:09:11,589 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.61 vs. limit=11.0485
2024-10-08 06:09:13,629 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=9462.666666666666, ans=0.027238888888888892
2024-10-08 06:09:16,603 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.07 vs. limit=9.731333333333332
2024-10-08 06:09:22,223 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.012e+02 1.287e+02 1.496e+02 1.688e+02 2.345e+02, threshold=2.991e+02, percent-clipped=0.0
2024-10-08 06:09:26,399 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=16.92 vs. limit=14.599499999999999
2024-10-08 06:09:30,001 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.67 vs. limit=14.599499999999999
2024-10-08 06:09:31,948 INFO [train.py:1153] Epoch 5, batch 1000, loss[loss=0.3181, simple_loss=0.3302, pruned_loss=0.1113, ctc_loss=0.2082, over 4932.00 frames. ], tot_loss[loss=0.3008, simple_loss=0.3088, pruned_loss=0.1039, ctc_loss=0.2127, over 960618.39 frames. ], batch size: 20, lr: 1.84e-02,
2024-10-08 06:09:43,752 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2.whitening_limit, batch_count=9469.333333333334, ans=9.734666666666667
2024-10-08 06:10:08,309 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.09 vs. limit=9.739666666666668
2024-10-08 06:10:23,627 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.23 vs. limit=4.4224
2024-10-08 06:10:32,817 INFO [train.py:1153] Epoch 5, batch 1050, loss[loss=0.2818, simple_loss=0.2963, pruned_loss=0.09479, ctc_loss=0.1944, over 4801.00 frames. ], tot_loss[loss=0.2992, simple_loss=0.3078, pruned_loss=0.1031, ctc_loss=0.2113, over 962650.70 frames. ], batch size: 25, lr: 1.84e-02,
2024-10-08 06:10:34,164 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=9486.0, ans=0.125
2024-10-08 06:10:37,764 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=9486.0, ans=0.025
2024-10-08 06:10:40,209 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=9486.0, ans=0.125
2024-10-08 06:10:42,671 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9486.0, ans=0.20514
2024-10-08 06:11:16,729 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=9496.0, ans=10.0
2024-10-08 06:11:20,364 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=9499.333333333334, ans=0.125
2024-10-08 06:11:23,924 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.697e+01 1.316e+02 1.417e+02 1.629e+02 2.318e+02, threshold=2.834e+02, percent-clipped=0.0
2024-10-08 06:11:26,568 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=9499.333333333334, ans=0.5675233333333334
2024-10-08 06:11:27,341 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module1.whiten, num_groups=1, num_channels=192, metric=4.11 vs. limit=11.06225
2024-10-08 06:11:30,202 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=9499.333333333334, ans=0.125
2024-10-08 06:11:33,639 INFO [train.py:1153] Epoch 5, batch 1100, loss[loss=0.2641, simple_loss=0.28, pruned_loss=0.08566, ctc_loss=0.1921, over 4865.00 frames. ], tot_loss[loss=0.2986, simple_loss=0.3074, pruned_loss=0.1027, ctc_loss=0.2107, over 964054.74 frames. ], batch size: 20, lr: 1.84e-02,
2024-10-08 06:11:46,025 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9506.0, ans=0.20494
2024-10-08 06:11:52,203 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 06:11:54,581 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=9506.0, ans=0.027058333333333334
2024-10-08 06:12:05,428 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.prob, batch_count=9509.333333333334, ans=0.125
2024-10-08 06:12:07,940 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=9509.333333333334, ans=0.5671733333333333
2024-10-08 06:12:12,912 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=9512.666666666666, ans=0.125
2024-10-08 06:12:17,752 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=9512.666666666666, ans=0.34269
2024-10-08 06:12:18,989 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=9512.666666666666, ans=0.008801594202898552
2024-10-08 06:12:29,791 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=9516.0, ans=0.0
2024-10-08 06:12:34,690 INFO [train.py:1153] Epoch 5, batch 1150, loss[loss=0.2621, simple_loss=0.2818, pruned_loss=0.08535, ctc_loss=0.1791, over 4862.00 frames. ], tot_loss[loss=0.2998, simple_loss=0.3082, pruned_loss=0.1033, ctc_loss=0.2121, over 964333.93 frames. ], batch size: 20, lr: 1.84e-02,
2024-10-08 06:12:36,569 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.35 vs. limit=7.379833333333334
2024-10-08 06:12:55,403 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.min_positive, batch_count=9522.666666666666, ans=0.15477333333333332
2024-10-08 06:13:00,243 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=9526.0, ans=0.09899494936611666
2024-10-08 06:13:14,798 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=9529.333333333334, ans=0.125
2024-10-08 06:13:25,678 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.075e+02 1.313e+02 1.493e+02 1.633e+02 2.467e+02, threshold=2.987e+02, percent-clipped=0.0
2024-10-08 06:13:35,415 INFO [train.py:1153] Epoch 5, batch 1200, loss[loss=0.3108, simple_loss=0.3146, pruned_loss=0.1065, ctc_loss=0.235, over 4807.00 frames. ], tot_loss[loss=0.3008, simple_loss=0.3093, pruned_loss=0.1036, ctc_loss=0.2127, over 964233.00 frames. ], batch size: 25, lr: 1.84e-02,
2024-10-08 06:13:44,620 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=17.04 vs. limit=14.652000000000001
2024-10-08 06:14:00,270 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=9542.666666666666, ans=0.07
2024-10-08 06:14:05,067 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=9542.666666666666, ans=0.125
2024-10-08 06:14:07,872 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.83 vs. limit=11.0785
2024-10-08 06:14:07,984 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.56 vs. limit=14.657
2024-10-08 06:14:09,340 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=14.62 vs. limit=14.657
2024-10-08 06:14:12,417 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=9546.0, ans=0.125
2024-10-08 06:14:12,473 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=9546.0, ans=0.008794347826086957
2024-10-08 06:14:36,346 INFO [train.py:1153] Epoch 5, batch 1250, loss[loss=0.293, simple_loss=0.2986, pruned_loss=0.1046, ctc_loss=0.1952, over 4757.00 frames. ], tot_loss[loss=0.3002, simple_loss=0.3088, pruned_loss=0.1034, ctc_loss=0.2119, over 964357.28 frames. ], batch size: 32, lr: 1.84e-02,
2024-10-08 06:14:41,286 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=9552.666666666666, ans=0.125
2024-10-08 06:14:57,433 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.84 vs. limit=11.0835
2024-10-08 06:15:05,716 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=9559.333333333334, ans=0.026836111111111113
2024-10-08 06:15:15,933 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.84 vs. limit=11.086
2024-10-08 06:15:24,664 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.78 vs. limit=14.6745
2024-10-08 06:15:27,592 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.056e+02 1.322e+02 1.496e+02 1.639e+02 2.345e+02, threshold=2.992e+02, percent-clipped=0.0
2024-10-08 06:15:37,348 INFO [train.py:1153] Epoch 5, batch 1300, loss[loss=0.3418, simple_loss=0.3368, pruned_loss=0.124, ctc_loss=0.2469, over 4849.00 frames. ], tot_loss[loss=0.3, simple_loss=0.3089, pruned_loss=0.1032, ctc_loss=0.2113, over 965764.91 frames. ], batch size: 43, lr: 1.84e-02,
2024-10-08 06:15:38,679 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=9569.333333333334, ans=0.125
2024-10-08 06:15:47,225 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=9569.333333333334, ans=0.125
2024-10-08 06:15:52,130 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=9572.666666666666, ans=0.125
2024-10-08 06:15:53,904 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.39 vs. limit=11.08975
2024-10-08 06:16:00,707 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=9576.0, ans=0.20424
2024-10-08 06:16:08,397 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.49 vs. limit=11.091000000000001
2024-10-08 06:16:09,336 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=9576.0, ans=0.0
2024-10-08 06:16:30,834 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten.whitening_limit, batch_count=9582.666666666666, ans=11.0935
2024-10-08 06:16:38,667 INFO [train.py:1153] Epoch 5, batch 1350, loss[loss=0.2756, simple_loss=0.2873, pruned_loss=0.09501, ctc_loss=0.185, over 4844.00 frames. ], tot_loss[loss=0.2973, simple_loss=0.3072, pruned_loss=0.102, ctc_loss=0.2086, over 966518.16 frames. ], batch size: 21, lr: 1.83e-02,
2024-10-08 06:16:48,650 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=9586.0, ans=0.0
2024-10-08 06:16:49,485 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.63 vs. limit=9.793
2024-10-08 06:16:55,294 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.68 vs. limit=14.692
2024-10-08 06:17:03,423 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=9592.666666666666, ans=0.5642566666666667
2024-10-08 06:17:06,237 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.14 vs. limit=14.6945
2024-10-08 06:17:10,596 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=9592.666666666666, ans=0.125
2024-10-08 06:17:22,948 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=9596.0, ans=0.125
2024-10-08 06:17:30,109 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.492e+01 1.296e+02 1.440e+02 1.636e+02 2.139e+02, threshold=2.881e+02, percent-clipped=0.0
2024-10-08 06:17:31,787 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.16 vs. limit=14.6995
2024-10-08 06:17:39,764 INFO [train.py:1153] Epoch 5, batch 1400, loss[loss=0.2188, simple_loss=0.2678, pruned_loss=0.0603, ctc_loss=0.1231, over 4940.00 frames. ], tot_loss[loss=0.2974, simple_loss=0.3071, pruned_loss=0.1021, ctc_loss=0.2085, over 966791.66 frames. ], batch size: 19, lr: 1.83e-02,
2024-10-08 06:17:59,242 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=9606.0, ans=0.125
2024-10-08 06:18:00,364 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer_ff2.min_abs, batch_count=9606.0, ans=0.1
2024-10-08 06:18:28,523 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.97 vs. limit=14.712
2024-10-08 06:18:31,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=9616.0, ans=0.125
2024-10-08 06:18:39,974 INFO [train.py:1153] Epoch 5, batch 1450, loss[loss=0.3178, simple_loss=0.3168, pruned_loss=0.1149, ctc_loss=0.2224, over 4834.00 frames. ], tot_loss[loss=0.299, simple_loss=0.3082, pruned_loss=0.1029, ctc_loss=0.2101, over 966733.24 frames. ], batch size: 34, lr: 1.83e-02,
2024-10-08 06:19:00,670 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=9622.666666666666, ans=0.125
2024-10-08 06:19:02,890 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.8.prob, batch_count=9626.0, ans=0.125
2024-10-08 06:19:19,021 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.53 vs. limit=14.722000000000001
2024-10-08 06:19:27,069 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9632.666666666666, ans=0.20367333333333332
2024-10-08 06:19:30,522 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.707e+01 1.339e+02 1.467e+02 1.593e+02 2.393e+02, threshold=2.934e+02, percent-clipped=0.0
2024-10-08 06:19:31,360 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.85 vs. limit=9.816333333333333
2024-10-08 06:19:32,476 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.41 vs. limit=4.4449000000000005
2024-10-08 06:19:38,754 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.45 vs. limit=14.724499999999999
2024-10-08 06:19:41,214 INFO [train.py:1153] Epoch 5, batch 1500, loss[loss=0.3069, simple_loss=0.3309, pruned_loss=0.1005, ctc_loss=0.2047, over 4698.00 frames. ], tot_loss[loss=0.2985, simple_loss=0.308, pruned_loss=0.1026, ctc_loss=0.2093, over 966423.47 frames. ], batch size: 26, lr: 1.83e-02,
2024-10-08 06:19:48,369 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=9636.0, ans=0.125
2024-10-08 06:19:55,379 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 06:19:57,571 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=9639.333333333334, ans=0.125
2024-10-08 06:20:23,362 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=16.22 vs. limit=14.7345
2024-10-08 06:20:34,904 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9649.333333333334, ans=0.20350666666666667
2024-10-08 06:20:40,804 INFO [train.py:1153] Epoch 5, batch 1550, loss[loss=0.3597, simple_loss=0.3416, pruned_loss=0.1345, ctc_loss=0.2722, over 4845.00 frames. ], tot_loss[loss=0.2972, simple_loss=0.307, pruned_loss=0.1021, ctc_loss=0.2081, over 966328.13 frames. ], batch size: 31, lr: 1.83e-02,
2024-10-08 06:20:45,717 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=9652.666666666666, ans=0.026447222222222226
2024-10-08 06:20:48,685 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.84 vs. limit=7.413166666666666
2024-10-08 06:20:51,863 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9656.0, ans=0.20344
2024-10-08 06:21:13,047 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.96 vs. limit=7.414833333333334
2024-10-08 06:21:19,836 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=9662.666666666666, ans=0.125
2024-10-08 06:21:31,869 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.037e+02 1.296e+02 1.445e+02 1.712e+02 2.812e+02, threshold=2.891e+02, percent-clipped=0.0
2024-10-08 06:21:41,639 INFO [train.py:1153] Epoch 5, batch 1600, loss[loss=0.2646, simple_loss=0.2891, pruned_loss=0.08473, ctc_loss=0.1764, over 4818.00 frames. ], tot_loss[loss=0.2952, simple_loss=0.3055, pruned_loss=0.1012, ctc_loss=0.2065, over 966550.04 frames. ], batch size: 25, lr: 1.83e-02,
2024-10-08 06:22:12,183 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=9676.0, ans=0.026350000000000002
2024-10-08 06:22:16,147 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.94 vs. limit=11.128499999999999
2024-10-08 06:22:23,021 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=9679.333333333334, ans=0.125
2024-10-08 06:22:30,338 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=9682.666666666666, ans=10.0
2024-10-08 06:22:32,854 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=9682.666666666666, ans=0.125
2024-10-08 06:22:40,122 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=9682.666666666666, ans=0.125
2024-10-08 06:22:42,369 INFO [train.py:1153] Epoch 5, batch 1650, loss[loss=0.2882, simple_loss=0.3196, pruned_loss=0.0893, ctc_loss=0.1957, over 4797.00 frames. ], tot_loss[loss=0.2963, simple_loss=0.3063, pruned_loss=0.1016, ctc_loss=0.2074, over 967039.32 frames. ], batch size: 29, lr: 1.83e-02,
2024-10-08 06:23:03,227 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=9689.333333333334, ans=0.125
2024-10-08 06:23:16,523 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=9692.666666666666, ans=0.125
2024-10-08 06:23:18,992 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=9696.0, ans=0.20304
2024-10-08 06:23:21,402 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=9696.0, ans=0.026266666666666667
2024-10-08 06:23:25,024 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 06:23:29,239 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.47 vs. limit=14.772
2024-10-08 06:23:33,400 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.005e+02 1.281e+02 1.450e+02 1.568e+02 2.552e+02, threshold=2.900e+02, percent-clipped=0.0
2024-10-08 06:23:33,561 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9699.333333333334, ans=0.20300666666666667
2024-10-08 06:23:38,901 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=6.44 vs. limit=11.13725
2024-10-08 06:23:41,182 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.87 vs. limit=9.849666666666668
2024-10-08 06:23:43,089 INFO [train.py:1153] Epoch 5, batch 1700, loss[loss=0.2521, simple_loss=0.2718, pruned_loss=0.08357, ctc_loss=0.163, over 4940.00 frames. ], tot_loss[loss=0.2953, simple_loss=0.3058, pruned_loss=0.1011, ctc_loss=0.2065, over 967044.60 frames. ], batch size: 19, lr: 1.82e-02,
2024-10-08 06:23:59,219 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=9706.0, ans=0.125
2024-10-08 06:24:44,094 INFO [train.py:1153] Epoch 5, batch 1750, loss[loss=0.2513, simple_loss=0.2757, pruned_loss=0.08209, ctc_loss=0.1567, over 4959.00 frames. ], tot_loss[loss=0.2933, simple_loss=0.3044, pruned_loss=0.1001, ctc_loss=0.2051, over 967182.88 frames. ], batch size: 19, lr: 1.82e-02,
2024-10-08 06:24:50,395 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.min_positive, batch_count=9719.333333333334, ans=0.025
2024-10-08 06:25:35,879 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.470e+01 1.304e+02 1.463e+02 1.622e+02 2.748e+02, threshold=2.926e+02, percent-clipped=0.0
2024-10-08 06:25:36,041 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=9732.666666666666, ans=0.008753768115942029
2024-10-08 06:25:45,582 INFO [train.py:1153] Epoch 5, batch 1800, loss[loss=0.2296, simple_loss=0.2698, pruned_loss=0.06745, ctc_loss=0.1361, over 4872.00 frames. ], tot_loss[loss=0.293, simple_loss=0.3043, pruned_loss=0.09993, ctc_loss=0.2046, over 967936.48 frames. ], batch size: 23, lr: 1.82e-02,
2024-10-08 06:25:48,463 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.25 vs. limit=11.151
2024-10-08 06:25:50,625 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.min_abs, batch_count=9736.0, ans=0.34604
2024-10-08 06:25:54,376 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 06:25:59,886 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=3.30 vs. limit=7.895733333333334
2024-10-08 06:26:06,653 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=9739.333333333334, ans=0.20260666666666666
2024-10-08 06:26:14,711 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=4.54 vs. limit=7.8970666666666665
2024-10-08 06:26:33,551 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=9749.333333333334, ans=0.125
2024-10-08 06:26:37,340 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=9749.333333333334, ans=0.5587733333333333
2024-10-08 06:26:39,649 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=9749.333333333334, ans=0.125
2024-10-08 06:26:39,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=9749.333333333334, ans=0.026044444444444443
2024-10-08 06:26:40,888 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=9749.333333333334, ans=0.09899494936611666
2024-10-08 06:26:46,690 INFO [train.py:1153] Epoch 5, batch 1850, loss[loss=0.3008, simple_loss=0.3041, pruned_loss=0.1039, ctc_loss=0.2242, over 4712.00 frames. ], tot_loss[loss=0.2938, simple_loss=0.305, pruned_loss=0.1003, ctc_loss=0.2048, over 968067.77 frames. ], batch size: 26, lr: 1.82e-02,
2024-10-08 06:27:05,136 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=9756.0, ans=0.34634
2024-10-08 06:27:16,150 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=9759.333333333334, ans=0.125
2024-10-08 06:27:33,134 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.prob, batch_count=9762.666666666666, ans=0.125
2024-10-08 06:27:37,776 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.317e+01 1.309e+02 1.437e+02 1.617e+02 2.548e+02, threshold=2.874e+02, percent-clipped=0.0
2024-10-08 06:27:39,153 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=9766.0, ans=0.125
2024-10-08 06:27:47,537 INFO [train.py:1153] Epoch 5, batch 1900, loss[loss=0.3173, simple_loss=0.3284, pruned_loss=0.1054, ctc_loss=0.2387, over 4793.00 frames. ], tot_loss[loss=0.2946, simple_loss=0.305, pruned_loss=0.1009, ctc_loss=0.2058, over 967890.41 frames. ], batch size: 29, lr: 1.82e-02,
2024-10-08 06:27:51,106 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.8.prob, batch_count=9769.333333333334, ans=0.125
2024-10-08 06:27:53,643 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=9769.333333333334, ans=0.125
2024-10-08 06:27:53,656 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=9769.333333333334, ans=0.125
2024-10-08 06:27:56,127 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=9769.333333333334, ans=0.025
2024-10-08 06:28:07,247 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=9772.666666666666, ans=0.025
2024-10-08 06:28:19,517 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=4.43 vs. limit=7.444
2024-10-08 06:28:32,697 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=9779.333333333334, ans=0.125
2024-10-08 06:28:41,794 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.27 vs. limit=11.1685
2024-10-08 06:28:48,283 INFO [train.py:1153] Epoch 5, batch 1950, loss[loss=0.3282, simple_loss=0.3307, pruned_loss=0.1192, ctc_loss=0.2183, over 4856.00 frames. ], tot_loss[loss=0.2957, simple_loss=0.3061, pruned_loss=0.1012, ctc_loss=0.207, over 966771.07 frames. ], batch size: 20, lr: 1.82e-02,
2024-10-08 06:28:54,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=9786.0, ans=0.0
2024-10-08 06:29:01,151 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=9789.333333333334, ans=0.125
2024-10-08 06:29:05,843 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=9789.333333333334, ans=0.125
2024-10-08 06:29:09,343 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=9789.333333333334, ans=0.125
2024-10-08 06:29:13,386 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=14.37 vs. limit=14.8445
2024-10-08 06:29:18,785 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=9792.666666666666, ans=0.025
2024-10-08 06:29:37,250 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.86 vs. limit=11.17475
2024-10-08 06:29:39,070 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.037e+02 1.343e+02 1.454e+02 1.613e+02 2.399e+02, threshold=2.908e+02, percent-clipped=0.0
2024-10-08 06:29:39,241 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=9799.333333333334, ans=0.125
2024-10-08 06:29:48,811 INFO [train.py:1153] Epoch 5, batch 2000, loss[loss=0.2489, simple_loss=0.2755, pruned_loss=0.07655, ctc_loss=0.173, over 4959.00 frames. ], tot_loss[loss=0.2962, simple_loss=0.306, pruned_loss=0.1016, ctc_loss=0.2076, over 966600.73 frames. ], batch size: 19, lr: 1.82e-02,
2024-10-08 06:29:59,761 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=9806.0, ans=0.125
2024-10-08 06:30:05,768 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=9806.0, ans=0.125
2024-10-08 06:30:06,574 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=10.11 vs. limit=9.902999999999999
2024-10-08 06:30:07,840 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.23 vs. limit=4.4709
2024-10-08 06:30:09,387 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=9806.0, ans=0.20194
2024-10-08 06:30:14,707 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=4.20 vs. limit=7.452333333333334
2024-10-08 06:30:16,855 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=9809.333333333334, ans=0.5566733333333334
2024-10-08 06:30:27,778 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=9812.666666666666, ans=0.5565566666666668
2024-10-08 06:30:49,231 INFO [train.py:1153] Epoch 5, batch 2050, loss[loss=0.2602, simple_loss=0.266, pruned_loss=0.08967, ctc_loss=0.1877, over 4911.00 frames. ], tot_loss[loss=0.2959, simple_loss=0.3057, pruned_loss=0.1015, ctc_loss=0.2075, over 966992.77 frames. ], batch size: 19, lr: 1.81e-02,
2024-10-08 06:30:52,373 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.52 vs. limit=4.4729
2024-10-08 06:30:56,701 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=9819.333333333334, ans=0.125
2024-10-08 06:31:13,824 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=9826.0, ans=0.34739
2024-10-08 06:31:15,038 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=9826.0, ans=10.0
2024-10-08 06:31:20,911 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=9826.0, ans=0.125
2024-10-08 06:31:40,222 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.039e+02 1.334e+02 1.491e+02 1.678e+02 2.196e+02, threshold=2.982e+02, percent-clipped=0.0
2024-10-08 06:31:41,758 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9832.666666666666, ans=0.20167333333333332
2024-10-08 06:31:50,236 INFO [train.py:1153] Epoch 5, batch 2100, loss[loss=0.2937, simple_loss=0.3059, pruned_loss=0.1015, ctc_loss=0.196, over 4848.00 frames. ], tot_loss[loss=0.2957, simple_loss=0.3059, pruned_loss=0.1013, ctc_loss=0.2072, over 967189.79 frames. ], batch size: 21, lr: 1.81e-02,
2024-10-08 06:31:52,657 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=9836.0, ans=0.109017
2024-10-08 06:32:02,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=9839.333333333334, ans=0.5556233333333334
2024-10-08 06:32:03,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=9839.333333333334, ans=0.125
2024-10-08 06:32:15,941 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=9842.666666666666, ans=0.5555066666666667
2024-10-08 06:32:30,520 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=9846.0, ans=0.125
2024-10-08 06:32:40,377 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=9849.333333333334, ans=0.025
2024-10-08 06:32:40,552 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 06:32:41,635 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=9849.333333333334, ans=0.125
2024-10-08 06:32:51,317 INFO [train.py:1153] Epoch 5, batch 2150, loss[loss=0.2355, simple_loss=0.2732, pruned_loss=0.07174, ctc_loss=0.1358, over 4857.00 frames. ], tot_loss[loss=0.2946, simple_loss=0.3056, pruned_loss=0.1006, ctc_loss=0.2061, over 967989.38 frames. ], batch size: 20, lr: 1.81e-02,
2024-10-08 06:32:51,485 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=9852.666666666666, ans=0.125
2024-10-08 06:32:53,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=9852.666666666666, ans=0.125
2024-10-08 06:32:58,777 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=9852.666666666666, ans=0.125
2024-10-08 06:33:02,765 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.16 vs. limit=11.196
2024-10-08 06:33:09,729 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=9856.0, ans=0.125
2024-10-08 06:33:13,358 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=9856.0, ans=0.125
2024-10-08 06:33:38,374 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=6.11 vs. limit=5.972533333333333
2024-10-08 06:33:42,342 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.763e+01 1.236e+02 1.417e+02 1.573e+02 2.144e+02, threshold=2.834e+02, percent-clipped=0.0
2024-10-08 06:33:51,924 INFO [train.py:1153] Epoch 5, batch 2200, loss[loss=0.2738, simple_loss=0.3032, pruned_loss=0.08381, ctc_loss=0.1917, over 4771.00 frames. ], tot_loss[loss=0.2942, simple_loss=0.3051, pruned_loss=0.1005, ctc_loss=0.2058, over 967731.18 frames. ], batch size: 26, lr: 1.81e-02,
2024-10-08 06:33:52,063 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=9869.333333333334, ans=0.125
2024-10-08 06:33:56,252 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=14.51 vs. limit=14.902000000000001
2024-10-08 06:33:58,146 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=9869.333333333334, ans=0.0
2024-10-08 06:34:18,671 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=9876.0, ans=0.125
2024-10-08 06:34:42,005 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=9882.666666666666, ans=0.025
2024-10-08 06:34:52,816 INFO [train.py:1153] Epoch 5, batch 2250, loss[loss=0.2635, simple_loss=0.281, pruned_loss=0.09242, ctc_loss=0.153, over 4875.00 frames. ], tot_loss[loss=0.2948, simple_loss=0.3059, pruned_loss=0.1008, ctc_loss=0.2054, over 967696.12 frames. ], batch size: 22, lr: 1.81e-02,
2024-10-08 06:34:54,134 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=9886.0, ans=0.34829
2024-10-08 06:34:54,141 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=9886.0, ans=0.008720434782608696
2024-10-08 06:35:09,119 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys.whitening_limit, batch_count=9889.333333333334, ans=4.4834
2024-10-08 06:35:11,119 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=9889.333333333334, ans=0.008719710144927537
2024-10-08 06:35:13,654 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=9889.333333333334, ans=0.025
2024-10-08 06:35:29,715 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=9896.0, ans=0.125
2024-10-08 06:35:37,000 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=9896.0, ans=0.025433333333333336
2024-10-08 06:35:43,971 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.076e+01 1.329e+02 1.489e+02 1.657e+02 2.652e+02, threshold=2.977e+02, percent-clipped=0.0
2024-10-08 06:35:53,845 INFO [train.py:1153] Epoch 5, batch 2300, loss[loss=0.2059, simple_loss=0.2527, pruned_loss=0.05498, ctc_loss=0.1229, over 4883.00 frames. ], tot_loss[loss=0.2931, simple_loss=0.3049, pruned_loss=0.09988, ctc_loss=0.2039, over 968347.95 frames. ], batch size: 19, lr: 1.81e-02,
2024-10-08 06:36:07,513 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=9906.0, ans=0.00871608695652174
2024-10-08 06:36:11,659 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.20 vs. limit=14.9295
2024-10-08 06:36:33,181 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=9912.666666666666, ans=0.125
2024-10-08 06:36:35,530 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=9912.666666666666, ans=0.5530566666666668
2024-10-08 06:36:39,278 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=9912.666666666666, ans=0.125
2024-10-08 06:36:55,003 INFO [train.py:1153] Epoch 5, batch 2350, loss[loss=0.281, simple_loss=0.302, pruned_loss=0.09278, ctc_loss=0.1862, over 4844.00 frames. ], tot_loss[loss=0.2923, simple_loss=0.3044, pruned_loss=0.09949, ctc_loss=0.2032, over 968402.15 frames. ], batch size: 23, lr: 1.81e-02,
2024-10-08 06:36:55,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=9919.333333333334, ans=0.125
2024-10-08 06:37:07,964 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.65 vs. limit=11.221
2024-10-08 06:37:08,600 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=9922.666666666666, ans=0.125
2024-10-08 06:37:10,952 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=9922.666666666666, ans=0.008712463768115943
2024-10-08 06:37:31,970 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.09 vs. limit=11.2235
2024-10-08 06:37:32,648 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.min_positive, batch_count=9929.333333333334, ans=0.15070666666666666
2024-10-08 06:37:38,051 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.02 vs. limit=9.964666666666666
2024-10-08 06:37:46,721 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.406e+01 1.291e+02 1.436e+02 1.530e+02 2.414e+02, threshold=2.871e+02, percent-clipped=0.0
2024-10-08 06:37:56,160 INFO [train.py:1153] Epoch 5, batch 2400, loss[loss=0.318, simple_loss=0.3303, pruned_loss=0.1109, ctc_loss=0.21, over 4750.00 frames. ], tot_loss[loss=0.2936, simple_loss=0.3051, pruned_loss=0.1001, ctc_loss=0.2045, over 967584.60 frames. ], batch size: 19, lr: 1.80e-02,
2024-10-08 06:38:46,085 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=9949.333333333334, ans=0.20050666666666667
2024-10-08 06:38:55,648 INFO [train.py:1153] Epoch 5, batch 2450, loss[loss=0.2723, simple_loss=0.2881, pruned_loss=0.08984, ctc_loss=0.1919, over 4885.00 frames. ], tot_loss[loss=0.2951, simple_loss=0.3059, pruned_loss=0.1009, ctc_loss=0.2064, over 966944.55 frames. ], batch size: 22, lr: 1.80e-02,
2024-10-08 06:39:02,926 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=9952.666666666666, ans=0.5516566666666667
2024-10-08 06:39:16,296 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=9956.0, ans=0.20044
2024-10-08 06:39:23,490 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=9959.333333333334, ans=0.00870449275362319
2024-10-08 06:39:46,010 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.013e+02 1.347e+02 1.474e+02 1.724e+02 2.292e+02, threshold=2.947e+02, percent-clipped=0.0
2024-10-08 06:39:55,835 INFO [train.py:1153] Epoch 5, batch 2500, loss[loss=0.2785, simple_loss=0.281, pruned_loss=0.09878, ctc_loss=0.1962, over 4727.00 frames. ], tot_loss[loss=0.2954, simple_loss=0.3059, pruned_loss=0.1011, ctc_loss=0.2067, over 966664.12 frames. ], batch size: 26, lr: 1.80e-02,
2024-10-08 06:40:07,077 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=9972.666666666666, ans=0.025113888888888894
2024-10-08 06:40:20,399 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=9976.0, ans=0.20024
2024-10-08 06:40:21,467 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=9976.0, ans=0.07
2024-10-08 06:40:23,899 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 06:40:30,869 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=3.86 vs. limit=5.0
2024-10-08 06:40:32,401 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=9979.333333333334, ans=0.07
2024-10-08 06:40:53,482 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.24 vs. limit=14.987
2024-10-08 06:40:55,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=9986.0, ans=0.125
2024-10-08 06:40:56,525 INFO [train.py:1153] Epoch 5, batch 2550, loss[loss=0.2791, simple_loss=0.2938, pruned_loss=0.09275, ctc_loss=0.1971, over 4959.00 frames. ], tot_loss[loss=0.2954, simple_loss=0.3061, pruned_loss=0.1011, ctc_loss=0.2064, over 966955.94 frames. ], batch size: 19, lr: 1.80e-02,
2024-10-08 06:41:05,641 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.84 vs. limit=14.9895
2024-10-08 06:41:20,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=9992.666666666666, ans=0.20007333333333333
2024-10-08 06:41:40,109 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=9996.0, ans=0.125
2024-10-08 06:41:47,271 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.953e+01 1.273e+02 1.434e+02 1.639e+02 2.447e+02, threshold=2.869e+02, percent-clipped=0.0
2024-10-08 06:41:56,924 INFO [train.py:1153] Epoch 5, batch 2600, loss[loss=0.229, simple_loss=0.2582, pruned_loss=0.06963, ctc_loss=0.1511, over 4860.00 frames. ], tot_loss[loss=0.295, simple_loss=0.3062, pruned_loss=0.1008, ctc_loss=0.2059, over 966357.51 frames. ], batch size: 20, lr: 1.80e-02,
2024-10-08 06:41:57,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=10002.666666666666, ans=0.5499066666666668
2024-10-08 06:42:01,910 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10002.666666666666, ans=0.19997333333333334
2024-10-08 06:42:01,914 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=10002.666666666666, ans=0.025
2024-10-08 06:42:04,335 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=10002.666666666666, ans=0.125
2024-10-08 06:42:09,058 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=10006.0, ans=0.008694347826086957
2024-10-08 06:42:12,644 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=10006.0, ans=0.125
2024-10-08 06:42:13,830 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=10006.0, ans=0.125
2024-10-08 06:42:19,831 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10009.333333333334, ans=0.19990666666666668
2024-10-08 06:42:21,072 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=10009.333333333334, ans=0.125
2024-10-08 06:42:57,279 INFO [train.py:1153] Epoch 5, batch 2650, loss[loss=0.3708, simple_loss=0.3463, pruned_loss=0.1399, ctc_loss=0.2889, over 4833.00 frames. ], tot_loss[loss=0.2964, simple_loss=0.3067, pruned_loss=0.1016, ctc_loss=0.2075, over 966014.43 frames. ], batch size: 38, lr: 1.80e-02,
2024-10-08 06:42:57,496 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=10019.333333333334, ans=0.024919444444444445
2024-10-08 06:43:07,604 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.68 vs. limit=15.0145
2024-10-08 06:43:44,695 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.15 vs. limit=11.26225
2024-10-08 06:43:47,798 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.621e+01 1.338e+02 1.505e+02 1.684e+02 2.688e+02, threshold=3.010e+02, percent-clipped=0.0
2024-10-08 06:43:52,839 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=10032.666666666666, ans=0.5488566666666668
2024-10-08 06:43:57,470 INFO [train.py:1153] Epoch 5, batch 2700, loss[loss=0.3208, simple_loss=0.3292, pruned_loss=0.1123, ctc_loss=0.22, over 4877.00 frames. ], tot_loss[loss=0.2966, simple_loss=0.3068, pruned_loss=0.1017, ctc_loss=0.2076, over 966188.02 frames. ], batch size: 28, lr: 1.80e-02,
2024-10-08 06:44:01,393 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=10036.0, ans=0.125
2024-10-08 06:44:03,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=10036.0, ans=0.125
2024-10-08 06:44:24,539 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=10042.666666666666, ans=0.125
2024-10-08 06:44:28,256 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=10042.666666666666, ans=0.19957333333333332
2024-10-08 06:44:33,413 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.89 vs. limit=11.26725
2024-10-08 06:44:36,633 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 06:44:58,548 INFO [train.py:1153] Epoch 5, batch 2750, loss[loss=0.3036, simple_loss=0.318, pruned_loss=0.1024, ctc_loss=0.2111, over 4801.00 frames. ], tot_loss[loss=0.294, simple_loss=0.3051, pruned_loss=0.1004, ctc_loss=0.2052, over 966945.19 frames. ], batch size: 19, lr: 1.79e-02,
2024-10-08 06:45:30,295 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=10059.333333333334, ans=0.125
2024-10-08 06:45:34,521 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.85 vs. limit=8.025066666666667
2024-10-08 06:45:50,865 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.135e+01 1.278e+02 1.443e+02 1.560e+02 2.391e+02, threshold=2.886e+02, percent-clipped=0.0
2024-10-08 06:45:50,983 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=10066.0, ans=0.125
2024-10-08 06:45:59,681 INFO [train.py:1153] Epoch 5, batch 2800, loss[loss=0.3428, simple_loss=0.3356, pruned_loss=0.1235, ctc_loss=0.2575, over 4782.00 frames. ], tot_loss[loss=0.2937, simple_loss=0.3046, pruned_loss=0.1003, ctc_loss=0.2054, over 966997.33 frames. ], batch size: 53, lr: 1.79e-02,
2024-10-08 06:46:05,685 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=10069.333333333334, ans=0.5475733333333334
2024-10-08 06:46:15,014 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=10072.666666666666, ans=0.025
2024-10-08 06:46:18,539 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=10072.666666666666, ans=0.125
2024-10-08 06:46:24,466 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=10076.0, ans=0.0
2024-10-08 06:46:38,713 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=10079.333333333334, ans=0.125
2024-10-08 06:46:42,353 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=10079.333333333334, ans=0.05
2024-10-08 06:46:43,657 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=2.525e-03
2024-10-08 06:46:46,354 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.17 vs. limit=4.5123999999999995
2024-10-08 06:46:59,168 INFO [train.py:1153] Epoch 5, batch 2850, loss[loss=0.2625, simple_loss=0.2958, pruned_loss=0.08037, ctc_loss=0.1713, over 4930.00 frames. ], tot_loss[loss=0.2959, simple_loss=0.306, pruned_loss=0.1013, ctc_loss=0.2079, over 966831.27 frames. ], batch size: 20, lr: 1.79e-02,
2024-10-08 06:47:11,310 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=10089.333333333334, ans=0.04949747468305833
2024-10-08 06:47:16,240 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=10089.333333333334, ans=0.5468733333333333
2024-10-08 06:47:25,692 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=10092.666666666666, ans=0.125
2024-10-08 06:47:32,999 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=10092.666666666666, ans=0.125
2024-10-08 06:47:49,597 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.013e+02 1.343e+02 1.478e+02 1.595e+02 2.909e+02, threshold=2.957e+02, percent-clipped=1.0
2024-10-08 06:47:59,239 INFO [train.py:1153] Epoch 5, batch 2900, loss[loss=0.2585, simple_loss=0.2816, pruned_loss=0.0817, ctc_loss=0.1798, over 4759.00 frames. ], tot_loss[loss=0.2952, simple_loss=0.3058, pruned_loss=0.1008, ctc_loss=0.2072, over 965955.50 frames. ], batch size: 20, lr: 1.79e-02,
2024-10-08 06:48:08,134 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=10102.666666666666, ans=0.024572222222222224
2024-10-08 06:48:10,574 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=10106.0, ans=0.0
2024-10-08 06:48:26,663 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=10109.333333333334, ans=0.024544444444444445
2024-10-08 06:48:29,038 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=10109.333333333334, ans=0.008671884057971015
2024-10-08 06:48:38,861 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=10112.666666666666, ans=0.125
2024-10-08 06:48:49,732 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=10116.0, ans=0.125
2024-10-08 06:48:53,411 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=10116.0, ans=0.125
2024-10-08 06:48:53,581 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 06:49:00,531 INFO [train.py:1153] Epoch 5, batch 2950, loss[loss=0.2411, simple_loss=0.2931, pruned_loss=0.06702, ctc_loss=0.1375, over 4797.00 frames. ], tot_loss[loss=0.2937, simple_loss=0.3051, pruned_loss=0.1001, ctc_loss=0.205, over 966541.58 frames. ], batch size: 19, lr: 1.79e-02,
2024-10-08 06:49:06,785 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=10119.333333333334, ans=0.125
2024-10-08 06:49:15,143 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=10122.666666666666, ans=0.5457066666666668
2024-10-08 06:49:19,312 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.11 vs. limit=15.091999999999999
2024-10-08 06:49:51,862 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.284e+01 1.267e+02 1.465e+02 1.621e+02 2.444e+02, threshold=2.930e+02, percent-clipped=0.0
2024-10-08 06:50:00,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=10136.0, ans=0.125
2024-10-08 06:50:01,667 INFO [train.py:1153] Epoch 5, batch 3000, loss[loss=0.3307, simple_loss=0.3372, pruned_loss=0.1148, ctc_loss=0.2367, over 4828.00 frames. ], tot_loss[loss=0.2922, simple_loss=0.3042, pruned_loss=0.09941, ctc_loss=0.2038, over 967193.04 frames. ], batch size: 21, lr: 1.79e-02,
2024-10-08 06:50:01,667 INFO [train.py:1176] Computing validation loss
2024-10-08 06:50:09,009 INFO [train.py:1185] Epoch 5, validation: loss=0.1915, simple_loss=0.2696, pruned_loss=0.04043, ctc_loss=0.08119, over 90464.00 frames.
2024-10-08 06:50:09,009 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 06:50:21,222 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=10139.333333333334, ans=0.125
2024-10-08 06:50:24,915 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=10139.333333333334, ans=0.5451233333333334
2024-10-08 06:50:28,516 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer_ff3.min_abs, batch_count=10139.333333333334, ans=0.2
2024-10-08 06:50:30,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=10139.333333333334, ans=0.00866536231884058
2024-10-08 06:50:32,562 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.46 vs. limit=15.107
2024-10-08 06:50:36,384 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.44 vs. limit=15.107
2024-10-08 06:51:09,813 INFO [train.py:1153] Epoch 5, batch 3050, loss[loss=0.2505, simple_loss=0.2836, pruned_loss=0.07646, ctc_loss=0.1614, over 4751.00 frames. ], tot_loss[loss=0.2942, simple_loss=0.3055, pruned_loss=0.1004, ctc_loss=0.2056, over 966691.06 frames. ], batch size: 19, lr: 1.79e-02,
2024-10-08 06:51:22,017 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=10156.0, ans=0.125
2024-10-08 06:51:23,271 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=10156.0, ans=0.02435
2024-10-08 06:51:41,536 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=10159.333333333334, ans=0.008661014492753622
2024-10-08 06:51:58,393 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=10166.0, ans=0.125
2024-10-08 06:52:00,674 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.197e+01 1.322e+02 1.484e+02 1.606e+02 2.973e+02, threshold=2.968e+02, percent-clipped=1.0
2024-10-08 06:52:00,833 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=10166.0, ans=0.125
2024-10-08 06:52:06,856 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10166.0, ans=0.19834000000000002
2024-10-08 06:52:08,055 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=10166.0, ans=0.54419
2024-10-08 06:52:10,248 INFO [train.py:1153] Epoch 5, batch 3100, loss[loss=0.3359, simple_loss=0.331, pruned_loss=0.1222, ctc_loss=0.2411, over 4843.00 frames. ], tot_loss[loss=0.2929, simple_loss=0.3047, pruned_loss=0.09968, ctc_loss=0.2045, over 966476.26 frames. ], batch size: 38, lr: 1.78e-02,
2024-10-08 06:52:55,972 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.64 vs. limit=15.1345
2024-10-08 06:53:07,953 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=10182.666666666666, ans=0.125
2024-10-08 06:53:11,319 INFO [train.py:1153] Epoch 5, batch 3150, loss[loss=0.311, simple_loss=0.2984, pruned_loss=0.1159, ctc_loss=0.2297, over 4779.00 frames. ], tot_loss[loss=0.2919, simple_loss=0.3039, pruned_loss=0.09921, ctc_loss=0.2037, over 966695.35 frames. ], batch size: 40, lr: 1.78e-02,
2024-10-08 06:53:19,072 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.02 vs. limit=10.093
2024-10-08 06:53:21,264 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=10186.0, ans=0.025
2024-10-08 06:53:55,264 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=10196.0, ans=0.54314
2024-10-08 06:53:56,829 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=3.68 vs. limit=8.0784
2024-10-08 06:54:00,104 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.prob, batch_count=10199.333333333334, ans=0.125
2024-10-08 06:54:02,508 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.015e+02 1.219e+02 1.364e+02 1.514e+02 2.210e+02, threshold=2.727e+02, percent-clipped=0.0
2024-10-08 06:54:12,100 INFO [train.py:1153] Epoch 5, batch 3200, loss[loss=0.2755, simple_loss=0.2856, pruned_loss=0.09309, ctc_loss=0.1983, over 4750.00 frames. ], tot_loss[loss=0.2913, simple_loss=0.3036, pruned_loss=0.09891, ctc_loss=0.2028, over 967311.93 frames. ], batch size: 20, lr: 1.78e-02,
2024-10-08 06:54:13,424 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=10202.666666666666, ans=0.125
2024-10-08 06:55:12,634 INFO [train.py:1153] Epoch 5, batch 3250, loss[loss=0.3111, simple_loss=0.3145, pruned_loss=0.1124, ctc_loss=0.2073, over 4849.00 frames. ], tot_loss[loss=0.2913, simple_loss=0.3037, pruned_loss=0.09902, ctc_loss=0.2022, over 967368.35 frames. ], batch size: 24, lr: 1.78e-02,
2024-10-08 06:55:18,837 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=10219.333333333334, ans=0.0
2024-10-08 06:55:21,387 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=10219.333333333334, ans=0.05
2024-10-08 06:55:21,433 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10219.333333333334, ans=0.19780666666666666
2024-10-08 06:55:34,763 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=10222.666666666666, ans=0.0
2024-10-08 06:55:37,658 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.72 vs. limit=15.1695
2024-10-08 06:55:47,379 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.26 vs. limit=15.1695
2024-10-08 06:55:49,779 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.07 vs. limit=15.172
2024-10-08 06:56:03,782 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.039e+01 1.321e+02 1.438e+02 1.570e+02 2.191e+02, threshold=2.875e+02, percent-clipped=0.0
2024-10-08 06:56:13,592 INFO [train.py:1153] Epoch 5, batch 3300, loss[loss=0.3089, simple_loss=0.3117, pruned_loss=0.1115, ctc_loss=0.2077, over 4836.00 frames. ], tot_loss[loss=0.2893, simple_loss=0.3023, pruned_loss=0.09812, ctc_loss=0.2003, over 967763.48 frames. ], batch size: 43, lr: 1.78e-02,
2024-10-08 06:56:19,703 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=10236.0, ans=0.125
2024-10-08 06:56:23,444 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=10236.0, ans=0.125
2024-10-08 06:56:55,025 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=10246.0, ans=0.19754
2024-10-08 06:56:56,274 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=10246.0, ans=0.125
2024-10-08 06:57:01,114 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=10249.333333333334, ans=0.02396111111111111
2024-10-08 06:57:13,069 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10252.666666666666, ans=0.19747333333333333
2024-10-08 06:57:14,141 INFO [train.py:1153] Epoch 5, batch 3350, loss[loss=0.2968, simple_loss=0.3041, pruned_loss=0.1013, ctc_loss=0.2169, over 4783.00 frames. ], tot_loss[loss=0.2908, simple_loss=0.3026, pruned_loss=0.0992, ctc_loss=0.2018, over 967037.70 frames. ], batch size: 40, lr: 1.78e-02,
2024-10-08 06:57:19,039 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=10252.666666666666, ans=0.023947222222222224
2024-10-08 06:57:35,493 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.04 vs. limit=11.346
2024-10-08 06:57:50,680 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=10262.666666666666, ans=0.125
2024-10-08 06:58:05,185 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.768e+01 1.320e+02 1.480e+02 1.597e+02 2.212e+02, threshold=2.960e+02, percent-clipped=0.0
2024-10-08 06:58:10,217 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=10266.0, ans=0.125
2024-10-08 06:58:12,014 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.97 vs. limit=7.5665
2024-10-08 06:58:13,843 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=10269.333333333334, ans=0.02387777777777778
2024-10-08 06:58:14,963 INFO [train.py:1153] Epoch 5, batch 3400, loss[loss=0.2444, simple_loss=0.2792, pruned_loss=0.07713, ctc_loss=0.1384, over 4959.00 frames. ], tot_loss[loss=0.2915, simple_loss=0.303, pruned_loss=0.09938, ctc_loss=0.203, over 966913.67 frames. ], batch size: 19, lr: 1.78e-02,
2024-10-08 06:58:38,200 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=10276.0, ans=0.125
2024-10-08 06:58:41,483 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=3.48 vs. limit=8.1104
2024-10-08 06:58:55,517 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=10279.333333333334, ans=0.008634927536231884
2024-10-08 06:59:10,108 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=10282.666666666666, ans=0.125
2024-10-08 06:59:13,826 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=10282.666666666666, ans=0.023822222222222227
2024-10-08 06:59:16,008 INFO [train.py:1153] Epoch 5, batch 3450, loss[loss=0.3009, simple_loss=0.3076, pruned_loss=0.1024, ctc_loss=0.2235, over 4828.00 frames. ], tot_loss[loss=0.2928, simple_loss=0.3041, pruned_loss=0.09986, ctc_loss=0.2044, over 967062.09 frames. ], batch size: 43, lr: 1.77e-02,
2024-10-08 06:59:30,486 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=10289.333333333334, ans=0.5398733333333334
2024-10-08 06:59:42,416 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10292.666666666666, ans=0.19707333333333335
2024-10-08 06:59:47,332 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=10292.666666666666, ans=0.07
2024-10-08 06:59:54,366 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=10296.0, ans=0.125
2024-10-08 06:59:59,359 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=10296.0, ans=0.125
2024-10-08 07:00:06,579 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.166e+01 1.258e+02 1.432e+02 1.614e+02 2.417e+02, threshold=2.865e+02, percent-clipped=0.0
2024-10-08 07:00:08,107 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=10299.333333333334, ans=0.125
2024-10-08 07:00:15,766 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.84 vs. limit=15.227
2024-10-08 07:00:16,549 INFO [train.py:1153] Epoch 5, batch 3500, loss[loss=0.2616, simple_loss=0.2892, pruned_loss=0.08325, ctc_loss=0.1689, over 4883.00 frames. ], tot_loss[loss=0.2914, simple_loss=0.303, pruned_loss=0.09923, ctc_loss=0.2031, over 967424.90 frames. ], batch size: 19, lr: 1.77e-02,
2024-10-08 07:00:19,450 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.42 vs. limit=11.3635
2024-10-08 07:00:38,518 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=4.316e-02
2024-10-08 07:00:59,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=10312.666666666666, ans=0.00862768115942029
2024-10-08 07:01:07,310 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=10316.0, ans=0.023683333333333334
2024-10-08 07:01:10,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=10316.0, ans=0.53894
2024-10-08 07:01:14,641 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=5.450e-02
2024-10-08 07:01:16,742 INFO [train.py:1153] Epoch 5, batch 3550, loss[loss=0.3417, simple_loss=0.3253, pruned_loss=0.1264, ctc_loss=0.2634, over 4771.00 frames. ], tot_loss[loss=0.2888, simple_loss=0.3016, pruned_loss=0.09791, ctc_loss=0.2006, over 967383.51 frames. ], batch size: 29, lr: 1.77e-02,
2024-10-08 07:02:07,157 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.137e+01 1.279e+02 1.440e+02 1.624e+02 2.818e+02, threshold=2.880e+02, percent-clipped=0.0
2024-10-08 07:02:07,524 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 07:02:12,279 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=10332.666666666666, ans=0.5383566666666667
2024-10-08 07:02:16,563 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=14.95 vs. limit=15.251999999999999
2024-10-08 07:02:16,909 INFO [train.py:1153] Epoch 5, batch 3600, loss[loss=0.2852, simple_loss=0.2902, pruned_loss=0.0979, ctc_loss=0.211, over 4938.00 frames. ], tot_loss[loss=0.2887, simple_loss=0.3016, pruned_loss=0.09781, ctc_loss=0.2004, over 967481.02 frames. ], batch size: 20, lr: 1.77e-02,
2024-10-08 07:02:19,484 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=10336.0, ans=0.125
2024-10-08 07:02:19,558 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=10336.0, ans=0.53824
2024-10-08 07:02:41,938 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.95 vs. limit=7.585666666666667
2024-10-08 07:02:47,569 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=10342.666666666666, ans=0.125
2024-10-08 07:02:51,189 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=10342.666666666666, ans=0.023572222222222227
2024-10-08 07:02:54,938 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=10346.0, ans=0.008620434782608696
2024-10-08 07:03:11,936 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10349.333333333334, ans=0.19650666666666666
2024-10-08 07:03:17,714 INFO [train.py:1153] Epoch 5, batch 3650, loss[loss=0.3043, simple_loss=0.3098, pruned_loss=0.1081, ctc_loss=0.2066, over 4879.00 frames. ], tot_loss[loss=0.2886, simple_loss=0.3014, pruned_loss=0.09785, ctc_loss=0.2004, over 967958.29 frames. ], batch size: 31, lr: 1.77e-02,
2024-10-08 07:03:28,097 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.34 vs. limit=4.5529
2024-10-08 07:04:08,772 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.956e+01 1.279e+02 1.425e+02 1.572e+02 2.943e+02, threshold=2.850e+02, percent-clipped=1.0
2024-10-08 07:04:14,305 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.21 vs. limit=11.38725
2024-10-08 07:04:17,331 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=10369.333333333334, ans=0.19630666666666663
2024-10-08 07:04:18,534 INFO [train.py:1153] Epoch 5, batch 3700, loss[loss=0.2694, simple_loss=0.2909, pruned_loss=0.08907, ctc_loss=0.1742, over 4844.00 frames. ], tot_loss[loss=0.2873, simple_loss=0.301, pruned_loss=0.09697, ctc_loss=0.1992, over 967353.81 frames. ], batch size: 24, lr: 1.77e-02,
2024-10-08 07:04:54,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=10379.333333333334, ans=0.0
2024-10-08 07:05:02,716 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10379.333333333334, ans=0.19620666666666664
2024-10-08 07:05:06,748 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.00 vs. limit=15.286999999999999
2024-10-08 07:05:07,570 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=10382.666666666666, ans=0.008612463768115943
2024-10-08 07:05:12,858 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.55 vs. limit=15.286999999999999
2024-10-08 07:05:17,265 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=10382.666666666666, ans=0.125
2024-10-08 07:05:19,717 INFO [train.py:1153] Epoch 5, batch 3750, loss[loss=0.3435, simple_loss=0.3379, pruned_loss=0.1261, ctc_loss=0.2424, over 4959.00 frames. ], tot_loss[loss=0.2878, simple_loss=0.301, pruned_loss=0.09745, ctc_loss=0.1994, over 967661.90 frames. ], batch size: 19, lr: 1.77e-02,
2024-10-08 07:05:43,083 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=10392.666666666666, ans=0.125
2024-10-08 07:05:44,310 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=10392.666666666666, ans=0.125
2024-10-08 07:05:53,302 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.17 vs. limit=7.598166666666666
2024-10-08 07:06:11,137 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.196e+01 1.317e+02 1.439e+02 1.556e+02 2.114e+02, threshold=2.879e+02, percent-clipped=0.0
2024-10-08 07:06:18,469 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=10399.333333333334, ans=0.008608840579710146
2024-10-08 07:06:20,773 INFO [train.py:1153] Epoch 5, batch 3800, loss[loss=0.3084, simple_loss=0.3128, pruned_loss=0.107, ctc_loss=0.2251, over 4748.00 frames. ], tot_loss[loss=0.2882, simple_loss=0.3017, pruned_loss=0.09746, ctc_loss=0.1994, over 967415.33 frames. ], batch size: 26, lr: 1.77e-02,
2024-10-08 07:06:20,945 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=10402.666666666666, ans=0.19597333333333333
2024-10-08 07:06:53,897 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=10409.333333333334, ans=0.125
2024-10-08 07:06:54,543 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.78 vs. limit=15.307
2024-10-08 07:07:00,403 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.03 vs. limit=7.603166666666667
2024-10-08 07:07:01,163 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=10412.666666666666, ans=0.025
2024-10-08 07:07:10,098 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.49 vs. limit=11.406
2024-10-08 07:07:14,693 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=10416.0, ans=0.125
2024-10-08 07:07:17,753 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten.whitening_limit, batch_count=10416.0, ans=11.406
2024-10-08 07:07:21,787 INFO [train.py:1153] Epoch 5, batch 3850, loss[loss=0.2962, simple_loss=0.303, pruned_loss=0.103, ctc_loss=0.2083, over 4820.00 frames. ], tot_loss[loss=0.2874, simple_loss=0.3012, pruned_loss=0.09707, ctc_loss=0.1984, over 967459.87 frames. ], batch size: 38, lr: 1.76e-02,
2024-10-08 07:07:30,479 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=10419.333333333334, ans=0.125
2024-10-08 07:07:47,493 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=10426.0, ans=0.125
2024-10-08 07:08:00,490 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer_na.min_abs, batch_count=10429.333333333334, ans=0.02
2024-10-08 07:08:12,085 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.000e+02 1.303e+02 1.460e+02 1.675e+02 2.298e+02, threshold=2.920e+02, percent-clipped=0.0
2024-10-08 07:08:12,232 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=10432.666666666666, ans=0.125
2024-10-08 07:08:21,654 INFO [train.py:1153] Epoch 5, batch 3900, loss[loss=0.254, simple_loss=0.2928, pruned_loss=0.07518, ctc_loss=0.162, over 4748.00 frames. ], tot_loss[loss=0.2892, simple_loss=0.3026, pruned_loss=0.09788, ctc_loss=0.2003, over 966946.03 frames. ], batch size: 26, lr: 1.76e-02,
2024-10-08 07:08:30,485 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=10.73 vs. limit=11.413499999999999
2024-10-08 07:08:38,730 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.61 vs. limit=4.5659
2024-10-08 07:08:43,376 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.15 vs. limit=11.41475
2024-10-08 07:09:03,788 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=10446.0, ans=0.125
2024-10-08 07:09:09,933 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=10449.333333333334, ans=0.125
2024-10-08 07:09:09,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=10449.333333333334, ans=0.5342733333333334
2024-10-08 07:09:13,628 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=10449.333333333334, ans=0.0
2024-10-08 07:09:21,872 INFO [train.py:1153] Epoch 5, batch 3950, loss[loss=0.3353, simple_loss=0.3346, pruned_loss=0.1222, ctc_loss=0.2296, over 4827.00 frames. ], tot_loss[loss=0.2864, simple_loss=0.3012, pruned_loss=0.09638, ctc_loss=0.1974, over 967138.74 frames. ], batch size: 36, lr: 1.76e-02,
2024-10-08 07:09:42,600 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=10456.0, ans=0.5340400000000001
2024-10-08 07:09:48,739 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=10459.333333333334, ans=0.125
2024-10-08 07:09:48,739 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=10459.333333333334, ans=0.008595797101449275
2024-10-08 07:09:57,693 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.77 vs. limit=7.615666666666666
2024-10-08 07:09:58,924 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten.whitening_limit, batch_count=10462.666666666666, ans=11.4235
2024-10-08 07:10:02,428 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.20 vs. limit=15.347000000000001
2024-10-08 07:10:11,998 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=10466.0, ans=0.125
2024-10-08 07:10:13,009 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.308e+01 1.244e+02 1.403e+02 1.544e+02 3.164e+02, threshold=2.806e+02, percent-clipped=1.0
2024-10-08 07:10:22,562 INFO [train.py:1153] Epoch 5, batch 4000, loss[loss=0.3031, simple_loss=0.3231, pruned_loss=0.1003, ctc_loss=0.206, over 4814.00 frames. ], tot_loss[loss=0.2872, simple_loss=0.3015, pruned_loss=0.09675, ctc_loss=0.1986, over 967113.19 frames. ], batch size: 19, lr: 1.76e-02,
2024-10-08 07:10:32,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=10469.333333333334, ans=0.125
2024-10-08 07:10:37,322 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=10472.666666666666, ans=0.008592898550724638
2024-10-08 07:10:54,357 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=10476.0, ans=0.125
2024-10-08 07:10:57,190 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.84 vs. limit=15.357
2024-10-08 07:11:04,104 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=10479.333333333334, ans=0.023002777777777778
2024-10-08 07:11:06,483 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=10479.333333333334, ans=0.125
2024-10-08 07:11:10,282 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=10482.666666666666, ans=0.125
2024-10-08 07:11:23,467 INFO [train.py:1153] Epoch 5, batch 4050, loss[loss=0.3803, simple_loss=0.3632, pruned_loss=0.1405, ctc_loss=0.2912, over 4805.00 frames. ], tot_loss[loss=0.2862, simple_loss=0.3006, pruned_loss=0.0962, ctc_loss=0.1983, over 967412.66 frames. ], batch size: 53, lr: 1.76e-02,
2024-10-08 07:11:51,411 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=10492.666666666666, ans=0.04949747468305833
2024-10-08 07:11:56,219 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=10492.666666666666, ans=0.125
2024-10-08 07:12:03,435 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=10496.0, ans=0.125
2024-10-08 07:12:11,855 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=10499.333333333334, ans=0.19500666666666666
2024-10-08 07:12:14,115 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.326e+01 1.274e+02 1.444e+02 1.651e+02 2.696e+02, threshold=2.889e+02, percent-clipped=0.0
2024-10-08 07:12:18,034 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=10499.333333333334, ans=0.022919444444444444
2024-10-08 07:12:22,848 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=10502.666666666666, ans=0.008586376811594203
2024-10-08 07:12:23,924 INFO [train.py:1153] Epoch 5, batch 4100, loss[loss=0.3349, simple_loss=0.328, pruned_loss=0.1224, ctc_loss=0.2426, over 4845.00 frames. ], tot_loss[loss=0.2871, simple_loss=0.3013, pruned_loss=0.09664, ctc_loss=0.1993, over 967006.92 frames. ], batch size: 31, lr: 1.76e-02,
2024-10-08 07:12:32,450 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10502.666666666666, ans=0.19497333333333333
2024-10-08 07:12:34,256 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=12.44 vs. limit=11.4385
2024-10-08 07:12:37,274 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=10506.0, ans=0.008585652173913043
2024-10-08 07:13:19,596 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=10516.0, ans=0.125
2024-10-08 07:13:23,267 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=10519.333333333334, ans=0.09899494936611666
2024-10-08 07:13:24,301 INFO [train.py:1153] Epoch 5, batch 4150, loss[loss=0.2519, simple_loss=0.2914, pruned_loss=0.07127, ctc_loss=0.1748, over 4754.00 frames. ], tot_loss[loss=0.2868, simple_loss=0.3009, pruned_loss=0.09652, ctc_loss=0.1992, over 967187.78 frames. ], batch size: 20, lr: 1.76e-02,
2024-10-08 07:13:25,623 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=10519.333333333334, ans=0.008582753623188406
2024-10-08 07:13:30,502 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=10519.333333333334, ans=0.0
2024-10-08 07:13:31,795 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=10519.333333333334, ans=0.022836111111111113
2024-10-08 07:13:34,219 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=10519.333333333334, ans=0.09899494936611666
2024-10-08 07:13:35,430 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10522.666666666666, ans=0.19477333333333335
2024-10-08 07:13:39,617 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten.whitening_limit, batch_count=10522.666666666666, ans=15.392
2024-10-08 07:14:10,462 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=10529.333333333334, ans=0.125
2024-10-08 07:14:15,645 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.117e+01 1.218e+02 1.362e+02 1.568e+02 2.220e+02, threshold=2.725e+02, percent-clipped=0.0
2024-10-08 07:14:19,445 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.82 vs. limit=7.633166666666666
2024-10-08 07:14:24,923 INFO [train.py:1153] Epoch 5, batch 4200, loss[loss=0.2906, simple_loss=0.2946, pruned_loss=0.09917, ctc_loss=0.2204, over 4842.00 frames. ], tot_loss[loss=0.2864, simple_loss=0.3004, pruned_loss=0.0964, ctc_loss=0.1988, over 967276.65 frames. ], batch size: 31, lr: 1.75e-02,
2024-10-08 07:14:34,470 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10536.0, ans=0.19463999999999998
2024-10-08 07:14:34,503 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10536.0, ans=0.19463999999999998
2024-10-08 07:14:44,016 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=10539.333333333334, ans=0.125
2024-10-08 07:14:57,770 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=20.68 vs. limit=15.407
2024-10-08 07:15:09,396 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=10546.0, ans=0.07
2024-10-08 07:15:24,822 INFO [train.py:1153] Epoch 5, batch 4250, loss[loss=0.2192, simple_loss=0.2571, pruned_loss=0.06521, ctc_loss=0.1273, over 4752.00 frames. ], tot_loss[loss=0.2856, simple_loss=0.2999, pruned_loss=0.09603, ctc_loss=0.198, over 967061.26 frames. ], batch size: 19, lr: 1.75e-02,
2024-10-08 07:15:42,998 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=10556.0, ans=0.125
2024-10-08 07:16:05,676 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.68 vs. limit=11.461
2024-10-08 07:16:15,898 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.206e+01 1.259e+02 1.390e+02 1.546e+02 2.315e+02, threshold=2.780e+02, percent-clipped=0.0
2024-10-08 07:16:17,250 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=10566.0, ans=0.125
2024-10-08 07:16:19,675 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=10566.0, ans=0.125
2024-10-08 07:16:22,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.49 vs. limit=11.462250000000001
2024-10-08 07:16:25,617 INFO [train.py:1153] Epoch 5, batch 4300, loss[loss=0.2412, simple_loss=0.2712, pruned_loss=0.07516, ctc_loss=0.1522, over 4845.00 frames. ], tot_loss[loss=0.2871, simple_loss=0.3013, pruned_loss=0.09663, ctc_loss=0.1991, over 967368.92 frames. ], batch size: 21, lr: 1.75e-02,
2024-10-08 07:16:36,470 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=10572.666666666666, ans=0.125
2024-10-08 07:16:55,494 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=10576.0, ans=0.022600000000000002
2024-10-08 07:17:00,255 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=10579.333333333334, ans=0.125
2024-10-08 07:17:01,294 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=10579.333333333334, ans=0.125
2024-10-08 07:17:05,127 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=10579.333333333334, ans=0.125
2024-10-08 07:17:07,384 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=10579.333333333334, ans=0.10214116666666667
2024-10-08 07:17:07,564 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=10579.333333333334, ans=0.8557933333333333
2024-10-08 07:17:13,494 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=10582.666666666666, ans=0.022572222222222226
2024-10-08 07:17:24,654 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=28.45 vs. limit=11.46975
2024-10-08 07:17:25,260 INFO [train.py:1153] Epoch 5, batch 4350, loss[loss=0.3066, simple_loss=0.3417, pruned_loss=0.0952, ctc_loss=0.203, over 4850.00 frames. ], tot_loss[loss=0.2875, simple_loss=0.3014, pruned_loss=0.09685, ctc_loss=0.1995, over 966276.84 frames. ], batch size: 21, lr: 1.75e-02,
2024-10-08 07:17:40,127 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.05 vs. limit=11.471
2024-10-08 07:18:06,210 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=10596.0, ans=0.0
2024-10-08 07:18:15,834 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.883e+01 1.270e+02 1.435e+02 1.581e+02 3.107e+02, threshold=2.871e+02, percent-clipped=1.0
2024-10-08 07:18:15,965 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=10599.333333333334, ans=0.125
2024-10-08 07:18:17,196 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=10599.333333333334, ans=0.022502777777777778
2024-10-08 07:18:25,456 INFO [train.py:1153] Epoch 5, batch 4400, loss[loss=0.2528, simple_loss=0.2853, pruned_loss=0.07717, ctc_loss=0.165, over 4735.00 frames. ], tot_loss[loss=0.2879, simple_loss=0.3018, pruned_loss=0.09702, ctc_loss=0.1996, over 965847.69 frames. ], batch size: 26, lr: 1.75e-02,
2024-10-08 07:18:39,938 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10606.0, ans=0.19394
2024-10-08 07:19:03,239 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.28 vs. limit=11.47975
2024-10-08 07:19:04,059 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=10612.666666666666, ans=0.125
2024-10-08 07:19:04,634 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten.whitening_limit, batch_count=10612.666666666666, ans=11.47975
2024-10-08 07:19:10,159 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=10612.666666666666, ans=0.125
2024-10-08 07:19:21,234 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=10616.0, ans=0.008561739130434783
2024-10-08 07:19:21,294 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=10616.0, ans=0.125
2024-10-08 07:19:24,234 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=5.59 vs. limit=11.481
2024-10-08 07:19:26,036 INFO [train.py:1153] Epoch 5, batch 4450, loss[loss=0.279, simple_loss=0.2959, pruned_loss=0.0938, ctc_loss=0.1861, over 4883.00 frames. ], tot_loss[loss=0.2907, simple_loss=0.3035, pruned_loss=0.09846, ctc_loss=0.2026, over 966186.83 frames. ], batch size: 19, lr: 1.75e-02,
2024-10-08 07:19:29,790 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10619.333333333334, ans=0.19380666666666665
2024-10-08 07:19:46,843 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=10622.666666666666, ans=0.5282066666666667
2024-10-08 07:20:03,836 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=10629.333333333334, ans=0.022377777777777778
2024-10-08 07:20:04,120 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.56 vs. limit=10.314666666666668
2024-10-08 07:20:16,888 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.046e+01 1.292e+02 1.482e+02 1.639e+02 2.406e+02, threshold=2.964e+02, percent-clipped=0.0
2024-10-08 07:20:18,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=10632.666666666666, ans=0.125
2024-10-08 07:20:24,580 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10632.666666666666, ans=0.19367333333333336
2024-10-08 07:20:26,759 INFO [train.py:1153] Epoch 5, batch 4500, loss[loss=0.3332, simple_loss=0.3224, pruned_loss=0.1226, ctc_loss=0.2474, over 4856.00 frames. ], tot_loss[loss=0.2898, simple_loss=0.303, pruned_loss=0.09792, ctc_loss=0.202, over 966043.26 frames. ], batch size: 28, lr: 1.75e-02,
2024-10-08 07:20:32,905 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=1.414e-02
2024-10-08 07:20:43,448 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=10639.333333333334, ans=0.125
2024-10-08 07:20:54,320 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=10642.666666666666, ans=0.5275066666666668
2024-10-08 07:21:06,015 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=10646.0, ans=0.025
2024-10-08 07:21:19,206 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=10649.333333333334, ans=0.022294444444444443
2024-10-08 07:21:26,229 INFO [train.py:1153] Epoch 5, batch 4550, loss[loss=0.2633, simple_loss=0.2996, pruned_loss=0.07811, ctc_loss=0.1772, over 4856.00 frames. ], tot_loss[loss=0.2881, simple_loss=0.3019, pruned_loss=0.09702, ctc_loss=0.2005, over 965929.37 frames. ], batch size: 20, lr: 1.75e-02,
2024-10-08 07:21:31,393 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 07:21:38,415 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=10656.0, ans=0.125
2024-10-08 07:21:47,555 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.32 vs. limit=7.664
2024-10-08 07:22:16,313 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/checkpoint-32000.pt
2024-10-08 07:22:18,233 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.572e+01 1.287e+02 1.404e+02 1.576e+02 2.170e+02, threshold=2.808e+02, percent-clipped=0.0
2024-10-08 07:22:19,278 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 07:22:21,444 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=10666.0, ans=0.5266900000000001
2024-10-08 07:22:25,486 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.24 vs. limit=15.499500000000001
2024-10-08 07:22:27,304 INFO [train.py:1153] Epoch 5, batch 4600, loss[loss=0.2893, simple_loss=0.292, pruned_loss=0.1009, ctc_loss=0.2116, over 4744.00 frames. ], tot_loss[loss=0.2861, simple_loss=0.3006, pruned_loss=0.09605, ctc_loss=0.1986, over 966235.90 frames. ], batch size: 45, lr: 1.74e-02,
2024-10-08 07:22:47,772 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=10672.666666666666, ans=0.125
2024-10-08 07:22:48,935 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10672.666666666666, ans=0.19327333333333335
2024-10-08 07:22:54,844 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=10676.0, ans=0.025
2024-10-08 07:22:56,644 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=14.40 vs. limit=15.507
2024-10-08 07:23:13,905 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=10682.666666666666, ans=0.125
2024-10-08 07:23:26,815 INFO [train.py:1153] Epoch 5, batch 4650, loss[loss=0.2775, simple_loss=0.2987, pruned_loss=0.09032, ctc_loss=0.189, over 4813.00 frames. ], tot_loss[loss=0.2856, simple_loss=0.3003, pruned_loss=0.09579, ctc_loss=0.1983, over 965610.09 frames. ], batch size: 36, lr: 1.74e-02,
2024-10-08 07:23:27,579 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.88 vs. limit=8.2744
2024-10-08 07:23:43,894 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=10689.333333333334, ans=0.125
2024-10-08 07:23:46,921 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.68 vs. limit=15.517
2024-10-08 07:23:50,099 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys.whitening_limit, batch_count=10692.666666666666, ans=4.6039
2024-10-08 07:24:09,262 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=10696.0, ans=0.5256400000000001
2024-10-08 07:24:10,956 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.36 vs. limit=7.6739999999999995
2024-10-08 07:24:16,375 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=10699.333333333334, ans=0.022086111111111112
2024-10-08 07:24:17,332 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.004e+02 1.289e+02 1.438e+02 1.598e+02 2.100e+02, threshold=2.876e+02, percent-clipped=0.0
2024-10-08 07:24:27,146 INFO [train.py:1153] Epoch 5, batch 4700, loss[loss=0.2446, simple_loss=0.2726, pruned_loss=0.07757, ctc_loss=0.1537, over 4940.00 frames. ], tot_loss[loss=0.2866, simple_loss=0.3011, pruned_loss=0.09621, ctc_loss=0.199, over 965588.22 frames. ], batch size: 19, lr: 1.74e-02,
2024-10-08 07:24:28,448 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=10702.666666666666, ans=0.025
2024-10-08 07:25:01,182 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=10709.333333333334, ans=0.00854144927536232
2024-10-08 07:25:20,752 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=10716.0, ans=0.19284
2024-10-08 07:25:23,092 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.max_positive, batch_count=10716.0, ans=0.8571599999999999
2024-10-08 07:25:24,311 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=10716.0, ans=0.52494
2024-10-08 07:25:27,668 INFO [train.py:1153] Epoch 5, batch 4750, loss[loss=0.2713, simple_loss=0.2919, pruned_loss=0.08598, ctc_loss=0.1969, over 4758.00 frames. ], tot_loss[loss=0.2879, simple_loss=0.3019, pruned_loss=0.09704, ctc_loss=0.1999, over 965460.25 frames. ], batch size: 45, lr: 1.74e-02,
2024-10-08 07:25:29,111 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=10719.333333333334, ans=0.125
2024-10-08 07:25:31,604 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=10719.333333333334, ans=0.125
2024-10-08 07:25:38,980 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=10722.666666666666, ans=0.5247066666666667
2024-10-08 07:25:39,797 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module2.whiten, num_groups=1, num_channels=192, metric=4.88 vs. limit=11.521
2024-10-08 07:25:47,395 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=10722.666666666666, ans=0.125
2024-10-08 07:25:50,997 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=10726.0, ans=0.125
2024-10-08 07:26:01,819 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=10726.0, ans=0.025
2024-10-08 07:26:12,987 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.14 vs. limit=15.547
2024-10-08 07:26:18,287 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.110e+01 1.207e+02 1.356e+02 1.516e+02 2.146e+02, threshold=2.712e+02, percent-clipped=0.0
2024-10-08 07:26:20,282 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=7.83 vs. limit=11.524750000000001
2024-10-08 07:26:27,610 INFO [train.py:1153] Epoch 5, batch 4800, loss[loss=0.3075, simple_loss=0.3288, pruned_loss=0.1039, ctc_loss=0.1962, over 4880.00 frames. ], tot_loss[loss=0.2875, simple_loss=0.3017, pruned_loss=0.0969, ctc_loss=0.199, over 965768.44 frames. ], batch size: 22, lr: 1.74e-02,
2024-10-08 07:27:11,705 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.64 vs. limit=15.5595
2024-10-08 07:27:26,015 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=16.02 vs. limit=15.562
2024-10-08 07:27:27,993 INFO [train.py:1153] Epoch 5, batch 4850, loss[loss=0.3063, simple_loss=0.3102, pruned_loss=0.1084, ctc_loss=0.2138, over 4831.00 frames. ], tot_loss[loss=0.2875, simple_loss=0.3018, pruned_loss=0.09695, ctc_loss=0.1981, over 966407.89 frames. ], batch size: 28, lr: 1.74e-02,
2024-10-08 07:27:45,802 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.56 vs. limit=11.5335
2024-10-08 07:27:58,613 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=10759.333333333334, ans=0.125
2024-10-08 07:28:13,056 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=10762.666666666666, ans=0.021822222222222225
2024-10-08 07:28:14,681 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.27 vs. limit=4.6144
2024-10-08 07:28:19,087 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.552e+01 1.319e+02 1.490e+02 1.686e+02 2.215e+02, threshold=2.980e+02, percent-clipped=0.0
2024-10-08 07:28:28,867 INFO [train.py:1153] Epoch 5, batch 4900, loss[loss=0.3129, simple_loss=0.3334, pruned_loss=0.104, ctc_loss=0.2113, over 4833.00 frames. ], tot_loss[loss=0.2887, simple_loss=0.3029, pruned_loss=0.09745, ctc_loss=0.1992, over 967086.61 frames. ], batch size: 21, lr: 1.74e-02,
2024-10-08 07:28:29,565 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.56 vs. limit=15.577
2024-10-08 07:28:53,249 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=10776.0, ans=0.125
2024-10-08 07:29:07,820 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=10779.333333333334, ans=0.125
2024-10-08 07:29:18,968 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=10782.666666666666, ans=0.125
2024-10-08 07:29:28,600 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=10786.0, ans=0.025
2024-10-08 07:29:29,601 INFO [train.py:1153] Epoch 5, batch 4950, loss[loss=0.3574, simple_loss=0.3488, pruned_loss=0.1292, ctc_loss=0.2688, over 4793.00 frames. ], tot_loss[loss=0.29, simple_loss=0.3035, pruned_loss=0.09809, ctc_loss=0.2006, over 966811.77 frames. ], batch size: 53, lr: 1.74e-02,
2024-10-08 07:29:35,017 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.50 vs. limit=10.393
2024-10-08 07:29:37,355 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=10786.0, ans=0.008524782608695653
2024-10-08 07:29:43,300 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=10789.333333333334, ans=0.10019866666666669
2024-10-08 07:29:45,907 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=10789.333333333334, ans=0.125
2024-10-08 07:29:58,246 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=10792.666666666666, ans=0.025
2024-10-08 07:30:08,108 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=10796.0, ans=0.125
2024-10-08 07:30:09,798 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.43 vs. limit=11.5485
2024-10-08 07:30:21,469 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.117e+01 1.270e+02 1.420e+02 1.580e+02 6.384e+02, threshold=2.841e+02, percent-clipped=2.0
2024-10-08 07:30:31,079 INFO [train.py:1153] Epoch 5, batch 5000, loss[loss=0.2444, simple_loss=0.2775, pruned_loss=0.07718, ctc_loss=0.1426, over 4786.00 frames. ], tot_loss[loss=0.2886, simple_loss=0.3026, pruned_loss=0.09741, ctc_loss=0.1994, over 967711.15 frames. ], batch size: 29, lr: 1.73e-02,
2024-10-08 07:30:31,222 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=10802.666666666666, ans=0.02165555555555556
2024-10-08 07:30:47,704 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=9.76 vs. limit=11.55225
2024-10-08 07:30:52,977 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer_ff3.min_abs, batch_count=10806.0, ans=0.2
2024-10-08 07:31:02,758 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10809.333333333334, ans=0.19190666666666667
2024-10-08 07:31:13,495 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=10812.666666666666, ans=0.125
2024-10-08 07:31:14,812 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=10812.666666666666, ans=0.125
2024-10-08 07:31:25,011 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.66 vs. limit=15.612
2024-10-08 07:31:31,543 INFO [train.py:1153] Epoch 5, batch 5050, loss[loss=0.3011, simple_loss=0.3199, pruned_loss=0.1005, ctc_loss=0.203, over 4855.00 frames. ], tot_loss[loss=0.2861, simple_loss=0.3006, pruned_loss=0.09629, ctc_loss=0.1974, over 968638.12 frames. ], batch size: 19, lr: 1.73e-02,
2024-10-08 07:31:45,690 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=10822.666666666666, ans=0.125
2024-10-08 07:31:51,620 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=10822.666666666666, ans=0.5212066666666667
2024-10-08 07:31:55,579 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten.whitening_limit, batch_count=10826.0, ans=11.559750000000001
2024-10-08 07:31:57,340 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=10826.0, ans=0.00851608695652174
2024-10-08 07:32:20,298 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=10832.666666666666, ans=0.02153055555555556
2024-10-08 07:32:21,275 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.009e+01 1.265e+02 1.447e+02 1.662e+02 2.330e+02, threshold=2.894e+02, percent-clipped=0.0
2024-10-08 07:32:21,467 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=10832.666666666666, ans=0.125
2024-10-08 07:32:25,456 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.86 vs. limit=8.333066666666667
2024-10-08 07:32:31,020 INFO [train.py:1153] Epoch 5, batch 5100, loss[loss=0.2498, simple_loss=0.2699, pruned_loss=0.07921, ctc_loss=0.178, over 4815.00 frames. ], tot_loss[loss=0.2884, simple_loss=0.3019, pruned_loss=0.09752, ctc_loss=0.1996, over 967886.19 frames. ], batch size: 19, lr: 1.73e-02,
2024-10-08 07:32:37,592 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.58 vs. limit=10.418
2024-10-08 07:32:45,725 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=10839.333333333334, ans=0.125
2024-10-08 07:33:05,721 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.75 vs. limit=7.710666666666667
2024-10-08 07:33:14,890 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=10846.0, ans=0.19154
2024-10-08 07:33:21,305 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=2.45 vs. limit=11.5685
2024-10-08 07:33:29,369 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=10849.333333333334, ans=0.125
2024-10-08 07:33:31,647 INFO [train.py:1153] Epoch 5, batch 5150, loss[loss=0.3238, simple_loss=0.3275, pruned_loss=0.115, ctc_loss=0.2253, over 4816.00 frames. ], tot_loss[loss=0.2877, simple_loss=0.3017, pruned_loss=0.09702, ctc_loss=0.1991, over 967929.06 frames. ], batch size: 36, lr: 1.73e-02,
2024-10-08 07:33:33,192 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 07:33:33,327 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.78 vs. limit=11.569749999999999
2024-10-08 07:33:35,357 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=10852.666666666666, ans=0.125
2024-10-08 07:33:44,186 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.70 vs. limit=11.571
2024-10-08 07:33:47,258 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=10856.0, ans=0.0
2024-10-08 07:33:57,007 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=10859.333333333334, ans=0.07
2024-10-08 07:34:03,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10859.333333333334, ans=0.19140666666666667
2024-10-08 07:34:03,651 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.56 vs. limit=4.6289
2024-10-08 07:34:12,735 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=10862.666666666666, ans=0.02140555555555556
2024-10-08 07:34:22,368 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.002e+02 1.233e+02 1.390e+02 1.541e+02 2.500e+02, threshold=2.781e+02, percent-clipped=0.0
2024-10-08 07:34:32,081 INFO [train.py:1153] Epoch 5, batch 5200, loss[loss=0.3331, simple_loss=0.3311, pruned_loss=0.117, ctc_loss=0.2529, over 4777.00 frames. ], tot_loss[loss=0.2875, simple_loss=0.3016, pruned_loss=0.0969, ctc_loss=0.1991, over 967559.85 frames. ], batch size: 29, lr: 1.73e-02,
2024-10-08 07:34:56,401 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=10876.0, ans=0.02135
2024-10-08 07:34:56,426 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=10876.0, ans=0.19124
2024-10-08 07:34:57,610 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=10876.0, ans=0.125
2024-10-08 07:35:08,702 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=10879.333333333334, ans=0.008504492753623189
2024-10-08 07:35:12,633 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.70 vs. limit=11.57975
2024-10-08 07:35:32,713 INFO [train.py:1153] Epoch 5, batch 5250, loss[loss=0.2292, simple_loss=0.2615, pruned_loss=0.06973, ctc_loss=0.1433, over 4863.00 frames. ], tot_loss[loss=0.2863, simple_loss=0.301, pruned_loss=0.09618, ctc_loss=0.1978, over 967655.21 frames. ], batch size: 20, lr: 1.73e-02,
2024-10-08 07:35:41,420 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=10886.0, ans=0.5189900000000001
2024-10-08 07:35:47,957 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.27 vs. limit=7.722333333333333
2024-10-08 07:36:24,126 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.074e+01 1.297e+02 1.535e+02 1.706e+02 2.953e+02, threshold=3.071e+02, percent-clipped=1.0
2024-10-08 07:36:30,158 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=10899.333333333334, ans=0.125
2024-10-08 07:36:33,721 INFO [train.py:1153] Epoch 5, batch 5300, loss[loss=0.288, simple_loss=0.2989, pruned_loss=0.09852, ctc_loss=0.2001, over 4825.00 frames. ], tot_loss[loss=0.2847, simple_loss=0.3, pruned_loss=0.09555, ctc_loss=0.196, over 967707.67 frames. ], batch size: 38, lr: 1.73e-02,
2024-10-08 07:36:48,567 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=10906.0, ans=0.51829
2024-10-08 07:37:00,437 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=10909.333333333334, ans=0.125
2024-10-08 07:37:02,067 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=14.50 vs. limit=11.591000000000001
2024-10-08 07:37:17,216 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=10912.666666666666, ans=0.125
2024-10-08 07:37:29,485 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=10916.0, ans=0.125
2024-10-08 07:37:31,914 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.min_positive, batch_count=10916.0, ans=0.025
2024-10-08 07:37:34,147 INFO [train.py:1153] Epoch 5, batch 5350, loss[loss=0.3097, simple_loss=0.3154, pruned_loss=0.1116, ctc_loss=0.202, over 4978.00 frames. ], tot_loss[loss=0.2873, simple_loss=0.3015, pruned_loss=0.09693, ctc_loss=0.1983, over 967177.66 frames. ], batch size: 19, lr: 1.73e-02,
2024-10-08 07:37:39,170 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=10919.333333333334, ans=0.0
2024-10-08 07:37:52,971 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.87 vs. limit=15.692
2024-10-08 07:38:04,803 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.69 vs. limit=15.6945
2024-10-08 07:38:20,564 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.93 vs. limit=15.697
2024-10-08 07:38:20,818 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.43 vs. limit=4.6394
2024-10-08 07:38:22,509 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=10932.666666666666, ans=0.021113888888888894
2024-10-08 07:38:24,825 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.203e+01 1.284e+02 1.454e+02 1.602e+02 2.249e+02, threshold=2.908e+02, percent-clipped=0.0
2024-10-08 07:38:34,322 INFO [train.py:1153] Epoch 5, batch 5400, loss[loss=0.3313, simple_loss=0.3321, pruned_loss=0.1167, ctc_loss=0.243, over 4785.00 frames. ], tot_loss[loss=0.2886, simple_loss=0.3021, pruned_loss=0.09764, ctc_loss=0.1998, over 966318.24 frames. ], batch size: 49, lr: 1.72e-02,
2024-10-08 07:38:36,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=10936.0, ans=0.51724
2024-10-08 07:38:49,092 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=10939.333333333334, ans=0.125
2024-10-08 07:38:51,552 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=10939.333333333334, ans=0.008491449275362318
2024-10-08 07:38:55,298 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=10939.333333333334, ans=0.125
2024-10-08 07:39:03,708 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=10942.666666666666, ans=0.19057333333333334
2024-10-08 07:39:31,522 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=10949.333333333334, ans=0.125
2024-10-08 07:39:35,040 INFO [train.py:1153] Epoch 5, batch 5450, loss[loss=0.249, simple_loss=0.2848, pruned_loss=0.0764, ctc_loss=0.1511, over 4940.00 frames. ], tot_loss[loss=0.2874, simple_loss=0.3016, pruned_loss=0.09686, ctc_loss=0.1989, over 967015.37 frames. ], batch size: 19, lr: 1.72e-02,
2024-10-08 07:39:35,933 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=7.74 vs. limit=11.60725
2024-10-08 07:39:36,439 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=10952.666666666666, ans=0.5166566666666668
2024-10-08 07:40:19,045 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 07:40:25,134 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=10966.0, ans=0.51619
2024-10-08 07:40:26,169 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.243e+01 1.328e+02 1.448e+02 1.577e+02 2.055e+02, threshold=2.896e+02, percent-clipped=0.0
2024-10-08 07:40:26,389 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=10966.0, ans=0.025
2024-10-08 07:40:32,330 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=10966.0, ans=0.85966
2024-10-08 07:40:32,349 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=10966.0, ans=0.008485652173913043
2024-10-08 07:40:35,918 INFO [train.py:1153] Epoch 5, batch 5500, loss[loss=0.3539, simple_loss=0.3348, pruned_loss=0.136, ctc_loss=0.2525, over 4774.00 frames. ], tot_loss[loss=0.288, simple_loss=0.302, pruned_loss=0.09718, ctc_loss=0.1993, over 967352.84 frames. ], batch size: 49, lr: 1.72e-02,
2024-10-08 07:40:40,277 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.07 vs. limit=10.484666666666667
2024-10-08 07:40:42,017 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=10969.333333333334, ans=0.02096111111111111
2024-10-08 07:40:43,254 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=10969.333333333334, ans=0.125
2024-10-08 07:41:17,907 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=10979.333333333334, ans=15.7345
2024-10-08 07:41:28,453 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=10982.666666666666, ans=0.5156066666666668
2024-10-08 07:41:36,825 INFO [train.py:1153] Epoch 5, batch 5550, loss[loss=0.2953, simple_loss=0.3139, pruned_loss=0.09703, ctc_loss=0.2068, over 4800.00 frames. ], tot_loss[loss=0.2866, simple_loss=0.3014, pruned_loss=0.09639, ctc_loss=0.1977, over 967060.33 frames. ], batch size: 19, lr: 1.72e-02,
2024-10-08 07:41:40,552 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=10986.0, ans=0.0
2024-10-08 07:41:42,106 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.24 vs. limit=4.6479
2024-10-08 07:42:18,264 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=10996.0, ans=0.125
2024-10-08 07:42:21,874 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=10996.0, ans=0.125
2024-10-08 07:42:27,691 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.546e+01 1.241e+02 1.373e+02 1.585e+02 2.047e+02, threshold=2.746e+02, percent-clipped=0.0
2024-10-08 07:42:37,166 INFO [train.py:1153] Epoch 5, batch 5600, loss[loss=0.3401, simple_loss=0.3394, pruned_loss=0.1239, ctc_loss=0.2325, over 4848.00 frames. ], tot_loss[loss=0.2874, simple_loss=0.3014, pruned_loss=0.09701, ctc_loss=0.1987, over 966960.04 frames. ], batch size: 28, lr: 1.72e-02,
2024-10-08 07:42:39,742 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=11002.666666666666, ans=0.025
2024-10-08 07:42:42,135 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=11002.666666666666, ans=0.18997333333333333
2024-10-08 07:42:46,317 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.10 vs. limit=11.626000000000001
2024-10-08 07:42:56,871 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=11006.0, ans=0.020808333333333335
2024-10-08 07:43:02,871 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=11009.333333333334, ans=0.5146733333333333
2024-10-08 07:43:15,156 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11012.666666666666, ans=0.18987333333333334
2024-10-08 07:43:27,163 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=11016.0, ans=0.125
2024-10-08 07:43:27,668 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.92 vs. limit=15.762
2024-10-08 07:43:34,619 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11016.0, ans=0.18984
2024-10-08 07:43:37,922 INFO [train.py:1153] Epoch 5, batch 5650, loss[loss=0.3247, simple_loss=0.3304, pruned_loss=0.1145, ctc_loss=0.2246, over 4761.00 frames. ], tot_loss[loss=0.2862, simple_loss=0.3008, pruned_loss=0.09623, ctc_loss=0.1976, over 967045.54 frames. ], batch size: 45, lr: 1.72e-02,
2024-10-08 07:43:51,286 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=11022.666666666666, ans=0.18977333333333335
2024-10-08 07:43:57,206 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=11022.666666666666, ans=0.04949747468305833
2024-10-08 07:43:59,566 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=11022.666666666666, ans=0.0
2024-10-08 07:44:02,099 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=11026.0, ans=0.125
2024-10-08 07:44:06,742 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=11026.0, ans=0.025
2024-10-08 07:44:07,165 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.74 vs. limit=10.513
2024-10-08 07:44:09,185 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=11026.0, ans=0.020725
2024-10-08 07:44:13,915 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11029.333333333334, ans=0.18970666666666663
2024-10-08 07:44:25,767 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=9.49 vs. limit=11.63725
2024-10-08 07:44:28,360 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.814e+01 1.293e+02 1.400e+02 1.578e+02 3.196e+02, threshold=2.800e+02, percent-clipped=1.0
2024-10-08 07:44:37,090 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=11036.0, ans=0.020683333333333335
2024-10-08 07:44:38,159 INFO [train.py:1153] Epoch 5, batch 5700, loss[loss=0.2662, simple_loss=0.2827, pruned_loss=0.09007, ctc_loss=0.1737, over 4862.00 frames. ], tot_loss[loss=0.2851, simple_loss=0.2999, pruned_loss=0.09583, ctc_loss=0.1966, over 966522.90 frames. ], batch size: 22, lr: 1.72e-02,
2024-10-08 07:45:33,679 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=11049.333333333334, ans=0.18950666666666666
2024-10-08 07:45:33,832 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=11049.333333333334, ans=0.008467536231884058
2024-10-08 07:45:37,790 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=11052.666666666666, ans=0.125
2024-10-08 07:45:38,721 INFO [train.py:1153] Epoch 5, batch 5750, loss[loss=0.3181, simple_loss=0.3258, pruned_loss=0.1073, ctc_loss=0.2395, over 4856.00 frames. ], tot_loss[loss=0.2868, simple_loss=0.301, pruned_loss=0.09666, ctc_loss=0.1982, over 966824.21 frames. ], batch size: 43, lr: 1.72e-02,
2024-10-08 07:45:44,677 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11052.666666666666, ans=0.18947333333333333
2024-10-08 07:45:45,804 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.min_positive, batch_count=11052.666666666666, ans=0.025
2024-10-08 07:45:59,414 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=13.60 vs. limit=11.646
2024-10-08 07:46:13,539 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=11062.666666666666, ans=0.5128066666666667
2024-10-08 07:46:26,024 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=14.35 vs. limit=11.649750000000001
2024-10-08 07:46:28,693 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=2.47 vs. limit=11.649750000000001
2024-10-08 07:46:29,175 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.365e+01 1.259e+02 1.447e+02 1.590e+02 2.190e+02, threshold=2.895e+02, percent-clipped=0.0
2024-10-08 07:46:38,625 INFO [train.py:1153] Epoch 5, batch 5800, loss[loss=0.2955, simple_loss=0.3015, pruned_loss=0.1031, ctc_loss=0.2086, over 4819.00 frames. ], tot_loss[loss=0.2869, simple_loss=0.3012, pruned_loss=0.09674, ctc_loss=0.1982, over 966239.77 frames. ], batch size: 43, lr: 1.71e-02,
2024-10-08 07:46:47,842 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.34 vs. limit=11.651
2024-10-08 07:47:01,507 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=11076.0, ans=0.125
2024-10-08 07:47:29,908 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=11082.666666666666, ans=15.812
2024-10-08 07:47:38,863 INFO [train.py:1153] Epoch 5, batch 5850, loss[loss=0.3175, simple_loss=0.3186, pruned_loss=0.1131, ctc_loss=0.2251, over 4745.00 frames. ], tot_loss[loss=0.2874, simple_loss=0.3017, pruned_loss=0.09679, ctc_loss=0.1986, over 966573.55 frames. ], batch size: 45, lr: 1.71e-02,
2024-10-08 07:47:39,497 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.11 vs. limit=15.8145
2024-10-08 07:47:56,356 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.48 vs. limit=15.817
2024-10-08 07:47:57,579 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=5.36 vs. limit=11.6585
2024-10-08 07:48:00,737 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=11089.333333333334, ans=0.125
2024-10-08 07:48:08,036 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=11092.666666666666, ans=10.0
2024-10-08 07:48:14,077 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=11096.0, ans=0.125
2024-10-08 07:48:21,314 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=11096.0, ans=0.125
2024-10-08 07:48:22,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=11096.0, ans=0.125
2024-10-08 07:48:29,646 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.043e+01 1.236e+02 1.416e+02 1.558e+02 2.493e+02, threshold=2.832e+02, percent-clipped=0.0
2024-10-08 07:48:33,508 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=11099.333333333334, ans=0.125
2024-10-08 07:48:39,374 INFO [train.py:1153] Epoch 5, batch 5900, loss[loss=0.334, simple_loss=0.3272, pruned_loss=0.1195, ctc_loss=0.2545, over 4818.00 frames. ], tot_loss[loss=0.2855, simple_loss=0.3002, pruned_loss=0.09589, ctc_loss=0.1974, over 966685.35 frames. ], batch size: 34, lr: 1.71e-02,
2024-10-08 07:49:16,972 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=11112.666666666666, ans=0.125
2024-10-08 07:49:22,431 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.42 vs. limit=10.556333333333333
2024-10-08 07:49:39,939 INFO [train.py:1153] Epoch 5, batch 5950, loss[loss=0.3182, simple_loss=0.3216, pruned_loss=0.1117, ctc_loss=0.2286, over 4804.00 frames. ], tot_loss[loss=0.2831, simple_loss=0.2988, pruned_loss=0.09471, ctc_loss=0.1949, over 966139.14 frames. ], batch size: 34, lr: 1.71e-02,
2024-10-08 07:50:04,553 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=11126.0, ans=0.020308333333333334
2024-10-08 07:50:23,960 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=11129.333333333334, ans=0.125
2024-10-08 07:50:25,175 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=11129.333333333334, ans=0.125
2024-10-08 07:50:31,129 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.559e+01 1.197e+02 1.350e+02 1.490e+02 2.050e+02, threshold=2.700e+02, percent-clipped=0.0
2024-10-08 07:50:40,688 INFO [train.py:1153] Epoch 5, batch 6000, loss[loss=0.3584, simple_loss=0.351, pruned_loss=0.1321, ctc_loss=0.2541, over 4772.00 frames. ], tot_loss[loss=0.2839, simple_loss=0.2994, pruned_loss=0.09508, ctc_loss=0.1954, over 966715.72 frames. ], batch size: 49, lr: 1.71e-02,
2024-10-08 07:50:40,688 INFO [train.py:1176] Computing validation loss
2024-10-08 07:50:48,050 INFO [train.py:1185] Epoch 5, validation: loss=0.1859, simple_loss=0.2657, pruned_loss=0.03801, ctc_loss=0.0753, over 90464.00 frames.
2024-10-08 07:50:48,051 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 07:50:50,843 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=4.27 vs. limit=7.784
2024-10-08 07:50:57,688 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=11136.0, ans=0.008448695652173913
2024-10-08 07:51:01,334 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=11139.333333333334, ans=0.025
2024-10-08 07:51:21,970 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=11142.666666666666, ans=0.020238888888888893
2024-10-08 07:51:23,456 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=15.67 vs. limit=15.8595
2024-10-08 07:51:35,296 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=11149.333333333334, ans=0.09899494936611666
2024-10-08 07:51:35,363 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=11149.333333333334, ans=0.125
2024-10-08 07:51:41,859 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=12.65 vs. limit=11.681000000000001
2024-10-08 07:51:43,893 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11149.333333333334, ans=0.18850666666666666
2024-10-08 07:51:48,564 INFO [train.py:1153] Epoch 5, batch 6050, loss[loss=0.2012, simple_loss=0.2505, pruned_loss=0.05181, ctc_loss=0.1207, over 4817.00 frames. ], tot_loss[loss=0.2831, simple_loss=0.2992, pruned_loss=0.09463, ctc_loss=0.1944, over 966680.87 frames. ], batch size: 19, lr: 1.71e-02,
2024-10-08 07:52:03,273 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=11156.0, ans=0.125
2024-10-08 07:52:04,497 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=11156.0, ans=0.18844
2024-10-08 07:52:08,712 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.15 vs. limit=15.867
2024-10-08 07:52:23,859 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=11162.666666666666, ans=0.5093066666666668
2024-10-08 07:52:29,360 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.53 vs. limit=4.6744
2024-10-08 07:52:39,159 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.117e+01 1.234e+02 1.367e+02 1.508e+02 1.935e+02, threshold=2.734e+02, percent-clipped=0.0
2024-10-08 07:52:42,267 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.39 vs. limit=7.7915
2024-10-08 07:52:44,235 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.prob, batch_count=11166.0, ans=0.125
2024-10-08 07:52:48,905 INFO [train.py:1153] Epoch 5, batch 6100, loss[loss=0.3113, simple_loss=0.3124, pruned_loss=0.1097, ctc_loss=0.2272, over 4806.00 frames. ], tot_loss[loss=0.2839, simple_loss=0.2998, pruned_loss=0.09503, ctc_loss=0.1948, over 966341.20 frames. ], batch size: 34, lr: 1.71e-02,
2024-10-08 07:53:04,897 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=11172.666666666666, ans=0.020113888888888893
2024-10-08 07:53:17,184 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=11176.0, ans=0.00844
2024-10-08 07:53:49,951 INFO [train.py:1153] Epoch 5, batch 6150, loss[loss=0.336, simple_loss=0.3294, pruned_loss=0.1197, ctc_loss=0.2582, over 4835.00 frames. ], tot_loss[loss=0.283, simple_loss=0.2994, pruned_loss=0.09449, ctc_loss=0.194, over 966524.93 frames. ], batch size: 43, lr: 1.71e-02,
2024-10-08 07:54:00,109 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=11186.0, ans=0.125
2024-10-08 07:54:01,230 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=11189.333333333334, ans=0.36784000000000006
2024-10-08 07:54:04,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=11189.333333333334, ans=0.04949747468305833
2024-10-08 07:54:06,686 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.11 vs. limit=15.892
2024-10-08 07:54:24,048 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=11192.666666666666, ans=0.02003055555555556
2024-10-08 07:54:41,035 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.717e+01 1.190e+02 1.368e+02 1.521e+02 2.198e+02, threshold=2.736e+02, percent-clipped=0.0
2024-10-08 07:54:44,061 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.51 vs. limit=7.799833333333334
2024-10-08 07:54:50,600 INFO [train.py:1153] Epoch 5, batch 6200, loss[loss=0.2612, simple_loss=0.2816, pruned_loss=0.08068, ctc_loss=0.1986, over 4756.00 frames. ], tot_loss[loss=0.2824, simple_loss=0.2988, pruned_loss=0.09429, ctc_loss=0.1935, over 966695.89 frames. ], batch size: 29, lr: 1.70e-02,
2024-10-08 07:54:50,747 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=11202.666666666666, ans=0.125
2024-10-08 07:54:53,134 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=11202.666666666666, ans=0.025
2024-10-08 07:54:54,355 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=11202.666666666666, ans=0.125
2024-10-08 07:55:07,819 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=11206.0, ans=0.019975000000000003
2024-10-08 07:55:08,974 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=11206.0, ans=0.019975000000000003
2024-10-08 07:55:16,293 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=11209.333333333334, ans=0.008432753623188405
2024-10-08 07:55:17,500 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=11209.333333333334, ans=0.025
2024-10-08 07:55:37,029 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=11212.666666666666, ans=0.125
2024-10-08 07:55:42,657 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=17.00 vs. limit=15.912
2024-10-08 07:55:49,377 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=11216.0, ans=0.125
2024-10-08 07:55:51,647 INFO [train.py:1153] Epoch 5, batch 6250, loss[loss=0.3113, simple_loss=0.329, pruned_loss=0.103, ctc_loss=0.2192, over 4719.00 frames. ], tot_loss[loss=0.2808, simple_loss=0.2979, pruned_loss=0.09343, ctc_loss=0.1919, over 966988.83 frames. ], batch size: 26, lr: 1.70e-02,
2024-10-08 07:55:56,607 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=11219.333333333334, ans=0.008430579710144928
2024-10-08 07:56:23,292 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=11226.0, ans=0.18774000000000002
2024-10-08 07:56:25,038 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.81 vs. limit=15.9195
2024-10-08 07:56:29,208 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=11229.333333333334, ans=0.04949747468305833
2024-10-08 07:56:32,122 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=5.67 vs. limit=8.491733333333332
2024-10-08 07:56:32,753 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=11229.333333333334, ans=0.125
2024-10-08 07:56:42,164 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.022e+02 1.277e+02 1.414e+02 1.570e+02 2.209e+02, threshold=2.828e+02, percent-clipped=0.0
2024-10-08 07:56:51,965 INFO [train.py:1153] Epoch 5, batch 6300, loss[loss=0.3432, simple_loss=0.335, pruned_loss=0.1274, ctc_loss=0.2414, over 4978.00 frames. ], tot_loss[loss=0.2825, simple_loss=0.299, pruned_loss=0.09437, ctc_loss=0.1929, over 966697.35 frames. ], batch size: 19, lr: 1.70e-02,
2024-10-08 07:56:58,863 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.88 vs. limit=11.7135
2024-10-08 07:57:21,981 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.84 vs. limit=7.810666666666666
2024-10-08 07:57:37,864 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.15 vs. limit=7.8115000000000006
2024-10-08 07:57:40,769 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=11249.333333333334, ans=0.125
2024-10-08 07:57:49,939 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=15.94 vs. limit=15.937
2024-10-08 07:57:52,870 INFO [train.py:1153] Epoch 5, batch 6350, loss[loss=0.3376, simple_loss=0.3353, pruned_loss=0.1206, ctc_loss=0.2467, over 4814.00 frames. ], tot_loss[loss=0.2821, simple_loss=0.2989, pruned_loss=0.09416, ctc_loss=0.1927, over 966377.00 frames. ], batch size: 36, lr: 1.70e-02,
2024-10-08 07:57:58,026 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=11252.666666666666, ans=0.5061566666666668
2024-10-08 07:58:01,154 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=3.69 vs. limit=5.0
2024-10-08 07:58:12,406 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer2.prob, batch_count=11256.0, ans=0.125
2024-10-08 07:58:15,341 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=5.58 vs. limit=11.721
2024-10-08 07:58:18,422 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=11259.333333333334, ans=0.008421884057971014
2024-10-08 07:58:35,931 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.61 vs. limit=15.947
2024-10-08 07:58:43,889 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.923e+01 1.280e+02 1.406e+02 1.599e+02 3.636e+02, threshold=2.813e+02, percent-clipped=1.0
2024-10-08 07:58:44,029 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=11266.0, ans=0.019725000000000003
2024-10-08 07:58:51,221 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=11266.0, ans=0.125
2024-10-08 07:58:53,287 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.99 vs. limit=7.817333333333334
2024-10-08 07:58:53,656 INFO [train.py:1153] Epoch 5, batch 6400, loss[loss=0.232, simple_loss=0.2753, pruned_loss=0.06611, ctc_loss=0.141, over 4870.00 frames. ], tot_loss[loss=0.28, simple_loss=0.2977, pruned_loss=0.09297, ctc_loss=0.1909, over 966060.27 frames. ], batch size: 23, lr: 1.70e-02,
2024-10-08 07:59:10,830 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=11272.666666666666, ans=0.5054566666666667
2024-10-08 07:59:51,971 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=11282.666666666666, ans=0.5051066666666667
2024-10-08 07:59:54,275 INFO [train.py:1153] Epoch 5, batch 6450, loss[loss=0.2481, simple_loss=0.2768, pruned_loss=0.07827, ctc_loss=0.1571, over 4738.00 frames. ], tot_loss[loss=0.2797, simple_loss=0.2975, pruned_loss=0.09297, ctc_loss=0.19, over 965414.42 frames. ], batch size: 26, lr: 1.70e-02,
2024-10-08 07:59:59,935 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=9.86 vs. limit=10.643
2024-10-08 08:00:01,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=11286.0, ans=0.125
2024-10-08 08:00:05,462 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=11289.333333333334, ans=0.125
2024-10-08 08:00:08,969 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=11289.333333333334, ans=0.125
2024-10-08 08:00:13,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=11289.333333333334, ans=0.01962777777777778
2024-10-08 08:00:38,124 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.min_positive, batch_count=11296.0, ans=0.025
2024-10-08 08:00:44,250 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=11299.333333333334, ans=0.125
2024-10-08 08:00:45,313 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.533e+01 1.236e+02 1.376e+02 1.523e+02 2.101e+02, threshold=2.752e+02, percent-clipped=0.0
2024-10-08 08:00:55,183 INFO [train.py:1153] Epoch 5, batch 6500, loss[loss=0.2576, simple_loss=0.2896, pruned_loss=0.08054, ctc_loss=0.1615, over 4744.00 frames. ], tot_loss[loss=0.2776, simple_loss=0.2967, pruned_loss=0.09173, ctc_loss=0.1878, over 965068.74 frames. ], batch size: 26, lr: 1.70e-02,
2024-10-08 08:00:57,765 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 08:01:01,339 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=11302.666666666666, ans=0.019572222222222227
2024-10-08 08:01:12,208 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=11306.0, ans=0.125
2024-10-08 08:01:15,848 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=11306.0, ans=0.125
2024-10-08 08:01:40,293 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=11312.666666666666, ans=0.035
2024-10-08 08:01:40,412 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=11312.666666666666, ans=0.019530555555555558
2024-10-08 08:01:52,533 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=11316.0, ans=0.125
2024-10-08 08:01:56,073 INFO [train.py:1153] Epoch 5, batch 6550, loss[loss=0.267, simple_loss=0.2886, pruned_loss=0.08784, ctc_loss=0.1743, over 4978.00 frames. ], tot_loss[loss=0.2767, simple_loss=0.2963, pruned_loss=0.09118, ctc_loss=0.1871, over 964717.63 frames. ], batch size: 19, lr: 1.70e-02,
2024-10-08 08:02:12,710 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.38 vs. limit=11.746
2024-10-08 08:02:16,919 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=11322.666666666666, ans=0.0
2024-10-08 08:02:23,040 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11326.0, ans=0.18674000000000002
2024-10-08 08:02:47,526 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.586e+01 1.292e+02 1.407e+02 1.584e+02 2.314e+02, threshold=2.813e+02, percent-clipped=0.0
2024-10-08 08:02:48,151 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.89 vs. limit=15.9995
2024-10-08 08:02:56,701 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.43 vs. limit=11.751000000000001
2024-10-08 08:02:57,321 INFO [train.py:1153] Epoch 5, batch 6600, loss[loss=0.336, simple_loss=0.3198, pruned_loss=0.1234, ctc_loss=0.2637, over 4864.00 frames. ], tot_loss[loss=0.2755, simple_loss=0.295, pruned_loss=0.09076, ctc_loss=0.1866, over 965430.86 frames. ], batch size: 23, lr: 1.69e-02,
2024-10-08 08:03:02,441 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=11336.0, ans=0.125
2024-10-08 08:03:10,197 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.88 vs. limit=16.0045
2024-10-08 08:03:24,497 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=11342.666666666666, ans=0.125
2024-10-08 08:03:26,935 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=11342.666666666666, ans=0.019405555555555558
2024-10-08 08:03:32,359 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=18.24 vs. limit=16.006999999999998
2024-10-08 08:03:39,312 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=11346.0, ans=0.125
2024-10-08 08:03:41,907 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=11346.0, ans=0.0
2024-10-08 08:03:58,927 INFO [train.py:1153] Epoch 5, batch 6650, loss[loss=0.2477, simple_loss=0.279, pruned_loss=0.07367, ctc_loss=0.1726, over 4748.00 frames. ], tot_loss[loss=0.2733, simple_loss=0.2938, pruned_loss=0.08954, ctc_loss=0.1842, over 967137.21 frames. ], batch size: 20, lr: 1.69e-02,
2024-10-08 08:04:05,262 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=11352.666666666666, ans=0.125
2024-10-08 08:04:11,442 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=11356.0, ans=0.125
2024-10-08 08:04:22,366 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=11359.333333333334, ans=0.125
2024-10-08 08:04:40,284 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.59 vs. limit=4.7044
2024-10-08 08:04:48,237 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=11366.0, ans=0.0
2024-10-08 08:04:49,428 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.min_positive, batch_count=11366.0, ans=0.025
2024-10-08 08:04:50,495 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.023e+01 1.250e+02 1.429e+02 1.595e+02 2.105e+02, threshold=2.857e+02, percent-clipped=0.0
2024-10-08 08:05:00,394 INFO [train.py:1153] Epoch 5, batch 6700, loss[loss=0.2813, simple_loss=0.3029, pruned_loss=0.09046, ctc_loss=0.1971, over 4935.00 frames. ], tot_loss[loss=0.2722, simple_loss=0.2936, pruned_loss=0.08886, ctc_loss=0.1829, over 969192.10 frames. ], batch size: 20, lr: 1.69e-02,
2024-10-08 08:05:24,818 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.76 vs. limit=7.843999999999999
2024-10-08 08:05:26,880 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11376.0, ans=0.18624000000000002
2024-10-08 08:05:29,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=11376.0, ans=0.125
2024-10-08 08:05:57,013 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.08 vs. limit=16.037
2024-10-08 08:05:59,299 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=11382.666666666666, ans=0.125
2024-10-08 08:06:00,458 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=11382.666666666666, ans=0.125
2024-10-08 08:06:02,839 INFO [train.py:1153] Epoch 5, batch 6750, loss[loss=0.248, simple_loss=0.2711, pruned_loss=0.08069, ctc_loss=0.1587, over 4909.00 frames. ], tot_loss[loss=0.2679, simple_loss=0.2904, pruned_loss=0.08695, ctc_loss=0.1785, over 972273.57 frames. ], batch size: 19, lr: 1.69e-02,
2024-10-08 08:06:05,663 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 08:06:20,522 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=11389.333333333334, ans=0.019211111111111113
2024-10-08 08:06:27,192 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.48 vs. limit=11.77225
2024-10-08 08:06:32,288 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.91 vs. limit=16.0445
2024-10-08 08:06:49,242 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=11396.0, ans=0.125
2024-10-08 08:06:52,871 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=11399.333333333334, ans=0.025
2024-10-08 08:06:55,342 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.020e+02 1.255e+02 1.352e+02 1.488e+02 2.468e+02, threshold=2.704e+02, percent-clipped=0.0
2024-10-08 08:07:05,183 INFO [train.py:1153] Epoch 5, batch 6800, loss[loss=0.2712, simple_loss=0.2945, pruned_loss=0.0894, ctc_loss=0.1729, over 4908.00 frames. ], tot_loss[loss=0.2648, simple_loss=0.2881, pruned_loss=0.08574, ctc_loss=0.1751, over 974593.25 frames. ], batch size: 19, lr: 1.69e-02,
2024-10-08 08:07:10,313 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=11402.666666666666, ans=0.025
2024-10-08 08:07:17,771 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=11406.0, ans=0.019141666666666668
2024-10-08 08:07:25,853 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.78 vs. limit=16.0545
2024-10-08 08:07:40,388 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=11409.333333333334, ans=0.18590666666666666
2024-10-08 08:07:50,566 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11412.666666666666, ans=0.18587333333333333
2024-10-08 08:08:08,037 INFO [train.py:1153] Epoch 5, batch 6850, loss[loss=0.2146, simple_loss=0.2433, pruned_loss=0.06374, ctc_loss=0.146, over 4978.00 frames. ], tot_loss[loss=0.2618, simple_loss=0.2858, pruned_loss=0.08443, ctc_loss=0.1725, over 978923.04 frames. ], batch size: 19, lr: 1.69e-02,
2024-10-08 08:08:09,394 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/epoch-5.pt
2024-10-08 08:08:36,636 INFO [train.py:1153] Epoch 6, batch 0, loss[loss=0.2807, simple_loss=0.3042, pruned_loss=0.08865, ctc_loss=0.1996, over 4856.00 frames. ], tot_loss[loss=0.2807, simple_loss=0.3042, pruned_loss=0.08865, ctc_loss=0.1996, over 4856.00 frames. ], batch size: 19, lr: 1.58e-02,
2024-10-08 08:08:36,636 INFO [train.py:1176] Computing validation loss
2024-10-08 08:08:42,458 INFO [train.py:1185] Epoch 6, validation: loss=0.1915, simple_loss=0.2685, pruned_loss=0.04142, ctc_loss=0.07922, over 90464.00 frames.
2024-10-08 08:08:42,459 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 08:08:55,591 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=15.84 vs. limit=16.067500000000003
2024-10-08 08:09:25,041 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.13 vs. limit=16.072499999999998
2024-10-08 08:09:27,031 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=11433.333333333334, ans=0.49983333333333335
2024-10-08 08:09:28,122 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.844e+01 1.126e+02 1.311e+02 1.472e+02 2.528e+02, threshold=2.622e+02, percent-clipped=0.0
2024-10-08 08:09:28,287 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=11433.333333333334, ans=0.125
2024-10-08 08:09:30,041 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.09 vs. limit=16.075
2024-10-08 08:09:31,968 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=11433.333333333334, ans=0.49983333333333335
2024-10-08 08:09:40,434 INFO [train.py:1153] Epoch 6, batch 50, loss[loss=0.2901, simple_loss=0.2979, pruned_loss=0.1016, ctc_loss=0.1978, over 4908.00 frames. ], tot_loss[loss=0.2898, simple_loss=0.3029, pruned_loss=0.09745, ctc_loss=0.2048, over 217728.37 frames. ], batch size: 19, lr: 1.57e-02,
2024-10-08 08:09:49,106 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11436.666666666666, ans=0.18563333333333332
2024-10-08 08:10:01,623 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=11440.0, ans=0.07
2024-10-08 08:10:11,463 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=11443.333333333334, ans=0.125
2024-10-08 08:10:16,355 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=11446.666666666666, ans=0.125
2024-10-08 08:10:41,938 INFO [train.py:1153] Epoch 6, batch 100, loss[loss=0.2592, simple_loss=0.2956, pruned_loss=0.07734, ctc_loss=0.1707, over 4746.00 frames. ], tot_loss[loss=0.2902, simple_loss=0.3032, pruned_loss=0.09798, ctc_loss=0.2029, over 383329.15 frames. ], batch size: 19, lr: 1.57e-02,
2024-10-08 08:10:46,159 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.40 vs. limit=10.726666666666667
2024-10-08 08:11:26,618 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.60 vs. limit=8.585333333333335
2024-10-08 08:11:31,170 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.617e+01 1.222e+02 1.391e+02 1.535e+02 1.897e+02, threshold=2.782e+02, percent-clipped=0.0
2024-10-08 08:11:34,919 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=11466.666666666666, ans=0.025
2024-10-08 08:11:37,580 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.51 vs. limit=11.8
2024-10-08 08:11:42,205 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11470.0, ans=0.1853
2024-10-08 08:11:43,309 INFO [train.py:1153] Epoch 6, batch 150, loss[loss=0.2379, simple_loss=0.2659, pruned_loss=0.07334, ctc_loss=0.1581, over 4910.00 frames. ], tot_loss[loss=0.2837, simple_loss=0.2993, pruned_loss=0.09481, ctc_loss=0.196, over 513281.80 frames. ], batch size: 19, lr: 1.57e-02,
2024-10-08 08:12:17,709 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=11476.666666666666, ans=0.018847222222222227
2024-10-08 08:12:27,835 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=16.88 vs. limit=11.805
2024-10-08 08:12:34,724 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=11483.333333333334, ans=0.4980833333333334
2024-10-08 08:12:44,258 INFO [train.py:1153] Epoch 6, batch 200, loss[loss=0.3536, simple_loss=0.3455, pruned_loss=0.1324, ctc_loss=0.2424, over 4740.00 frames. ], tot_loss[loss=0.282, simple_loss=0.2982, pruned_loss=0.09415, ctc_loss=0.1939, over 613748.79 frames. ], batch size: 45, lr: 1.57e-02,
2024-10-08 08:12:44,429 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=11486.666666666666, ans=0.125
2024-10-08 08:12:59,613 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.88 vs. limit=11.80875
2024-10-08 08:13:12,464 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=11493.333333333334, ans=0.008371014492753624
2024-10-08 08:13:33,198 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.963e+01 1.239e+02 1.371e+02 1.574e+02 2.229e+02, threshold=2.743e+02, percent-clipped=0.0
2024-10-08 08:13:39,280 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=11500.0, ans=0.018750000000000003
2024-10-08 08:13:41,529 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer_ff2.min_abs, batch_count=11500.0, ans=0.1
2024-10-08 08:13:43,888 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=11503.333333333334, ans=0.125
2024-10-08 08:13:44,945 INFO [train.py:1153] Epoch 6, batch 250, loss[loss=0.3147, simple_loss=0.3221, pruned_loss=0.1087, ctc_loss=0.2245, over 4825.00 frames. ], tot_loss[loss=0.279, simple_loss=0.2967, pruned_loss=0.09252, ctc_loss=0.1906, over 692431.51 frames. ], batch size: 38, lr: 1.57e-02,
2024-10-08 08:13:56,216 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.66 vs. limit=4.726
2024-10-08 08:14:05,373 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=11506.666666666666, ans=0.3726
2024-10-08 08:14:07,721 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=11510.0, ans=0.05
2024-10-08 08:14:10,215 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=11510.0, ans=0.125
2024-10-08 08:14:20,010 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=11513.333333333334, ans=0.008366666666666666
2024-10-08 08:14:23,139 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=15.58 vs. limit=16.134999999999998
2024-10-08 08:14:38,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11516.666666666666, ans=0.18483333333333335
2024-10-08 08:14:45,058 INFO [train.py:1153] Epoch 6, batch 300, loss[loss=0.3389, simple_loss=0.3286, pruned_loss=0.1276, ctc_loss=0.2351, over 4752.00 frames. ], tot_loss[loss=0.2788, simple_loss=0.2965, pruned_loss=0.09259, ctc_loss=0.1901, over 752777.12 frames. ], batch size: 32, lr: 1.57e-02,
2024-10-08 08:15:13,866 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11526.666666666666, ans=0.18473333333333333
2024-10-08 08:15:17,610 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=11526.666666666666, ans=0.125
2024-10-08 08:15:25,943 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=11530.0, ans=0.49645000000000006
2024-10-08 08:15:30,838 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11530.0, ans=0.18469999999999998
2024-10-08 08:15:33,194 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.385e+01 1.209e+02 1.323e+02 1.485e+02 1.866e+02, threshold=2.647e+02, percent-clipped=0.0
2024-10-08 08:15:35,765 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=11533.333333333334, ans=0.0
2024-10-08 08:15:38,411 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=16.61 vs. limit=16.15
2024-10-08 08:15:44,122 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=11536.666666666666, ans=0.125
2024-10-08 08:15:45,178 INFO [train.py:1153] Epoch 6, batch 350, loss[loss=0.2384, simple_loss=0.2804, pruned_loss=0.06983, ctc_loss=0.1421, over 4883.00 frames. ], tot_loss[loss=0.2763, simple_loss=0.295, pruned_loss=0.09122, ctc_loss=0.1879, over 800327.94 frames. ], batch size: 19, lr: 1.57e-02,
2024-10-08 08:15:51,333 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=11536.666666666666, ans=0.4962166666666667
2024-10-08 08:16:12,960 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=11543.333333333334, ans=0.125
2024-10-08 08:16:22,373 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=11546.666666666666, ans=0.125
2024-10-08 08:16:29,536 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=11546.666666666666, ans=0.125
2024-10-08 08:16:35,795 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=11.72 vs. limit=11.83125
2024-10-08 08:16:42,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=11550.0, ans=0.01854166666666667
2024-10-08 08:16:45,078 INFO [train.py:1153] Epoch 6, batch 400, loss[loss=0.2302, simple_loss=0.2633, pruned_loss=0.06866, ctc_loss=0.1492, over 4883.00 frames. ], tot_loss[loss=0.2738, simple_loss=0.2931, pruned_loss=0.09009, ctc_loss=0.1858, over 836993.22 frames. ], batch size: 22, lr: 1.57e-02,
2024-10-08 08:17:01,360 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=6.65 vs. limit=11.83375
2024-10-08 08:17:03,224 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=11556.666666666666, ans=0.125
2024-10-08 08:17:22,743 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=11563.333333333334, ans=0.125
2024-10-08 08:17:28,051 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.92 vs. limit=16.1725
2024-10-08 08:17:33,314 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.818e+01 1.193e+02 1.363e+02 1.538e+02 2.190e+02, threshold=2.726e+02, percent-clipped=0.0
2024-10-08 08:17:39,532 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=11566.666666666666, ans=0.125
2024-10-08 08:17:45,292 INFO [train.py:1153] Epoch 6, batch 450, loss[loss=0.2337, simple_loss=0.2829, pruned_loss=0.06473, ctc_loss=0.1376, over 4866.00 frames. ], tot_loss[loss=0.2725, simple_loss=0.2928, pruned_loss=0.08931, ctc_loss=0.1839, over 865666.32 frames. ], batch size: 23, lr: 1.57e-02,
2024-10-08 08:18:03,506 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=11573.333333333334, ans=0.4949333333333334
2024-10-08 08:18:09,599 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=11576.666666666666, ans=0.025
2024-10-08 08:18:19,262 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=11576.666666666666, ans=0.18423333333333333
2024-10-08 08:18:21,695 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=11580.0, ans=0.18419999999999997
2024-10-08 08:18:33,938 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=11583.333333333334, ans=0.125
2024-10-08 08:18:39,604 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=3.77 vs. limit=5.0
2024-10-08 08:18:45,895 INFO [train.py:1153] Epoch 6, batch 500, loss[loss=0.306, simple_loss=0.3145, pruned_loss=0.1045, ctc_loss=0.2208, over 4822.00 frames. ], tot_loss[loss=0.2694, simple_loss=0.2909, pruned_loss=0.08774, ctc_loss=0.1809, over 888156.62 frames. ], batch size: 34, lr: 1.56e-02,
2024-10-08 08:18:49,720 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=11586.666666666666, ans=0.125
2024-10-08 08:19:04,837 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.56 vs. limit=4.7385
2024-10-08 08:19:12,687 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=11593.333333333334, ans=0.125
2024-10-08 08:19:18,928 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=11593.333333333334, ans=0.125
2024-10-08 08:19:21,313 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=11596.666666666666, ans=0.125
2024-10-08 08:19:34,622 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.965e+01 1.154e+02 1.364e+02 1.518e+02 2.068e+02, threshold=2.728e+02, percent-clipped=0.0
2024-10-08 08:19:37,553 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=23.54 vs. limit=16.2
2024-10-08 08:19:46,666 INFO [train.py:1153] Epoch 6, batch 550, loss[loss=0.2486, simple_loss=0.2695, pruned_loss=0.07841, ctc_loss=0.1773, over 4804.00 frames. ], tot_loss[loss=0.27, simple_loss=0.291, pruned_loss=0.08813, ctc_loss=0.1818, over 905589.97 frames. ], batch size: 40, lr: 1.56e-02,
2024-10-08 08:20:03,839 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=11606.666666666666, ans=0.01830555555555556
2024-10-08 08:20:25,560 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=11613.333333333334, ans=0.125
2024-10-08 08:20:32,981 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=11613.333333333334, ans=0.125
2024-10-08 08:20:34,205 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=11616.666666666666, ans=0.018263888888888892
2024-10-08 08:20:36,670 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11616.666666666666, ans=0.18383333333333335
2024-10-08 08:20:39,168 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=11616.666666666666, ans=0.125
2024-10-08 08:20:47,422 INFO [train.py:1153] Epoch 6, batch 600, loss[loss=0.293, simple_loss=0.3087, pruned_loss=0.1009, ctc_loss=0.1884, over 4833.00 frames. ], tot_loss[loss=0.2696, simple_loss=0.291, pruned_loss=0.08782, ctc_loss=0.1813, over 919510.78 frames. ], batch size: 38, lr: 1.56e-02,
2024-10-08 08:21:02,823 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.08 vs. limit=7.905833333333334
2024-10-08 08:21:07,017 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=11623.333333333334, ans=0.49318333333333336
2024-10-08 08:21:09,420 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=11623.333333333334, ans=0.125
2024-10-08 08:21:15,622 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=11626.666666666666, ans=0.4930666666666667
2024-10-08 08:21:35,965 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.038e+01 1.176e+02 1.328e+02 1.473e+02 1.841e+02, threshold=2.657e+02, percent-clipped=0.0
2024-10-08 08:21:47,985 INFO [train.py:1153] Epoch 6, batch 650, loss[loss=0.2661, simple_loss=0.2962, pruned_loss=0.08447, ctc_loss=0.1678, over 4841.00 frames. ], tot_loss[loss=0.2686, simple_loss=0.2905, pruned_loss=0.0873, ctc_loss=0.1801, over 930358.40 frames. ], batch size: 21, lr: 1.56e-02,
2024-10-08 08:21:57,652 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=11636.666666666666, ans=0.008339855072463768
2024-10-08 08:22:12,152 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer1.prob, batch_count=11643.333333333334, ans=0.125
2024-10-08 08:22:23,488 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.25 vs. limit=11.8675
2024-10-08 08:22:48,320 INFO [train.py:1153] Epoch 6, batch 700, loss[loss=0.2671, simple_loss=0.2878, pruned_loss=0.08719, ctc_loss=0.1803, over 4750.00 frames. ], tot_loss[loss=0.2694, simple_loss=0.2906, pruned_loss=0.0879, ctc_loss=0.1811, over 938185.56 frames. ], batch size: 19, lr: 1.56e-02,
2024-10-08 08:22:54,574 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=1.019e-01
2024-10-08 08:22:55,980 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.71 vs. limit=11.870000000000001
2024-10-08 08:23:01,729 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer_ff3.min_abs, batch_count=11656.666666666666, ans=0.2
2024-10-08 08:23:10,275 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11656.666666666666, ans=0.18343333333333334
2024-10-08 08:23:28,273 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=11663.333333333334, ans=0.008334057971014493
2024-10-08 08:23:36,776 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.741e+01 1.196e+02 1.333e+02 1.467e+02 2.139e+02, threshold=2.667e+02, percent-clipped=0.0
2024-10-08 08:23:47,757 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=11670.0, ans=0.008332608695652175
2024-10-08 08:23:48,798 INFO [train.py:1153] Epoch 6, batch 750, loss[loss=0.2543, simple_loss=0.2765, pruned_loss=0.0837, ctc_loss=0.1618, over 4883.00 frames. ], tot_loss[loss=0.2665, simple_loss=0.289, pruned_loss=0.08633, ctc_loss=0.1784, over 944972.82 frames. ], batch size: 22, lr: 1.56e-02,
2024-10-08 08:23:52,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=11670.0, ans=0.125
2024-10-08 08:24:25,260 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=11680.0, ans=0.125
2024-10-08 08:24:26,443 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=11680.0, ans=0.4912000000000001
2024-10-08 08:24:41,405 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=11683.333333333334, ans=0.125
2024-10-08 08:24:48,509 INFO [train.py:1153] Epoch 6, batch 800, loss[loss=0.2103, simple_loss=0.2371, pruned_loss=0.06339, ctc_loss=0.1419, over 4850.00 frames. ], tot_loss[loss=0.2657, simple_loss=0.2886, pruned_loss=0.08593, ctc_loss=0.1774, over 949735.99 frames. ], batch size: 19, lr: 1.56e-02,
2024-10-08 08:24:49,832 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=11686.666666666666, ans=0.125
2024-10-08 08:25:15,369 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.65 vs. limit=4.754
2024-10-08 08:25:25,436 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=11696.666666666666, ans=0.01793055555555556
2024-10-08 08:25:35,855 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.210e+01 1.170e+02 1.328e+02 1.535e+02 2.068e+02, threshold=2.656e+02, percent-clipped=0.0
2024-10-08 08:25:47,921 INFO [train.py:1153] Epoch 6, batch 850, loss[loss=0.2964, simple_loss=0.3156, pruned_loss=0.1004, ctc_loss=0.1909, over 4789.00 frames. ], tot_loss[loss=0.267, simple_loss=0.2898, pruned_loss=0.08653, ctc_loss=0.178, over 953982.07 frames. ], batch size: 29, lr: 1.56e-02,
2024-10-08 08:25:48,573 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=15.72 vs. limit=16.2775
2024-10-08 08:25:57,439 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=11703.333333333334, ans=0.49038333333333334
2024-10-08 08:26:00,939 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=11706.666666666666, ans=0.49026666666666674
2024-10-08 08:26:04,613 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=11706.666666666666, ans=0.125
2024-10-08 08:26:07,041 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=11706.666666666666, ans=0.125
2024-10-08 08:26:12,792 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=11710.0, ans=0.125
2024-10-08 08:26:35,327 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=11716.666666666666, ans=0.125
2024-10-08 08:26:39,977 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11716.666666666666, ans=0.18283333333333335
2024-10-08 08:26:45,440 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=16.64 vs. limit=16.2875
2024-10-08 08:26:47,107 INFO [train.py:1153] Epoch 6, batch 900, loss[loss=0.2601, simple_loss=0.291, pruned_loss=0.08099, ctc_loss=0.1681, over 4859.00 frames. ], tot_loss[loss=0.2674, simple_loss=0.2901, pruned_loss=0.08667, ctc_loss=0.1786, over 956959.21 frames. ], batch size: 19, lr: 1.56e-02,
2024-10-08 08:26:55,461 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11720.0, ans=0.1828
2024-10-08 08:27:04,236 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.83 vs. limit=8.689333333333334
2024-10-08 08:27:09,591 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=11726.666666666666, ans=0.125
2024-10-08 08:27:34,278 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.950e+01 1.209e+02 1.362e+02 1.537e+02 2.109e+02, threshold=2.725e+02, percent-clipped=0.0
2024-10-08 08:27:34,853 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=11733.333333333334, ans=16.3
2024-10-08 08:27:36,673 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=11733.333333333334, ans=0.008318840579710145
2024-10-08 08:27:46,110 INFO [train.py:1153] Epoch 6, batch 950, loss[loss=0.2652, simple_loss=0.295, pruned_loss=0.08326, ctc_loss=0.1723, over 4816.00 frames. ], tot_loss[loss=0.2702, simple_loss=0.2917, pruned_loss=0.08794, ctc_loss=0.1819, over 958743.56 frames. ], batch size: 19, lr: 1.56e-02,
2024-10-08 08:27:55,709 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=11736.666666666666, ans=0.125
2024-10-08 08:28:04,968 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=11740.0, ans=0.18259999999999998
2024-10-08 08:28:11,469 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=13.05 vs. limit=11.903749999999999
2024-10-08 08:28:15,148 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=6.09 vs. limit=8.697333333333333
2024-10-08 08:28:30,179 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=11746.666666666666, ans=0.01772222222222223
2024-10-08 08:28:36,094 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=11750.0, ans=0.008315217391304348
2024-10-08 08:28:41,952 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=11750.0, ans=0.125
2024-10-08 08:28:43,145 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer_ff3.min_abs, batch_count=11750.0, ans=0.2
2024-10-08 08:28:45,442 INFO [train.py:1153] Epoch 6, batch 1000, loss[loss=0.2241, simple_loss=0.2634, pruned_loss=0.06414, ctc_loss=0.1412, over 4935.00 frames. ], tot_loss[loss=0.2718, simple_loss=0.2928, pruned_loss=0.08868, ctc_loss=0.1836, over 960553.03 frames. ], batch size: 20, lr: 1.55e-02,
2024-10-08 08:28:50,377 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=11753.333333333334, ans=0.025
2024-10-08 08:29:08,273 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=11760.0, ans=0.01766666666666667
2024-10-08 08:29:10,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=11760.0, ans=0.125
2024-10-08 08:29:20,211 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11763.333333333334, ans=0.18236666666666668
2024-10-08 08:29:33,137 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.251e+01 1.223e+02 1.412e+02 1.503e+02 2.035e+02, threshold=2.824e+02, percent-clipped=0.0
2024-10-08 08:29:36,941 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=11766.666666666666, ans=0.0
2024-10-08 08:29:45,007 INFO [train.py:1153] Epoch 6, batch 1050, loss[loss=0.264, simple_loss=0.2823, pruned_loss=0.08678, ctc_loss=0.1801, over 4818.00 frames. ], tot_loss[loss=0.271, simple_loss=0.2922, pruned_loss=0.08832, ctc_loss=0.1828, over 962776.70 frames. ], batch size: 25, lr: 1.55e-02,
2024-10-08 08:29:53,512 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=11770.0, ans=0.125
2024-10-08 08:29:58,467 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=11773.333333333334, ans=0.125
2024-10-08 08:30:00,054 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.97 vs. limit=4.766
2024-10-08 08:30:09,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=11776.666666666666, ans=0.125
2024-10-08 08:30:44,518 INFO [train.py:1153] Epoch 6, batch 1100, loss[loss=0.2966, simple_loss=0.3163, pruned_loss=0.09928, ctc_loss=0.1959, over 4853.00 frames. ], tot_loss[loss=0.2718, simple_loss=0.2926, pruned_loss=0.08887, ctc_loss=0.1833, over 964195.05 frames. ], batch size: 20, lr: 1.55e-02,
2024-10-08 08:30:46,819 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.8.prob, batch_count=11786.666666666666, ans=0.125
2024-10-08 08:31:02,406 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=11790.0, ans=0.008306521739130435
2024-10-08 08:31:12,594 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=6.06 vs. limit=6.358666666666666
2024-10-08 08:31:13,634 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.27 vs. limit=4.769
2024-10-08 08:31:15,777 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.84 vs. limit=11.9225
2024-10-08 08:31:31,929 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.046e+01 1.230e+02 1.389e+02 1.578e+02 2.300e+02, threshold=2.779e+02, percent-clipped=0.0
2024-10-08 08:31:43,727 INFO [train.py:1153] Epoch 6, batch 1150, loss[loss=0.2382, simple_loss=0.2766, pruned_loss=0.06883, ctc_loss=0.1551, over 4862.00 frames. ], tot_loss[loss=0.2707, simple_loss=0.2918, pruned_loss=0.0883, ctc_loss=0.1823, over 964509.46 frames. ], batch size: 20, lr: 1.55e-02,
2024-10-08 08:32:01,029 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.39 vs. limit=11.9275
2024-10-08 08:32:08,154 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.23 vs. limit=4.7715
2024-10-08 08:32:15,583 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=11810.0, ans=0.017458333333333333
2024-10-08 08:32:23,712 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=11813.333333333334, ans=0.125
2024-10-08 08:32:25,996 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=11813.333333333334, ans=0.125
2024-10-08 08:32:35,401 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=11816.666666666666, ans=0.01743055555555556
2024-10-08 08:32:35,450 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=11816.666666666666, ans=0.125
2024-10-08 08:32:42,349 INFO [train.py:1153] Epoch 6, batch 1200, loss[loss=0.3127, simple_loss=0.334, pruned_loss=0.1049, ctc_loss=0.204, over 4811.00 frames. ], tot_loss[loss=0.271, simple_loss=0.2921, pruned_loss=0.08838, ctc_loss=0.1828, over 964453.90 frames. ], batch size: 25, lr: 1.55e-02,
2024-10-08 08:33:16,923 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=11830.0, ans=0.48595000000000005
2024-10-08 08:33:29,569 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.869e+01 1.239e+02 1.380e+02 1.532e+02 2.412e+02, threshold=2.760e+02, percent-clipped=0.0
2024-10-08 08:33:41,402 INFO [train.py:1153] Epoch 6, batch 1250, loss[loss=0.2882, simple_loss=0.2961, pruned_loss=0.09976, ctc_loss=0.202, over 4741.00 frames. ], tot_loss[loss=0.2717, simple_loss=0.2923, pruned_loss=0.08883, ctc_loss=0.1836, over 964373.59 frames. ], batch size: 32, lr: 1.55e-02,
2024-10-08 08:33:58,708 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.08 vs. limit=11.940000000000001
2024-10-08 08:34:15,448 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=11843.333333333334, ans=0.125
2024-10-08 08:34:16,618 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=11846.666666666666, ans=0.01730555555555556
2024-10-08 08:34:24,559 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=11846.666666666666, ans=0.125
2024-10-08 08:34:30,489 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=11850.0, ans=0.008293478260869565
2024-10-08 08:34:33,827 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=11850.0, ans=0.48525
2024-10-08 08:34:37,354 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11850.0, ans=0.1815
2024-10-08 08:34:40,675 INFO [train.py:1153] Epoch 6, batch 1300, loss[loss=0.3235, simple_loss=0.3225, pruned_loss=0.1166, ctc_loss=0.2282, over 4828.00 frames. ], tot_loss[loss=0.2711, simple_loss=0.2921, pruned_loss=0.08851, ctc_loss=0.1829, over 965644.07 frames. ], batch size: 43, lr: 1.55e-02,
2024-10-08 08:34:53,624 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=11856.666666666666, ans=0.125
2024-10-08 08:34:57,076 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=11856.666666666666, ans=0.025
2024-10-08 08:35:01,723 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=11856.666666666666, ans=0.125
2024-10-08 08:35:08,953 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=11860.0, ans=0.01725
2024-10-08 08:35:22,027 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11863.333333333334, ans=0.18136666666666668
2024-10-08 08:35:27,823 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.726e+01 1.180e+02 1.330e+02 1.549e+02 1.910e+02, threshold=2.661e+02, percent-clipped=0.0
2024-10-08 08:35:30,264 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=11866.666666666666, ans=0.01722222222222223
2024-10-08 08:35:39,830 INFO [train.py:1153] Epoch 6, batch 1350, loss[loss=0.2633, simple_loss=0.2861, pruned_loss=0.08357, ctc_loss=0.1836, over 4842.00 frames. ], tot_loss[loss=0.2688, simple_loss=0.2902, pruned_loss=0.08752, ctc_loss=0.1808, over 966366.26 frames. ], batch size: 21, lr: 1.55e-02,
2024-10-08 08:35:47,190 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=11870.0, ans=0.008289130434782608
2024-10-08 08:35:48,502 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 08:36:38,937 INFO [train.py:1153] Epoch 6, batch 1400, loss[loss=0.2531, simple_loss=0.2776, pruned_loss=0.08138, ctc_loss=0.1643, over 4940.00 frames. ], tot_loss[loss=0.2693, simple_loss=0.2902, pruned_loss=0.08796, ctc_loss=0.181, over 966644.81 frames. ], batch size: 19, lr: 1.55e-02,
2024-10-08 08:36:57,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=11890.0, ans=0.008284782608695652
2024-10-08 08:37:08,583 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=11893.333333333334, ans=0.48373333333333335
2024-10-08 08:37:17,556 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=5.26 vs. limit=11.96125
2024-10-08 08:37:18,558 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.63 vs. limit=11.96125
2024-10-08 08:37:26,067 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.468e+01 1.232e+02 1.383e+02 1.524e+02 2.222e+02, threshold=2.766e+02, percent-clipped=0.0
2024-10-08 08:37:37,837 INFO [train.py:1153] Epoch 6, batch 1450, loss[loss=0.2635, simple_loss=0.2936, pruned_loss=0.08229, ctc_loss=0.1721, over 4810.00 frames. ], tot_loss[loss=0.2704, simple_loss=0.2912, pruned_loss=0.08835, ctc_loss=0.1822, over 966496.85 frames. ], batch size: 34, lr: 1.54e-02,
2024-10-08 08:37:48,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=11906.666666666666, ans=0.0
2024-10-08 08:37:59,142 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=11906.666666666666, ans=0.48326666666666673
2024-10-08 08:38:01,458 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=11910.0, ans=0.125
2024-10-08 08:38:13,155 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=11913.333333333334, ans=0.125
2024-10-08 08:38:15,884 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.42 vs. limit=4.787
2024-10-08 08:38:28,414 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=11916.666666666666, ans=0.125
2024-10-08 08:38:31,099 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.49 vs. limit=11.96875
2024-10-08 08:38:36,450 INFO [train.py:1153] Epoch 6, batch 1500, loss[loss=0.2637, simple_loss=0.2875, pruned_loss=0.08539, ctc_loss=0.1729, over 4749.00 frames. ], tot_loss[loss=0.2697, simple_loss=0.2911, pruned_loss=0.0878, ctc_loss=0.1819, over 966213.36 frames. ], batch size: 26, lr: 1.54e-02,
2024-10-08 08:39:00,069 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 08:39:11,863 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=11930.0, ans=0.125
2024-10-08 08:39:23,512 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.211e+01 1.186e+02 1.328e+02 1.478e+02 1.994e+02, threshold=2.656e+02, percent-clipped=0.0
2024-10-08 08:39:24,348 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module1.whiten, num_groups=1, num_channels=192, metric=3.39 vs. limit=11.975
2024-10-08 08:39:35,194 INFO [train.py:1153] Epoch 6, batch 1550, loss[loss=0.2858, simple_loss=0.2898, pruned_loss=0.09988, ctc_loss=0.2053, over 4859.00 frames. ], tot_loss[loss=0.2701, simple_loss=0.2911, pruned_loss=0.0881, ctc_loss=0.1821, over 966062.58 frames. ], batch size: 31, lr: 1.54e-02,
2024-10-08 08:39:38,810 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.8.prob, batch_count=11936.666666666666, ans=0.125
2024-10-08 08:39:40,020 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=11936.666666666666, ans=0.025
2024-10-08 08:39:41,541 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=18.01 vs. limit=16.4525
2024-10-08 08:39:46,975 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.min_positive, batch_count=11940.0, ans=0.025
2024-10-08 08:39:54,525 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.88 vs. limit=4.791
2024-10-08 08:40:07,196 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.20 vs. limit=11.97875
2024-10-08 08:40:24,375 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=11950.0, ans=0.016875
2024-10-08 08:40:25,546 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=11950.0, ans=0.07
2024-10-08 08:40:26,738 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=11950.0, ans=0.125
2024-10-08 08:40:33,568 INFO [train.py:1153] Epoch 6, batch 1600, loss[loss=0.2434, simple_loss=0.2826, pruned_loss=0.06943, ctc_loss=0.1633, over 4826.00 frames. ], tot_loss[loss=0.2683, simple_loss=0.2902, pruned_loss=0.08704, ctc_loss=0.1806, over 966319.82 frames. ], batch size: 25, lr: 1.54e-02,
2024-10-08 08:40:51,285 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=11956.666666666666, ans=0.008270289855072464
2024-10-08 08:41:05,740 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=16.50 vs. limit=16.47
2024-10-08 08:41:20,508 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.259e+01 1.196e+02 1.332e+02 1.475e+02 2.139e+02, threshold=2.665e+02, percent-clipped=0.0
2024-10-08 08:41:32,218 INFO [train.py:1153] Epoch 6, batch 1650, loss[loss=0.2753, simple_loss=0.3041, pruned_loss=0.08771, ctc_loss=0.1777, over 4778.00 frames. ], tot_loss[loss=0.2688, simple_loss=0.2906, pruned_loss=0.08724, ctc_loss=0.1812, over 966617.45 frames. ], batch size: 29, lr: 1.54e-02,
2024-10-08 08:41:41,741 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11970.0, ans=0.1803
2024-10-08 08:41:45,425 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11973.333333333334, ans=0.18026666666666666
2024-10-08 08:41:53,528 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11973.333333333334, ans=0.18026666666666666
2024-10-08 08:42:00,662 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=11976.666666666666, ans=0.008265942028985507
2024-10-08 08:42:06,962 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=16.05 vs. limit=16.485
2024-10-08 08:42:12,314 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=11980.0, ans=0.01675
2024-10-08 08:42:15,895 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=11980.0, ans=0.125
2024-10-08 08:42:22,876 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=11983.333333333334, ans=0.18016666666666667
2024-10-08 08:42:22,959 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=11983.333333333334, ans=0.125
2024-10-08 08:42:31,010 INFO [train.py:1153] Epoch 6, batch 1700, loss[loss=0.2152, simple_loss=0.2491, pruned_loss=0.06455, ctc_loss=0.1307, over 4940.00 frames. ], tot_loss[loss=0.2675, simple_loss=0.2901, pruned_loss=0.08659, ctc_loss=0.1794, over 966739.34 frames. ], batch size: 19, lr: 1.54e-02,
2024-10-08 08:42:45,147 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=11990.0, ans=0.01670833333333334
2024-10-08 08:43:07,407 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=11996.666666666666, ans=0.18003333333333332
2024-10-08 08:43:16,898 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/checkpoint-36000.pt
2024-10-08 08:43:20,448 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.921e+01 1.172e+02 1.317e+02 1.491e+02 2.259e+02, threshold=2.635e+02, percent-clipped=0.0
2024-10-08 08:43:21,886 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.22 vs. limit=8.0
2024-10-08 08:43:31,526 INFO [train.py:1153] Epoch 6, batch 1750, loss[loss=0.2717, simple_loss=0.2982, pruned_loss=0.08606, ctc_loss=0.1826, over 4959.00 frames. ], tot_loss[loss=0.2684, simple_loss=0.2905, pruned_loss=0.08711, ctc_loss=0.1804, over 966927.25 frames. ], batch size: 19, lr: 1.54e-02,
2024-10-08 08:43:38,376 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=12003.333333333334, ans=0.125
2024-10-08 08:44:10,751 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=12013.333333333334, ans=0.125
2024-10-08 08:44:12,317 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.38 vs. limit=8.805333333333333
2024-10-08 08:44:23,007 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=4.39 vs. limit=12.00625
2024-10-08 08:44:29,215 INFO [train.py:1153] Epoch 6, batch 1800, loss[loss=0.2396, simple_loss=0.2805, pruned_loss=0.06799, ctc_loss=0.157, over 4868.00 frames. ], tot_loss[loss=0.2703, simple_loss=0.2922, pruned_loss=0.08802, ctc_loss=0.181, over 967564.83 frames. ], batch size: 23, lr: 1.54e-02,
2024-10-08 08:45:11,433 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12030.0, ans=0.17969999999999997
2024-10-08 08:45:15,933 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.211e+01 1.180e+02 1.323e+02 1.531e+02 1.972e+02, threshold=2.645e+02, percent-clipped=0.0
2024-10-08 08:45:16,666 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.40 vs. limit=16.525
2024-10-08 08:45:21,866 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=12033.333333333334, ans=0.125
2024-10-08 08:45:27,532 INFO [train.py:1153] Epoch 6, batch 1850, loss[loss=0.2868, simple_loss=0.3071, pruned_loss=0.09544, ctc_loss=0.1892, over 4757.00 frames. ], tot_loss[loss=0.269, simple_loss=0.2915, pruned_loss=0.08722, ctc_loss=0.18, over 967884.72 frames. ], batch size: 26, lr: 1.54e-02,
2024-10-08 08:45:29,100 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.65 vs. limit=12.01375
2024-10-08 08:45:35,805 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=12036.666666666666, ans=0.47871666666666673
2024-10-08 08:45:39,211 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=12040.0, ans=0.125
2024-10-08 08:45:52,973 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=12043.333333333334, ans=0.4784833333333333
2024-10-08 08:45:54,056 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=12043.333333333334, ans=0.025
2024-10-08 08:46:25,244 INFO [train.py:1153] Epoch 6, batch 1900, loss[loss=0.3135, simple_loss=0.3299, pruned_loss=0.1057, ctc_loss=0.2143, over 4802.00 frames. ], tot_loss[loss=0.2686, simple_loss=0.2911, pruned_loss=0.087, ctc_loss=0.1801, over 967726.16 frames. ], batch size: 29, lr: 1.54e-02,
2024-10-08 08:46:28,867 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=12053.333333333334, ans=0.016444444444444442
2024-10-08 08:46:35,783 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=12056.666666666666, ans=0.025
2024-10-08 08:46:38,187 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=12056.666666666666, ans=0.125
2024-10-08 08:46:38,191 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=12056.666666666666, ans=0.125
2024-10-08 08:46:42,899 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=12056.666666666666, ans=0.125
2024-10-08 08:47:04,045 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.10 vs. limit=12.02375
2024-10-08 08:47:11,837 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.162e+01 1.164e+02 1.283e+02 1.402e+02 1.971e+02, threshold=2.566e+02, percent-clipped=0.0
2024-10-08 08:47:14,243 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=12066.666666666666, ans=0.09899494936611666
2024-10-08 08:47:23,322 INFO [train.py:1153] Epoch 6, batch 1950, loss[loss=0.2752, simple_loss=0.3035, pruned_loss=0.08865, ctc_loss=0.1742, over 4867.00 frames. ], tot_loss[loss=0.2692, simple_loss=0.2916, pruned_loss=0.0873, ctc_loss=0.1805, over 966726.57 frames. ], batch size: 20, lr: 1.53e-02,
2024-10-08 08:47:36,097 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=12073.333333333334, ans=0.125
2024-10-08 08:47:40,596 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=12073.333333333334, ans=0.125
2024-10-08 08:47:50,989 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=12076.666666666666, ans=0.17923333333333336
2024-10-08 08:48:07,323 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=12080.0, ans=0.008243478260869566
2024-10-08 08:48:21,262 INFO [train.py:1153] Epoch 6, batch 2000, loss[loss=0.2303, simple_loss=0.2577, pruned_loss=0.07245, ctc_loss=0.1449, over 4959.00 frames. ], tot_loss[loss=0.2693, simple_loss=0.2917, pruned_loss=0.08739, ctc_loss=0.1806, over 966489.75 frames. ], batch size: 19, lr: 1.53e-02,
2024-10-08 08:48:37,596 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=12090.0, ans=0.008241304347826087
2024-10-08 08:48:55,040 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=12096.666666666666, ans=0.125
2024-10-08 08:49:07,655 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.054e+01 1.198e+02 1.376e+02 1.510e+02 3.642e+02, threshold=2.753e+02, percent-clipped=1.0
2024-10-08 08:49:15,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=12100.0, ans=0.01625
2024-10-08 08:49:19,249 INFO [train.py:1153] Epoch 6, batch 2050, loss[loss=0.2757, simple_loss=0.2866, pruned_loss=0.09224, ctc_loss=0.2006, over 4910.00 frames. ], tot_loss[loss=0.2694, simple_loss=0.2912, pruned_loss=0.0876, ctc_loss=0.1811, over 966908.52 frames. ], batch size: 19, lr: 1.53e-02,
2024-10-08 08:49:52,140 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=12110.0, ans=0.035
2024-10-08 08:49:56,828 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=12113.333333333334, ans=0.016194444444444442
2024-10-08 08:50:14,476 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=12116.666666666666, ans=0.008235507246376813
2024-10-08 08:50:18,078 INFO [train.py:1153] Epoch 6, batch 2100, loss[loss=0.2474, simple_loss=0.272, pruned_loss=0.0794, ctc_loss=0.1602, over 4852.00 frames. ], tot_loss[loss=0.2694, simple_loss=0.2915, pruned_loss=0.08741, ctc_loss=0.1811, over 967074.82 frames. ], batch size: 21, lr: 1.53e-02,
2024-10-08 08:50:43,920 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer_ff2.min_abs, batch_count=12126.666666666666, ans=0.1
2024-10-08 08:50:54,737 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=12130.0, ans=0.025
2024-10-08 08:51:05,143 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.240e+01 1.256e+02 1.464e+02 1.626e+02 3.173e+02, threshold=2.927e+02, percent-clipped=1.0
2024-10-08 08:51:05,398 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=12133.333333333334, ans=0.125
2024-10-08 08:51:14,597 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=12133.333333333334, ans=0.17866666666666667
2024-10-08 08:51:16,706 INFO [train.py:1153] Epoch 6, batch 2150, loss[loss=0.2599, simple_loss=0.2892, pruned_loss=0.08191, ctc_loss=0.1671, over 4871.00 frames. ], tot_loss[loss=0.2684, simple_loss=0.2907, pruned_loss=0.08711, ctc_loss=0.1799, over 967787.36 frames. ], batch size: 20, lr: 1.53e-02,
2024-10-08 08:51:33,067 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=12140.0, ans=0.01608333333333334
2024-10-08 08:51:52,596 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=10.05 vs. limit=11.073333333333334
2024-10-08 08:51:58,450 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=12146.666666666666, ans=0.4748666666666667
2024-10-08 08:51:59,492 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=12146.666666666666, ans=0.125
2024-10-08 08:52:03,821 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12150.0, ans=0.1785
2024-10-08 08:52:03,844 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=12150.0, ans=0.47475
2024-10-08 08:52:12,268 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.30 vs. limit=12.05625
2024-10-08 08:52:14,200 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=12153.333333333334, ans=0.016027777777777773
2024-10-08 08:52:15,134 INFO [train.py:1153] Epoch 6, batch 2200, loss[loss=0.2768, simple_loss=0.2998, pruned_loss=0.08907, ctc_loss=0.1891, over 4741.00 frames. ], tot_loss[loss=0.2689, simple_loss=0.2913, pruned_loss=0.08726, ctc_loss=0.1802, over 967525.41 frames. ], batch size: 26, lr: 1.53e-02,
2024-10-08 08:52:25,490 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=12156.666666666666, ans=0.4745166666666667
2024-10-08 08:52:46,567 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=4.17 vs. limit=8.04
2024-10-08 08:53:01,160 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.076e+01 1.202e+02 1.310e+02 1.507e+02 4.941e+02, threshold=2.621e+02, percent-clipped=1.0
2024-10-08 08:53:12,796 INFO [train.py:1153] Epoch 6, batch 2250, loss[loss=0.2346, simple_loss=0.2699, pruned_loss=0.07085, ctc_loss=0.1437, over 4892.00 frames. ], tot_loss[loss=0.2684, simple_loss=0.291, pruned_loss=0.08696, ctc_loss=0.1796, over 967656.19 frames. ], batch size: 22, lr: 1.53e-02,
2024-10-08 08:53:33,992 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=12173.333333333334, ans=0.01594444444444444
2024-10-08 08:53:35,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.prob, batch_count=12176.666666666666, ans=0.125
2024-10-08 08:53:41,625 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.36 vs. limit=16.6325
2024-10-08 08:53:43,609 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=12176.666666666666, ans=0.125
2024-10-08 08:53:45,934 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=12176.666666666666, ans=0.0
2024-10-08 08:53:56,690 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=12180.0, ans=0.025
2024-10-08 08:53:57,002 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=2.99 vs. limit=12.067499999999999
2024-10-08 08:53:57,171 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.58 vs. limit=8.045
2024-10-08 08:54:00,125 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12183.333333333334, ans=0.17816666666666667
2024-10-08 08:54:06,283 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.24 vs. limit=12.06875
2024-10-08 08:54:10,759 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=12186.666666666666, ans=0.125
2024-10-08 08:54:11,689 INFO [train.py:1153] Epoch 6, batch 2300, loss[loss=0.2315, simple_loss=0.2624, pruned_loss=0.06976, ctc_loss=0.1529, over 4883.00 frames. ], tot_loss[loss=0.2661, simple_loss=0.2897, pruned_loss=0.08575, ctc_loss=0.1775, over 968338.17 frames. ], batch size: 19, lr: 1.53e-02,
2024-10-08 08:54:15,379 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=12186.666666666666, ans=0.4734666666666667
2024-10-08 08:54:17,270 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=5.27 vs. limit=6.437333333333333
2024-10-08 08:54:23,457 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=12190.0, ans=0.17809999999999998
2024-10-08 08:54:37,924 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.86 vs. limit=12.0725
2024-10-08 08:54:46,880 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=12196.666666666666, ans=0.015847222222222228
2024-10-08 08:54:50,241 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=12196.666666666666, ans=0.17803333333333332
2024-10-08 08:54:58,391 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.268e+01 1.160e+02 1.290e+02 1.441e+02 1.796e+02, threshold=2.580e+02, percent-clipped=0.0
2024-10-08 08:55:10,007 INFO [train.py:1153] Epoch 6, batch 2350, loss[loss=0.2502, simple_loss=0.2792, pruned_loss=0.08003, ctc_loss=0.1527, over 4860.00 frames. ], tot_loss[loss=0.2669, simple_loss=0.29, pruned_loss=0.08628, ctc_loss=0.1779, over 968426.32 frames. ], batch size: 23, lr: 1.53e-02,
2024-10-08 08:55:22,207 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.11 vs. limit=12.0775
2024-10-08 08:55:26,529 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=12206.666666666666, ans=0.125
2024-10-08 08:55:36,520 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=4.90 vs. limit=5.0
2024-10-08 08:55:48,456 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=12213.333333333334, ans=0.125
2024-10-08 08:55:49,643 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=12213.333333333334, ans=0.125
2024-10-08 08:56:02,628 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12216.666666666666, ans=0.17783333333333334
2024-10-08 08:56:04,916 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.min_positive, batch_count=12216.666666666666, ans=0.025
2024-10-08 08:56:08,303 INFO [train.py:1153] Epoch 6, batch 2400, loss[loss=0.2489, simple_loss=0.2676, pruned_loss=0.08028, ctc_loss=0.1744, over 4750.00 frames. ], tot_loss[loss=0.2673, simple_loss=0.2902, pruned_loss=0.08648, ctc_loss=0.1785, over 967708.88 frames. ], batch size: 19, lr: 1.53e-02,
2024-10-08 08:56:15,512 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=12220.0, ans=0.125
2024-10-08 08:56:18,172 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=6.98 vs. limit=8.888
2024-10-08 08:56:18,846 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=12223.333333333334, ans=0.00821231884057971
2024-10-08 08:56:49,191 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.46 vs. limit=16.6725
2024-10-08 08:56:53,485 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=12233.333333333334, ans=0.125
2024-10-08 08:56:53,580 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=12233.333333333334, ans=0.125
2024-10-08 08:56:54,467 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.365e+01 1.213e+02 1.352e+02 1.436e+02 2.643e+02, threshold=2.704e+02, percent-clipped=1.0
2024-10-08 08:56:55,765 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12233.333333333334, ans=0.17766666666666667
2024-10-08 08:57:02,704 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=12233.333333333334, ans=0.125
2024-10-08 08:57:06,042 INFO [train.py:1153] Epoch 6, batch 2450, loss[loss=0.2207, simple_loss=0.2507, pruned_loss=0.06559, ctc_loss=0.1487, over 4878.00 frames. ], tot_loss[loss=0.2679, simple_loss=0.2902, pruned_loss=0.08692, ctc_loss=0.1795, over 966876.52 frames. ], batch size: 22, lr: 1.52e-02,
2024-10-08 08:57:40,442 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=17.14 vs. limit=16.685000000000002
2024-10-08 08:57:44,725 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=12246.666666666666, ans=0.125
2024-10-08 08:57:55,079 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12250.0, ans=0.1775
2024-10-08 08:58:03,113 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=12253.333333333334, ans=0.125
2024-10-08 08:58:04,236 INFO [train.py:1153] Epoch 6, batch 2500, loss[loss=0.304, simple_loss=0.2993, pruned_loss=0.1083, ctc_loss=0.2302, over 4757.00 frames. ], tot_loss[loss=0.2677, simple_loss=0.2905, pruned_loss=0.08665, ctc_loss=0.1789, over 966490.08 frames. ], batch size: 26, lr: 1.52e-02,
2024-10-08 08:58:31,994 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=12260.0, ans=0.08659500000000002
2024-10-08 08:58:47,434 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=12263.333333333334, ans=0.125
2024-10-08 08:58:50,958 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.707e+01 1.184e+02 1.310e+02 1.425e+02 1.927e+02, threshold=2.621e+02, percent-clipped=0.0
2024-10-08 08:58:53,597 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 08:58:56,857 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12266.666666666666, ans=0.17733333333333334
2024-10-08 08:59:02,669 INFO [train.py:1153] Epoch 6, batch 2550, loss[loss=0.224, simple_loss=0.2621, pruned_loss=0.06444, ctc_loss=0.1425, over 4959.00 frames. ], tot_loss[loss=0.2678, simple_loss=0.2904, pruned_loss=0.08671, ctc_loss=0.1792, over 966851.53 frames. ], batch size: 19, lr: 1.52e-02,
2024-10-08 08:59:05,103 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=12270.0, ans=0.125
2024-10-08 08:59:07,385 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=12270.0, ans=0.0
2024-10-08 08:59:13,764 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=16.54 vs. limit=16.705
2024-10-08 08:59:22,544 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=12273.333333333334, ans=0.04949747468305833
2024-10-08 08:59:32,994 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=12276.666666666666, ans=0.0
2024-10-08 08:59:38,825 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=12280.0, ans=0.125
2024-10-08 08:59:40,792 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=6.44 vs. limit=12.105
2024-10-08 08:59:49,408 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=12283.333333333334, ans=0.125
2024-10-08 08:59:55,177 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=12283.333333333334, ans=0.01548611111111111
2024-10-08 09:00:00,745 INFO [train.py:1153] Epoch 6, batch 2600, loss[loss=0.2035, simple_loss=0.2588, pruned_loss=0.04874, ctc_loss=0.1267, over 4847.00 frames. ], tot_loss[loss=0.2689, simple_loss=0.2914, pruned_loss=0.08715, ctc_loss=0.1804, over 966280.04 frames. ], batch size: 20, lr: 1.52e-02,
2024-10-08 09:00:03,519 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=12286.666666666666, ans=0.4699666666666667
2024-10-08 09:00:07,137 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=12286.666666666666, ans=0.125
2024-10-08 09:00:09,411 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 09:00:15,827 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.82 vs. limit=12.10875
2024-10-08 09:00:16,350 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=12290.0, ans=0.07
2024-10-08 09:00:38,975 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=12296.666666666666, ans=0.125
2024-10-08 09:00:46,970 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.520e+01 1.205e+02 1.362e+02 1.553e+02 2.167e+02, threshold=2.724e+02, percent-clipped=0.0
2024-10-08 09:00:52,296 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=11.41 vs. limit=12.1125
2024-10-08 09:00:54,220 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=12300.0, ans=0.46950000000000003
2024-10-08 09:00:58,599 INFO [train.py:1153] Epoch 6, batch 2650, loss[loss=0.3414, simple_loss=0.3481, pruned_loss=0.117, ctc_loss=0.2518, over 4832.00 frames. ], tot_loss[loss=0.2704, simple_loss=0.2921, pruned_loss=0.08802, ctc_loss=0.1818, over 966038.66 frames. ], batch size: 38, lr: 1.52e-02,
2024-10-08 09:01:56,862 INFO [train.py:1153] Epoch 6, batch 2700, loss[loss=0.3382, simple_loss=0.3426, pruned_loss=0.12, ctc_loss=0.2348, over 4856.00 frames. ], tot_loss[loss=0.2692, simple_loss=0.2911, pruned_loss=0.08751, ctc_loss=0.1809, over 966317.31 frames. ], batch size: 28, lr: 1.52e-02,
2024-10-08 09:02:10,503 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.06 vs. limit=12.12125
2024-10-08 09:02:19,433 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=12326.666666666666, ans=0.46856666666666674
2024-10-08 09:02:24,532 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.22 vs. limit=12.122499999999999
2024-10-08 09:02:43,857 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.214e+01 1.172e+02 1.316e+02 1.438e+02 1.858e+02, threshold=2.632e+02, percent-clipped=0.0
2024-10-08 09:02:55,513 INFO [train.py:1153] Epoch 6, batch 2750, loss[loss=0.2739, simple_loss=0.3064, pruned_loss=0.08707, ctc_loss=0.1681, over 4799.00 frames. ], tot_loss[loss=0.2679, simple_loss=0.2903, pruned_loss=0.08702, ctc_loss=0.1788, over 966952.46 frames. ], batch size: 19, lr: 1.52e-02,
2024-10-08 09:03:01,600 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=12336.666666666666, ans=0.125
2024-10-08 09:03:17,865 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=12343.333333333334, ans=0.125
2024-10-08 09:03:27,210 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=12343.333333333334, ans=0.38515
2024-10-08 09:03:32,339 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.47 vs. limit=16.759999999999998
2024-10-08 09:03:53,966 INFO [train.py:1153] Epoch 6, batch 2800, loss[loss=0.2776, simple_loss=0.3082, pruned_loss=0.0819, ctc_loss=0.2078, over 4773.00 frames. ], tot_loss[loss=0.2664, simple_loss=0.2894, pruned_loss=0.0862, ctc_loss=0.1776, over 967144.54 frames. ], batch size: 53, lr: 1.52e-02,
2024-10-08 09:03:57,511 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12353.333333333334, ans=0.17646666666666666
2024-10-08 09:03:58,682 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=12353.333333333334, ans=0.015194444444444441
2024-10-08 09:04:13,123 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.94 vs. limit=16.7675
2024-10-08 09:04:40,620 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.533e+01 1.211e+02 1.367e+02 1.552e+02 2.538e+02, threshold=2.733e+02, percent-clipped=0.0
2024-10-08 09:04:41,967 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=12366.666666666666, ans=0.125
2024-10-08 09:04:52,560 INFO [train.py:1153] Epoch 6, batch 2850, loss[loss=0.224, simple_loss=0.2656, pruned_loss=0.063, ctc_loss=0.1409, over 4937.00 frames. ], tot_loss[loss=0.2697, simple_loss=0.2916, pruned_loss=0.08769, ctc_loss=0.1809, over 966850.50 frames. ], batch size: 20, lr: 1.52e-02,
2024-10-08 09:04:59,770 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=12370.0, ans=0.015125
2024-10-08 09:05:25,876 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=12376.666666666666, ans=0.4668166666666667
2024-10-08 09:05:34,206 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12380.0, ans=0.1762
2024-10-08 09:05:42,477 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=12383.333333333334, ans=0.125
2024-10-08 09:05:51,642 INFO [train.py:1153] Epoch 6, batch 2900, loss[loss=0.2662, simple_loss=0.2949, pruned_loss=0.08288, ctc_loss=0.1795, over 4752.00 frames. ], tot_loss[loss=0.2697, simple_loss=0.2916, pruned_loss=0.08779, ctc_loss=0.1808, over 965923.95 frames. ], batch size: 20, lr: 1.52e-02,
2024-10-08 09:05:52,933 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=6.135e-03
2024-10-08 09:05:57,684 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=12386.666666666666, ans=0.015055555555555558
2024-10-08 09:06:07,234 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=12390.0, ans=0.015041666666666668
2024-10-08 09:06:10,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=12390.0, ans=0.125
2024-10-08 09:06:16,167 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn2.whiten.whitening_limit, batch_count=12393.333333333334, ans=16.795
2024-10-08 09:06:16,738 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=12393.333333333334, ans=0.125
2024-10-08 09:06:36,594 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12396.666666666666, ans=0.17603333333333332
2024-10-08 09:06:36,966 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.56 vs. limit=16.7975
2024-10-08 09:06:38,937 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.548e+01 1.187e+02 1.338e+02 1.502e+02 2.412e+02, threshold=2.675e+02, percent-clipped=0.0
2024-10-08 09:06:50,403 INFO [train.py:1153] Epoch 6, batch 2950, loss[loss=0.2067, simple_loss=0.2669, pruned_loss=0.05414, ctc_loss=0.0958, over 4797.00 frames. ], tot_loss[loss=0.2676, simple_loss=0.2902, pruned_loss=0.08678, ctc_loss=0.1787, over 966428.26 frames. ], batch size: 19, lr: 1.51e-02,
2024-10-08 09:06:56,546 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=12403.333333333334, ans=0.125
2024-10-08 09:07:17,910 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12410.0, ans=0.1759
2024-10-08 09:07:28,556 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=12413.333333333334, ans=0.01494444444444444
2024-10-08 09:07:45,795 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.18 vs. limit=12.15625
2024-10-08 09:07:50,469 INFO [train.py:1153] Epoch 6, batch 3000, loss[loss=0.2287, simple_loss=0.2752, pruned_loss=0.06505, ctc_loss=0.1303, over 4855.00 frames. ], tot_loss[loss=0.2686, simple_loss=0.2906, pruned_loss=0.08736, ctc_loss=0.1797, over 967161.23 frames. ], batch size: 21, lr: 1.51e-02,
2024-10-08 09:07:50,469 INFO [train.py:1176] Computing validation loss
2024-10-08 09:07:57,284 INFO [train.py:1185] Epoch 6, validation: loss=0.1776, simple_loss=0.2606, pruned_loss=0.03409, ctc_loss=0.06629, over 90464.00 frames.
2024-10-08 09:07:57,284 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 09:08:25,011 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=12426.666666666666, ans=0.125
2024-10-08 09:08:43,647 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.614e+01 1.235e+02 1.379e+02 1.568e+02 2.220e+02, threshold=2.758e+02, percent-clipped=0.0
2024-10-08 09:08:55,242 INFO [train.py:1153] Epoch 6, batch 3050, loss[loss=0.201, simple_loss=0.2495, pruned_loss=0.05403, ctc_loss=0.1111, over 4742.00 frames. ], tot_loss[loss=0.268, simple_loss=0.2904, pruned_loss=0.08696, ctc_loss=0.1794, over 966574.69 frames. ], batch size: 19, lr: 1.51e-02,
2024-10-08 09:09:15,458 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=12440.0, ans=0.025
2024-10-08 09:09:29,720 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=12446.666666666666, ans=0.008163768115942029
2024-10-08 09:09:33,286 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=12446.666666666666, ans=0.125
2024-10-08 09:09:40,491 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=12446.666666666666, ans=0.125
2024-10-08 09:09:54,612 INFO [train.py:1153] Epoch 6, batch 3100, loss[loss=0.3086, simple_loss=0.3215, pruned_loss=0.103, ctc_loss=0.224, over 4833.00 frames. ], tot_loss[loss=0.2665, simple_loss=0.2897, pruned_loss=0.08605, ctc_loss=0.1779, over 966312.07 frames. ], batch size: 38, lr: 1.51e-02,
2024-10-08 09:09:54,715 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=12453.333333333334, ans=0.125
2024-10-08 09:09:57,524 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.54 vs. limit=12.17
2024-10-08 09:09:59,858 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.39 vs. limit=12.17
2024-10-08 09:10:09,172 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=12456.666666666666, ans=0.025
2024-10-08 09:10:15,246 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=12456.666666666666, ans=0.014763888888888896
2024-10-08 09:10:38,970 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=12463.333333333334, ans=0.0
2024-10-08 09:10:42,702 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.342e+01 1.120e+02 1.289e+02 1.423e+02 1.873e+02, threshold=2.578e+02, percent-clipped=0.0
2024-10-08 09:10:46,312 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=12466.666666666666, ans=0.4636666666666667
2024-10-08 09:10:52,280 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=12466.666666666666, ans=0.125
2024-10-08 09:10:54,540 INFO [train.py:1153] Epoch 6, batch 3150, loss[loss=0.2879, simple_loss=0.2899, pruned_loss=0.101, ctc_loss=0.2094, over 4796.00 frames. ], tot_loss[loss=0.2655, simple_loss=0.289, pruned_loss=0.08557, ctc_loss=0.1774, over 966581.13 frames. ], batch size: 40, lr: 1.51e-02,
2024-10-08 09:10:54,658 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12470.0, ans=0.17529999999999998
2024-10-08 09:11:06,884 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12473.333333333334, ans=0.17526666666666668
2024-10-08 09:11:09,186 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=12473.333333333334, ans=0.01469444444444444
2024-10-08 09:11:18,677 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=12476.666666666666, ans=0.125
2024-10-08 09:11:29,468 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=12480.0, ans=0.125
2024-10-08 09:11:40,415 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.62 vs. limit=12.18
2024-10-08 09:11:40,651 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.59 vs. limit=4.872
2024-10-08 09:11:53,366 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=12486.666666666666, ans=0.125
2024-10-08 09:11:54,354 INFO [train.py:1153] Epoch 6, batch 3200, loss[loss=0.2641, simple_loss=0.2904, pruned_loss=0.08631, ctc_loss=0.1629, over 4748.00 frames. ], tot_loss[loss=0.2645, simple_loss=0.2881, pruned_loss=0.08515, ctc_loss=0.1762, over 967086.26 frames. ], batch size: 20, lr: 1.51e-02,
2024-10-08 09:11:58,132 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=12486.666666666666, ans=0.008155072463768117
2024-10-08 09:12:01,651 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=12486.666666666666, ans=0.014638888888888896
2024-10-08 09:12:14,959 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12490.0, ans=0.17509999999999998
2024-10-08 09:12:41,882 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.279e+01 1.127e+02 1.305e+02 1.453e+02 2.461e+02, threshold=2.610e+02, percent-clipped=0.0
2024-10-08 09:12:42,162 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12500.0, ans=0.175
2024-10-08 09:12:50,423 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=12500.0, ans=0.125
2024-10-08 09:12:53,817 INFO [train.py:1153] Epoch 6, batch 3250, loss[loss=0.3133, simple_loss=0.3144, pruned_loss=0.1092, ctc_loss=0.2341, over 4860.00 frames. ], tot_loss[loss=0.2658, simple_loss=0.2888, pruned_loss=0.08595, ctc_loss=0.1773, over 967224.98 frames. ], batch size: 24, lr: 1.51e-02,
2024-10-08 09:13:00,414 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.19 vs. limit=16.877499999999998
2024-10-08 09:13:07,263 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=12506.666666666666, ans=0.125
2024-10-08 09:13:13,093 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=12506.666666666666, ans=0.125
2024-10-08 09:13:19,581 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.39 vs. limit=12.19125
2024-10-08 09:13:21,578 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=12510.0, ans=0.014541666666666668
2024-10-08 09:13:41,244 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module1.whiten, num_groups=1, num_channels=192, metric=3.09 vs. limit=12.19375
2024-10-08 09:13:53,589 INFO [train.py:1153] Epoch 6, batch 3300, loss[loss=0.2559, simple_loss=0.2914, pruned_loss=0.07778, ctc_loss=0.1619, over 4839.00 frames. ], tot_loss[loss=0.2654, simple_loss=0.2887, pruned_loss=0.0856, ctc_loss=0.1771, over 967695.14 frames. ], batch size: 43, lr: 1.51e-02,
2024-10-08 09:13:53,704 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=12520.0, ans=0.125
2024-10-08 09:14:05,453 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=12523.333333333334, ans=0.125
2024-10-08 09:14:08,294 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=11.10 vs. limit=12.19625
2024-10-08 09:14:09,629 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=18.65 vs. limit=16.8925
2024-10-08 09:14:10,386 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=12523.333333333334, ans=0.125
2024-10-08 09:14:11,589 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=12523.333333333334, ans=0.125
2024-10-08 09:14:15,090 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=12523.333333333334, ans=0.17476666666666665
2024-10-08 09:14:15,122 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=12523.333333333334, ans=0.125
2024-10-08 09:14:32,877 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=12530.0, ans=0.8753
2024-10-08 09:14:40,990 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.093e+01 1.162e+02 1.318e+02 1.495e+02 2.025e+02, threshold=2.635e+02, percent-clipped=0.0
2024-10-08 09:14:52,799 INFO [train.py:1153] Epoch 6, batch 3350, loss[loss=0.2704, simple_loss=0.2833, pruned_loss=0.09202, ctc_loss=0.1839, over 4804.00 frames. ], tot_loss[loss=0.2671, simple_loss=0.2894, pruned_loss=0.08663, ctc_loss=0.1789, over 966986.43 frames. ], batch size: 40, lr: 1.51e-02,
2024-10-08 09:15:03,569 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=12540.0, ans=0.125
2024-10-08 09:15:18,103 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=12543.333333333334, ans=0.014402777777777771
2024-10-08 09:15:19,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=12543.333333333334, ans=0.014402777777777771
2024-10-08 09:15:19,280 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=12543.333333333334, ans=0.4609833333333333
2024-10-08 09:15:38,955 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=12550.0, ans=0.008141304347826088
2024-10-08 09:15:43,766 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=12550.0, ans=0.125
2024-10-08 09:15:45,013 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12550.0, ans=0.1745
2024-10-08 09:15:51,958 INFO [train.py:1153] Epoch 6, batch 3400, loss[loss=0.1876, simple_loss=0.246, pruned_loss=0.04396, ctc_loss=0.1034, over 4959.00 frames. ], tot_loss[loss=0.267, simple_loss=0.2895, pruned_loss=0.08657, ctc_loss=0.1786, over 966808.60 frames. ], batch size: 19, lr: 1.51e-02,
2024-10-08 09:15:57,896 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=12553.333333333334, ans=0.04949747468305833
2024-10-08 09:16:04,180 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=3.83 vs. limit=9.022666666666666
2024-10-08 09:16:20,705 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=12560.0, ans=0.125
2024-10-08 09:16:39,311 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.837e+01 1.199e+02 1.337e+02 1.553e+02 2.197e+02, threshold=2.675e+02, percent-clipped=0.0
2024-10-08 09:16:51,125 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.out_whiten.whitening_limit, batch_count=12570.0, ans=6.513999999999999
2024-10-08 09:16:51,509 INFO [train.py:1153] Epoch 6, batch 3450, loss[loss=0.2737, simple_loss=0.2983, pruned_loss=0.08818, ctc_loss=0.1818, over 4847.00 frames. ], tot_loss[loss=0.266, simple_loss=0.2893, pruned_loss=0.08586, ctc_loss=0.1774, over 967059.85 frames. ], batch size: 43, lr: 1.51e-02,
2024-10-08 09:17:15,482 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=12576.666666666666, ans=0.008135507246376811
2024-10-08 09:17:24,065 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=12576.666666666666, ans=0.04949747468305833
2024-10-08 09:17:51,601 INFO [train.py:1153] Epoch 6, batch 3500, loss[loss=0.2595, simple_loss=0.2905, pruned_loss=0.08052, ctc_loss=0.1687, over 4883.00 frames. ], tot_loss[loss=0.2667, simple_loss=0.2897, pruned_loss=0.0862, ctc_loss=0.1781, over 967418.48 frames. ], batch size: 19, lr: 1.50e-02,
2024-10-08 09:17:56,679 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=6.75 vs. limit=9.034666666666666
2024-10-08 09:18:15,670 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=12593.333333333334, ans=0.01419444444444444
2024-10-08 09:18:23,583 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.41 vs. limit=12.2225
2024-10-08 09:18:26,711 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 09:18:30,126 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=12596.666666666666, ans=0.0
2024-10-08 09:18:37,457 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12596.666666666666, ans=0.17403333333333335
2024-10-08 09:18:39,907 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.957e+01 1.215e+02 1.353e+02 1.531e+02 2.234e+02, threshold=2.707e+02, percent-clipped=0.0
2024-10-08 09:18:51,057 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.19 vs. limit=12.22625
2024-10-08 09:18:51,707 INFO [train.py:1153] Epoch 6, batch 3550, loss[loss=0.285, simple_loss=0.2964, pruned_loss=0.09463, ctc_loss=0.2105, over 4778.00 frames. ], tot_loss[loss=0.2654, simple_loss=0.289, pruned_loss=0.08539, ctc_loss=0.1774, over 967455.86 frames. ], batch size: 29, lr: 1.50e-02,
2024-10-08 09:18:55,422 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=12603.333333333334, ans=0.04949747468305833
2024-10-08 09:19:23,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=12610.0, ans=0.45865
2024-10-08 09:19:33,766 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=12613.333333333334, ans=0.125
2024-10-08 09:19:38,989 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.80 vs. limit=4.8925
2024-10-08 09:19:51,787 INFO [train.py:1153] Epoch 6, batch 3600, loss[loss=0.2134, simple_loss=0.2566, pruned_loss=0.05966, ctc_loss=0.127, over 4935.00 frames. ], tot_loss[loss=0.2653, simple_loss=0.2891, pruned_loss=0.08543, ctc_loss=0.1768, over 967649.45 frames. ], batch size: 20, lr: 1.50e-02,
2024-10-08 09:20:17,561 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.35 vs. limit=8.156666666666666
2024-10-08 09:20:31,078 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.50 vs. limit=8.1575
2024-10-08 09:20:39,734 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.791e+01 1.187e+02 1.290e+02 1.451e+02 2.085e+02, threshold=2.580e+02, percent-clipped=0.0
2024-10-08 09:20:41,200 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=12633.333333333334, ans=0.4578333333333333
2024-10-08 09:20:42,507 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=12633.333333333334, ans=0.125
2024-10-08 09:20:51,765 INFO [train.py:1153] Epoch 6, batch 3650, loss[loss=0.2507, simple_loss=0.2741, pruned_loss=0.07853, ctc_loss=0.1757, over 4868.00 frames. ], tot_loss[loss=0.2648, simple_loss=0.2883, pruned_loss=0.08533, ctc_loss=0.1766, over 968022.85 frames. ], batch size: 31, lr: 1.50e-02,
2024-10-08 09:21:02,868 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=12640.0, ans=0.125
2024-10-08 09:21:07,682 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer_ff3.min_abs, batch_count=12640.0, ans=0.2
2024-10-08 09:21:33,401 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.28 vs. limit=11.323333333333334
2024-10-08 09:21:44,413 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module2.whiten, num_groups=1, num_channels=192, metric=4.34 vs. limit=12.24375
2024-10-08 09:21:47,327 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12650.0, ans=0.1735
2024-10-08 09:21:51,898 INFO [train.py:1153] Epoch 6, batch 3700, loss[loss=0.2643, simple_loss=0.2892, pruned_loss=0.0832, ctc_loss=0.1828, over 4839.00 frames. ], tot_loss[loss=0.2648, simple_loss=0.2884, pruned_loss=0.08526, ctc_loss=0.1766, over 967382.46 frames. ], batch size: 24, lr: 1.50e-02,
2024-10-08 09:22:33,987 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12663.333333333334, ans=0.17336666666666667
2024-10-08 09:22:35,616 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=5.08 vs. limit=4.8995
2024-10-08 09:22:36,351 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=12663.333333333334, ans=0.01390277777777777
2024-10-08 09:22:37,478 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=12663.333333333334, ans=0.01390277777777777
2024-10-08 09:22:39,901 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.043e+01 1.211e+02 1.363e+02 1.486e+02 2.461e+02, threshold=2.726e+02, percent-clipped=0.0
2024-10-08 09:22:44,704 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=12666.666666666666, ans=0.125
2024-10-08 09:22:48,272 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=12666.666666666666, ans=0.125
2024-10-08 09:22:51,586 INFO [train.py:1153] Epoch 6, batch 3750, loss[loss=0.202, simple_loss=0.2459, pruned_loss=0.05361, ctc_loss=0.127, over 4959.00 frames. ], tot_loss[loss=0.2647, simple_loss=0.2889, pruned_loss=0.08511, ctc_loss=0.1759, over 967775.69 frames. ], batch size: 19, lr: 1.50e-02,
2024-10-08 09:23:01,257 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=12670.0, ans=0.17329999999999998
2024-10-08 09:23:03,662 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=12673.333333333334, ans=0.0
2024-10-08 09:23:19,115 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=12676.666666666666, ans=0.025
2024-10-08 09:23:21,614 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=12676.666666666666, ans=0.4563166666666667
2024-10-08 09:23:22,784 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=12676.666666666666, ans=0.013847222222222226
2024-10-08 09:23:23,288 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten.whitening_limit, batch_count=12676.666666666666, ans=12.25375
2024-10-08 09:23:51,721 INFO [train.py:1153] Epoch 6, batch 3800, loss[loss=0.2836, simple_loss=0.2928, pruned_loss=0.09821, ctc_loss=0.1948, over 4732.00 frames. ], tot_loss[loss=0.2652, simple_loss=0.2892, pruned_loss=0.08537, ctc_loss=0.176, over 967592.00 frames. ], batch size: 26, lr: 1.50e-02,
2024-10-08 09:23:55,590 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=12686.666666666666, ans=0.125
2024-10-08 09:23:59,073 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=12686.666666666666, ans=0.013805555555555557
2024-10-08 09:24:13,613 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=12690.0, ans=0.013791666666666667
2024-10-08 09:24:40,094 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.755e+01 1.162e+02 1.323e+02 1.495e+02 2.961e+02, threshold=2.646e+02, percent-clipped=1.0
2024-10-08 09:24:52,268 INFO [train.py:1153] Epoch 6, batch 3850, loss[loss=0.2752, simple_loss=0.2959, pruned_loss=0.09059, ctc_loss=0.1833, over 4820.00 frames. ], tot_loss[loss=0.2652, simple_loss=0.2892, pruned_loss=0.08536, ctc_loss=0.1762, over 967510.09 frames. ], batch size: 38, lr: 1.50e-02,
2024-10-08 09:24:52,896 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=16.39 vs. limit=17.0275
2024-10-08 09:25:11,735 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=12706.666666666666, ans=0.125
2024-10-08 09:25:20,012 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=12710.0, ans=0.125
2024-10-08 09:25:29,602 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=12713.333333333334, ans=0.17286666666666667
2024-10-08 09:25:42,425 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=12716.666666666666, ans=0.008105072463768116
2024-10-08 09:25:48,388 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=12716.666666666666, ans=0.025
2024-10-08 09:25:51,814 INFO [train.py:1153] Epoch 6, batch 3900, loss[loss=0.2205, simple_loss=0.2665, pruned_loss=0.06141, ctc_loss=0.129, over 4728.00 frames. ], tot_loss[loss=0.2655, simple_loss=0.2891, pruned_loss=0.08563, ctc_loss=0.1768, over 967184.00 frames. ], batch size: 26, lr: 1.50e-02,
2024-10-08 09:26:13,056 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 09:26:39,365 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.401e+01 1.183e+02 1.294e+02 1.498e+02 2.029e+02, threshold=2.588e+02, percent-clipped=0.0
2024-10-08 09:26:51,089 INFO [train.py:1153] Epoch 6, batch 3950, loss[loss=0.2447, simple_loss=0.2663, pruned_loss=0.0778, ctc_loss=0.1686, over 4824.00 frames. ], tot_loss[loss=0.2633, simple_loss=0.2879, pruned_loss=0.08452, ctc_loss=0.1744, over 967323.90 frames. ], batch size: 36, lr: 1.50e-02,
2024-10-08 09:26:51,314 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=12736.666666666666, ans=0.013597222222222226
2024-10-08 09:26:53,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=12736.666666666666, ans=0.125
2024-10-08 09:27:00,871 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=12736.666666666666, ans=0.013597222222222226
2024-10-08 09:27:04,382 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=12740.0, ans=0.125
2024-10-08 09:27:11,656 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=12740.0, ans=0.125
2024-10-08 09:27:29,481 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=12746.666666666666, ans=0.125
2024-10-08 09:27:39,237 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.94 vs. limit=17.0625
2024-10-08 09:27:43,144 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=7.99 vs. limit=12.28125
2024-10-08 09:27:50,668 INFO [train.py:1153] Epoch 6, batch 4000, loss[loss=0.2588, simple_loss=0.2946, pruned_loss=0.07919, ctc_loss=0.1616, over 4818.00 frames. ], tot_loss[loss=0.2635, simple_loss=0.2877, pruned_loss=0.08472, ctc_loss=0.1746, over 967349.27 frames. ], batch size: 19, lr: 1.49e-02,
2024-10-08 09:28:27,206 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.79 vs. limit=9.105333333333334
2024-10-08 09:28:38,638 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.787e+01 1.114e+02 1.290e+02 1.439e+02 2.071e+02, threshold=2.581e+02, percent-clipped=0.0
2024-10-08 09:28:50,688 INFO [train.py:1153] Epoch 6, batch 4050, loss[loss=0.2886, simple_loss=0.2992, pruned_loss=0.0983, ctc_loss=0.2032, over 4794.00 frames. ], tot_loss[loss=0.2656, simple_loss=0.2895, pruned_loss=0.08562, ctc_loss=0.1761, over 967587.69 frames. ], batch size: 53, lr: 1.49e-02,
2024-10-08 09:28:50,898 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=12770.0, ans=0.008093478260869565
2024-10-08 09:29:06,445 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=12773.333333333334, ans=0.01344444444444444
2024-10-08 09:29:13,321 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=12776.666666666666, ans=0.125
2024-10-08 09:29:35,142 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=12780.0, ans=0.05
2024-10-08 09:29:41,013 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=12783.333333333334, ans=0.125
2024-10-08 09:29:49,146 INFO [train.py:1153] Epoch 6, batch 4100, loss[loss=0.2641, simple_loss=0.2849, pruned_loss=0.08384, ctc_loss=0.1892, over 4843.00 frames. ], tot_loss[loss=0.2659, simple_loss=0.2895, pruned_loss=0.0859, ctc_loss=0.1764, over 966987.75 frames. ], batch size: 31, lr: 1.49e-02,
2024-10-08 09:29:49,806 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.15 vs. limit=12.295
2024-10-08 09:29:59,928 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=12790.0, ans=0.45235000000000003
2024-10-08 09:30:24,766 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten.whitening_limit, batch_count=12796.666666666666, ans=12.29875
2024-10-08 09:30:26,857 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=12796.666666666666, ans=0.125
2024-10-08 09:30:36,273 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.951e+01 1.209e+02 1.339e+02 1.499e+02 2.406e+02, threshold=2.678e+02, percent-clipped=0.0
2024-10-08 09:30:36,848 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=9.83 vs. limit=12.3
2024-10-08 09:30:46,263 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.87 vs. limit=4.92
2024-10-08 09:30:48,063 INFO [train.py:1153] Epoch 6, batch 4150, loss[loss=0.2693, simple_loss=0.3054, pruned_loss=0.08203, ctc_loss=0.1731, over 4757.00 frames. ], tot_loss[loss=0.2652, simple_loss=0.2889, pruned_loss=0.08553, ctc_loss=0.1759, over 967122.11 frames. ], batch size: 20, lr: 1.49e-02,
2024-10-08 09:31:12,492 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.84 vs. limit=17.1075
2024-10-08 09:31:21,304 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=12810.0, ans=0.125
2024-10-08 09:31:38,733 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=8.30 vs. limit=12.30625
2024-10-08 09:31:47,162 INFO [train.py:1153] Epoch 6, batch 4200, loss[loss=0.2961, simple_loss=0.3065, pruned_loss=0.1012, ctc_loss=0.2085, over 4866.00 frames. ], tot_loss[loss=0.2642, simple_loss=0.2884, pruned_loss=0.08509, ctc_loss=0.1748, over 967394.54 frames. ], batch size: 31, lr: 1.49e-02,
2024-10-08 09:32:01,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=12823.333333333334, ans=0.17176666666666665
2024-10-08 09:32:09,793 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=12826.666666666666, ans=0.125
2024-10-08 09:32:34,265 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.531e+01 1.151e+02 1.308e+02 1.474e+02 1.989e+02, threshold=2.616e+02, percent-clipped=0.0
2024-10-08 09:32:34,453 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=12833.333333333334, ans=0.4508333333333333
2024-10-08 09:32:38,089 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=12833.333333333334, ans=0.125
2024-10-08 09:32:46,207 INFO [train.py:1153] Epoch 6, batch 4250, loss[loss=0.2113, simple_loss=0.2612, pruned_loss=0.05766, ctc_loss=0.1151, over 4749.00 frames. ], tot_loss[loss=0.2633, simple_loss=0.2877, pruned_loss=0.08456, ctc_loss=0.1743, over 967107.10 frames. ], batch size: 19, lr: 1.49e-02,
2024-10-08 09:32:57,170 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=12840.0, ans=0.013166666666666667
2024-10-08 09:33:05,588 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.20 vs. limit=11.42
2024-10-08 09:33:19,497 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=12843.333333333334, ans=0.013152777777777777
2024-10-08 09:33:37,059 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=12850.0, ans=0.013125000000000005
2024-10-08 09:33:38,674 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.61 vs. limit=12.31875
2024-10-08 09:33:45,205 INFO [train.py:1153] Epoch 6, batch 4300, loss[loss=0.2944, simple_loss=0.3328, pruned_loss=0.09226, ctc_loss=0.1786, over 4845.00 frames. ], tot_loss[loss=0.2644, simple_loss=0.2883, pruned_loss=0.08517, ctc_loss=0.1754, over 967357.89 frames. ], batch size: 21, lr: 1.49e-02,
2024-10-08 09:33:52,338 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=12853.333333333334, ans=0.125
2024-10-08 09:33:56,399 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=4.84 vs. limit=12.32125
2024-10-08 09:34:01,647 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=12856.666666666666, ans=0.45001666666666673
2024-10-08 09:34:03,503 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=11.35 vs. limit=12.32125
2024-10-08 09:34:31,841 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.120e+01 1.222e+02 1.358e+02 1.501e+02 2.636e+02, threshold=2.717e+02, percent-clipped=1.0
2024-10-08 09:34:43,846 INFO [train.py:1153] Epoch 6, batch 4350, loss[loss=0.2354, simple_loss=0.2723, pruned_loss=0.07057, ctc_loss=0.1437, over 4835.00 frames. ], tot_loss[loss=0.2654, simple_loss=0.2892, pruned_loss=0.08559, ctc_loss=0.1761, over 966358.20 frames. ], batch size: 21, lr: 1.49e-02,
2024-10-08 09:34:44,443 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.76 vs. limit=12.32625
2024-10-08 09:34:44,472 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.86 vs. limit=4.9305
2024-10-08 09:35:09,403 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=12876.666666666666, ans=0.125
2024-10-08 09:35:14,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=12876.666666666666, ans=0.013013888888888894
2024-10-08 09:35:20,375 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=12880.0, ans=0.025
2024-10-08 09:35:40,841 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=12883.333333333334, ans=0.125
2024-10-08 09:35:42,110 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=12883.333333333334, ans=0.025
2024-10-08 09:35:43,405 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.min_positive, batch_count=12886.666666666666, ans=0.025
2024-10-08 09:35:44,398 INFO [train.py:1153] Epoch 6, batch 4400, loss[loss=0.2776, simple_loss=0.2941, pruned_loss=0.09102, ctc_loss=0.1978, over 4726.00 frames. ], tot_loss[loss=0.265, simple_loss=0.2889, pruned_loss=0.08534, ctc_loss=0.1758, over 966006.38 frames. ], batch size: 26, lr: 1.49e-02,
2024-10-08 09:35:59,116 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=12890.0, ans=0.44885
2024-10-08 09:36:06,934 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.04 vs. limit=12.33375
2024-10-08 09:36:21,045 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=12896.666666666666, ans=0.125
2024-10-08 09:36:28,469 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=12896.666666666666, ans=0.125
2024-10-08 09:36:33,301 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.370e+01 1.282e+02 1.423e+02 1.592e+02 2.103e+02, threshold=2.846e+02, percent-clipped=0.0
2024-10-08 09:36:38,439 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=12900.0, ans=0.4485
2024-10-08 09:36:45,647 INFO [train.py:1153] Epoch 6, batch 4450, loss[loss=0.2207, simple_loss=0.2671, pruned_loss=0.06008, ctc_loss=0.1354, over 4883.00 frames. ], tot_loss[loss=0.2674, simple_loss=0.2904, pruned_loss=0.08655, ctc_loss=0.1784, over 966301.18 frames. ], batch size: 19, lr: 1.49e-02,
2024-10-08 09:37:05,594 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=12906.666666666666, ans=0.0
2024-10-08 09:37:34,225 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=12916.666666666666, ans=0.125
2024-10-08 09:37:37,827 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=12916.666666666666, ans=0.125
2024-10-08 09:37:47,633 INFO [train.py:1153] Epoch 6, batch 4500, loss[loss=0.3235, simple_loss=0.3191, pruned_loss=0.1178, ctc_loss=0.2311, over 4835.00 frames. ], tot_loss[loss=0.2665, simple_loss=0.2901, pruned_loss=0.08598, ctc_loss=0.1772, over 966292.91 frames. ], batch size: 28, lr: 1.49e-02,
2024-10-08 09:37:49,236 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.17 vs. limit=12.344999999999999
2024-10-08 09:37:53,258 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.36 vs. limit=17.189999999999998
2024-10-08 09:38:12,769 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=12926.666666666666, ans=0.125
2024-10-08 09:38:18,858 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=12926.666666666666, ans=0.0
2024-10-08 09:38:37,181 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.097e+01 1.157e+02 1.295e+02 1.477e+02 1.956e+02, threshold=2.590e+02, percent-clipped=0.0
2024-10-08 09:38:39,800 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=12933.333333333334, ans=0.012777777777777777
2024-10-08 09:38:43,307 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=12933.333333333334, ans=0.08036666666666666
2024-10-08 09:38:49,649 INFO [train.py:1153] Epoch 6, batch 4550, loss[loss=0.3027, simple_loss=0.3312, pruned_loss=0.09841, ctc_loss=0.1936, over 4852.00 frames. ], tot_loss[loss=0.2664, simple_loss=0.2902, pruned_loss=0.08585, ctc_loss=0.177, over 966144.97 frames. ], batch size: 20, lr: 1.48e-02,
2024-10-08 09:39:03,680 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=1.135e-02
2024-10-08 09:39:12,332 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=12940.0, ans=0.125
2024-10-08 09:39:18,477 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=12943.333333333334, ans=0.44698333333333334
2024-10-08 09:39:33,523 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=12946.666666666666, ans=0.125
2024-10-08 09:39:34,730 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.attention_skip_rate, batch_count=12946.666666666666, ans=0.012722222222222225
2024-10-08 09:39:36,539 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=18.36 vs. limit=17.21
2024-10-08 09:39:51,924 INFO [train.py:1153] Epoch 6, batch 4600, loss[loss=0.3007, simple_loss=0.3136, pruned_loss=0.1004, ctc_loss=0.2175, over 4753.00 frames. ], tot_loss[loss=0.2648, simple_loss=0.2892, pruned_loss=0.08515, ctc_loss=0.1752, over 966421.84 frames. ], batch size: 45, lr: 1.48e-02,
2024-10-08 09:40:17,570 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.82 vs. limit=4.944
2024-10-08 09:40:21,939 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=12960.0, ans=0.125
2024-10-08 09:40:38,156 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=12963.333333333334, ans=0.125
2024-10-08 09:40:41,262 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.74 vs. limit=4.945
2024-10-08 09:40:41,761 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.337e+01 1.153e+02 1.271e+02 1.438e+02 1.835e+02, threshold=2.543e+02, percent-clipped=0.0
2024-10-08 09:40:44,401 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=12966.666666666666, ans=0.125
2024-10-08 09:40:46,925 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=12966.666666666666, ans=0.00805072463768116
2024-10-08 09:40:48,137 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=12966.666666666666, ans=0.025
2024-10-08 09:40:48,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=12966.666666666666, ans=0.125
2024-10-08 09:40:51,259 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.74 vs. limit=4.945
2024-10-08 09:40:54,278 INFO [train.py:1153] Epoch 6, batch 4650, loss[loss=0.2768, simple_loss=0.302, pruned_loss=0.08965, ctc_loss=0.1805, over 4828.00 frames. ], tot_loss[loss=0.2635, simple_loss=0.2886, pruned_loss=0.08443, ctc_loss=0.174, over 965792.37 frames. ], batch size: 36, lr: 1.48e-02,
2024-10-08 09:41:06,933 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=12973.333333333334, ans=0.125
2024-10-08 09:41:12,426 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=12973.333333333334, ans=0.125
2024-10-08 09:41:29,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=12976.666666666666, ans=0.17023333333333335
2024-10-08 09:41:34,780 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=12980.0, ans=0.1702
2024-10-08 09:41:40,861 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=12980.0, ans=0.1702
2024-10-08 09:41:43,474 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=12983.333333333334, ans=0.125
2024-10-08 09:41:44,763 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=12983.333333333334, ans=0.125
2024-10-08 09:41:57,033 INFO [train.py:1153] Epoch 6, batch 4700, loss[loss=0.232, simple_loss=0.2658, pruned_loss=0.07066, ctc_loss=0.1423, over 4940.00 frames. ], tot_loss[loss=0.2645, simple_loss=0.2891, pruned_loss=0.08509, ctc_loss=0.1746, over 965749.14 frames. ], batch size: 19, lr: 1.48e-02,
2024-10-08 09:42:00,354 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=3.95 vs. limit=9.194666666666667
2024-10-08 09:42:47,515 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.107e+01 1.232e+02 1.323e+02 1.487e+02 2.255e+02, threshold=2.645e+02, percent-clipped=0.0
2024-10-08 09:42:50,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=13000.0, ans=0.125
2024-10-08 09:42:59,735 INFO [train.py:1153] Epoch 6, batch 4750, loss[loss=0.2609, simple_loss=0.2884, pruned_loss=0.07966, ctc_loss=0.1849, over 4735.00 frames. ], tot_loss[loss=0.2663, simple_loss=0.2902, pruned_loss=0.08597, ctc_loss=0.1763, over 965514.47 frames. ], batch size: 45, lr: 1.48e-02,
2024-10-08 09:43:11,294 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=13006.666666666666, ans=0.16993333333333333
2024-10-08 09:43:40,717 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.44 vs. limit=12.379999999999999
2024-10-08 09:44:02,552 INFO [train.py:1153] Epoch 6, batch 4800, loss[loss=0.2206, simple_loss=0.2503, pruned_loss=0.06546, ctc_loss=0.1499, over 4862.00 frames. ], tot_loss[loss=0.2649, simple_loss=0.2889, pruned_loss=0.0853, ctc_loss=0.1756, over 965882.46 frames. ], batch size: 22, lr: 1.48e-02,
2024-10-08 09:44:03,937 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=13020.0, ans=0.00803913043478261
2024-10-08 09:44:16,752 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=13023.333333333334, ans=0.125
2024-10-08 09:44:20,369 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=13023.333333333334, ans=0.125
2024-10-08 09:44:26,845 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=13026.666666666666, ans=0.125
2024-10-08 09:44:37,310 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=6.46 vs. limit=9.210666666666667
2024-10-08 09:44:44,182 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=13030.0, ans=0.125
2024-10-08 09:44:50,528 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=13030.0, ans=0.125
2024-10-08 09:44:52,932 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.695e+01 1.183e+02 1.313e+02 1.481e+02 2.632e+02, threshold=2.627e+02, percent-clipped=0.0
2024-10-08 09:45:01,908 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 09:45:05,473 INFO [train.py:1153] Epoch 6, batch 4850, loss[loss=0.3023, simple_loss=0.315, pruned_loss=0.1068, ctc_loss=0.1899, over 4848.00 frames. ], tot_loss[loss=0.2641, simple_loss=0.2884, pruned_loss=0.08488, ctc_loss=0.1751, over 966552.15 frames. ], batch size: 28, lr: 1.48e-02,
2024-10-08 09:45:06,845 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=13036.666666666666, ans=0.025
2024-10-08 09:45:09,452 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=13036.666666666666, ans=0.125
2024-10-08 09:45:25,661 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=13040.0, ans=0.44360000000000005
2024-10-08 09:46:03,826 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=2.54 vs. limit=12.39375
2024-10-08 09:46:08,374 INFO [train.py:1153] Epoch 6, batch 4900, loss[loss=0.2731, simple_loss=0.2846, pruned_loss=0.09282, ctc_loss=0.19, over 4861.00 frames. ], tot_loss[loss=0.2639, simple_loss=0.2881, pruned_loss=0.08483, ctc_loss=0.1752, over 967209.03 frames. ], batch size: 21, lr: 1.48e-02,
2024-10-08 09:46:17,173 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 09:46:40,472 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.59 vs. limit=11.530000000000001
2024-10-08 09:46:58,622 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.677e+01 1.173e+02 1.310e+02 1.479e+02 2.083e+02, threshold=2.621e+02, percent-clipped=0.0
2024-10-08 09:47:00,385 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.34 vs. limit=9.226666666666667
2024-10-08 09:47:01,176 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=13066.666666666666, ans=0.012222222222222225
2024-10-08 09:47:10,805 INFO [train.py:1153] Epoch 6, batch 4950, loss[loss=0.3026, simple_loss=0.3122, pruned_loss=0.1064, ctc_loss=0.2003, over 4767.00 frames. ], tot_loss[loss=0.2657, simple_loss=0.2889, pruned_loss=0.08582, ctc_loss=0.1772, over 966824.30 frames. ], batch size: 53, lr: 1.48e-02,
2024-10-08 09:48:00,214 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=13083.333333333334, ans=0.00802536231884058
2024-10-08 09:48:01,850 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.85 vs. limit=4.9625
2024-10-08 09:48:05,711 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.86 vs. limit=8.270833333333334
2024-10-08 09:48:12,929 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=13086.666666666666, ans=0.16913333333333333
2024-10-08 09:48:13,947 INFO [train.py:1153] Epoch 6, batch 5000, loss[loss=0.2246, simple_loss=0.2586, pruned_loss=0.06473, ctc_loss=0.1526, over 4796.00 frames. ], tot_loss[loss=0.2642, simple_loss=0.2885, pruned_loss=0.08492, ctc_loss=0.1752, over 967794.65 frames. ], batch size: 29, lr: 1.48e-02,
2024-10-08 09:48:35,598 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=13090.0, ans=0.125
2024-10-08 09:49:01,240 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.48 vs. limit=11.548333333333332
2024-10-08 09:49:04,533 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.181e+01 1.171e+02 1.277e+02 1.458e+02 1.869e+02, threshold=2.554e+02, percent-clipped=0.0
2024-10-08 09:49:07,186 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 09:49:17,325 INFO [train.py:1153] Epoch 6, batch 5050, loss[loss=0.2404, simple_loss=0.2813, pruned_loss=0.06987, ctc_loss=0.1495, over 4850.00 frames. ], tot_loss[loss=0.2634, simple_loss=0.2875, pruned_loss=0.08473, ctc_loss=0.1748, over 968681.88 frames. ], batch size: 19, lr: 1.48e-02,
2024-10-08 09:49:34,062 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.93 vs. limit=17.33
2024-10-08 09:49:36,057 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 09:49:55,936 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=13113.333333333334, ans=0.125
2024-10-08 09:50:04,697 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=13113.333333333334, ans=0.8811333333333333
2024-10-08 09:50:16,330 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.60 vs. limit=8.279166666666667
2024-10-08 09:50:19,615 INFO [train.py:1153] Epoch 6, batch 5100, loss[loss=0.2259, simple_loss=0.256, pruned_loss=0.06857, ctc_loss=0.1465, over 4819.00 frames. ], tot_loss[loss=0.2652, simple_loss=0.2884, pruned_loss=0.08568, ctc_loss=0.1765, over 967802.20 frames. ], batch size: 19, lr: 1.47e-02,
2024-10-08 09:50:30,572 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.07 vs. limit=17.34
2024-10-08 09:50:40,490 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=10.56 vs. limit=12.42125
2024-10-08 09:51:00,828 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten.whitening_limit, batch_count=13130.0, ans=12.42375
2024-10-08 09:51:09,119 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=13133.333333333334, ans=0.125
2024-10-08 09:51:10,146 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.006e+01 1.160e+02 1.279e+02 1.460e+02 1.916e+02, threshold=2.558e+02, percent-clipped=0.0
2024-10-08 09:51:10,896 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.17 vs. limit=11.566666666666666
2024-10-08 09:51:19,076 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=13133.333333333334, ans=0.125
2024-10-08 09:51:22,647 INFO [train.py:1153] Epoch 6, batch 5150, loss[loss=0.3149, simple_loss=0.311, pruned_loss=0.114, ctc_loss=0.2271, over 4837.00 frames. ], tot_loss[loss=0.265, simple_loss=0.2886, pruned_loss=0.08547, ctc_loss=0.176, over 967954.38 frames. ], batch size: 36, lr: 1.47e-02,
2024-10-08 09:51:32,854 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=13136.666666666666, ans=0.16863333333333333
2024-10-08 09:51:34,710 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.78 vs. limit=17.355
2024-10-08 09:51:53,199 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=3.73 vs. limit=9.257333333333333
2024-10-08 09:52:00,633 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=13146.666666666666, ans=0.3972
2024-10-08 09:52:09,684 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=13146.666666666666, ans=0.011888888888888893
2024-10-08 09:52:25,482 INFO [train.py:1153] Epoch 6, batch 5200, loss[loss=0.3879, simple_loss=0.3744, pruned_loss=0.1449, ctc_loss=0.2792, over 4764.00 frames. ], tot_loss[loss=0.266, simple_loss=0.2894, pruned_loss=0.08591, ctc_loss=0.177, over 967608.75 frames. ], batch size: 29, lr: 1.47e-02,
2024-10-08 09:52:28,095 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=13153.333333333334, ans=0.008010144927536232
2024-10-08 09:52:40,494 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=13156.666666666666, ans=0.008009420289855074
2024-10-08 09:52:55,430 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer2.prob, batch_count=13160.0, ans=0.125
2024-10-08 09:53:15,292 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.591e+01 1.139e+02 1.314e+02 1.449e+02 2.042e+02, threshold=2.628e+02, percent-clipped=0.0
2024-10-08 09:53:28,009 INFO [train.py:1153] Epoch 6, batch 5250, loss[loss=0.2294, simple_loss=0.2779, pruned_loss=0.06437, ctc_loss=0.1303, over 4862.00 frames. ], tot_loss[loss=0.2638, simple_loss=0.2881, pruned_loss=0.0848, ctc_loss=0.1747, over 967693.52 frames. ], batch size: 20, lr: 1.47e-02,
2024-10-08 09:53:28,123 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=13170.0, ans=0.008006521739130435
2024-10-08 09:53:40,647 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=13173.333333333334, ans=0.011777777777777776
2024-10-08 09:53:43,653 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.57 vs. limit=17.380000000000003
2024-10-08 09:53:49,574 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=13173.333333333334, ans=0.125
2024-10-08 09:53:52,097 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=13176.666666666666, ans=0.125
2024-10-08 09:54:04,661 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=13180.0, ans=0.008004347826086958
2024-10-08 09:54:14,485 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer1.prob, batch_count=13180.0, ans=0.125
2024-10-08 09:54:22,248 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=13183.333333333334, ans=0.025
2024-10-08 09:54:30,776 INFO [train.py:1153] Epoch 6, batch 5300, loss[loss=0.2639, simple_loss=0.2926, pruned_loss=0.08244, ctc_loss=0.176, over 4819.00 frames. ], tot_loss[loss=0.2628, simple_loss=0.287, pruned_loss=0.08451, ctc_loss=0.1739, over 967712.04 frames. ], batch size: 38, lr: 1.47e-02,
2024-10-08 09:54:37,214 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=13186.666666666666, ans=0.008002898550724637
2024-10-08 09:54:38,005 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module1.whiten, num_groups=1, num_channels=192, metric=2.93 vs. limit=12.445
2024-10-08 09:55:06,365 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=1.119e-01
2024-10-08 09:55:12,580 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=13196.666666666666, ans=0.125
2024-10-08 09:55:12,774 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 09:55:21,438 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.350e+01 1.226e+02 1.346e+02 1.486e+02 1.946e+02, threshold=2.692e+02, percent-clipped=0.0
2024-10-08 09:55:34,089 INFO [train.py:1153] Epoch 6, batch 5350, loss[loss=0.2345, simple_loss=0.2564, pruned_loss=0.07525, ctc_loss=0.1551, over 4978.00 frames. ], tot_loss[loss=0.2634, simple_loss=0.2876, pruned_loss=0.08464, ctc_loss=0.1746, over 967055.17 frames. ], batch size: 19, lr: 1.47e-02,
2024-10-08 09:55:44,416 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=13203.333333333334, ans=0.00799927536231884
2024-10-08 09:56:01,244 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.52 vs. limit=12.45375
2024-10-08 09:56:13,399 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=13213.333333333334, ans=0.125
2024-10-08 09:56:36,554 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.30 vs. limit=12.4575
2024-10-08 09:56:37,246 INFO [train.py:1153] Epoch 6, batch 5400, loss[loss=0.2713, simple_loss=0.2932, pruned_loss=0.08941, ctc_loss=0.1763, over 4817.00 frames. ], tot_loss[loss=0.2648, simple_loss=0.2888, pruned_loss=0.08526, ctc_loss=0.1759, over 966332.77 frames. ], batch size: 49, lr: 1.47e-02,
2024-10-08 09:56:38,757 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=13220.0, ans=0.125
2024-10-08 09:56:43,757 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=13220.0, ans=0.125
2024-10-08 09:56:47,434 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=13220.0, ans=0.4373
2024-10-08 09:56:56,356 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=13223.333333333334, ans=0.16776666666666668
2024-10-08 09:56:57,505 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=13223.333333333334, ans=0.125
2024-10-08 09:57:02,603 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=13226.666666666666, ans=0.0
2024-10-08 09:57:09,172 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=13226.666666666666, ans=0.007994202898550726
2024-10-08 09:57:19,053 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=13230.0, ans=0.011541666666666672
2024-10-08 09:57:27,774 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.909e+01 1.190e+02 1.311e+02 1.448e+02 2.025e+02, threshold=2.621e+02, percent-clipped=0.0
2024-10-08 09:57:40,609 INFO [train.py:1153] Epoch 6, batch 5450, loss[loss=0.2225, simple_loss=0.267, pruned_loss=0.06186, ctc_loss=0.1356, over 4940.00 frames. ], tot_loss[loss=0.2621, simple_loss=0.287, pruned_loss=0.084, ctc_loss=0.1732, over 967053.34 frames. ], batch size: 19, lr: 1.47e-02,
2024-10-08 09:57:42,069 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=13236.666666666666, ans=0.025
2024-10-08 09:57:45,918 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=13236.666666666666, ans=0.011513888888888893
2024-10-08 09:58:04,997 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=13243.333333333334, ans=0.125
2024-10-08 09:58:21,386 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.max_abs, batch_count=13246.666666666666, ans=10.0
2024-10-08 09:58:30,504 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=6.27 vs. limit=9.3
2024-10-08 09:58:32,586 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=13250.0, ans=0.00798913043478261
2024-10-08 09:58:41,488 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=13250.0, ans=0.125
2024-10-08 09:58:43,947 INFO [train.py:1153] Epoch 6, batch 5500, loss[loss=0.2897, simple_loss=0.3011, pruned_loss=0.1011, ctc_loss=0.19, over 4814.00 frames. ], tot_loss[loss=0.2616, simple_loss=0.2867, pruned_loss=0.08375, ctc_loss=0.1724, over 967382.54 frames. ], batch size: 49, lr: 1.47e-02,
2024-10-08 09:59:09,533 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=13260.0, ans=0.125
2024-10-08 09:59:29,864 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=13263.333333333334, ans=0.011402777777777776
2024-10-08 09:59:30,584 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.46 vs. limit=17.447499999999998
2024-10-08 09:59:32,376 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=13263.333333333334, ans=0.04949747468305833
2024-10-08 09:59:34,934 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.682e+01 1.176e+02 1.301e+02 1.466e+02 1.842e+02, threshold=2.601e+02, percent-clipped=0.0
2024-10-08 09:59:47,490 INFO [train.py:1153] Epoch 6, batch 5550, loss[loss=0.2124, simple_loss=0.2518, pruned_loss=0.0601, ctc_loss=0.1321, over 4796.00 frames. ], tot_loss[loss=0.261, simple_loss=0.2867, pruned_loss=0.08335, ctc_loss=0.1715, over 967261.57 frames. ], batch size: 19, lr: 1.47e-02,
2024-10-08 09:59:47,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=13270.0, ans=0.16729999999999998
2024-10-08 09:59:47,695 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=13270.0, ans=0.07
2024-10-08 10:00:10,643 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=13273.333333333334, ans=0.011361111111111107
2024-10-08 10:00:10,650 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=13273.333333333334, ans=0.007984057971014494
2024-10-08 10:00:11,953 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=13276.666666666666, ans=0.16723333333333334
2024-10-08 10:00:13,087 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=13276.666666666666, ans=0.011347222222222224
2024-10-08 10:00:23,164 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=13276.666666666666, ans=0.125
2024-10-08 10:00:38,294 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=13283.333333333334, ans=0.125
2024-10-08 10:00:40,897 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=13283.333333333334, ans=0.0
2024-10-08 10:00:43,378 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=13283.333333333334, ans=0.09899494936611666
2024-10-08 10:00:48,435 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=13283.333333333334, ans=0.011319444444444438
2024-10-08 10:00:50,788 INFO [train.py:1153] Epoch 6, batch 5600, loss[loss=0.2207, simple_loss=0.2611, pruned_loss=0.06242, ctc_loss=0.1385, over 4874.00 frames. ], tot_loss[loss=0.2623, simple_loss=0.2872, pruned_loss=0.08415, ctc_loss=0.1731, over 967176.01 frames. ], batch size: 28, lr: 1.47e-02,
2024-10-08 10:00:50,920 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=13286.666666666666, ans=0.125
2024-10-08 10:00:59,123 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.96 vs. limit=4.993
2024-10-08 10:01:07,560 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=13290.0, ans=0.011291666666666672
2024-10-08 10:01:20,995 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.80 vs. limit=8.323333333333334
2024-10-08 10:01:32,445 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=3.87 vs. limit=12.48625
2024-10-08 10:01:33,024 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=13296.666666666666, ans=0.4346166666666667
2024-10-08 10:01:35,990 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=2.58 vs. limit=12.48625
2024-10-08 10:01:41,876 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.427e+01 1.167e+02 1.346e+02 1.520e+02 2.200e+02, threshold=2.691e+02, percent-clipped=0.0
2024-10-08 10:01:55,020 INFO [train.py:1153] Epoch 6, batch 5650, loss[loss=0.2637, simple_loss=0.2854, pruned_loss=0.08676, ctc_loss=0.1712, over 4738.00 frames. ], tot_loss[loss=0.2602, simple_loss=0.2858, pruned_loss=0.08307, ctc_loss=0.1711, over 967015.01 frames. ], batch size: 45, lr: 1.46e-02,
2024-10-08 10:02:04,410 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.06 vs. limit=11.651666666666667
2024-10-08 10:02:10,755 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.37 vs. limit=12.49
2024-10-08 10:02:18,898 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=13310.0, ans=0.011208333333333334
2024-10-08 10:02:23,318 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.99 vs. limit=4.9965
2024-10-08 10:02:23,994 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=13310.0, ans=0.025
2024-10-08 10:02:30,286 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=13310.0, ans=0.125
2024-10-08 10:02:35,283 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=13313.333333333334, ans=0.0
2024-10-08 10:02:57,944 INFO [train.py:1153] Epoch 6, batch 5700, loss[loss=0.2141, simple_loss=0.2498, pruned_loss=0.06071, ctc_loss=0.1427, over 4878.00 frames. ], tot_loss[loss=0.2597, simple_loss=0.2856, pruned_loss=0.08284, ctc_loss=0.1705, over 966433.68 frames. ], batch size: 22, lr: 1.46e-02,
2024-10-08 10:03:06,854 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer_ff3.min_abs, batch_count=13320.0, ans=0.2
2024-10-08 10:03:09,472 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=13323.333333333334, ans=0.4336833333333333
2024-10-08 10:03:12,460 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.40 vs. limit=17.4925
2024-10-08 10:03:27,292 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=13326.666666666666, ans=0.025
2024-10-08 10:03:38,901 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=13330.0, ans=0.125
2024-10-08 10:03:47,805 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/checkpoint-40000.pt
2024-10-08 10:03:49,719 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.651e+01 1.158e+02 1.306e+02 1.450e+02 1.957e+02, threshold=2.611e+02, percent-clipped=0.0
2024-10-08 10:03:53,883 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=7.82 vs. limit=12.5
2024-10-08 10:04:01,858 INFO [train.py:1153] Epoch 6, batch 5750, loss[loss=0.3492, simple_loss=0.352, pruned_loss=0.1257, ctc_loss=0.2374, over 4845.00 frames. ], tot_loss[loss=0.2614, simple_loss=0.2867, pruned_loss=0.0836, ctc_loss=0.1723, over 966843.93 frames. ], batch size: 43, lr: 1.46e-02,
2024-10-08 10:04:24,850 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.51 vs. limit=17.505000000000003
2024-10-08 10:04:31,779 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=13343.333333333334, ans=0.007968840579710146
2024-10-08 10:04:41,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=13346.666666666666, ans=0.0
2024-10-08 10:04:46,984 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=13346.666666666666, ans=0.011055555555555562
2024-10-08 10:04:56,523 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.98 vs. limit=12.50625
2024-10-08 10:05:04,533 INFO [train.py:1153] Epoch 6, batch 5800, loss[loss=0.3117, simple_loss=0.3274, pruned_loss=0.1057, ctc_loss=0.2116, over 4835.00 frames. ], tot_loss[loss=0.2626, simple_loss=0.2874, pruned_loss=0.08422, ctc_loss=0.1735, over 966290.47 frames. ], batch size: 43, lr: 1.46e-02,
2024-10-08 10:05:07,694 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=12.82 vs. limit=12.5075
2024-10-08 10:05:40,128 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=13360.0, ans=0.0
2024-10-08 10:05:55,501 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.522e+01 1.142e+02 1.272e+02 1.432e+02 2.163e+02, threshold=2.543e+02, percent-clipped=0.0
2024-10-08 10:06:08,229 INFO [train.py:1153] Epoch 6, batch 5850, loss[loss=0.3343, simple_loss=0.3171, pruned_loss=0.1241, ctc_loss=0.2587, over 4775.00 frames. ], tot_loss[loss=0.2614, simple_loss=0.2865, pruned_loss=0.08372, ctc_loss=0.1724, over 966729.88 frames. ], batch size: 45, lr: 1.46e-02,
2024-10-08 10:06:14,003 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=13370.0, ans=0.11629999999999999
2024-10-08 10:06:16,321 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=13370.0, ans=0.00796304347826087
2024-10-08 10:06:30,312 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=13373.333333333334, ans=0.125
2024-10-08 10:06:41,946 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=18.52 vs. limit=17.5325
2024-10-08 10:06:52,053 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.91 vs. limit=12.5175
2024-10-08 10:07:04,350 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=13383.333333333334, ans=0.125
2024-10-08 10:07:11,819 INFO [train.py:1153] Epoch 6, batch 5900, loss[loss=0.3225, simple_loss=0.3199, pruned_loss=0.1141, ctc_loss=0.2424, over 4789.00 frames. ], tot_loss[loss=0.2617, simple_loss=0.2866, pruned_loss=0.08381, ctc_loss=0.1728, over 966826.63 frames. ], batch size: 34, lr: 1.46e-02,
2024-10-08 10:07:22,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=13386.666666666666, ans=0.125
2024-10-08 10:07:41,612 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.05 vs. limit=12.5225
2024-10-08 10:08:02,478 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.626e+01 1.138e+02 1.294e+02 1.452e+02 1.839e+02, threshold=2.589e+02, percent-clipped=0.0
2024-10-08 10:08:07,786 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=13400.0, ans=0.43100000000000005
2024-10-08 10:08:14,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=13403.333333333334, ans=0.010819444444444444
2024-10-08 10:08:15,194 INFO [train.py:1153] Epoch 6, batch 5950, loss[loss=0.2816, simple_loss=0.2972, pruned_loss=0.09229, ctc_loss=0.2037, over 4804.00 frames. ], tot_loss[loss=0.2596, simple_loss=0.2853, pruned_loss=0.0828, ctc_loss=0.1707, over 966222.51 frames. ], batch size: 34, lr: 1.46e-02,
2024-10-08 10:08:32,023 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=13406.666666666666, ans=0.0
2024-10-08 10:08:34,289 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.65 vs. limit=5.011
2024-10-08 10:08:42,385 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=13410.0, ans=0.007954347826086956
2024-10-08 10:09:09,104 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=13416.666666666666, ans=0.007952898550724638
2024-10-08 10:09:11,633 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=13416.666666666666, ans=0.125
2024-10-08 10:09:14,263 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=13416.666666666666, ans=0.4304166666666667
2024-10-08 10:09:19,177 INFO [train.py:1153] Epoch 6, batch 6000, loss[loss=0.3194, simple_loss=0.3218, pruned_loss=0.1138, ctc_loss=0.2237, over 4778.00 frames. ], tot_loss[loss=0.2616, simple_loss=0.2866, pruned_loss=0.08371, ctc_loss=0.1729, over 966774.00 frames. ], batch size: 49, lr: 1.46e-02,
2024-10-08 10:09:19,178 INFO [train.py:1176] Computing validation loss
2024-10-08 10:09:19,974 INFO [zipformer.py:1858] name=encoder.encoders.5.encoder.layers.1.self_attn_weights, attn_weights_entropy = tensor([4.1703, 4.3379, 2.7143, 2.1711], device='cuda:0')
2024-10-08 10:09:26,842 INFO [train.py:1185] Epoch 6, validation: loss=0.1758, simple_loss=0.2583, pruned_loss=0.03348, ctc_loss=0.06592, over 90464.00 frames.
2024-10-08 10:09:26,842 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 10:09:41,298 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.83 vs. limit=5.0135000000000005
2024-10-08 10:09:50,969 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=13426.666666666666, ans=0.125
2024-10-08 10:09:59,912 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=13426.666666666666, ans=0.4300666666666667
2024-10-08 10:10:03,568 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=13430.0, ans=0.1657
2024-10-08 10:10:17,331 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.171e+01 1.175e+02 1.318e+02 1.499e+02 2.096e+02, threshold=2.636e+02, percent-clipped=0.0
2024-10-08 10:10:30,232 INFO [train.py:1153] Epoch 6, batch 6050, loss[loss=0.2175, simple_loss=0.266, pruned_loss=0.05997, ctc_loss=0.1226, over 4812.00 frames. ], tot_loss[loss=0.2611, simple_loss=0.2864, pruned_loss=0.08353, ctc_loss=0.1719, over 966635.51 frames. ], batch size: 19, lr: 1.46e-02,
2024-10-08 10:10:31,566 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=13436.666666666666, ans=0.125
2024-10-08 10:10:58,429 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=13443.333333333334, ans=0.125
2024-10-08 10:11:26,802 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.62 vs. limit=17.5875
2024-10-08 10:11:31,313 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=13450.0, ans=0.010625000000000002
2024-10-08 10:11:33,711 INFO [train.py:1153] Epoch 6, batch 6100, loss[loss=0.3061, simple_loss=0.32, pruned_loss=0.1033, ctc_loss=0.2137, over 4804.00 frames. ], tot_loss[loss=0.2617, simple_loss=0.2869, pruned_loss=0.08377, ctc_loss=0.1723, over 966299.25 frames. ], batch size: 34, lr: 1.46e-02,
2024-10-08 10:11:36,334 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=13453.333333333334, ans=0.125
2024-10-08 10:11:47,805 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=13456.666666666666, ans=0.125
2024-10-08 10:11:59,479 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=13460.0, ans=0.010583333333333333
2024-10-08 10:12:19,344 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=5.43 vs. limit=9.385333333333334
2024-10-08 10:12:25,140 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.210e+01 1.157e+02 1.320e+02 1.460e+02 2.058e+02, threshold=2.640e+02, percent-clipped=0.0
2024-10-08 10:12:25,375 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=13466.666666666666, ans=0.125
2024-10-08 10:12:37,674 INFO [train.py:1153] Epoch 6, batch 6150, loss[loss=0.245, simple_loss=0.2752, pruned_loss=0.07506, ctc_loss=0.1617, over 4828.00 frames. ], tot_loss[loss=0.2605, simple_loss=0.2861, pruned_loss=0.08321, ctc_loss=0.1714, over 966461.75 frames. ], batch size: 43, lr: 1.46e-02,
2024-10-08 10:12:37,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=13470.0, ans=0.125
2024-10-08 10:12:39,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=13470.0, ans=0.007941304347826087
2024-10-08 10:12:51,823 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 10:13:09,231 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=11.05 vs. limit=11.738333333333333
2024-10-08 10:13:09,830 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=13476.666666666666, ans=0.010513888888888892
2024-10-08 10:13:10,304 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.54 vs. limit=17.6075
2024-10-08 10:13:13,487 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=13476.666666666666, ans=0.16523333333333334
2024-10-08 10:13:19,092 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.46 vs. limit=8.370000000000001
2024-10-08 10:13:19,913 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=13480.0, ans=0.125
2024-10-08 10:13:30,204 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=13483.333333333334, ans=0.025
2024-10-08 10:13:41,521 INFO [train.py:1153] Epoch 6, batch 6200, loss[loss=0.3152, simple_loss=0.317, pruned_loss=0.1127, ctc_loss=0.2201, over 4782.00 frames. ], tot_loss[loss=0.2601, simple_loss=0.2856, pruned_loss=0.08317, ctc_loss=0.1709, over 966706.95 frames. ], batch size: 29, lr: 1.45e-02,
2024-10-08 10:13:50,836 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=13486.666666666666, ans=0.0
2024-10-08 10:13:52,050 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 10:14:32,954 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.151e+01 1.214e+02 1.327e+02 1.487e+02 1.937e+02, threshold=2.655e+02, percent-clipped=0.0
2024-10-08 10:14:33,120 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=13500.0, ans=0.125
2024-10-08 10:14:36,925 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=13500.0, ans=0.010416666666666671
2024-10-08 10:14:44,941 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=13503.333333333334, ans=0.007934057971014492
2024-10-08 10:14:46,004 INFO [train.py:1153] Epoch 6, batch 6250, loss[loss=0.2249, simple_loss=0.2612, pruned_loss=0.06543, ctc_loss=0.1441, over 4735.00 frames. ], tot_loss[loss=0.26, simple_loss=0.2858, pruned_loss=0.08303, ctc_loss=0.1701, over 967023.65 frames. ], batch size: 26, lr: 1.45e-02,
2024-10-08 10:14:57,779 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=13506.666666666666, ans=0.09899494936611666
2024-10-08 10:15:06,584 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=13506.666666666666, ans=0.125
2024-10-08 10:15:08,286 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=15.99 vs. limit=12.565000000000001
2024-10-08 10:15:11,651 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=13510.0, ans=0.1649
2024-10-08 10:15:37,355 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=13516.666666666666, ans=0.125
2024-10-08 10:15:42,425 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=13516.666666666666, ans=0.01034722222222223
2024-10-08 10:15:50,003 INFO [train.py:1153] Epoch 6, batch 6300, loss[loss=0.246, simple_loss=0.2694, pruned_loss=0.07935, ctc_loss=0.1596, over 4978.00 frames. ], tot_loss[loss=0.2591, simple_loss=0.2849, pruned_loss=0.08273, ctc_loss=0.1694, over 966690.60 frames. ], batch size: 19, lr: 1.45e-02,
2024-10-08 10:15:53,241 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=5.23 vs. limit=12.57
2024-10-08 10:16:07,948 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=13523.333333333334, ans=0.16476666666666667
2024-10-08 10:16:09,917 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.26 vs. limit=17.6425
2024-10-08 10:16:10,411 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=13523.333333333334, ans=0.125
2024-10-08 10:16:22,000 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 10:16:27,087 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=13530.0, ans=0.010291666666666671
2024-10-08 10:16:30,937 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=13530.0, ans=0.010291666666666671
2024-10-08 10:16:32,584 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.36 vs. limit=17.6475
2024-10-08 10:16:40,978 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.061e+01 1.186e+02 1.323e+02 1.420e+02 2.088e+02, threshold=2.647e+02, percent-clipped=0.0
2024-10-08 10:16:47,474 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=13533.333333333334, ans=0.125
2024-10-08 10:16:50,484 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.35 vs. limit=17.65
2024-10-08 10:16:53,701 INFO [train.py:1153] Epoch 6, batch 6350, loss[loss=0.2681, simple_loss=0.2912, pruned_loss=0.08776, ctc_loss=0.1738, over 4821.00 frames. ], tot_loss[loss=0.2586, simple_loss=0.2848, pruned_loss=0.08243, ctc_loss=0.169, over 966247.99 frames. ], batch size: 36, lr: 1.45e-02,
2024-10-08 10:16:54,628 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=3.79 vs. limit=12.57625
2024-10-08 10:16:56,465 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=13536.666666666666, ans=0.125
2024-10-08 10:17:04,891 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=13.90 vs. limit=17.6525
2024-10-08 10:17:27,004 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=13543.333333333334, ans=0.025
2024-10-08 10:17:35,766 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=13546.666666666666, ans=0.16453333333333334
2024-10-08 10:17:37,105 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=13546.666666666666, ans=0.16453333333333334
2024-10-08 10:17:40,897 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=13546.666666666666, ans=0.01022222222222223
2024-10-08 10:17:40,912 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=13546.666666666666, ans=0.01022222222222223
2024-10-08 10:17:46,077 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=13550.0, ans=0.125
2024-10-08 10:17:54,756 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=13550.0, ans=0.125
2024-10-08 10:17:57,291 INFO [train.py:1153] Epoch 6, batch 6400, loss[loss=0.2325, simple_loss=0.2706, pruned_loss=0.06869, ctc_loss=0.1426, over 4838.00 frames. ], tot_loss[loss=0.2568, simple_loss=0.2836, pruned_loss=0.08151, ctc_loss=0.1676, over 965935.74 frames. ], batch size: 23, lr: 1.45e-02,
2024-10-08 10:18:06,476 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=13553.333333333334, ans=0.125
2024-10-08 10:18:37,117 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=13563.333333333334, ans=0.125
2024-10-08 10:18:42,163 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=13563.333333333334, ans=0.16436666666666666
2024-10-08 10:18:48,440 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.925e+01 1.136e+02 1.269e+02 1.405e+02 1.936e+02, threshold=2.539e+02, percent-clipped=0.0
2024-10-08 10:19:01,059 INFO [train.py:1153] Epoch 6, batch 6450, loss[loss=0.2078, simple_loss=0.2513, pruned_loss=0.05729, ctc_loss=0.1241, over 4740.00 frames. ], tot_loss[loss=0.2557, simple_loss=0.2827, pruned_loss=0.08098, ctc_loss=0.1667, over 965357.05 frames. ], batch size: 26, lr: 1.45e-02,
2024-10-08 10:19:16,910 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=2.671e-03
2024-10-08 10:19:19,395 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=13573.333333333334, ans=0.125
2024-10-08 10:19:59,591 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=13583.333333333334, ans=0.125
2024-10-08 10:20:05,722 INFO [train.py:1153] Epoch 6, batch 6500, loss[loss=0.22, simple_loss=0.2613, pruned_loss=0.06252, ctc_loss=0.1342, over 4761.00 frames. ], tot_loss[loss=0.2541, simple_loss=0.2823, pruned_loss=0.08003, ctc_loss=0.1649, over 964946.36 frames. ], batch size: 26, lr: 1.45e-02,
2024-10-08 10:20:29,090 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=13590.0, ans=0.09899494936611666
2024-10-08 10:20:34,150 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=13593.333333333334, ans=0.010027777777777774
2024-10-08 10:20:34,204 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=13593.333333333334, ans=0.16406666666666667
2024-10-08 10:20:57,230 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.192e+01 1.135e+02 1.257e+02 1.393e+02 2.275e+02, threshold=2.514e+02, percent-clipped=0.0
2024-10-08 10:21:05,532 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.57 vs. limit=17.7
2024-10-08 10:21:10,028 INFO [train.py:1153] Epoch 6, batch 6550, loss[loss=0.2328, simple_loss=0.2523, pruned_loss=0.07823, ctc_loss=0.1421, over 4978.00 frames. ], tot_loss[loss=0.2533, simple_loss=0.2816, pruned_loss=0.07966, ctc_loss=0.1641, over 964630.89 frames. ], batch size: 19, lr: 1.45e-02,
2024-10-08 10:21:16,605 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=13603.333333333334, ans=0.025
2024-10-08 10:21:25,516 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.prob, batch_count=13606.666666666666, ans=0.125
2024-10-08 10:21:33,254 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=13606.666666666666, ans=0.125
2024-10-08 10:21:56,587 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=13613.333333333334, ans=0.4235333333333333
2024-10-08 10:22:08,246 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=13616.666666666666, ans=0.4234166666666667
2024-10-08 10:22:14,474 INFO [train.py:1153] Epoch 6, batch 6600, loss[loss=0.2661, simple_loss=0.2878, pruned_loss=0.08661, ctc_loss=0.1778, over 4847.00 frames. ], tot_loss[loss=0.2528, simple_loss=0.2813, pruned_loss=0.07937, ctc_loss=0.1638, over 965268.23 frames. ], batch size: 23, lr: 1.45e-02,
2024-10-08 10:22:52,366 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=13630.0, ans=0.125
2024-10-08 10:23:06,626 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.411e+01 1.150e+02 1.298e+02 1.432e+02 2.437e+02, threshold=2.597e+02, percent-clipped=0.0
2024-10-08 10:23:06,788 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=13633.333333333334, ans=0.42283333333333334
2024-10-08 10:23:19,645 INFO [train.py:1153] Epoch 6, batch 6650, loss[loss=0.2288, simple_loss=0.2596, pruned_loss=0.07044, ctc_loss=0.1427, over 4744.00 frames. ], tot_loss[loss=0.2507, simple_loss=0.2803, pruned_loss=0.07829, ctc_loss=0.1614, over 967184.55 frames. ], batch size: 20, lr: 1.45e-02,
2024-10-08 10:23:21,133 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=13636.666666666666, ans=0.16363333333333333
2024-10-08 10:23:26,609 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.38 vs. limit=12.61375
2024-10-08 10:23:36,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=13640.0, ans=0.125
2024-10-08 10:23:41,920 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=13640.0, ans=0.125
2024-10-08 10:23:47,133 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=13643.333333333334, ans=0.009819444444444443
2024-10-08 10:23:57,474 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=13646.666666666666, ans=0.00980555555555556
2024-10-08 10:24:10,513 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=13650.0, ans=0.025
2024-10-08 10:24:24,783 INFO [train.py:1153] Epoch 6, batch 6700, loss[loss=0.2675, simple_loss=0.2966, pruned_loss=0.08495, ctc_loss=0.1712, over 4936.00 frames. ], tot_loss[loss=0.2479, simple_loss=0.2788, pruned_loss=0.07686, ctc_loss=0.1583, over 969249.92 frames. ], batch size: 20, lr: 1.45e-02,
2024-10-08 10:24:29,425 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.52 vs. limit=11.826666666666668
2024-10-08 10:24:45,425 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=17.33 vs. limit=17.7425
2024-10-08 10:24:47,383 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=13656.666666666666, ans=0.125
2024-10-08 10:24:48,749 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=13656.666666666666, ans=0.16343333333333335
2024-10-08 10:25:07,147 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=13663.333333333334, ans=0.42178333333333334
2024-10-08 10:25:17,752 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.179e+01 1.143e+02 1.225e+02 1.364e+02 1.848e+02, threshold=2.449e+02, percent-clipped=0.0
2024-10-08 10:25:19,652 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.88 vs. limit=8.416666666666666
2024-10-08 10:25:30,920 INFO [train.py:1153] Epoch 6, batch 6750, loss[loss=0.2243, simple_loss=0.2572, pruned_loss=0.06809, ctc_loss=0.1381, over 4911.00 frames. ], tot_loss[loss=0.2453, simple_loss=0.2766, pruned_loss=0.07577, ctc_loss=0.1561, over 972296.35 frames. ], batch size: 19, lr: 1.45e-02,
2024-10-08 10:25:33,761 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=13670.0, ans=0.009708333333333333
2024-10-08 10:25:37,664 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=13670.0, ans=0.007897826086956522
2024-10-08 10:25:58,975 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=13676.666666666666, ans=10.0
2024-10-08 10:26:08,126 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=13676.666666666666, ans=0.42131666666666673
2024-10-08 10:26:14,746 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=13680.0, ans=0.125
2024-10-08 10:26:17,437 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=13680.0, ans=0.4212
2024-10-08 10:26:36,936 INFO [train.py:1153] Epoch 6, batch 6800, loss[loss=0.2667, simple_loss=0.282, pruned_loss=0.09127, ctc_loss=0.1723, over 4912.00 frames. ], tot_loss[loss=0.2432, simple_loss=0.2751, pruned_loss=0.07477, ctc_loss=0.1542, over 974619.50 frames. ], batch size: 19, lr: 1.44e-02,
2024-10-08 10:27:07,522 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=13693.333333333334, ans=0.125
2024-10-08 10:27:08,190 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=5.10 vs. limit=5.054
2024-10-08 10:27:29,997 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.925e+01 1.077e+02 1.164e+02 1.297e+02 2.007e+02, threshold=2.329e+02, percent-clipped=0.0
2024-10-08 10:27:43,344 INFO [train.py:1153] Epoch 6, batch 6850, loss[loss=0.2267, simple_loss=0.2656, pruned_loss=0.06728, ctc_loss=0.1329, over 4978.00 frames. ], tot_loss[loss=0.2419, simple_loss=0.274, pruned_loss=0.07432, ctc_loss=0.153, over 978951.43 frames. ], batch size: 19, lr: 1.44e-02,
2024-10-08 10:27:44,792 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/epoch-6.pt
2024-10-08 10:28:08,184 INFO [train.py:1153] Epoch 7, batch 0, loss[loss=0.1977, simple_loss=0.2466, pruned_loss=0.05267, ctc_loss=0.1084, over 4852.00 frames. ], tot_loss[loss=0.1977, simple_loss=0.2466, pruned_loss=0.05267, ctc_loss=0.1084, over 4852.00 frames. ], batch size: 19, lr: 1.35e-02,
2024-10-08 10:28:08,184 INFO [train.py:1176] Computing validation loss
2024-10-08 10:28:14,307 INFO [train.py:1185] Epoch 7, validation: loss=0.1744, simple_loss=0.2561, pruned_loss=0.03351, ctc_loss=0.06417, over 90464.00 frames.
2024-10-08 10:28:14,307 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 10:28:19,390 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.06 vs. limit=12.639
2024-10-08 10:29:10,929 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=13717.333333333334, ans=0.41989333333333334
2024-10-08 10:29:15,943 INFO [train.py:1153] Epoch 7, batch 50, loss[loss=0.2935, simple_loss=0.3092, pruned_loss=0.09985, ctc_loss=0.1952, over 4913.00 frames. ], tot_loss[loss=0.2631, simple_loss=0.2876, pruned_loss=0.08442, ctc_loss=0.1745, over 217676.27 frames. ], batch size: 19, lr: 1.35e-02,
2024-10-08 10:29:43,184 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=5.298e-03
2024-10-08 10:29:58,374 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=13730.666666666666, ans=0.125
2024-10-08 10:30:04,700 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.361e+01 1.167e+02 1.312e+02 1.455e+02 2.592e+02, threshold=2.624e+02, percent-clipped=1.0
2024-10-08 10:30:19,998 INFO [train.py:1153] Epoch 7, batch 100, loss[loss=0.1858, simple_loss=0.2215, pruned_loss=0.05381, ctc_loss=0.1063, over 4753.00 frames. ], tot_loss[loss=0.2612, simple_loss=0.2862, pruned_loss=0.08343, ctc_loss=0.1734, over 383403.88 frames. ], batch size: 19, lr: 1.35e-02,
2024-10-08 10:30:34,137 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=13740.666666666666, ans=0.025
2024-10-08 10:30:50,829 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=13744.0, ans=0.009399999999999999
2024-10-08 10:31:10,337 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=13750.666666666666, ans=0.125
2024-10-08 10:31:14,138 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=13750.666666666666, ans=0.125
2024-10-08 10:31:18,464 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.34 vs. limit=12.656500000000001
2024-10-08 10:31:24,339 INFO [train.py:1153] Epoch 7, batch 150, loss[loss=0.2215, simple_loss=0.2653, pruned_loss=0.0633, ctc_loss=0.1276, over 4911.00 frames. ], tot_loss[loss=0.2576, simple_loss=0.2842, pruned_loss=0.08165, ctc_loss=0.1694, over 513290.65 frames. ], batch size: 19, lr: 1.35e-02,
2024-10-08 10:31:24,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=13754.0, ans=0.09899494936611666
2024-10-08 10:31:27,742 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=4.26 vs. limit=12.65775
2024-10-08 10:31:31,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=13754.0, ans=17.8155
2024-10-08 10:31:39,902 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=13757.333333333334, ans=0.07
2024-10-08 10:31:58,754 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=8.45 vs. limit=8.440166666666666
2024-10-08 10:32:13,057 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.923e+01 1.124e+02 1.280e+02 1.457e+02 1.947e+02, threshold=2.561e+02, percent-clipped=0.0
2024-10-08 10:32:15,790 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=13767.333333333334, ans=0.125
2024-10-08 10:32:16,343 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=17.17 vs. limit=17.825499999999998
2024-10-08 10:32:23,481 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=13767.333333333334, ans=0.41814333333333337
2024-10-08 10:32:28,382 INFO [train.py:1153] Epoch 7, batch 200, loss[loss=0.328, simple_loss=0.3325, pruned_loss=0.1164, ctc_loss=0.2264, over 4760.00 frames. ], tot_loss[loss=0.2565, simple_loss=0.2831, pruned_loss=0.08123, ctc_loss=0.1684, over 613680.41 frames. ], batch size: 45, lr: 1.35e-02,
2024-10-08 10:32:28,488 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=13770.666666666666, ans=0.125
2024-10-08 10:32:37,492 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=13770.666666666666, ans=0.07
2024-10-08 10:33:07,384 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=13780.666666666666, ans=0.07
2024-10-08 10:33:13,929 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=13780.666666666666, ans=0.0
2024-10-08 10:33:33,011 INFO [train.py:1153] Epoch 7, batch 250, loss[loss=0.2974, simple_loss=0.3131, pruned_loss=0.1008, ctc_loss=0.2006, over 4813.00 frames. ], tot_loss[loss=0.2571, simple_loss=0.2837, pruned_loss=0.0815, ctc_loss=0.1688, over 692452.27 frames. ], batch size: 38, lr: 1.35e-02,
2024-10-08 10:33:33,783 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.52 vs. limit=17.8405
2024-10-08 10:33:39,415 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer1.prob, batch_count=13787.333333333334, ans=0.125
2024-10-08 10:34:06,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=13794.0, ans=0.125
2024-10-08 10:34:14,331 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=13797.333333333334, ans=0.40696
2024-10-08 10:34:21,886 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.159e+01 1.166e+02 1.274e+02 1.397e+02 1.901e+02, threshold=2.548e+02, percent-clipped=0.0
2024-10-08 10:34:27,107 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=13800.666666666666, ans=0.125
2024-10-08 10:34:37,110 INFO [train.py:1153] Epoch 7, batch 300, loss[loss=0.2679, simple_loss=0.2723, pruned_loss=0.0909, ctc_loss=0.2042, over 4754.00 frames. ], tot_loss[loss=0.2563, simple_loss=0.2833, pruned_loss=0.08095, ctc_loss=0.1685, over 752822.41 frames. ], batch size: 32, lr: 1.35e-02,
2024-10-08 10:34:50,692 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=17.66 vs. limit=17.8555
2024-10-08 10:35:00,427 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=13807.333333333334, ans=0.125
2024-10-08 10:35:08,210 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=13810.666666666666, ans=0.05
2024-10-08 10:35:40,299 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=13820.666666666666, ans=0.125
2024-10-08 10:35:41,394 INFO [train.py:1153] Epoch 7, batch 350, loss[loss=0.1765, simple_loss=0.2287, pruned_loss=0.04335, ctc_loss=0.09385, over 4883.00 frames. ], tot_loss[loss=0.2552, simple_loss=0.2827, pruned_loss=0.08036, ctc_loss=0.1675, over 800267.52 frames. ], batch size: 19, lr: 1.35e-02,
2024-10-08 10:35:44,090 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=13820.666666666666, ans=0.125
2024-10-08 10:35:58,860 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.01 vs. limit=12.684000000000001
2024-10-08 10:36:10,063 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 10:36:24,611 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.05 vs. limit=12.686499999999999
2024-10-08 10:36:30,183 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.782e+01 1.159e+02 1.285e+02 1.422e+02 2.035e+02, threshold=2.571e+02, percent-clipped=0.0
2024-10-08 10:36:33,571 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.10 vs. limit=12.687750000000001
2024-10-08 10:36:36,738 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer_ff3.min_abs, batch_count=13834.0, ans=0.2
2024-10-08 10:36:36,845 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=13834.0, ans=0.009024999999999998
2024-10-08 10:36:45,711 INFO [train.py:1153] Epoch 7, batch 400, loss[loss=0.2409, simple_loss=0.2831, pruned_loss=0.07059, ctc_loss=0.1437, over 4862.00 frames. ], tot_loss[loss=0.2543, simple_loss=0.2822, pruned_loss=0.08004, ctc_loss=0.1661, over 837011.03 frames. ], batch size: 22, lr: 1.35e-02,
2024-10-08 10:36:59,840 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=13840.666666666666, ans=0.008997222222222226
2024-10-08 10:37:06,196 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=13840.666666666666, ans=0.05
2024-10-08 10:37:07,516 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=13840.666666666666, ans=0.07
2024-10-08 10:37:18,033 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=13844.0, ans=0.125
2024-10-08 10:37:19,192 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=13844.0, ans=0.41546000000000005
2024-10-08 10:37:25,661 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=13847.333333333334, ans=0.00896944444444444
2024-10-08 10:37:26,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=13847.333333333334, ans=0.125
2024-10-08 10:37:27,095 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=13847.333333333334, ans=0.125
2024-10-08 10:37:38,640 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=13850.666666666666, ans=0.125
2024-10-08 10:37:43,815 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=13850.666666666666, ans=0.008955555555555564
2024-10-08 10:37:46,388 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=13850.666666666666, ans=0.125
2024-10-08 10:37:50,094 INFO [train.py:1153] Epoch 7, batch 450, loss[loss=0.2003, simple_loss=0.2427, pruned_loss=0.05567, ctc_loss=0.1163, over 4870.00 frames. ], tot_loss[loss=0.2537, simple_loss=0.2816, pruned_loss=0.07982, ctc_loss=0.1653, over 865603.97 frames. ], batch size: 23, lr: 1.35e-02,
2024-10-08 10:38:00,454 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=13854.0, ans=0.125
2024-10-08 10:38:01,871 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=13857.333333333334, ans=0.125
2024-10-08 10:38:09,412 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=13857.333333333334, ans=0.125
2024-10-08 10:38:10,776 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=13857.333333333334, ans=0.4149933333333334
2024-10-08 10:38:10,782 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=13857.333333333334, ans=0.008927777777777778
2024-10-08 10:38:27,447 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=13864.0, ans=0.125
2024-10-08 10:38:39,078 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.016e+01 1.088e+02 1.248e+02 1.410e+02 1.897e+02, threshold=2.496e+02, percent-clipped=0.0
2024-10-08 10:38:41,853 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=13867.333333333334, ans=0.16132666666666665
2024-10-08 10:38:48,236 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=13867.333333333334, ans=0.125
2024-10-08 10:38:54,527 INFO [train.py:1153] Epoch 7, batch 500, loss[loss=0.2863, simple_loss=0.307, pruned_loss=0.0938, ctc_loss=0.195, over 4826.00 frames. ], tot_loss[loss=0.2525, simple_loss=0.2815, pruned_loss=0.07896, ctc_loss=0.1639, over 888135.94 frames. ], batch size: 34, lr: 1.35e-02,
2024-10-08 10:38:54,735 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=13870.666666666666, ans=0.125
2024-10-08 10:39:02,442 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=13870.666666666666, ans=0.125
2024-10-08 10:39:06,320 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=13874.0, ans=0.16126000000000001
2024-10-08 10:39:36,291 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=13880.666666666666, ans=0.125
2024-10-08 10:39:43,840 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.min_positive, batch_count=13880.666666666666, ans=0.11119333333333334
2024-10-08 10:39:59,119 INFO [train.py:1153] Epoch 7, batch 550, loss[loss=0.2834, simple_loss=0.3129, pruned_loss=0.08866, ctc_loss=0.1913, over 4790.00 frames. ], tot_loss[loss=0.2527, simple_loss=0.2816, pruned_loss=0.07919, ctc_loss=0.1636, over 905553.28 frames. ], batch size: 40, lr: 1.34e-02,
2024-10-08 10:40:06,953 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=13887.333333333334, ans=0.16112666666666664
2024-10-08 10:40:10,798 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=13890.666666666666, ans=0.008788888888888895
2024-10-08 10:40:19,054 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=12.14 vs. limit=12.709
2024-10-08 10:40:39,528 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.42 vs. limit=17.923000000000002
2024-10-08 10:40:47,634 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.747e+01 1.100e+02 1.259e+02 1.456e+02 2.080e+02, threshold=2.518e+02, percent-clipped=0.0
2024-10-08 10:40:47,900 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=13897.333333333334, ans=0.125
2024-10-08 10:41:03,338 INFO [train.py:1153] Epoch 7, batch 600, loss[loss=0.2322, simple_loss=0.2736, pruned_loss=0.06498, ctc_loss=0.1522, over 4816.00 frames. ], tot_loss[loss=0.2502, simple_loss=0.28, pruned_loss=0.0779, ctc_loss=0.1614, over 919311.11 frames. ], batch size: 38, lr: 1.34e-02,
2024-10-08 10:41:44,231 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.88 vs. limit=17.935499999999998
2024-10-08 10:42:07,894 INFO [train.py:1153] Epoch 7, batch 650, loss[loss=0.2373, simple_loss=0.2825, pruned_loss=0.06489, ctc_loss=0.156, over 4835.00 frames. ], tot_loss[loss=0.2488, simple_loss=0.279, pruned_loss=0.07726, ctc_loss=0.1603, over 930237.52 frames. ], batch size: 21, lr: 1.34e-02,
2024-10-08 10:42:08,061 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=13920.666666666666, ans=0.09899494936611666
2024-10-08 10:42:32,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=13927.333333333334, ans=0.04949747468305833
2024-10-08 10:42:56,892 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.092e+01 1.111e+02 1.242e+02 1.381e+02 2.066e+02, threshold=2.485e+02, percent-clipped=0.0
2024-10-08 10:43:06,874 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=10.56 vs. limit=12.725249999999999
2024-10-08 10:43:12,120 INFO [train.py:1153] Epoch 7, batch 700, loss[loss=0.2239, simple_loss=0.2637, pruned_loss=0.06677, ctc_loss=0.1264, over 4749.00 frames. ], tot_loss[loss=0.2488, simple_loss=0.2791, pruned_loss=0.07725, ctc_loss=0.1601, over 938172.54 frames. ], batch size: 19, lr: 1.34e-02,
2024-10-08 10:43:14,850 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=13937.333333333334, ans=0.125
2024-10-08 10:43:23,596 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=13940.666666666666, ans=0.125
2024-10-08 10:43:26,071 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=13940.666666666666, ans=0.125
2024-10-08 10:43:45,967 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.45 vs. limit=12.729
2024-10-08 10:43:50,677 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=13947.333333333334, ans=0.007837536231884058
2024-10-08 10:44:11,523 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=13950.666666666666, ans=0.125
2024-10-08 10:44:16,689 INFO [train.py:1153] Epoch 7, batch 750, loss[loss=0.2789, simple_loss=0.3086, pruned_loss=0.08885, ctc_loss=0.1784, over 4865.00 frames. ], tot_loss[loss=0.2466, simple_loss=0.2774, pruned_loss=0.0762, ctc_loss=0.1584, over 945034.68 frames. ], batch size: 22, lr: 1.34e-02,
2024-10-08 10:44:23,433 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=13954.0, ans=0.41161000000000003
2024-10-08 10:44:54,823 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.49 vs. limit=17.973
2024-10-08 10:45:05,778 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.339e+01 1.084e+02 1.237e+02 1.402e+02 2.071e+02, threshold=2.474e+02, percent-clipped=0.0
2024-10-08 10:45:09,801 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=13967.333333333334, ans=0.125
2024-10-08 10:45:11,525 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=22.13 vs. limit=17.9755
2024-10-08 10:45:21,330 INFO [train.py:1153] Epoch 7, batch 800, loss[loss=0.2143, simple_loss=0.2472, pruned_loss=0.06306, ctc_loss=0.138, over 4856.00 frames. ], tot_loss[loss=0.2466, simple_loss=0.2777, pruned_loss=0.07609, ctc_loss=0.1586, over 949825.84 frames. ], batch size: 19, lr: 1.34e-02,
2024-10-08 10:45:22,792 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=13970.666666666666, ans=0.007832463768115942
2024-10-08 10:45:42,175 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=13974.0, ans=0.008441666666666667
2024-10-08 10:45:45,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=13977.333333333334, ans=0.125
2024-10-08 10:45:49,234 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.09 vs. limit=17.983
2024-10-08 10:46:00,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=13980.666666666666, ans=0.125
2024-10-08 10:46:02,742 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=13980.666666666666, ans=0.125
2024-10-08 10:46:03,808 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=13980.666666666666, ans=0.07067883333333336
2024-10-08 10:46:25,816 INFO [train.py:1153] Epoch 7, batch 850, loss[loss=0.2484, simple_loss=0.2762, pruned_loss=0.0769, ctc_loss=0.1669, over 4753.00 frames. ], tot_loss[loss=0.2454, simple_loss=0.2769, pruned_loss=0.07542, ctc_loss=0.1574, over 953955.53 frames. ], batch size: 29, lr: 1.34e-02,
2024-10-08 10:46:31,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=13987.333333333334, ans=0.16012666666666667
2024-10-08 10:46:41,228 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=13990.666666666666, ans=0.05
2024-10-08 10:46:45,242 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.min_positive, batch_count=13990.666666666666, ans=0.025
2024-10-08 10:46:51,711 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=13994.0, ans=0.16006
2024-10-08 10:47:00,745 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=13994.0, ans=0.04949747468305833
2024-10-08 10:47:02,281 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=18.23 vs. limit=17.9955
2024-10-08 10:47:05,883 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=13997.333333333334, ans=0.41009333333333337
2024-10-08 10:47:14,799 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.183e+01 1.051e+02 1.207e+02 1.362e+02 3.090e+02, threshold=2.415e+02, percent-clipped=1.0
2024-10-08 10:47:16,228 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=14000.666666666666, ans=0.15999333333333335
2024-10-08 10:47:30,356 INFO [train.py:1153] Epoch 7, batch 900, loss[loss=0.2131, simple_loss=0.2706, pruned_loss=0.05474, ctc_loss=0.1156, over 4853.00 frames. ], tot_loss[loss=0.2456, simple_loss=0.2772, pruned_loss=0.07558, ctc_loss=0.1573, over 956855.99 frames. ], batch size: 19, lr: 1.34e-02,
2024-10-08 10:47:32,858 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=14004.0, ans=0.125
2024-10-08 10:47:38,185 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=14004.0, ans=0.07
2024-10-08 10:47:58,672 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=14010.666666666666, ans=0.15989333333333336
2024-10-08 10:47:58,780 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=14010.666666666666, ans=0.025
2024-10-08 10:48:30,957 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=14017.333333333334, ans=0.125
2024-10-08 10:48:34,720 INFO [train.py:1153] Epoch 7, batch 950, loss[loss=0.2143, simple_loss=0.2607, pruned_loss=0.05911, ctc_loss=0.1241, over 4812.00 frames. ], tot_loss[loss=0.2452, simple_loss=0.2769, pruned_loss=0.07536, ctc_loss=0.1572, over 958732.65 frames. ], batch size: 19, lr: 1.34e-02,
2024-10-08 10:48:45,077 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=14020.666666666666, ans=0.025
2024-10-08 10:48:46,468 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=14024.0, ans=0.007820869565217391
2024-10-08 10:49:01,739 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=14027.333333333334, ans=0.125
2024-10-08 10:49:05,655 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=14027.333333333334, ans=0.008219444444444439
2024-10-08 10:49:09,597 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=14027.333333333334, ans=0.15972666666666666
2024-10-08 10:49:23,610 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.037e+01 1.186e+02 1.291e+02 1.458e+02 1.971e+02, threshold=2.582e+02, percent-clipped=0.0
2024-10-08 10:49:30,215 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=14034.0, ans=0.125
2024-10-08 10:49:39,205 INFO [train.py:1153] Epoch 7, batch 1000, loss[loss=0.2145, simple_loss=0.265, pruned_loss=0.05731, ctc_loss=0.1234, over 4939.00 frames. ], tot_loss[loss=0.2468, simple_loss=0.2777, pruned_loss=0.07613, ctc_loss=0.1591, over 960552.60 frames. ], batch size: 20, lr: 1.34e-02,
2024-10-08 10:49:44,631 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 10:49:55,044 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=14040.666666666666, ans=0.125
2024-10-08 10:50:15,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=14044.0, ans=0.125
2024-10-08 10:50:22,491 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=14047.333333333334, ans=0.0
2024-10-08 10:50:29,226 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=14047.333333333334, ans=0.125
2024-10-08 10:50:43,207 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=14050.666666666666, ans=0.025
2024-10-08 10:50:45,360 INFO [train.py:1153] Epoch 7, batch 1050, loss[loss=0.2442, simple_loss=0.2778, pruned_loss=0.0744, ctc_loss=0.1545, over 4815.00 frames. ], tot_loss[loss=0.2459, simple_loss=0.2769, pruned_loss=0.0758, ctc_loss=0.1581, over 962571.14 frames. ], batch size: 25, lr: 1.34e-02,
2024-10-08 10:50:45,940 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.79 vs. limit=18.0405
2024-10-08 10:50:49,232 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=14054.0, ans=0.008108333333333335
2024-10-08 10:51:08,497 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=14057.333333333334, ans=0.008094444444444439
2024-10-08 10:51:11,013 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=14060.666666666666, ans=0.125
2024-10-08 10:51:23,896 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=14064.0, ans=0.125
2024-10-08 10:51:23,923 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14064.0, ans=0.15936
2024-10-08 10:51:31,095 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.60 vs. limit=12.774000000000001
2024-10-08 10:51:34,133 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.962e+01 1.107e+02 1.232e+02 1.424e+02 1.698e+02, threshold=2.464e+02, percent-clipped=0.0
2024-10-08 10:51:43,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=18.75 vs. limit=18.0505
2024-10-08 10:51:48,331 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=14070.666666666666, ans=0.125
2024-10-08 10:51:49,524 INFO [train.py:1153] Epoch 7, batch 1100, loss[loss=0.2337, simple_loss=0.2656, pruned_loss=0.07245, ctc_loss=0.1423, over 4861.00 frames. ], tot_loss[loss=0.2453, simple_loss=0.2769, pruned_loss=0.07551, ctc_loss=0.1569, over 964024.39 frames. ], batch size: 20, lr: 1.34e-02,
2024-10-08 10:51:50,340 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.60 vs. limit=12.7765
2024-10-08 10:51:50,953 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=14070.666666666666, ans=0.125
2024-10-08 10:51:52,231 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=14070.666666666666, ans=0.007810724637681159
2024-10-08 10:51:53,590 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=14070.666666666666, ans=0.125
2024-10-08 10:51:56,069 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=14070.666666666666, ans=0.125
2024-10-08 10:51:57,429 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=14070.666666666666, ans=0.125
2024-10-08 10:52:05,208 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=14074.0, ans=0.125
2024-10-08 10:52:35,090 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 10:52:53,354 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.89 vs. limit=9.634933333333333
2024-10-08 10:52:53,921 INFO [train.py:1153] Epoch 7, batch 1150, loss[loss=0.2504, simple_loss=0.2798, pruned_loss=0.08032, ctc_loss=0.1507, over 4868.00 frames. ], tot_loss[loss=0.2468, simple_loss=0.2774, pruned_loss=0.07633, ctc_loss=0.1586, over 964386.48 frames. ], batch size: 20, lr: 1.34e-02,
2024-10-08 10:52:54,145 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=14087.333333333334, ans=0.007807101449275362
2024-10-08 10:52:55,409 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=14087.333333333334, ans=0.125
2024-10-08 10:53:15,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=14090.666666666666, ans=0.15909333333333336
2024-10-08 10:53:20,471 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.75 vs. limit=18.0705
2024-10-08 10:53:22,272 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=14094.0, ans=0.125
2024-10-08 10:53:42,509 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.468e+01 1.150e+02 1.277e+02 1.417e+02 1.866e+02, threshold=2.554e+02, percent-clipped=0.0
2024-10-08 10:53:49,080 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=14100.666666666666, ans=0.40647666666666676
2024-10-08 10:53:51,857 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=14100.666666666666, ans=0.125
2024-10-08 10:53:58,039 INFO [train.py:1153] Epoch 7, batch 1200, loss[loss=0.2711, simple_loss=0.294, pruned_loss=0.08847, ctc_loss=0.1783, over 4803.00 frames. ], tot_loss[loss=0.2476, simple_loss=0.2781, pruned_loss=0.07671, ctc_loss=0.1593, over 964217.27 frames. ], batch size: 25, lr: 1.33e-02,
2024-10-08 10:54:03,399 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=14104.0, ans=0.05
2024-10-08 10:54:37,891 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.55 vs. limit=12.79275
2024-10-08 10:54:53,971 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=14117.333333333334, ans=0.125
2024-10-08 10:55:02,891 INFO [train.py:1153] Epoch 7, batch 1250, loss[loss=0.2509, simple_loss=0.2738, pruned_loss=0.08154, ctc_loss=0.1623, over 4754.00 frames. ], tot_loss[loss=0.2478, simple_loss=0.278, pruned_loss=0.07698, ctc_loss=0.1592, over 964314.24 frames. ], batch size: 32, lr: 1.33e-02,
2024-10-08 10:55:22,427 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.prob, batch_count=14124.0, ans=0.125
2024-10-08 10:55:26,274 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer1.prob, batch_count=14124.0, ans=0.125
2024-10-08 10:55:46,090 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=14130.666666666666, ans=0.40542666666666677
2024-10-08 10:55:51,506 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=14130.666666666666, ans=0.40542666666666677
2024-10-08 10:55:51,851 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.51 vs. limit=12.799
2024-10-08 10:55:52,555 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.452e+01 1.101e+02 1.262e+02 1.339e+02 2.279e+02, threshold=2.523e+02, percent-clipped=0.0
2024-10-08 10:56:05,652 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=14134.0, ans=0.125
2024-10-08 10:56:08,116 INFO [train.py:1153] Epoch 7, batch 1300, loss[loss=0.335, simple_loss=0.3376, pruned_loss=0.1186, ctc_loss=0.2377, over 4836.00 frames. ], tot_loss[loss=0.2487, simple_loss=0.2786, pruned_loss=0.07736, ctc_loss=0.1602, over 965556.58 frames. ], batch size: 43, lr: 1.33e-02,
2024-10-08 10:56:55,765 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=10.27 vs. limit=12.805250000000001
2024-10-08 10:57:04,294 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=14150.666666666666, ans=0.04949747468305833
2024-10-08 10:57:05,486 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=14150.666666666666, ans=0.125
2024-10-08 10:57:08,278 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=14150.666666666666, ans=0.007705555555555563
2024-10-08 10:57:11,546 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.73 vs. limit=5.1226
2024-10-08 10:57:13,333 INFO [train.py:1153] Epoch 7, batch 1350, loss[loss=0.2776, simple_loss=0.2986, pruned_loss=0.09256, ctc_loss=0.1789, over 4854.00 frames. ], tot_loss[loss=0.2461, simple_loss=0.2773, pruned_loss=0.07588, ctc_loss=0.1578, over 966389.87 frames. ], batch size: 21, lr: 1.33e-02,
2024-10-08 10:57:22,765 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=14154.0, ans=0.125
2024-10-08 10:57:23,063 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.28 vs. limit=12.80775
2024-10-08 10:57:46,235 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=14160.666666666666, ans=0.15839333333333336
2024-10-08 10:57:52,909 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14164.0, ans=0.15836
2024-10-08 10:58:02,932 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.220e+01 1.108e+02 1.227e+02 1.406e+02 6.487e+02, threshold=2.455e+02, percent-clipped=1.0
2024-10-08 10:58:07,147 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=14167.333333333334, ans=0.125
2024-10-08 10:58:12,256 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14167.333333333334, ans=0.15832666666666664
2024-10-08 10:58:18,630 INFO [train.py:1153] Epoch 7, batch 1400, loss[loss=0.2127, simple_loss=0.2643, pruned_loss=0.05767, ctc_loss=0.1145, over 4940.00 frames. ], tot_loss[loss=0.2469, simple_loss=0.2779, pruned_loss=0.07625, ctc_loss=0.1583, over 966682.64 frames. ], batch size: 19, lr: 1.33e-02,
2024-10-08 10:58:55,143 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=14177.333333333334, ans=0.035
2024-10-08 10:58:55,191 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=14177.333333333334, ans=0.125
2024-10-08 10:59:13,061 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14184.0, ans=0.15816
2024-10-08 10:59:20,077 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.10 vs. limit=12.091999999999999
2024-10-08 10:59:23,208 INFO [train.py:1153] Epoch 7, batch 1450, loss[loss=0.2971, simple_loss=0.3103, pruned_loss=0.1018, ctc_loss=0.2006, over 4808.00 frames. ], tot_loss[loss=0.2501, simple_loss=0.2797, pruned_loss=0.07798, ctc_loss=0.1611, over 966575.43 frames. ], batch size: 34, lr: 1.33e-02,
2024-10-08 10:59:23,335 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=14187.333333333334, ans=0.8918733333333333
2024-10-08 10:59:29,687 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=14187.333333333334, ans=0.007552777777777776
2024-10-08 10:59:41,777 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.64 vs. limit=12.8215
2024-10-08 11:00:12,325 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.098e+01 1.141e+02 1.259e+02 1.394e+02 2.667e+02, threshold=2.517e+02, percent-clipped=1.0
2024-10-08 11:00:27,829 INFO [train.py:1153] Epoch 7, batch 1500, loss[loss=0.199, simple_loss=0.248, pruned_loss=0.05376, ctc_loss=0.106, over 4756.00 frames. ], tot_loss[loss=0.2495, simple_loss=0.2795, pruned_loss=0.07768, ctc_loss=0.1605, over 966267.78 frames. ], batch size: 26, lr: 1.33e-02,
2024-10-08 11:00:44,911 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 11:00:55,799 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.66 vs. limit=12.829
2024-10-08 11:01:16,675 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=17.88 vs. limit=18.1605
2024-10-08 11:01:19,453 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.64 vs. limit=12.83025
2024-10-08 11:01:22,537 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.46 vs. limit=18.163
2024-10-08 11:01:25,604 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=14217.333333333334, ans=0.07
2024-10-08 11:01:33,332 INFO [train.py:1153] Epoch 7, batch 1550, loss[loss=0.252, simple_loss=0.2696, pruned_loss=0.08071, ctc_loss=0.1824, over 4848.00 frames. ], tot_loss[loss=0.2486, simple_loss=0.2788, pruned_loss=0.07718, ctc_loss=0.1602, over 966133.70 frames. ], batch size: 31, lr: 1.33e-02,
2024-10-08 11:01:42,174 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=14220.666666666666, ans=0.125
2024-10-08 11:01:47,257 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=14224.0, ans=0.05
2024-10-08 11:01:55,737 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=11.19 vs. limit=12.834
2024-10-08 11:02:10,566 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=14230.666666666666, ans=0.125
2024-10-08 11:02:22,065 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.388e+01 1.111e+02 1.212e+02 1.398e+02 2.587e+02, threshold=2.423e+02, percent-clipped=1.0
2024-10-08 11:02:24,050 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=18.30 vs. limit=18.1755
2024-10-08 11:02:34,023 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=14234.0, ans=0.0077752173913043475
2024-10-08 11:02:34,115 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=14234.0, ans=0.40181000000000006
2024-10-08 11:02:37,840 INFO [train.py:1153] Epoch 7, batch 1600, loss[loss=0.2033, simple_loss=0.2578, pruned_loss=0.05262, ctc_loss=0.1091, over 4796.00 frames. ], tot_loss[loss=0.2469, simple_loss=0.2778, pruned_loss=0.07632, ctc_loss=0.1584, over 966407.60 frames. ], batch size: 25, lr: 1.33e-02,
2024-10-08 11:02:42,041 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 11:02:48,767 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.10 vs. limit=12.839
2024-10-08 11:02:53,452 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.min_abs, batch_count=14240.666666666666, ans=0.41361000000000003
2024-10-08 11:03:30,048 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=14250.666666666666, ans=0.125
2024-10-08 11:03:37,901 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=14250.666666666666, ans=0.125
2024-10-08 11:03:42,889 INFO [train.py:1153] Epoch 7, batch 1650, loss[loss=0.2652, simple_loss=0.313, pruned_loss=0.07796, ctc_loss=0.1535, over 4792.00 frames. ], tot_loss[loss=0.2474, simple_loss=0.2781, pruned_loss=0.07653, ctc_loss=0.1591, over 966888.53 frames. ], batch size: 29, lr: 1.33e-02,
2024-10-08 11:03:45,638 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=14254.0, ans=0.125
2024-10-08 11:03:45,695 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=14254.0, ans=0.125
2024-10-08 11:03:46,511 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=11.52 vs. limit=12.84525
2024-10-08 11:03:47,687 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.25 vs. limit=12.84525
2024-10-08 11:04:07,981 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=14260.666666666666, ans=0.125
2024-10-08 11:04:20,995 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=14264.0, ans=0.007233333333333335
2024-10-08 11:04:29,603 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=16.14 vs. limit=18.198
2024-10-08 11:04:30,160 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=14264.0, ans=0.007233333333333335
2024-10-08 11:04:32,750 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.695e+01 1.088e+02 1.190e+02 1.347e+02 1.874e+02, threshold=2.381e+02, percent-clipped=0.0
2024-10-08 11:04:48,286 INFO [train.py:1153] Epoch 7, batch 1700, loss[loss=0.2568, simple_loss=0.2967, pruned_loss=0.07579, ctc_loss=0.163, over 4940.00 frames. ], tot_loss[loss=0.2461, simple_loss=0.2771, pruned_loss=0.07598, ctc_loss=0.1577, over 967044.50 frames. ], batch size: 19, lr: 1.33e-02,
2024-10-08 11:04:51,118 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=14270.666666666666, ans=0.125
2024-10-08 11:05:06,924 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=14274.0, ans=0.125
2024-10-08 11:05:19,947 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=14277.333333333334, ans=0.15722666666666665
2024-10-08 11:05:33,015 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.46 vs. limit=5.1421
2024-10-08 11:05:35,763 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=512, metric=5.40 vs. limit=12.85525
2024-10-08 11:05:45,645 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer2.prob, batch_count=14284.0, ans=0.125
2024-10-08 11:05:53,359 INFO [train.py:1153] Epoch 7, batch 1750, loss[loss=0.2258, simple_loss=0.2716, pruned_loss=0.06236, ctc_loss=0.1384, over 4959.00 frames. ], tot_loss[loss=0.2455, simple_loss=0.2765, pruned_loss=0.07582, ctc_loss=0.1569, over 967209.11 frames. ], batch size: 19, lr: 1.33e-02,
2024-10-08 11:06:13,363 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.max_abs, batch_count=14290.666666666666, ans=10.0
2024-10-08 11:06:19,122 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.24 vs. limit=18.2205
2024-10-08 11:06:39,311 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=14297.333333333334, ans=0.125
2024-10-08 11:06:42,970 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.731e+01 1.188e+02 1.313e+02 1.505e+02 2.224e+02, threshold=2.627e+02, percent-clipped=0.0
2024-10-08 11:06:49,151 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.50 vs. limit=5.145099999999999
2024-10-08 11:06:50,887 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.8.prob, batch_count=14300.666666666666, ans=0.125
2024-10-08 11:06:52,444 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=14300.666666666666, ans=0.007080555555555562
2024-10-08 11:06:53,679 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=14300.666666666666, ans=0.125
2024-10-08 11:06:55,056 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=14300.666666666666, ans=0.00776072463768116
2024-10-08 11:06:58,803 INFO [train.py:1153] Epoch 7, batch 1800, loss[loss=0.1954, simple_loss=0.2532, pruned_loss=0.04748, ctc_loss=0.1067, over 4871.00 frames. ], tot_loss[loss=0.246, simple_loss=0.2772, pruned_loss=0.07595, ctc_loss=0.1573, over 967881.18 frames. ], batch size: 23, lr: 1.33e-02,
2024-10-08 11:07:04,144 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=14304.0, ans=0.125
2024-10-08 11:07:24,266 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.26 vs. limit=18.233
2024-10-08 11:07:32,004 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.77 vs. limit=12.8665
2024-10-08 11:07:40,637 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14314.0, ans=0.15686
2024-10-08 11:07:50,070 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2.whitening_limit, batch_count=14317.333333333334, ans=12.158666666666667
2024-10-08 11:07:56,463 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=14317.333333333334, ans=0.3988933333333333
2024-10-08 11:08:03,868 INFO [train.py:1153] Epoch 7, batch 1850, loss[loss=0.199, simple_loss=0.2496, pruned_loss=0.05139, ctc_loss=0.1137, over 4746.00 frames. ], tot_loss[loss=0.2455, simple_loss=0.2771, pruned_loss=0.07562, ctc_loss=0.1568, over 968022.70 frames. ], batch size: 26, lr: 1.32e-02,
2024-10-08 11:08:24,861 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=14324.0, ans=0.0069833333333333344
2024-10-08 11:08:26,191 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=14324.0, ans=0.0069833333333333344
2024-10-08 11:08:28,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=14327.333333333334, ans=0.39854333333333336
2024-10-08 11:08:31,830 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.22 vs. limit=18.2455
2024-10-08 11:08:53,284 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.427e+01 1.135e+02 1.317e+02 1.472e+02 1.834e+02, threshold=2.634e+02, percent-clipped=0.0
2024-10-08 11:09:05,100 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14334.0, ans=0.15666
2024-10-08 11:09:08,823 INFO [train.py:1153] Epoch 7, batch 1900, loss[loss=0.2521, simple_loss=0.2758, pruned_loss=0.08235, ctc_loss=0.1589, over 4777.00 frames. ], tot_loss[loss=0.2476, simple_loss=0.2786, pruned_loss=0.07658, ctc_loss=0.1587, over 967783.91 frames. ], batch size: 29, lr: 1.32e-02,
2024-10-08 11:09:11,556 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=14337.333333333334, ans=0.0
2024-10-08 11:09:18,679 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=2.88 vs. limit=12.8765
2024-10-08 11:09:19,319 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=14337.333333333334, ans=0.3981933333333334
2024-10-08 11:09:27,073 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=14340.666666666666, ans=0.39807666666666675
2024-10-08 11:09:31,424 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=5.29 vs. limit=5.1511
2024-10-08 11:10:11,181 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=14350.666666666666, ans=0.3977266666666668
2024-10-08 11:10:13,709 INFO [train.py:1153] Epoch 7, batch 1950, loss[loss=0.228, simple_loss=0.2673, pruned_loss=0.06594, ctc_loss=0.1422, over 4849.00 frames. ], tot_loss[loss=0.2478, simple_loss=0.2787, pruned_loss=0.07667, ctc_loss=0.1589, over 966729.65 frames. ], batch size: 20, lr: 1.32e-02,
2024-10-08 11:10:19,759 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=3.55 vs. limit=12.88275
2024-10-08 11:10:20,352 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=14354.0, ans=0.125
2024-10-08 11:10:20,406 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=14354.0, ans=0.125
2024-10-08 11:10:59,037 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=14364.0, ans=0.125
2024-10-08 11:10:59,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=14364.0, ans=0.125
2024-10-08 11:11:02,937 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.321e+01 1.174e+02 1.270e+02 1.426e+02 1.823e+02, threshold=2.540e+02, percent-clipped=0.0
2024-10-08 11:11:10,275 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer_ff2.min_abs, batch_count=14367.333333333334, ans=0.1
2024-10-08 11:11:12,698 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=14367.333333333334, ans=0.07
2024-10-08 11:11:19,074 INFO [train.py:1153] Epoch 7, batch 2000, loss[loss=0.2413, simple_loss=0.2687, pruned_loss=0.07615, ctc_loss=0.154, over 4959.00 frames. ], tot_loss[loss=0.2488, simple_loss=0.2791, pruned_loss=0.07717, ctc_loss=0.1602, over 966464.17 frames. ], batch size: 19, lr: 1.32e-02,
2024-10-08 11:11:25,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=14370.666666666666, ans=0.007745507246376812
2024-10-08 11:11:33,268 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=14374.0, ans=0.006774999999999996
2024-10-08 11:11:41,014 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=14374.0, ans=0.125
2024-10-08 11:11:44,947 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=14377.333333333334, ans=0.007744057971014493
2024-10-08 11:11:48,866 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=14377.333333333334, ans=0.125
2024-10-08 11:11:54,261 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=3.94 vs. limit=8.594333333333333
2024-10-08 11:11:56,583 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14380.666666666666, ans=0.15619333333333335
2024-10-08 11:12:14,801 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=14384.0, ans=0.125
2024-10-08 11:12:23,697 INFO [train.py:1153] Epoch 7, batch 2050, loss[loss=0.2589, simple_loss=0.2806, pruned_loss=0.08472, ctc_loss=0.1692, over 4915.00 frames. ], tot_loss[loss=0.2508, simple_loss=0.2804, pruned_loss=0.07817, ctc_loss=0.162, over 966787.06 frames. ], batch size: 19, lr: 1.32e-02,
2024-10-08 11:12:30,402 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=14387.333333333334, ans=0.05
2024-10-08 11:12:35,584 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=14390.666666666666, ans=0.125
2024-10-08 11:12:38,104 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=14390.666666666666, ans=0.125
2024-10-08 11:13:06,695 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=14397.333333333334, ans=0.125
2024-10-08 11:13:07,298 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.26 vs. limit=18.298000000000002
2024-10-08 11:13:13,209 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.309e+01 1.181e+02 1.345e+02 1.543e+02 1.885e+02, threshold=2.691e+02, percent-clipped=0.0
2024-10-08 11:13:23,821 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14400.666666666666, ans=0.15599333333333334
2024-10-08 11:13:26,552 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=14400.666666666666, ans=0.007738985507246377
2024-10-08 11:13:29,082 INFO [train.py:1153] Epoch 7, batch 2100, loss[loss=0.2289, simple_loss=0.2687, pruned_loss=0.06852, ctc_loss=0.1302, over 4854.00 frames. ], tot_loss[loss=0.2494, simple_loss=0.28, pruned_loss=0.07739, ctc_loss=0.1602, over 966978.51 frames. ], batch size: 21, lr: 1.32e-02,
2024-10-08 11:13:36,392 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.95 vs. limit=12.9015
2024-10-08 11:13:50,002 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=14407.333333333334, ans=0.125
2024-10-08 11:13:55,474 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=14410.666666666666, ans=0.006622222222222231
2024-10-08 11:13:58,086 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=14410.666666666666, ans=0.3956266666666668
2024-10-08 11:14:21,737 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.attention_skip_rate, batch_count=14417.333333333334, ans=0.0065944444444444444
2024-10-08 11:14:21,806 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=14417.333333333334, ans=0.125
2024-10-08 11:14:22,099 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten.whitening_limit, batch_count=14417.333333333334, ans=12.906500000000001
2024-10-08 11:14:34,769 INFO [train.py:1153] Epoch 7, batch 2150, loss[loss=0.1783, simple_loss=0.2287, pruned_loss=0.04637, ctc_loss=0.08773, over 4859.00 frames. ], tot_loss[loss=0.2486, simple_loss=0.2795, pruned_loss=0.077, ctc_loss=0.1593, over 967872.13 frames. ], batch size: 20, lr: 1.32e-02,
2024-10-08 11:14:40,157 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=14420.666666666666, ans=0.025
2024-10-08 11:14:45,392 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=14420.666666666666, ans=0.3952766666666667
2024-10-08 11:14:54,617 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=14424.0, ans=0.025
2024-10-08 11:15:02,566 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=14427.333333333334, ans=0.007733188405797101
2024-10-08 11:15:24,569 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.951e+01 1.066e+02 1.231e+02 1.382e+02 1.669e+02, threshold=2.463e+02, percent-clipped=0.0
2024-10-08 11:15:26,049 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=14434.0, ans=0.006524999999999996
2024-10-08 11:15:40,272 INFO [train.py:1153] Epoch 7, batch 2200, loss[loss=0.2516, simple_loss=0.2845, pruned_loss=0.07723, ctc_loss=0.1603, over 4716.00 frames. ], tot_loss[loss=0.2468, simple_loss=0.278, pruned_loss=0.07621, ctc_loss=0.158, over 967639.97 frames. ], batch size: 26, lr: 1.32e-02,
2024-10-08 11:15:46,284 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.11 vs. limit=8.609333333333334
2024-10-08 11:15:46,956 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14437.333333333334, ans=0.15562666666666666
2024-10-08 11:15:47,013 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=14437.333333333334, ans=0.125
2024-10-08 11:16:11,936 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=14444.0, ans=0.125
2024-10-08 11:16:24,960 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=14447.333333333334, ans=0.007728840579710145
2024-10-08 11:16:36,151 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.42 vs. limit=12.919
2024-10-08 11:16:45,988 INFO [train.py:1153] Epoch 7, batch 2250, loss[loss=0.2528, simple_loss=0.2948, pruned_loss=0.0743, ctc_loss=0.1555, over 4866.00 frames. ], tot_loss[loss=0.2461, simple_loss=0.2777, pruned_loss=0.07579, ctc_loss=0.1571, over 967785.71 frames. ], batch size: 22, lr: 1.32e-02,
2024-10-08 11:16:57,481 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.43 vs. limit=5.1681
2024-10-08 11:17:06,428 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.07 vs. limit=12.9215
2024-10-08 11:17:31,589 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.97 vs. limit=12.232
2024-10-08 11:17:36,018 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.275e+01 1.112e+02 1.219e+02 1.372e+02 2.069e+02, threshold=2.438e+02, percent-clipped=0.0
2024-10-08 11:17:41,242 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=14467.333333333334, ans=0.0077244927536231885
2024-10-08 11:17:41,413 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=5.801e-02
2024-10-08 11:17:41,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.90 vs. limit=12.92525
2024-10-08 11:17:46,632 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=14467.333333333334, ans=0.125
2024-10-08 11:17:51,607 INFO [train.py:1153] Epoch 7, batch 2300, loss[loss=0.2136, simple_loss=0.2649, pruned_loss=0.05401, ctc_loss=0.1357, over 4883.00 frames. ], tot_loss[loss=0.245, simple_loss=0.2775, pruned_loss=0.07508, ctc_loss=0.156, over 968303.23 frames. ], batch size: 19, lr: 1.32e-02,
2024-10-08 11:18:11,642 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=14474.0, ans=0.125
2024-10-08 11:18:19,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=14477.333333333334, ans=0.006344444444444444
2024-10-08 11:18:23,855 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=18.08 vs. limit=18.358
2024-10-08 11:18:57,231 INFO [train.py:1153] Epoch 7, batch 2350, loss[loss=0.2611, simple_loss=0.2963, pruned_loss=0.08154, ctc_loss=0.1573, over 4867.00 frames. ], tot_loss[loss=0.2465, simple_loss=0.278, pruned_loss=0.07595, ctc_loss=0.1577, over 968482.23 frames. ], batch size: 23, lr: 1.32e-02,
2024-10-08 11:18:57,241 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=14487.333333333334, ans=0.125
2024-10-08 11:19:36,435 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=14497.333333333334, ans=0.006261111111111113
2024-10-08 11:19:37,723 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=14497.333333333334, ans=0.125
2024-10-08 11:19:42,939 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=14497.333333333334, ans=0.007717971014492753
2024-10-08 11:19:46,749 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.274e+01 1.099e+02 1.261e+02 1.423e+02 2.099e+02, threshold=2.523e+02, percent-clipped=0.0
2024-10-08 11:20:02,540 INFO [train.py:1153] Epoch 7, batch 2400, loss[loss=0.2234, simple_loss=0.2653, pruned_loss=0.06319, ctc_loss=0.1376, over 4752.00 frames. ], tot_loss[loss=0.2469, simple_loss=0.2783, pruned_loss=0.07611, ctc_loss=0.1581, over 967769.84 frames. ], batch size: 19, lr: 1.32e-02,
2024-10-08 11:20:14,761 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.28 vs. limit=12.940249999999999
2024-10-08 11:20:20,723 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=14507.333333333334, ans=0.125
2024-10-08 11:20:41,372 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.84 vs. limit=18.3855
2024-10-08 11:21:05,200 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.82 vs. limit=12.943999999999999
2024-10-08 11:21:07,280 INFO [train.py:1153] Epoch 7, batch 2450, loss[loss=0.2175, simple_loss=0.2553, pruned_loss=0.06328, ctc_loss=0.1329, over 4878.00 frames. ], tot_loss[loss=0.2488, simple_loss=0.2793, pruned_loss=0.07715, ctc_loss=0.1598, over 967019.31 frames. ], batch size: 22, lr: 1.32e-02,
2024-10-08 11:21:12,624 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.min_positive, batch_count=14520.666666666666, ans=0.025
2024-10-08 11:21:13,791 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=14520.666666666666, ans=0.125
2024-10-08 11:21:28,868 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.69 vs. limit=12.9465
2024-10-08 11:21:42,540 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=14527.333333333334, ans=0.15472666666666665
2024-10-08 11:21:42,600 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=14527.333333333334, ans=0.15472666666666665
2024-10-08 11:21:56,916 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.395e+01 1.199e+02 1.323e+02 1.464e+02 3.353e+02, threshold=2.645e+02, percent-clipped=1.0
2024-10-08 11:22:02,337 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=14534.0, ans=0.04949747468305833
2024-10-08 11:22:12,509 INFO [train.py:1153] Epoch 7, batch 2500, loss[loss=0.2926, simple_loss=0.3151, pruned_loss=0.09638, ctc_loss=0.1935, over 4745.00 frames. ], tot_loss[loss=0.2496, simple_loss=0.28, pruned_loss=0.07754, ctc_loss=0.1602, over 966698.46 frames. ], batch size: 26, lr: 1.32e-02,
2024-10-08 11:22:45,619 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=2.058e-01
2024-10-08 11:22:50,764 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=14547.333333333334, ans=0.125
2024-10-08 11:22:52,050 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14547.333333333334, ans=0.15452666666666665
2024-10-08 11:22:54,738 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=14547.333333333334, ans=0.09899494936611666
2024-10-08 11:23:07,778 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=14550.666666666666, ans=0.007706376811594203
2024-10-08 11:23:18,052 INFO [train.py:1153] Epoch 7, batch 2550, loss[loss=0.2303, simple_loss=0.2588, pruned_loss=0.07104, ctc_loss=0.1493, over 4959.00 frames. ], tot_loss[loss=0.2507, simple_loss=0.2806, pruned_loss=0.07813, ctc_loss=0.1614, over 967200.22 frames. ], batch size: 19, lr: 1.31e-02,
2024-10-08 11:23:36,492 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=14557.333333333334, ans=0.125
2024-10-08 11:23:37,776 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=14557.333333333334, ans=0.007704927536231884
2024-10-08 11:24:06,523 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=14564.0, ans=0.125
2024-10-08 11:24:07,178 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.53 vs. limit=12.961500000000001
2024-10-08 11:24:07,564 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.969e+01 1.130e+02 1.282e+02 1.412e+02 1.774e+02, threshold=2.563e+02, percent-clipped=0.0
2024-10-08 11:24:15,483 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=14567.333333333334, ans=0.09899494936611666
2024-10-08 11:24:16,797 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=14567.333333333334, ans=0.125
2024-10-08 11:24:23,183 INFO [train.py:1153] Epoch 7, batch 2600, loss[loss=0.2219, simple_loss=0.2593, pruned_loss=0.06582, ctc_loss=0.1322, over 4855.00 frames. ], tot_loss[loss=0.2506, simple_loss=0.2807, pruned_loss=0.07802, ctc_loss=0.1612, over 966573.61 frames. ], batch size: 20, lr: 1.31e-02,
2024-10-08 11:24:32,558 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=14570.666666666666, ans=0.39002666666666674
2024-10-08 11:24:36,360 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=14574.0, ans=0.10425999999999999
2024-10-08 11:24:42,946 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=14574.0, ans=0.125
2024-10-08 11:24:47,283 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=14.93 vs. limit=12.965250000000001
2024-10-08 11:24:53,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=14577.333333333334, ans=0.3897933333333333
2024-10-08 11:25:06,939 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.38 vs. limit=9.832266666666666
2024-10-08 11:25:09,082 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=14580.666666666666, ans=0.007699855072463769
2024-10-08 11:25:28,617 INFO [train.py:1153] Epoch 7, batch 2650, loss[loss=0.2933, simple_loss=0.3022, pruned_loss=0.1024, ctc_loss=0.1986, over 4810.00 frames. ], tot_loss[loss=0.2512, simple_loss=0.2812, pruned_loss=0.07823, ctc_loss=0.1619, over 966241.96 frames. ], batch size: 38, lr: 1.31e-02,
2024-10-08 11:25:47,370 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=14590.666666666666, ans=0.125
2024-10-08 11:26:18,630 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.103e+01 1.119e+02 1.230e+02 1.364e+02 1.983e+02, threshold=2.460e+02, percent-clipped=0.0
2024-10-08 11:26:21,283 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=14600.666666666666, ans=0.007695507246376812
2024-10-08 11:26:34,221 INFO [train.py:1153] Epoch 7, batch 2700, loss[loss=0.2444, simple_loss=0.2817, pruned_loss=0.07404, ctc_loss=0.1478, over 4879.00 frames. ], tot_loss[loss=0.2497, simple_loss=0.28, pruned_loss=0.07757, ctc_loss=0.1603, over 966442.48 frames. ], batch size: 28, lr: 1.31e-02,
2024-10-08 11:26:42,337 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=14604.0, ans=0.125
2024-10-08 11:26:46,397 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=14607.333333333334, ans=0.025
2024-10-08 11:26:53,557 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=5.60 vs. limit=12.97775
2024-10-08 11:27:00,011 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=15.12 vs. limit=12.979
2024-10-08 11:27:09,384 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.29 vs. limit=12.979
2024-10-08 11:27:17,160 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten.whitening_limit, batch_count=14614.0, ans=12.98025
2024-10-08 11:27:40,262 INFO [train.py:1153] Epoch 7, batch 2750, loss[loss=0.1903, simple_loss=0.2357, pruned_loss=0.04825, ctc_loss=0.1209, over 4796.00 frames. ], tot_loss[loss=0.25, simple_loss=0.2803, pruned_loss=0.0777, ctc_loss=0.1606, over 967165.58 frames. ], batch size: 19, lr: 1.31e-02,
2024-10-08 11:27:43,414 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.95 vs. limit=12.98275
2024-10-08 11:27:59,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=14624.0, ans=0.005733333333333333
2024-10-08 11:28:29,881 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.653e+01 1.146e+02 1.259e+02 1.453e+02 2.478e+02, threshold=2.517e+02, percent-clipped=1.0
2024-10-08 11:28:45,521 INFO [train.py:1153] Epoch 7, batch 2800, loss[loss=0.2865, simple_loss=0.3009, pruned_loss=0.09533, ctc_loss=0.2039, over 4802.00 frames. ], tot_loss[loss=0.2524, simple_loss=0.2822, pruned_loss=0.07874, ctc_loss=0.1627, over 967372.70 frames. ], batch size: 53, lr: 1.31e-02,
2024-10-08 11:29:03,703 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=14640.666666666666, ans=0.025
2024-10-08 11:29:19,832 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=14644.0, ans=0.125
2024-10-08 11:29:22,186 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=5.707e-03
2024-10-08 11:29:50,618 INFO [train.py:1153] Epoch 7, batch 2850, loss[loss=0.2125, simple_loss=0.2733, pruned_loss=0.05317, ctc_loss=0.1136, over 4929.00 frames. ], tot_loss[loss=0.2527, simple_loss=0.2823, pruned_loss=0.07889, ctc_loss=0.1634, over 967006.55 frames. ], batch size: 20, lr: 1.31e-02,
2024-10-08 11:30:09,659 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.98 vs. limit=9.862933333333334
2024-10-08 11:30:11,593 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=14657.333333333334, ans=0.0
2024-10-08 11:30:23,947 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.49 vs. limit=18.4955
2024-10-08 11:30:26,056 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=14660.666666666666, ans=0.125
2024-10-08 11:30:28,680 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=14664.0, ans=0.125
2024-10-08 11:30:39,212 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/checkpoint-44000.pt
2024-10-08 11:30:41,195 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.694e+01 1.121e+02 1.284e+02 1.435e+02 1.958e+02, threshold=2.569e+02, percent-clipped=0.0
2024-10-08 11:30:51,161 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14667.333333333334, ans=0.15332666666666667
2024-10-08 11:30:56,086 INFO [train.py:1153] Epoch 7, batch 2900, loss[loss=0.2241, simple_loss=0.2617, pruned_loss=0.06501, ctc_loss=0.141, over 4740.00 frames. ], tot_loss[loss=0.2524, simple_loss=0.2819, pruned_loss=0.07884, ctc_loss=0.1632, over 966142.02 frames. ], batch size: 20, lr: 1.31e-02,
2024-10-08 11:30:56,230 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=14670.666666666666, ans=0.3865266666666668
2024-10-08 11:31:08,426 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=5.15 vs. limit=5.2011
2024-10-08 11:31:21,382 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=14.05 vs. limit=13.004
2024-10-08 11:31:22,346 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=14677.333333333334, ans=0.15322666666666665
2024-10-08 11:31:24,885 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=14677.333333333334, ans=0.0076788405797101455
2024-10-08 11:31:26,269 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=14677.333333333334, ans=0.125
2024-10-08 11:31:31,422 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 11:32:01,166 INFO [train.py:1153] Epoch 7, batch 2950, loss[loss=0.1754, simple_loss=0.2279, pruned_loss=0.0412, ctc_loss=0.1011, over 4798.00 frames. ], tot_loss[loss=0.2508, simple_loss=0.2809, pruned_loss=0.07803, ctc_loss=0.1616, over 966673.06 frames. ], batch size: 19, lr: 1.31e-02,
2024-10-08 11:32:51,294 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.815e+01 1.144e+02 1.270e+02 1.452e+02 2.372e+02, threshold=2.539e+02, percent-clipped=0.0
2024-10-08 11:32:55,551 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=14700.666666666666, ans=0.125
2024-10-08 11:33:02,218 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=14700.666666666666, ans=0.125
2024-10-08 11:33:07,313 INFO [train.py:1153] Epoch 7, batch 3000, loss[loss=0.2081, simple_loss=0.26, pruned_loss=0.05613, ctc_loss=0.1096, over 4832.00 frames. ], tot_loss[loss=0.2503, simple_loss=0.2805, pruned_loss=0.0779, ctc_loss=0.1607, over 967217.06 frames. ], batch size: 21, lr: 1.31e-02,
2024-10-08 11:33:07,313 INFO [train.py:1176] Computing validation loss
2024-10-08 11:33:15,047 INFO [train.py:1185] Epoch 7, validation: loss=0.1691, simple_loss=0.2517, pruned_loss=0.03164, ctc_loss=0.05786, over 90464.00 frames.
2024-10-08 11:33:15,047 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 11:33:15,280 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=14704.0, ans=0.15296
2024-10-08 11:33:24,202 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=14704.0, ans=0.15296
2024-10-08 11:33:33,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=14707.333333333334, ans=0.025
2024-10-08 11:33:47,555 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=14710.666666666666, ans=0.007671594202898551
2024-10-08 11:34:07,360 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=14717.333333333334, ans=0.125
2024-10-08 11:34:20,088 INFO [train.py:1153] Epoch 7, batch 3050, loss[loss=0.1943, simple_loss=0.2455, pruned_loss=0.05135, ctc_loss=0.1008, over 4757.00 frames. ], tot_loss[loss=0.25, simple_loss=0.28, pruned_loss=0.07789, ctc_loss=0.1608, over 966766.96 frames. ], batch size: 19, lr: 1.31e-02,
2024-10-08 11:34:20,929 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=13.23 vs. limit=13.02025
2024-10-08 11:34:36,480 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=3.81 vs. limit=9.8896
2024-10-08 11:34:38,579 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=14724.0, ans=0.0
2024-10-08 11:35:02,093 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=14730.666666666666, ans=0.007667246376811595
2024-10-08 11:35:10,080 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.940e+01 1.102e+02 1.283e+02 1.417e+02 2.038e+02, threshold=2.566e+02, percent-clipped=0.0
2024-10-08 11:35:22,026 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14734.0, ans=0.15266
2024-10-08 11:35:25,782 INFO [train.py:1153] Epoch 7, batch 3100, loss[loss=0.2745, simple_loss=0.2829, pruned_loss=0.09644, ctc_loss=0.1829, over 4825.00 frames. ], tot_loss[loss=0.2493, simple_loss=0.2796, pruned_loss=0.07752, ctc_loss=0.16, over 966512.99 frames. ], batch size: 38, lr: 1.31e-02,
2024-10-08 11:35:31,287 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=14737.333333333334, ans=0.007665797101449275
2024-10-08 11:35:44,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=14740.666666666666, ans=0.15259333333333336
2024-10-08 11:36:00,455 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=14744.0, ans=0.15256
2024-10-08 11:36:21,882 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=14750.666666666666, ans=0.0
2024-10-08 11:36:21,903 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=14750.666666666666, ans=0.15249333333333334
2024-10-08 11:36:32,315 INFO [train.py:1153] Epoch 7, batch 3150, loss[loss=0.2886, simple_loss=0.2923, pruned_loss=0.1007, ctc_loss=0.2091, over 4773.00 frames. ], tot_loss[loss=0.2491, simple_loss=0.2795, pruned_loss=0.07742, ctc_loss=0.1598, over 966757.66 frames. ], batch size: 40, lr: 1.31e-02,
2024-10-08 11:36:37,821 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=14754.0, ans=0.15246
2024-10-08 11:36:56,715 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=6.81 vs. limit=9.902933333333333
2024-10-08 11:37:12,744 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.42 vs. limit=18.573
2024-10-08 11:37:13,704 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=14764.0, ans=0.125
2024-10-08 11:37:22,832 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.479e+01 1.102e+02 1.199e+02 1.383e+02 1.917e+02, threshold=2.398e+02, percent-clipped=0.0
2024-10-08 11:37:25,829 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=14767.333333333334, ans=0.125
2024-10-08 11:37:38,940 INFO [train.py:1153] Epoch 7, batch 3200, loss[loss=0.2427, simple_loss=0.2799, pruned_loss=0.07483, ctc_loss=0.1398, over 4740.00 frames. ], tot_loss[loss=0.248, simple_loss=0.2787, pruned_loss=0.0769, ctc_loss=0.1588, over 967270.83 frames. ], batch size: 20, lr: 1.30e-02,
2024-10-08 11:37:51,879 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14774.0, ans=0.15226
2024-10-08 11:37:55,784 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=14774.0, ans=0.0
2024-10-08 11:38:32,736 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 11:38:44,232 INFO [train.py:1153] Epoch 7, batch 3250, loss[loss=0.269, simple_loss=0.29, pruned_loss=0.08931, ctc_loss=0.1735, over 4864.00 frames. ], tot_loss[loss=0.2487, simple_loss=0.2795, pruned_loss=0.07712, ctc_loss=0.1592, over 967320.84 frames. ], batch size: 24, lr: 1.30e-02,
2024-10-08 11:38:44,402 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=14787.333333333334, ans=0.025
2024-10-08 11:39:17,631 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=14794.0, ans=0.04949747468305833
2024-10-08 11:39:34,635 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.842e+01 1.133e+02 1.302e+02 1.445e+02 1.938e+02, threshold=2.604e+02, percent-clipped=0.0
2024-10-08 11:39:50,587 INFO [train.py:1153] Epoch 7, batch 3300, loss[loss=0.2526, simple_loss=0.2735, pruned_loss=0.08181, ctc_loss=0.1701, over 4830.00 frames. ], tot_loss[loss=0.2482, simple_loss=0.279, pruned_loss=0.07699, ctc_loss=0.1587, over 967748.62 frames. ], batch size: 43, lr: 1.30e-02,
2024-10-08 11:40:01,807 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=4.37 vs. limit=13.0515
2024-10-08 11:40:14,305 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff2_skip_rate, batch_count=14807.333333333334, ans=0.0076505797101449275
2024-10-08 11:40:19,539 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=14810.666666666666, ans=0.125
2024-10-08 11:40:27,779 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.59 vs. limit=8.702666666666666
2024-10-08 11:40:36,562 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=14814.0, ans=0.125
2024-10-08 11:40:55,007 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=14820.666666666666, ans=0.3812766666666667
2024-10-08 11:40:56,144 INFO [train.py:1153] Epoch 7, batch 3350, loss[loss=0.2599, simple_loss=0.2943, pruned_loss=0.08042, ctc_loss=0.1615, over 4788.00 frames. ], tot_loss[loss=0.2506, simple_loss=0.2806, pruned_loss=0.07806, ctc_loss=0.1609, over 967050.78 frames. ], batch size: 40, lr: 1.30e-02,
2024-10-08 11:41:00,563 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=7.93 vs. limit=13.05775
2024-10-08 11:41:02,989 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=14820.666666666666, ans=0.00764768115942029
2024-10-08 11:41:08,368 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=14824.0, ans=0.125
2024-10-08 11:41:16,405 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=14824.0, ans=0.38116000000000005
2024-10-08 11:41:20,832 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.35 vs. limit=8.706
2024-10-08 11:41:46,645 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.778e+01 1.146e+02 1.274e+02 1.487e+02 2.189e+02, threshold=2.548e+02, percent-clipped=0.0
2024-10-08 11:42:02,835 INFO [train.py:1153] Epoch 7, batch 3400, loss[loss=0.1876, simple_loss=0.2477, pruned_loss=0.04608, ctc_loss=0.08844, over 4959.00 frames. ], tot_loss[loss=0.2504, simple_loss=0.2805, pruned_loss=0.07803, ctc_loss=0.1608, over 966914.04 frames. ], batch size: 19, lr: 1.30e-02,
2024-10-08 11:42:15,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=14840.666666666666, ans=0.3805766666666668
2024-10-08 11:42:15,063 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14840.666666666666, ans=0.15159333333333336
2024-10-08 11:42:28,311 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=14844.0, ans=0.04949747468305833
2024-10-08 11:42:32,835 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=19.45 vs. limit=18.633
2024-10-08 11:42:34,910 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14844.0, ans=0.15156
2024-10-08 11:42:57,330 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=14850.666666666666, ans=0.125
2024-10-08 11:43:09,297 INFO [train.py:1153] Epoch 7, batch 3450, loss[loss=0.273, simple_loss=0.2942, pruned_loss=0.09065, ctc_loss=0.1762, over 4805.00 frames. ], tot_loss[loss=0.2485, simple_loss=0.2792, pruned_loss=0.07707, ctc_loss=0.1591, over 967051.34 frames. ], batch size: 43, lr: 1.30e-02,
2024-10-08 11:43:14,233 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1.whitening_limit, batch_count=14854.0, ans=8.7135
2024-10-08 11:43:28,068 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=14857.333333333334, ans=0.8985733333333333
2024-10-08 11:43:35,156 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.06 vs. limit=12.430333333333333
2024-10-08 11:44:00,042 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.454e+01 1.069e+02 1.200e+02 1.341e+02 2.054e+02, threshold=2.401e+02, percent-clipped=0.0
2024-10-08 11:44:15,990 INFO [train.py:1153] Epoch 7, batch 3500, loss[loss=0.2446, simple_loss=0.2809, pruned_loss=0.07454, ctc_loss=0.1481, over 4883.00 frames. ], tot_loss[loss=0.2484, simple_loss=0.2791, pruned_loss=0.07709, ctc_loss=0.1587, over 967262.50 frames. ], batch size: 19, lr: 1.30e-02,
2024-10-08 11:44:32,012 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=14874.0, ans=0.42311
2024-10-08 11:44:37,490 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=14874.0, ans=0.004691666666666663
2024-10-08 11:44:42,750 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=14877.333333333334, ans=0.00763536231884058
2024-10-08 11:44:45,559 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=14877.333333333334, ans=0.00763536231884058
2024-10-08 11:44:52,259 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14877.333333333334, ans=0.15122666666666665
2024-10-08 11:44:59,074 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=14880.666666666666, ans=0.125
2024-10-08 11:45:08,207 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=14884.0, ans=0.125
2024-10-08 11:45:23,049 INFO [train.py:1153] Epoch 7, batch 3550, loss[loss=0.2789, simple_loss=0.3092, pruned_loss=0.08618, ctc_loss=0.1904, over 4770.00 frames. ], tot_loss[loss=0.2476, simple_loss=0.2787, pruned_loss=0.07666, ctc_loss=0.1581, over 967213.62 frames. ], batch size: 29, lr: 1.30e-02,
2024-10-08 11:45:25,473 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=14887.333333333334, ans=0.004636111111111112
2024-10-08 11:45:35,441 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.29 vs. limit=18.668
2024-10-08 11:45:46,258 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=14890.666666666666, ans=0.42336
2024-10-08 11:45:52,791 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=14894.0, ans=0.125
2024-10-08 11:46:12,191 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.325e+01 1.094e+02 1.227e+02 1.348e+02 1.672e+02, threshold=2.454e+02, percent-clipped=0.0
2024-10-08 11:46:28,267 INFO [train.py:1153] Epoch 7, batch 3600, loss[loss=0.2497, simple_loss=0.2917, pruned_loss=0.07334, ctc_loss=0.1525, over 4920.00 frames. ], tot_loss[loss=0.2478, simple_loss=0.2788, pruned_loss=0.07678, ctc_loss=0.158, over 967270.94 frames. ], batch size: 20, lr: 1.30e-02,
2024-10-08 11:46:37,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=14904.0, ans=0.37836000000000003
2024-10-08 11:47:07,190 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=14914.0, ans=0.37801000000000007
2024-10-08 11:47:34,770 INFO [train.py:1153] Epoch 7, batch 3650, loss[loss=0.2468, simple_loss=0.2729, pruned_loss=0.07488, ctc_loss=0.1776, over 4851.00 frames. ], tot_loss[loss=0.2464, simple_loss=0.2778, pruned_loss=0.07612, ctc_loss=0.1569, over 967723.97 frames. ], batch size: 31, lr: 1.30e-02,
2024-10-08 11:48:25,144 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.683e+01 1.043e+02 1.177e+02 1.356e+02 1.731e+02, threshold=2.355e+02, percent-clipped=0.0
2024-10-08 11:48:40,896 INFO [train.py:1153] Epoch 7, batch 3700, loss[loss=0.2725, simple_loss=0.2951, pruned_loss=0.08497, ctc_loss=0.2, over 4852.00 frames. ], tot_loss[loss=0.2441, simple_loss=0.2765, pruned_loss=0.07487, ctc_loss=0.1549, over 967224.62 frames. ], batch size: 24, lr: 1.30e-02,
2024-10-08 11:48:49,668 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.86 vs. limit=8.734333333333334
2024-10-08 11:48:52,558 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.01 vs. limit=5.240600000000001
2024-10-08 11:48:54,305 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=14940.666666666666, ans=0.125
2024-10-08 11:49:14,575 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=14944.0, ans=0.125
2024-10-08 11:49:43,838 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=14950.666666666666, ans=0.007619420289855072
2024-10-08 11:49:47,628 INFO [train.py:1153] Epoch 7, batch 3750, loss[loss=0.2245, simple_loss=0.2534, pruned_loss=0.06891, ctc_loss=0.1447, over 4959.00 frames. ], tot_loss[loss=0.2434, simple_loss=0.2761, pruned_loss=0.07447, ctc_loss=0.1542, over 967692.03 frames. ], batch size: 19, lr: 1.30e-02,
2024-10-08 11:50:08,749 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.76 vs. limit=5.2436
2024-10-08 11:50:10,388 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=14957.333333333334, ans=0.06164466666666668
2024-10-08 11:50:10,542 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=14957.333333333334, ans=0.004344444444444449
2024-10-08 11:50:19,966 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.83 vs. limit=5.2440999999999995
2024-10-08 11:50:38,046 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.598e+01 1.101e+02 1.221e+02 1.351e+02 1.819e+02, threshold=2.442e+02, percent-clipped=0.0
2024-10-08 11:50:53,918 INFO [train.py:1153] Epoch 7, batch 3800, loss[loss=0.2811, simple_loss=0.3018, pruned_loss=0.09252, ctc_loss=0.1882, over 4733.00 frames. ], tot_loss[loss=0.2426, simple_loss=0.2756, pruned_loss=0.07406, ctc_loss=0.1537, over 967494.11 frames. ], batch size: 26, lr: 1.30e-02,
2024-10-08 11:50:58,562 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.90 vs. limit=9.988266666666666
2024-10-08 11:51:15,468 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=14974.0, ans=0.025
2024-10-08 11:51:16,788 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=14974.0, ans=0.125
2024-10-08 11:51:20,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=14977.333333333334, ans=0.15022666666666665
2024-10-08 11:51:24,153 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten.whitening_limit, batch_count=14977.333333333334, ans=13.1165
2024-10-08 11:51:39,396 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=14980.666666666666, ans=0.37567666666666677
2024-10-08 11:51:56,039 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.62 vs. limit=8.746
2024-10-08 11:52:00,366 INFO [train.py:1153] Epoch 7, batch 3850, loss[loss=0.2797, simple_loss=0.3021, pruned_loss=0.09346, ctc_loss=0.1758, over 4823.00 frames. ], tot_loss[loss=0.2434, simple_loss=0.2761, pruned_loss=0.07443, ctc_loss=0.1546, over 967486.79 frames. ], batch size: 38, lr: 1.30e-02,
2024-10-08 11:52:01,902 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=14987.333333333334, ans=0.004219444444444449
2024-10-08 11:52:07,323 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=14987.333333333334, ans=0.125
2024-10-08 11:52:09,918 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=14987.333333333334, ans=0.007611449275362319
2024-10-08 11:52:26,973 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=14994.0, ans=0.125
2024-10-08 11:52:28,357 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=14994.0, ans=0.09899494936611666
2024-10-08 11:52:34,833 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=14994.0, ans=0.0
2024-10-08 11:52:40,507 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.71 vs. limit=13.123999999999999
2024-10-08 11:52:50,476 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=3.61 vs. limit=9.998933333333333
2024-10-08 11:52:50,769 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.611e+01 1.124e+02 1.247e+02 1.350e+02 1.740e+02, threshold=2.494e+02, percent-clipped=0.0
2024-10-08 11:52:52,141 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=15000.666666666666, ans=0.125
2024-10-08 11:52:52,149 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=15000.666666666666, ans=0.125
2024-10-08 11:52:58,096 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=18.22 vs. limit=18.750500000000002
2024-10-08 11:53:06,478 INFO [train.py:1153] Epoch 7, batch 3900, loss[loss=0.2167, simple_loss=0.2726, pruned_loss=0.05551, ctc_loss=0.1242, over 4727.00 frames. ], tot_loss[loss=0.2429, simple_loss=0.2756, pruned_loss=0.07429, ctc_loss=0.154, over 967012.83 frames. ], batch size: 26, lr: 1.29e-02,
2024-10-08 11:53:10,573 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=15004.0, ans=0.14996
2024-10-08 11:53:10,730 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 11:53:19,074 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module1.whiten, num_groups=1, num_channels=192, metric=2.95 vs. limit=13.127749999999999
2024-10-08 11:53:22,102 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=15007.333333333334, ans=0.14992666666666665
2024-10-08 11:53:57,121 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=5.10 vs. limit=5.2521
2024-10-08 11:54:07,259 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=15017.333333333334, ans=0.125
2024-10-08 11:54:12,327 INFO [train.py:1153] Epoch 7, batch 3950, loss[loss=0.2441, simple_loss=0.2672, pruned_loss=0.07924, ctc_loss=0.1561, over 4823.00 frames. ], tot_loss[loss=0.2405, simple_loss=0.2742, pruned_loss=0.07311, ctc_loss=0.1513, over 967167.58 frames. ], batch size: 36, lr: 1.29e-02,
2024-10-08 11:54:12,572 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=15020.666666666666, ans=0.0040805555555555595
2024-10-08 11:54:16,320 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=15020.666666666666, ans=0.125
2024-10-08 11:54:17,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=15020.666666666666, ans=0.0040805555555555595
2024-10-08 11:54:19,749 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=3.95 vs. limit=13.13275
2024-10-08 11:54:31,268 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.14 vs. limit=13.134
2024-10-08 11:54:44,183 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=15027.333333333334, ans=0.125
2024-10-08 11:54:45,396 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=15027.333333333334, ans=0.125
2024-10-08 11:55:02,571 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.609e+01 1.029e+02 1.172e+02 1.337e+02 1.907e+02, threshold=2.343e+02, percent-clipped=0.0
2024-10-08 11:55:15,869 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=15034.0, ans=0.14966000000000002
2024-10-08 11:55:17,633 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.25 vs. limit=13.139
2024-10-08 11:55:18,368 INFO [train.py:1153] Epoch 7, batch 4000, loss[loss=0.2546, simple_loss=0.2947, pruned_loss=0.07814, ctc_loss=0.1457, over 4815.00 frames. ], tot_loss[loss=0.2408, simple_loss=0.2745, pruned_loss=0.07327, ctc_loss=0.1513, over 967259.28 frames. ], batch size: 19, lr: 1.29e-02,
2024-10-08 11:55:43,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=15044.0, ans=0.003983333333333332
2024-10-08 11:55:54,637 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=27.48 vs. limit=18.783
2024-10-08 11:56:00,737 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=15047.333333333334, ans=0.125
2024-10-08 11:56:10,153 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=15050.666666666666, ans=0.125
2024-10-08 11:56:24,490 INFO [train.py:1153] Epoch 7, batch 4050, loss[loss=0.2998, simple_loss=0.3056, pruned_loss=0.1051, ctc_loss=0.2096, over 4783.00 frames. ], tot_loss[loss=0.2422, simple_loss=0.2752, pruned_loss=0.07407, ctc_loss=0.1527, over 967627.63 frames. ], batch size: 53, lr: 1.29e-02,
2024-10-08 11:56:35,384 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=15054.0, ans=0.14946
2024-10-08 11:56:35,715 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.28 vs. limit=5.258100000000001
2024-10-08 11:57:14,857 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.304e+01 1.117e+02 1.210e+02 1.393e+02 1.920e+02, threshold=2.420e+02, percent-clipped=0.0
2024-10-08 11:57:18,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=15067.333333333334, ans=0.3726433333333333
2024-10-08 11:57:19,295 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=18.40 vs. limit=13.15025
2024-10-08 11:57:30,709 INFO [train.py:1153] Epoch 7, batch 4100, loss[loss=0.2973, simple_loss=0.3087, pruned_loss=0.1014, ctc_loss=0.208, over 4854.00 frames. ], tot_loss[loss=0.2443, simple_loss=0.277, pruned_loss=0.07483, ctc_loss=0.1548, over 966954.24 frames. ], batch size: 31, lr: 1.29e-02,
2024-10-08 11:57:40,121 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=15070.666666666666, ans=0.125
2024-10-08 11:57:43,152 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=19.48 vs. limit=18.805500000000002
2024-10-08 11:57:43,487 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=17.83 vs. limit=18.805500000000002
2024-10-08 11:57:45,428 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=15074.0, ans=0.125
2024-10-08 11:58:01,288 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=15077.333333333334, ans=0.125
2024-10-08 11:58:17,099 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=15080.666666666666, ans=0.0038305555555555593
2024-10-08 11:58:22,608 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=15084.0, ans=0.125
2024-10-08 11:58:23,922 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=15084.0, ans=0.0
2024-10-08 11:58:30,477 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=15084.0, ans=0.37206000000000006
2024-10-08 11:58:31,867 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=15084.0, ans=0.14916000000000001
2024-10-08 11:58:33,166 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=15084.0, ans=0.007590434782608696
2024-10-08 11:58:37,019 INFO [train.py:1153] Epoch 7, batch 4150, loss[loss=0.239, simple_loss=0.2721, pruned_loss=0.07208, ctc_loss=0.1542, over 4757.00 frames. ], tot_loss[loss=0.2441, simple_loss=0.2768, pruned_loss=0.07471, ctc_loss=0.1547, over 967059.73 frames. ], batch size: 20, lr: 1.29e-02,
2024-10-08 11:59:13,164 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=15094.0, ans=0.125
2024-10-08 11:59:19,679 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=15097.333333333334, ans=0.125
2024-10-08 11:59:21,096 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=15097.333333333334, ans=0.003761111111111111
2024-10-08 11:59:27,434 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.255e+01 1.077e+02 1.246e+02 1.402e+02 2.250e+02, threshold=2.492e+02, percent-clipped=0.0
2024-10-08 11:59:27,634 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=15097.333333333334, ans=0.125
2024-10-08 11:59:43,369 INFO [train.py:1153] Epoch 7, batch 4200, loss[loss=0.2739, simple_loss=0.2888, pruned_loss=0.09502, ctc_loss=0.1723, over 4870.00 frames. ], tot_loss[loss=0.2444, simple_loss=0.2767, pruned_loss=0.07514, ctc_loss=0.1547, over 967291.00 frames. ], batch size: 31, lr: 1.29e-02,
2024-10-08 11:59:46,251 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=15104.0, ans=0.14896
2024-10-08 12:00:05,988 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.max_positive, batch_count=15107.333333333334, ans=0.9010733333333333
2024-10-08 12:00:07,353 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=15107.333333333334, ans=0.07
2024-10-08 12:00:19,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=15110.666666666666, ans=0.0075846376811594205
2024-10-08 12:00:26,041 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.84 vs. limit=13.16775
2024-10-08 12:00:28,234 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=15114.0, ans=0.125
2024-10-08 12:00:35,115 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=16.73 vs. limit=13.169
2024-10-08 12:00:40,262 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.69 vs. limit=8.779333333333334
2024-10-08 12:00:48,962 INFO [train.py:1153] Epoch 7, batch 4250, loss[loss=0.2155, simple_loss=0.2629, pruned_loss=0.06152, ctc_loss=0.1128, over 4748.00 frames. ], tot_loss[loss=0.243, simple_loss=0.2753, pruned_loss=0.07457, ctc_loss=0.1538, over 967108.96 frames. ], batch size: 19, lr: 1.29e-02,
2024-10-08 12:01:13,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=15124.0, ans=0.125
2024-10-08 12:01:14,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=5.36 vs. limit=5.2691
2024-10-08 12:01:38,266 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=15130.666666666666, ans=0.125
2024-10-08 12:01:39,287 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.013e+01 1.103e+02 1.250e+02 1.403e+02 1.927e+02, threshold=2.500e+02, percent-clipped=0.0
2024-10-08 12:01:53,213 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.94 vs. limit=18.8505
2024-10-08 12:01:54,992 INFO [train.py:1153] Epoch 7, batch 4300, loss[loss=0.2313, simple_loss=0.2592, pruned_loss=0.07056, ctc_loss=0.1557, over 4832.00 frames. ], tot_loss[loss=0.2419, simple_loss=0.2743, pruned_loss=0.07406, ctc_loss=0.1534, over 967428.81 frames. ], batch size: 21, lr: 1.29e-02,
2024-10-08 12:01:59,135 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=15137.333333333334, ans=0.14862666666666666
2024-10-08 12:02:02,373 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=21.99 vs. limit=18.853
2024-10-08 12:02:05,755 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=15137.333333333334, ans=10.0
2024-10-08 12:02:12,266 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=15140.666666666666, ans=0.025
2024-10-08 12:02:15,378 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.43 vs. limit=13.17775
2024-10-08 12:02:25,487 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=15144.0, ans=0.36996000000000007
2024-10-08 12:02:29,352 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=15144.0, ans=0.025
2024-10-08 12:02:32,259 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten.whitening_limit, batch_count=15144.0, ans=13.179
2024-10-08 12:02:35,959 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=15147.333333333334, ans=0.125
2024-10-08 12:02:43,775 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=15147.333333333334, ans=0.125
2024-10-08 12:03:00,865 INFO [train.py:1153] Epoch 7, batch 4350, loss[loss=0.2279, simple_loss=0.269, pruned_loss=0.06495, ctc_loss=0.1421, over 4835.00 frames. ], tot_loss[loss=0.2438, simple_loss=0.2758, pruned_loss=0.07496, ctc_loss=0.1548, over 966281.10 frames. ], batch size: 21, lr: 1.29e-02,
2024-10-08 12:03:40,643 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=15164.0, ans=0.125
2024-10-08 12:03:45,988 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=15164.0, ans=0.14836
2024-10-08 12:03:50,987 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.979e+01 1.146e+02 1.272e+02 1.395e+02 1.922e+02, threshold=2.544e+02, percent-clipped=0.0
2024-10-08 12:03:52,559 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=15167.333333333334, ans=0.07
2024-10-08 12:04:00,478 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=15167.333333333334, ans=0.0034694444444444486
2024-10-08 12:04:01,771 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=15167.333333333334, ans=0.0034694444444444486
2024-10-08 12:04:05,783 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=15170.666666666666, ans=0.125
2024-10-08 12:04:06,834 INFO [train.py:1153] Epoch 7, batch 4400, loss[loss=0.2546, simple_loss=0.2919, pruned_loss=0.07634, ctc_loss=0.1613, over 4751.00 frames. ], tot_loss[loss=0.245, simple_loss=0.277, pruned_loss=0.07545, ctc_loss=0.1552, over 965841.20 frames. ], batch size: 26, lr: 1.29e-02,
2024-10-08 12:04:20,170 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=15174.0, ans=0.14826
2024-10-08 12:04:22,883 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=15174.0, ans=0.14826
2024-10-08 12:04:29,709 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.69 vs. limit=13.190249999999999
2024-10-08 12:04:49,160 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 12:04:51,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=15180.666666666666, ans=0.125
2024-10-08 12:05:11,636 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=15187.333333333334, ans=0.125
2024-10-08 12:05:12,727 INFO [train.py:1153] Epoch 7, batch 4450, loss[loss=0.2378, simple_loss=0.2724, pruned_loss=0.07159, ctc_loss=0.1502, over 4883.00 frames. ], tot_loss[loss=0.2471, simple_loss=0.2781, pruned_loss=0.07656, ctc_loss=0.1575, over 966093.91 frames. ], batch size: 19, lr: 1.29e-02,
2024-10-08 12:05:13,463 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=19.05 vs. limit=18.8905
2024-10-08 12:05:24,805 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=15190.666666666666, ans=0.125
2024-10-08 12:05:44,601 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=15194.0, ans=0.125
2024-10-08 12:05:51,136 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=15197.333333333334, ans=0.125
2024-10-08 12:05:52,477 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=15197.333333333334, ans=0.125
2024-10-08 12:05:59,414 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.66 vs. limit=13.199
2024-10-08 12:06:02,282 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=13.21 vs. limit=13.199
2024-10-08 12:06:02,961 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.285e+01 1.156e+02 1.286e+02 1.461e+02 2.595e+02, threshold=2.572e+02, percent-clipped=1.0
2024-10-08 12:06:18,186 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=18.21 vs. limit=12.602
2024-10-08 12:06:18,687 INFO [train.py:1153] Epoch 7, batch 4500, loss[loss=0.2381, simple_loss=0.2716, pruned_loss=0.07383, ctc_loss=0.1423, over 4847.00 frames. ], tot_loss[loss=0.2484, simple_loss=0.2791, pruned_loss=0.07717, ctc_loss=0.1582, over 966107.63 frames. ], batch size: 28, lr: 1.29e-02,
2024-10-08 12:06:34,733 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=15207.333333333334, ans=0.125
2024-10-08 12:06:37,416 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=15207.333333333334, ans=0.3677433333333333
2024-10-08 12:07:00,376 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=15214.0, ans=0.36751
2024-10-08 12:07:18,506 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=15217.333333333334, ans=0.0032611111111111105
2024-10-08 12:07:19,887 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=15217.333333333334, ans=0.36739333333333335
2024-10-08 12:07:25,023 INFO [train.py:1153] Epoch 7, batch 4550, loss[loss=0.2042, simple_loss=0.2587, pruned_loss=0.05241, ctc_loss=0.1121, over 4860.00 frames. ], tot_loss[loss=0.2469, simple_loss=0.278, pruned_loss=0.0765, ctc_loss=0.157, over 966008.28 frames. ], batch size: 20, lr: 1.29e-02,
2024-10-08 12:07:49,178 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=15224.0, ans=0.14776
2024-10-08 12:08:02,309 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer1.prob, batch_count=15227.333333333334, ans=0.125
2024-10-08 12:08:05,520 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.47 vs. limit=8.807666666666666
2024-10-08 12:08:13,007 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=15230.666666666666, ans=0.14769333333333334
2024-10-08 12:08:14,561 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.38 vs. limit=13.211500000000001
2024-10-08 12:08:15,328 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.074e+01 1.080e+02 1.235e+02 1.340e+02 2.205e+02, threshold=2.470e+02, percent-clipped=0.0
2024-10-08 12:08:24,787 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.max_abs, batch_count=15234.0, ans=10.0
2024-10-08 12:08:31,203 INFO [train.py:1153] Epoch 7, batch 4600, loss[loss=0.279, simple_loss=0.2966, pruned_loss=0.09393, ctc_loss=0.1836, over 4744.00 frames. ], tot_loss[loss=0.2468, simple_loss=0.2782, pruned_loss=0.07632, ctc_loss=0.1568, over 966413.94 frames. ], batch size: 45, lr: 1.29e-02,
2024-10-08 12:08:39,279 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=15237.333333333334, ans=0.125
2024-10-08 12:08:41,913 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=15237.333333333334, ans=0.125
2024-10-08 12:08:49,841 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=15240.666666666666, ans=0.05
2024-10-08 12:08:51,296 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=15240.666666666666, ans=0.125
2024-10-08 12:09:00,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=15244.0, ans=0.00315
2024-10-08 12:09:03,059 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=15244.0, ans=0.125
2024-10-08 12:09:12,218 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=15247.333333333334, ans=0.025
2024-10-08 12:09:16,027 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=15247.333333333334, ans=0.14752666666666667
2024-10-08 12:09:16,158 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=15247.333333333334, ans=0.007554927536231884
2024-10-08 12:09:17,544 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=15247.333333333334, ans=0.0
2024-10-08 12:09:18,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=15247.333333333334, ans=0.125
2024-10-08 12:09:37,159 INFO [train.py:1153] Epoch 7, batch 4650, loss[loss=0.2359, simple_loss=0.2704, pruned_loss=0.0702, ctc_loss=0.1527, over 4840.00 frames. ], tot_loss[loss=0.2469, simple_loss=0.2783, pruned_loss=0.07634, ctc_loss=0.1573, over 965703.45 frames. ], batch size: 36, lr: 1.28e-02,
2024-10-08 12:09:45,429 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=15254.0, ans=0.003108333333333331
2024-10-08 12:09:53,204 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=15257.333333333334, ans=0.125
2024-10-08 12:10:11,189 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=7.94 vs. limit=13.22275
2024-10-08 12:10:27,570 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.108e+01 1.150e+02 1.324e+02 1.420e+02 2.196e+02, threshold=2.648e+02, percent-clipped=0.0
2024-10-08 12:10:31,633 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=15267.333333333334, ans=0.125
2024-10-08 12:10:33,420 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.05 vs. limit=10.106933333333334
2024-10-08 12:10:43,354 INFO [train.py:1153] Epoch 7, batch 4700, loss[loss=0.2256, simple_loss=0.2669, pruned_loss=0.06531, ctc_loss=0.1339, over 4940.00 frames. ], tot_loss[loss=0.2453, simple_loss=0.2771, pruned_loss=0.07551, ctc_loss=0.1563, over 965721.79 frames. ], batch size: 19, lr: 1.28e-02,
2024-10-08 12:10:46,862 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=5.26 vs. limit=10.108266666666665
2024-10-08 12:11:04,135 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn2.whiten.whitening_limit, batch_count=15274.0, ans=18.9555
2024-10-08 12:11:09,264 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=6.41 vs. limit=13.229
2024-10-08 12:11:27,150 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer2.prob, batch_count=15280.666666666666, ans=0.125
2024-10-08 12:11:29,864 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=15280.666666666666, ans=0.0029972222222222275
2024-10-08 12:11:31,693 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=13.64 vs. limit=13.23025
2024-10-08 12:11:49,394 INFO [train.py:1153] Epoch 7, batch 4750, loss[loss=0.2399, simple_loss=0.272, pruned_loss=0.07173, ctc_loss=0.161, over 4732.00 frames. ], tot_loss[loss=0.2452, simple_loss=0.2768, pruned_loss=0.07556, ctc_loss=0.1561, over 965471.83 frames. ], batch size: 45, lr: 1.28e-02,
2024-10-08 12:12:08,128 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=15290.666666666666, ans=0.058561333333333354
2024-10-08 12:12:08,268 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=15290.666666666666, ans=0.125
2024-10-08 12:12:13,677 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=15290.666666666666, ans=0.0029555555555555585
2024-10-08 12:12:16,389 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=15294.0, ans=0.125
2024-10-08 12:12:33,602 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff2_skip_rate, batch_count=15297.333333333334, ans=0.007544057971014493
2024-10-08 12:12:34,215 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.83 vs. limit=18.973
2024-10-08 12:12:38,895 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=15297.333333333334, ans=0.125
2024-10-08 12:12:39,969 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.809e+01 1.083e+02 1.206e+02 1.337e+02 1.768e+02, threshold=2.413e+02, percent-clipped=0.0
2024-10-08 12:12:56,009 INFO [train.py:1153] Epoch 7, batch 4800, loss[loss=0.2691, simple_loss=0.2954, pruned_loss=0.08428, ctc_loss=0.1854, over 4877.00 frames. ], tot_loss[loss=0.2453, simple_loss=0.2769, pruned_loss=0.0756, ctc_loss=0.1561, over 965856.88 frames. ], batch size: 22, lr: 1.28e-02,
2024-10-08 12:12:56,124 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=15304.0, ans=0.14696
2024-10-08 12:13:14,539 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=15307.333333333334, ans=0.05840716666666668
2024-10-08 12:13:33,056 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=15310.666666666666, ans=0.3641266666666667
2024-10-08 12:13:39,457 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=15314.0, ans=0.14686
2024-10-08 12:13:55,147 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=15317.333333333334, ans=0.125
2024-10-08 12:14:01,753 INFO [train.py:1153] Epoch 7, batch 4850, loss[loss=0.2561, simple_loss=0.2788, pruned_loss=0.08329, ctc_loss=0.1672, over 4829.00 frames. ], tot_loss[loss=0.2455, simple_loss=0.2768, pruned_loss=0.07576, ctc_loss=0.1564, over 966563.04 frames. ], batch size: 28, lr: 1.28e-02,
2024-10-08 12:14:16,636 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=15324.0, ans=0.0028166666666666687
2024-10-08 12:14:50,123 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.13 vs. limit=13.248999999999999
2024-10-08 12:14:52,089 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.302e+01 1.106e+02 1.264e+02 1.433e+02 1.989e+02, threshold=2.527e+02, percent-clipped=0.0
2024-10-08 12:15:08,205 INFO [train.py:1153] Epoch 7, batch 4900, loss[loss=0.2353, simple_loss=0.2724, pruned_loss=0.06912, ctc_loss=0.1498, over 4848.00 frames. ], tot_loss[loss=0.245, simple_loss=0.2762, pruned_loss=0.07581, ctc_loss=0.1557, over 967186.06 frames. ], batch size: 21, lr: 1.28e-02,
2024-10-08 12:15:19,321 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=12.15 vs. limit=13.2515
2024-10-08 12:15:20,059 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=15340.666666666666, ans=0.3630766666666667
2024-10-08 12:15:23,827 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=15340.666666666666, ans=0.125
2024-10-08 12:15:29,931 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.61 vs. limit=19.005499999999998
2024-10-08 12:15:42,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=15344.0, ans=0.025
2024-10-08 12:15:54,517 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=15347.333333333334, ans=0.007533188405797101
2024-10-08 12:16:02,409 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=15350.666666666666, ans=0.0027055555555555583
2024-10-08 12:16:14,082 INFO [train.py:1153] Epoch 7, batch 4950, loss[loss=0.3011, simple_loss=0.3071, pruned_loss=0.1044, ctc_loss=0.2157, over 4788.00 frames. ], tot_loss[loss=0.2475, simple_loss=0.2781, pruned_loss=0.07679, ctc_loss=0.1583, over 966791.12 frames. ], batch size: 53, lr: 1.28e-02,
2024-10-08 12:16:32,959 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=15357.333333333334, ans=0.04949747468305833
2024-10-08 12:16:33,002 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=15357.333333333334, ans=0.125
2024-10-08 12:16:35,727 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=15357.333333333334, ans=0.002677777777777779
2024-10-08 12:16:53,074 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=15364.0, ans=0.0
2024-10-08 12:17:04,797 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.329e+01 1.116e+02 1.302e+02 1.423e+02 1.924e+02, threshold=2.604e+02, percent-clipped=0.0
2024-10-08 12:17:20,887 INFO [train.py:1153] Epoch 7, batch 5000, loss[loss=0.2382, simple_loss=0.2836, pruned_loss=0.06602, ctc_loss=0.1519, over 4776.00 frames. ], tot_loss[loss=0.245, simple_loss=0.2767, pruned_loss=0.0755, ctc_loss=0.1556, over 967683.68 frames. ], batch size: 29, lr: 1.28e-02,
2024-10-08 12:17:21,102 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=15370.666666666666, ans=0.3620266666666667
2024-10-08 12:17:32,916 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=15374.0, ans=0.125
2024-10-08 12:17:37,615 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.86 vs. limit=8.8435
2024-10-08 12:17:46,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=15377.333333333334, ans=0.3617933333333334
2024-10-08 12:17:55,935 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=15377.333333333334, ans=0.002594444444444448
2024-10-08 12:18:21,247 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=15384.0, ans=0.0
2024-10-08 12:18:22,605 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=15384.0, ans=0.125
2024-10-08 12:18:27,921 INFO [train.py:1153] Epoch 7, batch 5050, loss[loss=0.2261, simple_loss=0.2604, pruned_loss=0.06733, ctc_loss=0.143, over 4852.00 frames. ], tot_loss[loss=0.2434, simple_loss=0.2753, pruned_loss=0.07488, ctc_loss=0.1544, over 968582.78 frames. ], batch size: 19, lr: 1.28e-02,
2024-10-08 12:18:38,821 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=15387.333333333334, ans=0.0
2024-10-08 12:19:09,159 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=15397.333333333334, ans=0.3610933333333334
2024-10-08 12:19:15,620 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=15397.333333333334, ans=0.00251111111111111
2024-10-08 12:19:18,187 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.572e+01 1.084e+02 1.236e+02 1.376e+02 2.554e+02, threshold=2.473e+02, percent-clipped=0.0
2024-10-08 12:19:21,005 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=15400.666666666666, ans=0.125
2024-10-08 12:19:34,247 INFO [train.py:1153] Epoch 7, batch 5100, loss[loss=0.2264, simple_loss=0.2763, pruned_loss=0.06232, ctc_loss=0.1299, over 4815.00 frames. ], tot_loss[loss=0.2456, simple_loss=0.2768, pruned_loss=0.07595, ctc_loss=0.1566, over 967896.40 frames. ], batch size: 19, lr: 1.28e-02,
2024-10-08 12:20:06,501 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=15410.666666666666, ans=0.14589333333333335
2024-10-08 12:20:14,468 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=15414.0, ans=0.007518695652173913
2024-10-08 12:20:19,390 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=8.47 vs. limit=8.8535
2024-10-08 12:20:27,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=15417.333333333334, ans=0.125
2024-10-08 12:20:27,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=15417.333333333334, ans=0.0024277777777777787
2024-10-08 12:20:40,637 INFO [train.py:1153] Epoch 7, batch 5150, loss[loss=0.2832, simple_loss=0.3015, pruned_loss=0.09183, ctc_loss=0.2031, over 4818.00 frames. ], tot_loss[loss=0.2457, simple_loss=0.277, pruned_loss=0.07587, ctc_loss=0.1564, over 968077.01 frames. ], batch size: 36, lr: 1.28e-02,
2024-10-08 12:21:30,997 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.112e+01 1.121e+02 1.250e+02 1.359e+02 1.744e+02, threshold=2.501e+02, percent-clipped=0.0
2024-10-08 12:21:46,812 INFO [train.py:1153] Epoch 7, batch 5200, loss[loss=0.2596, simple_loss=0.2823, pruned_loss=0.08196, ctc_loss=0.1826, over 4761.00 frames. ], tot_loss[loss=0.2463, simple_loss=0.2774, pruned_loss=0.07609, ctc_loss=0.1573, over 967691.24 frames. ], batch size: 29, lr: 1.28e-02,
2024-10-08 12:21:51,435 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=13.52 vs. limit=13.289
2024-10-08 12:21:53,563 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=15437.333333333334, ans=0.125
2024-10-08 12:21:56,785 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.23 vs. limit=13.289
2024-10-08 12:22:09,600 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=15440.666666666666, ans=0.14559333333333335
2024-10-08 12:22:15,109 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 12:22:16,317 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=15444.0, ans=0.125
2024-10-08 12:22:33,882 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.32 vs. limit=13.29275
2024-10-08 12:22:36,241 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=15447.333333333334, ans=0.14552666666666667
2024-10-08 12:22:44,634 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=20.51 vs. limit=19.088
2024-10-08 12:22:48,215 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.min_positive, batch_count=15450.666666666666, ans=0.025
2024-10-08 12:22:53,473 INFO [train.py:1153] Epoch 7, batch 5250, loss[loss=0.1986, simple_loss=0.2526, pruned_loss=0.05146, ctc_loss=0.1043, over 4853.00 frames. ], tot_loss[loss=0.2446, simple_loss=0.2765, pruned_loss=0.07517, ctc_loss=0.1556, over 967699.88 frames. ], batch size: 20, lr: 1.28e-02,
2024-10-08 12:23:18,781 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 12:23:21,371 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=15460.666666666666, ans=0.125
2024-10-08 12:23:22,659 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=15460.666666666666, ans=0.14539333333333335
2024-10-08 12:23:28,145 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=15460.666666666666, ans=0.002247222222222227
2024-10-08 12:23:39,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=15464.0, ans=0.0022333333333333302
2024-10-08 12:23:43,708 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.798e+01 1.096e+02 1.224e+02 1.377e+02 1.778e+02, threshold=2.447e+02, percent-clipped=0.0
2024-10-08 12:23:49,308 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=15467.333333333334, ans=0.07
2024-10-08 12:23:50,659 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=15467.333333333334, ans=0.07
2024-10-08 12:23:52,525 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=13.20 vs. limit=13.30025
2024-10-08 12:23:58,631 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=15470.666666666666, ans=0.002205555555555558
2024-10-08 12:23:59,749 INFO [train.py:1153] Epoch 7, batch 5300, loss[loss=0.2713, simple_loss=0.3046, pruned_loss=0.08472, ctc_loss=0.1714, over 4832.00 frames. ], tot_loss[loss=0.2446, simple_loss=0.2766, pruned_loss=0.07517, ctc_loss=0.1556, over 967877.32 frames. ], batch size: 38, lr: 1.28e-02,
2024-10-08 12:24:19,448 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.24 vs. limit=19.1055
2024-10-08 12:24:38,562 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=15480.666666666666, ans=0.007504202898550725
2024-10-08 12:24:52,478 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=3.82 vs. limit=10.1936
2024-10-08 12:24:59,769 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 12:25:06,318 INFO [train.py:1153] Epoch 7, batch 5350, loss[loss=0.2524, simple_loss=0.2746, pruned_loss=0.08318, ctc_loss=0.1595, over 4978.00 frames. ], tot_loss[loss=0.2459, simple_loss=0.2775, pruned_loss=0.07586, ctc_loss=0.1565, over 967297.66 frames. ], batch size: 19, lr: 1.28e-02,
2024-10-08 12:25:11,816 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=15487.333333333334, ans=0.025
2024-10-08 12:25:26,392 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=15490.666666666666, ans=0.43235999999999997
2024-10-08 12:25:28,885 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=15490.666666666666, ans=0.125
2024-10-08 12:25:31,494 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=15494.0, ans=0.025
2024-10-08 12:25:42,102 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=15494.0, ans=0.14506
2024-10-08 12:25:56,571 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.922e+01 1.159e+02 1.278e+02 1.402e+02 2.240e+02, threshold=2.557e+02, percent-clipped=0.0
2024-10-08 12:26:12,400 INFO [train.py:1153] Epoch 7, batch 5400, loss[loss=0.2911, simple_loss=0.3049, pruned_loss=0.09915, ctc_loss=0.1976, over 4774.00 frames. ], tot_loss[loss=0.2481, simple_loss=0.2787, pruned_loss=0.07696, ctc_loss=0.159, over 966591.12 frames. ], batch size: 49, lr: 1.27e-02,
2024-10-08 12:26:12,658 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=15504.0, ans=0.125
2024-10-08 12:26:18,020 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=15504.0, ans=0.125
2024-10-08 12:26:21,909 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=15504.0, ans=0.025
2024-10-08 12:26:38,354 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.02 vs. limit=5.3266
2024-10-08 12:26:41,922 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=15510.666666666666, ans=0.3571266666666667
2024-10-08 12:26:43,189 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=15510.666666666666, ans=0.3571266666666667
2024-10-08 12:26:44,515 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=15510.666666666666, ans=0.09899494936611666
2024-10-08 12:26:53,773 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=15514.0, ans=0.025
2024-10-08 12:27:00,453 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=15514.0, ans=0.14486
2024-10-08 12:27:09,700 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=15517.333333333334, ans=0.007496231884057971
2024-10-08 12:27:18,820 INFO [train.py:1153] Epoch 7, batch 5450, loss[loss=0.227, simple_loss=0.2571, pruned_loss=0.06964, ctc_loss=0.1441, over 4940.00 frames. ], tot_loss[loss=0.2447, simple_loss=0.2769, pruned_loss=0.07517, ctc_loss=0.1553, over 967305.05 frames. ], batch size: 19, lr: 1.27e-02,
2024-10-08 12:27:26,148 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.87 vs. limit=10.208266666666667
2024-10-08 12:27:31,139 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=4.002e-02
2024-10-08 12:27:31,798 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=5.08 vs. limit=13.3215
2024-10-08 12:27:46,938 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=15527.333333333334, ans=0.14472666666666664
2024-10-08 12:27:53,652 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 12:28:09,359 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.673e+01 1.081e+02 1.235e+02 1.437e+02 2.768e+02, threshold=2.469e+02, percent-clipped=1.0
2024-10-08 12:28:25,441 INFO [train.py:1153] Epoch 7, batch 5500, loss[loss=0.3259, simple_loss=0.3186, pruned_loss=0.1175, ctc_loss=0.246, over 4774.00 frames. ], tot_loss[loss=0.2462, simple_loss=0.2777, pruned_loss=0.07601, ctc_loss=0.1567, over 967629.72 frames. ], batch size: 49, lr: 1.27e-02,
2024-10-08 12:28:28,188 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=15537.333333333334, ans=0.125
2024-10-08 12:28:42,837 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=15540.666666666666, ans=0.125
2024-10-08 12:28:57,509 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 12:29:06,652 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=15547.333333333334, ans=0.125
2024-10-08 12:29:31,623 INFO [train.py:1153] Epoch 7, batch 5550, loss[loss=0.21, simple_loss=0.2564, pruned_loss=0.05785, ctc_loss=0.1197, over 4798.00 frames. ], tot_loss[loss=0.2443, simple_loss=0.2766, pruned_loss=0.07503, ctc_loss=0.1548, over 967294.41 frames. ], batch size: 19, lr: 1.27e-02,
2024-10-08 12:29:47,683 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=15557.333333333334, ans=0.35549333333333333
2024-10-08 12:29:48,324 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.88 vs. limit=12.778666666666666
2024-10-08 12:29:49,076 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=15557.333333333334, ans=0.007487536231884058
2024-10-08 12:29:56,089 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.26 vs. limit=5.333600000000001
2024-10-08 12:30:12,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=15564.0, ans=0.025
2024-10-08 12:30:21,886 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.933e+01 1.098e+02 1.236e+02 1.381e+02 1.984e+02, threshold=2.472e+02, percent-clipped=0.0
2024-10-08 12:30:37,793 INFO [train.py:1153] Epoch 7, batch 5600, loss[loss=0.2232, simple_loss=0.2619, pruned_loss=0.06281, ctc_loss=0.1473, over 4853.00 frames. ], tot_loss[loss=0.2461, simple_loss=0.2779, pruned_loss=0.07594, ctc_loss=0.1564, over 967221.20 frames. ], batch size: 28, lr: 1.27e-02,
2024-10-08 12:30:56,677 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=15574.0, ans=0.0017749999999999988
2024-10-08 12:31:44,425 INFO [train.py:1153] Epoch 7, batch 5650, loss[loss=0.2661, simple_loss=0.2855, pruned_loss=0.09035, ctc_loss=0.1652, over 4752.00 frames. ], tot_loss[loss=0.2447, simple_loss=0.2765, pruned_loss=0.07544, ctc_loss=0.155, over 967230.91 frames. ], batch size: 45, lr: 1.27e-02,
2024-10-08 12:32:04,574 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=12.80 vs. limit=13.346499999999999
2024-10-08 12:32:07,410 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.00 vs. limit=13.346499999999999
2024-10-08 12:32:10,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=15594.0, ans=0.14406
2024-10-08 12:32:34,538 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.870e+01 1.078e+02 1.187e+02 1.307e+02 1.777e+02, threshold=2.374e+02, percent-clipped=0.0
2024-10-08 12:32:50,479 INFO [train.py:1153] Epoch 7, batch 5700, loss[loss=0.2563, simple_loss=0.285, pruned_loss=0.07735, ctc_loss=0.1824, over 4879.00 frames. ], tot_loss[loss=0.245, simple_loss=0.2765, pruned_loss=0.07556, ctc_loss=0.1557, over 966507.79 frames. ], batch size: 22, lr: 1.27e-02,
2024-10-08 12:33:07,788 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=15607.333333333334, ans=0.3537433333333334
2024-10-08 12:33:25,166 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=15610.666666666666, ans=0.0016222222222222263
2024-10-08 12:33:56,847 INFO [train.py:1153] Epoch 7, batch 5750, loss[loss=0.2842, simple_loss=0.3003, pruned_loss=0.09616, ctc_loss=0.1897, over 4855.00 frames. ], tot_loss[loss=0.2448, simple_loss=0.2766, pruned_loss=0.07545, ctc_loss=0.1554, over 966799.56 frames. ], batch size: 43, lr: 1.27e-02,
2024-10-08 12:34:10,262 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=15624.0, ans=0.0015666666666666676
2024-10-08 12:34:15,708 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=15624.0, ans=0.00747304347826087
2024-10-08 12:34:25,280 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=17.64 vs. limit=13.36025
2024-10-08 12:34:30,289 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer1.prob, batch_count=15627.333333333334, ans=0.125
2024-10-08 12:34:31,572 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=15627.333333333334, ans=0.125
2024-10-08 12:34:47,345 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.355e+01 1.086e+02 1.243e+02 1.412e+02 1.757e+02, threshold=2.487e+02, percent-clipped=0.0
2024-10-08 12:35:03,276 INFO [train.py:1153] Epoch 7, batch 5800, loss[loss=0.3159, simple_loss=0.3239, pruned_loss=0.1108, ctc_loss=0.2157, over 4843.00 frames. ], tot_loss[loss=0.2466, simple_loss=0.2777, pruned_loss=0.07635, ctc_loss=0.1572, over 966092.22 frames. ], batch size: 43, lr: 1.27e-02,
2024-10-08 12:35:03,443 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=15637.333333333334, ans=0.3526933333333334
2024-10-08 12:35:03,453 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=15637.333333333334, ans=0.0015111111111111158
2024-10-08 12:35:31,440 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=15644.0, ans=0.125
2024-10-08 12:36:09,813 INFO [train.py:1153] Epoch 7, batch 5850, loss[loss=0.2689, simple_loss=0.2856, pruned_loss=0.08858, ctc_loss=0.1876, over 4742.00 frames. ], tot_loss[loss=0.2455, simple_loss=0.2771, pruned_loss=0.07576, ctc_loss=0.156, over 966503.86 frames. ], batch size: 45, lr: 1.27e-02,
2024-10-08 12:36:11,347 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=15654.0, ans=0.14346
2024-10-08 12:36:12,634 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=15654.0, ans=0.125
2024-10-08 12:36:27,461 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=15657.333333333334, ans=0.025
2024-10-08 12:36:43,414 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=15660.666666666666, ans=0.001413888888888895
2024-10-08 12:37:00,671 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.716e+01 1.053e+02 1.185e+02 1.364e+02 1.909e+02, threshold=2.371e+02, percent-clipped=0.0
2024-10-08 12:37:16,588 INFO [train.py:1153] Epoch 7, batch 5900, loss[loss=0.2782, simple_loss=0.2926, pruned_loss=0.09441, ctc_loss=0.1876, over 4829.00 frames. ], tot_loss[loss=0.2451, simple_loss=0.2769, pruned_loss=0.07553, ctc_loss=0.1555, over 966628.57 frames. ], batch size: 34, lr: 1.27e-02,
2024-10-08 12:37:47,286 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=15677.333333333334, ans=0.125
2024-10-08 12:37:48,612 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=15677.333333333334, ans=0.125
2024-10-08 12:37:49,049 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.96 vs. limit=5.3515999999999995
2024-10-08 12:38:22,732 INFO [train.py:1153] Epoch 7, batch 5950, loss[loss=0.2406, simple_loss=0.2701, pruned_loss=0.07485, ctc_loss=0.1532, over 4799.00 frames. ], tot_loss[loss=0.2445, simple_loss=0.2765, pruned_loss=0.07534, ctc_loss=0.1547, over 966048.71 frames. ], batch size: 34, lr: 1.27e-02,
2024-10-08 12:38:25,653 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=15687.333333333334, ans=0.14312666666666665
2024-10-08 12:38:28,303 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=15687.333333333334, ans=0.007459275362318841
2024-10-08 12:38:45,110 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.25 vs. limit=8.922666666666666
2024-10-08 12:39:06,248 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1.whitening_limit, batch_count=15697.333333333334, ans=8.924333333333333
2024-10-08 12:39:13,692 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.114e+01 1.081e+02 1.221e+02 1.401e+02 1.840e+02, threshold=2.442e+02, percent-clipped=0.0
2024-10-08 12:39:15,221 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=15700.666666666666, ans=0.9070066666666666
2024-10-08 12:39:29,722 INFO [train.py:1153] Epoch 7, batch 6000, loss[loss=0.2536, simple_loss=0.2773, pruned_loss=0.08184, ctc_loss=0.1657, over 4789.00 frames. ], tot_loss[loss=0.2441, simple_loss=0.2762, pruned_loss=0.07511, ctc_loss=0.1542, over 966596.62 frames. ], batch size: 49, lr: 1.27e-02,
2024-10-08 12:39:29,723 INFO [train.py:1176] Computing validation loss
2024-10-08 12:39:37,600 INFO [train.py:1185] Epoch 7, validation: loss=0.1655, simple_loss=0.2507, pruned_loss=0.02887, ctc_loss=0.05659, over 90464.00 frames.
2024-10-08 12:39:37,601 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 12:39:44,416 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=15704.0, ans=0.125
2024-10-08 12:39:48,875 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=14.50 vs. limit=13.389
2024-10-08 12:39:53,827 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=15707.333333333334, ans=0.14292666666666665
2024-10-08 12:40:23,481 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=15714.0, ans=0.125
2024-10-08 12:40:30,246 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=15717.333333333334, ans=0.14282666666666666
2024-10-08 12:40:36,912 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=15717.333333333334, ans=0.0
2024-10-08 12:40:44,813 INFO [train.py:1153] Epoch 7, batch 6050, loss[loss=0.1908, simple_loss=0.2534, pruned_loss=0.04273, ctc_loss=0.1068, over 4815.00 frames. ], tot_loss[loss=0.2417, simple_loss=0.275, pruned_loss=0.07378, ctc_loss=0.1518, over 966539.90 frames. ], batch size: 19, lr: 1.27e-02,
2024-10-08 12:41:12,279 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=18.48 vs. limit=19.2955
2024-10-08 12:41:25,004 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=15730.666666666666, ans=0.125
2024-10-08 12:41:30,402 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.prob, batch_count=15730.666666666666, ans=0.125
2024-10-08 12:41:32,914 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=15730.666666666666, ans=0.125
2024-10-08 12:41:35,548 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.310e+01 1.060e+02 1.163e+02 1.367e+02 1.838e+02, threshold=2.326e+02, percent-clipped=0.0
2024-10-08 12:41:38,394 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=15734.0, ans=0.0011083333333333292
2024-10-08 12:41:47,666 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=15734.0, ans=0.125
2024-10-08 12:41:50,955 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.44 vs. limit=12.868666666666666
2024-10-08 12:41:51,626 INFO [train.py:1153] Epoch 7, batch 6100, loss[loss=0.2628, simple_loss=0.2912, pruned_loss=0.08043, ctc_loss=0.1837, over 4834.00 frames. ], tot_loss[loss=0.2414, simple_loss=0.2747, pruned_loss=0.07379, ctc_loss=0.1516, over 966241.16 frames. ], batch size: 34, lr: 1.27e-02,
2024-10-08 12:41:54,048 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=4.28 vs. limit=5.0
2024-10-08 12:41:59,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=15737.333333333334, ans=0.125
2024-10-08 12:42:02,794 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.67 vs. limit=5.3606
2024-10-08 12:42:03,747 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=15740.666666666666, ans=0.3490766666666667
2024-10-08 12:42:23,217 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.16 vs. limit=12.872
2024-10-08 12:42:57,044 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=15754.0, ans=0.125
2024-10-08 12:42:58,210 INFO [train.py:1153] Epoch 7, batch 6150, loss[loss=0.2839, simple_loss=0.3019, pruned_loss=0.09881, ctc_loss=0.1708, over 4817.00 frames. ], tot_loss[loss=0.2409, simple_loss=0.2746, pruned_loss=0.0735, ctc_loss=0.1508, over 966430.55 frames. ], batch size: 43, lr: 1.26e-02,
2024-10-08 12:42:58,386 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=15754.0, ans=0.14246
2024-10-08 12:43:07,989 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=15754.0, ans=0.125
2024-10-08 12:43:20,001 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=15757.333333333334, ans=0.0010111111111111154
2024-10-08 12:43:23,077 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=15757.333333333334, ans=0.0010111111111111154
2024-10-08 12:43:33,403 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=15760.666666666666, ans=0.0009972222222222257
2024-10-08 12:43:34,724 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=15760.666666666666, ans=0.125
2024-10-08 12:43:49,063 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.631e+01 1.061e+02 1.232e+02 1.364e+02 2.667e+02, threshold=2.465e+02, percent-clipped=1.0
2024-10-08 12:43:56,056 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=15767.333333333334, ans=0.34814333333333336
2024-10-08 12:44:05,121 INFO [train.py:1153] Epoch 7, batch 6200, loss[loss=0.3112, simple_loss=0.3197, pruned_loss=0.1049, ctc_loss=0.2321, over 4795.00 frames. ], tot_loss[loss=0.2407, simple_loss=0.2746, pruned_loss=0.07339, ctc_loss=0.1503, over 966598.93 frames. ], batch size: 29, lr: 1.26e-02,
2024-10-08 12:44:11,910 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=15770.666666666666, ans=0.125
2024-10-08 12:44:15,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=15770.666666666666, ans=0.14229333333333335
2024-10-08 12:44:18,559 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=15774.0, ans=0.125
2024-10-08 12:45:01,213 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=15784.0, ans=0.025
2024-10-08 12:45:01,273 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=15784.0, ans=0.3475600000000001
2024-10-08 12:45:11,745 INFO [train.py:1153] Epoch 7, batch 6250, loss[loss=0.2262, simple_loss=0.2697, pruned_loss=0.06344, ctc_loss=0.1397, over 4743.00 frames. ], tot_loss[loss=0.2396, simple_loss=0.2738, pruned_loss=0.07277, ctc_loss=0.1499, over 966877.53 frames. ], batch size: 26, lr: 1.26e-02,
2024-10-08 12:45:14,167 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=10.38 vs. limit=13.42025
2024-10-08 12:45:28,066 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=15790.666666666666, ans=0.125
2024-10-08 12:45:31,001 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.15 vs. limit=19.343
2024-10-08 12:45:31,833 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=15790.666666666666, ans=0.0
2024-10-08 12:45:38,622 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 12:45:47,156 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.24 vs. limit=13.42275
2024-10-08 12:45:48,021 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=15794.0, ans=0.14206
2024-10-08 12:46:00,423 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.09 vs. limit=13.424
2024-10-08 12:46:02,367 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.750e+01 1.139e+02 1.310e+02 1.479e+02 2.221e+02, threshold=2.620e+02, percent-clipped=0.0
2024-10-08 12:46:05,175 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=15800.666666666666, ans=0.07
2024-10-08 12:46:14,612 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 12:46:18,316 INFO [train.py:1153] Epoch 7, batch 6300, loss[loss=0.2382, simple_loss=0.2637, pruned_loss=0.07533, ctc_loss=0.1552, over 4978.00 frames. ], tot_loss[loss=0.2409, simple_loss=0.2748, pruned_loss=0.07337, ctc_loss=0.1507, over 966655.28 frames. ], batch size: 19, lr: 1.26e-02,
2024-10-08 12:46:22,472 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=15804.0, ans=0.125
2024-10-08 12:46:38,430 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=15807.333333333334, ans=0.125
2024-10-08 12:46:42,691 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.32 vs. limit=5.3711
2024-10-08 12:46:47,855 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=15810.666666666666, ans=0.0007888888888888945
2024-10-08 12:47:03,180 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=10.70 vs. limit=13.430250000000001
2024-10-08 12:47:09,002 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=15814.0, ans=0.3465100000000001
2024-10-08 12:47:14,306 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=15817.333333333334, ans=0.34639333333333333
2024-10-08 12:47:16,996 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=15817.333333333334, ans=0.125
2024-10-08 12:47:24,989 INFO [train.py:1153] Epoch 7, batch 6350, loss[loss=0.2534, simple_loss=0.286, pruned_loss=0.07735, ctc_loss=0.1653, over 4833.00 frames. ], tot_loss[loss=0.2401, simple_loss=0.2745, pruned_loss=0.0729, ctc_loss=0.1498, over 966184.18 frames. ], batch size: 36, lr: 1.26e-02,
2024-10-08 12:47:25,155 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=15820.666666666666, ans=0.3462766666666668
2024-10-08 12:47:36,230 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=14.34 vs. limit=13.43275
2024-10-08 12:47:42,445 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=15824.0, ans=0.0007333333333333289
2024-10-08 12:48:15,649 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.699e+01 1.081e+02 1.238e+02 1.363e+02 1.891e+02, threshold=2.476e+02, percent-clipped=0.0
2024-10-08 12:48:23,834 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=15834.0, ans=0.14166
2024-10-08 12:48:31,700 INFO [train.py:1153] Epoch 7, batch 6400, loss[loss=0.2396, simple_loss=0.2799, pruned_loss=0.07061, ctc_loss=0.1453, over 4856.00 frames. ], tot_loss[loss=0.2381, simple_loss=0.2733, pruned_loss=0.07194, ctc_loss=0.1475, over 965898.25 frames. ], batch size: 23, lr: 1.26e-02,
2024-10-08 12:48:32,365 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=19.14 vs. limit=19.378
2024-10-08 12:48:34,607 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=15837.333333333334, ans=0.09162666666666663
2024-10-08 12:49:06,657 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=15844.0, ans=0.05
2024-10-08 12:49:13,380 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=15847.333333333334, ans=0.34534333333333334
2024-10-08 12:49:27,968 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=15850.666666666666, ans=0.125
2024-10-08 12:49:29,272 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=15850.666666666666, ans=0.43776000000000004
2024-10-08 12:49:30,681 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=15850.666666666666, ans=0.0006222222222222254
2024-10-08 12:49:38,459 INFO [train.py:1153] Epoch 7, batch 6450, loss[loss=0.2469, simple_loss=0.2937, pruned_loss=0.06974, ctc_loss=0.1516, over 4719.00 frames. ], tot_loss[loss=0.2387, simple_loss=0.2736, pruned_loss=0.07234, ctc_loss=0.148, over 965312.45 frames. ], batch size: 26, lr: 1.26e-02,
2024-10-08 12:49:48,040 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=15854.0, ans=0.125
2024-10-08 12:50:00,789 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.70 vs. limit=13.4465
2024-10-08 12:50:04,133 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=15860.666666666666, ans=0.025
2024-10-08 12:50:07,374 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=7.00 vs. limit=13.44775
2024-10-08 12:50:16,155 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=15860.666666666666, ans=0.125
2024-10-08 12:50:29,376 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.572e+01 1.077e+02 1.182e+02 1.298e+02 1.644e+02, threshold=2.365e+02, percent-clipped=0.0
2024-10-08 12:50:45,532 INFO [train.py:1153] Epoch 7, batch 6500, loss[loss=0.2185, simple_loss=0.2598, pruned_loss=0.06163, ctc_loss=0.1346, over 4716.00 frames. ], tot_loss[loss=0.237, simple_loss=0.2723, pruned_loss=0.07153, ctc_loss=0.1468, over 964860.21 frames. ], batch size: 26, lr: 1.26e-02,
2024-10-08 12:50:51,010 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=15870.666666666666, ans=0.125
2024-10-08 12:50:55,231 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=15870.666666666666, ans=0.125
2024-10-08 12:50:55,249 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=15870.666666666666, ans=0.0
2024-10-08 12:50:56,508 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=15870.666666666666, ans=0.09899494936611666
2024-10-08 12:50:59,138 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=15874.0, ans=0.3444100000000001
2024-10-08 12:51:01,869 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=15874.0, ans=0.007418695652173913
2024-10-08 12:51:05,814 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=15874.0, ans=0.14126
2024-10-08 12:51:12,409 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=15877.333333333334, ans=0.025
2024-10-08 12:51:52,482 INFO [train.py:1153] Epoch 7, batch 6550, loss[loss=0.2062, simple_loss=0.2498, pruned_loss=0.0579, ctc_loss=0.1169, over 4978.00 frames. ], tot_loss[loss=0.2361, simple_loss=0.2718, pruned_loss=0.07109, ctc_loss=0.1457, over 964517.53 frames. ], batch size: 19, lr: 1.26e-02,
2024-10-08 12:51:56,636 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=15887.333333333334, ans=0.00046944444444444594
2024-10-08 12:51:58,001 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=15887.333333333334, ans=0.125
2024-10-08 12:52:14,090 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=15890.666666666666, ans=0.025
2024-10-08 12:52:16,636 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=15890.666666666666, ans=0.0004555555555555632
2024-10-08 12:52:28,802 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=15894.0, ans=0.125
2024-10-08 12:52:43,277 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.028e+01 1.091e+02 1.229e+02 1.386e+02 1.986e+02, threshold=2.457e+02, percent-clipped=0.0
2024-10-08 12:52:50,921 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.10 vs. limit=5.3850999999999996
2024-10-08 12:52:54,977 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.46 vs. limit=5.3850999999999996
2024-10-08 12:52:55,100 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=8.50 vs. limit=8.975166666666667
2024-10-08 12:52:58,259 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=15904.0, ans=0.125
2024-10-08 12:52:59,435 INFO [train.py:1153] Epoch 7, batch 6600, loss[loss=0.2386, simple_loss=0.2714, pruned_loss=0.07148, ctc_loss=0.1567, over 4869.00 frames. ], tot_loss[loss=0.2358, simple_loss=0.2716, pruned_loss=0.07089, ctc_loss=0.1454, over 965190.95 frames. ], batch size: 23, lr: 1.26e-02,
2024-10-08 12:53:09,133 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.max_abs, batch_count=15904.0, ans=10.0
2024-10-08 12:53:15,017 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.39 vs. limit=10.362933333333334
2024-10-08 12:53:36,241 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=15910.666666666666, ans=0.0
2024-10-08 12:53:46,025 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=15914.0, ans=0.00741
2024-10-08 12:53:59,570 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=15917.333333333334, ans=0.125
2024-10-08 12:54:07,585 INFO [train.py:1153] Epoch 7, batch 6650, loss[loss=0.2344, simple_loss=0.2729, pruned_loss=0.06974, ctc_loss=0.1409, over 4746.00 frames. ], tot_loss[loss=0.234, simple_loss=0.27, pruned_loss=0.07008, ctc_loss=0.1443, over 966944.52 frames. ], batch size: 20, lr: 1.26e-02,
2024-10-08 12:54:10,533 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=15920.666666666666, ans=0.007408550724637681
2024-10-08 12:54:13,182 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=15920.666666666666, ans=0.125
2024-10-08 12:54:20,259 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=5.79 vs. limit=10.3696
2024-10-08 12:54:27,368 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.87 vs. limit=19.442999999999998
2024-10-08 12:54:53,002 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=18.53 vs. limit=19.448
2024-10-08 12:54:59,248 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.017e+01 1.087e+02 1.181e+02 1.334e+02 2.482e+02, threshold=2.363e+02, percent-clipped=1.0
2024-10-08 12:55:13,144 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=15934.0, ans=0.34231
2024-10-08 12:55:15,724 INFO [train.py:1153] Epoch 7, batch 6700, loss[loss=0.2715, simple_loss=0.3079, pruned_loss=0.0836, ctc_loss=0.1698, over 4927.00 frames. ], tot_loss[loss=0.2332, simple_loss=0.2697, pruned_loss=0.06967, ctc_loss=0.1433, over 969091.47 frames. ], batch size: 20, lr: 1.26e-02,
2024-10-08 12:55:22,692 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=15937.333333333334, ans=0.0
2024-10-08 12:55:23,612 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.02 vs. limit=5.3906
2024-10-08 12:55:47,584 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=15944.0, ans=0.025
2024-10-08 12:55:57,144 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=15947.333333333334, ans=0.14052666666666666
2024-10-08 12:56:24,469 INFO [train.py:1153] Epoch 7, batch 6750, loss[loss=0.2747, simple_loss=0.2999, pruned_loss=0.0908, ctc_loss=0.1695, over 4911.00 frames. ], tot_loss[loss=0.2298, simple_loss=0.2672, pruned_loss=0.06816, ctc_loss=0.1401, over 972205.31 frames. ], batch size: 19, lr: 1.26e-02,
2024-10-08 12:56:27,372 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=15954.0, ans=0.125
2024-10-08 12:56:50,821 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=15960.666666666666, ans=0.0
2024-10-08 12:57:01,958 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=15960.666666666666, ans=0.025
2024-10-08 12:57:04,643 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=15964.0, ans=0.14036
2024-10-08 12:57:05,963 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=15964.0, ans=0.125
2024-10-08 12:57:10,186 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=15964.0, ans=0.125
2024-10-08 12:57:16,065 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.39 vs. limit=13.4865
2024-10-08 12:57:16,805 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.076e+01 1.080e+02 1.195e+02 1.315e+02 2.226e+02, threshold=2.390e+02, percent-clipped=0.0
2024-10-08 12:57:33,319 INFO [train.py:1153] Epoch 7, batch 6800, loss[loss=0.2454, simple_loss=0.2727, pruned_loss=0.07917, ctc_loss=0.1491, over 4911.00 frames. ], tot_loss[loss=0.2287, simple_loss=0.2662, pruned_loss=0.06777, ctc_loss=0.139, over 974514.17 frames. ], batch size: 19, lr: 1.26e-02,
2024-10-08 12:57:34,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=15970.666666666666, ans=0.125
2024-10-08 12:57:39,007 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=15970.666666666666, ans=0.0
2024-10-08 12:57:40,367 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=15970.666666666666, ans=0.0
2024-10-08 12:57:44,956 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.72 vs. limit=10.388266666666667
2024-10-08 12:58:21,963 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=15980.666666666666, ans=0.125
2024-10-08 12:58:39,848 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=15984.0, ans=0.125
2024-10-08 12:58:42,493 INFO [train.py:1153] Epoch 7, batch 6850, loss[loss=0.1942, simple_loss=0.2363, pruned_loss=0.05392, ctc_loss=0.1104, over 4978.00 frames. ], tot_loss[loss=0.2253, simple_loss=0.2631, pruned_loss=0.06651, ctc_loss=0.136, over 978874.19 frames. ], batch size: 19, lr: 1.26e-02,
2024-10-08 12:58:44,003 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/epoch-7.pt
2024-10-08 12:59:11,882 INFO [train.py:1153] Epoch 8, batch 0, loss[loss=0.2153, simple_loss=0.2539, pruned_loss=0.06161, ctc_loss=0.1339, over 4852.00 frames. ], tot_loss[loss=0.2153, simple_loss=0.2539, pruned_loss=0.06161, ctc_loss=0.1339, over 4852.00 frames. ], batch size: 19, lr: 1.18e-02,
2024-10-08 12:59:11,882 INFO [train.py:1176] Computing validation loss
2024-10-08 12:59:18,000 INFO [train.py:1185] Epoch 8, validation: loss=0.1693, simple_loss=0.2513, pruned_loss=0.03193, ctc_loss=0.0588, over 90464.00 frames.
2024-10-08 12:59:18,001 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 12:59:39,784 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=20.40 vs. limit=19.493499999999997
2024-10-08 13:00:01,526 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/checkpoint-48000.pt
2024-10-08 13:00:03,466 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.271e+01 9.704e+01 1.144e+02 1.345e+02 2.193e+02, threshold=2.288e+02, percent-clipped=0.0
2024-10-08 13:00:06,298 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.95 vs. limit=13.49925
2024-10-08 13:00:09,451 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=16001.333333333334, ans=0.13998666666666668
2024-10-08 13:00:20,868 INFO [train.py:1153] Epoch 8, batch 50, loss[loss=0.2653, simple_loss=0.291, pruned_loss=0.08623, ctc_loss=0.1675, over 4911.00 frames. ], tot_loss[loss=0.2543, simple_loss=0.2834, pruned_loss=0.07986, ctc_loss=0.1635, over 217748.21 frames. ], batch size: 19, lr: 1.18e-02,
2024-10-08 13:01:22,463 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.58 vs. limit=13.50675
2024-10-08 13:01:25,945 INFO [train.py:1153] Epoch 8, batch 100, loss[loss=0.2129, simple_loss=0.2649, pruned_loss=0.05619, ctc_loss=0.1209, over 4755.00 frames. ], tot_loss[loss=0.2448, simple_loss=0.2769, pruned_loss=0.07527, ctc_loss=0.1555, over 383344.98 frames. ], batch size: 19, lr: 1.18e-02,
2024-10-08 13:01:31,942 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.51 vs. limit=13.010666666666667
2024-10-08 13:01:35,735 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.24 vs. limit=13.508
2024-10-08 13:01:49,575 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=16024.666666666666, ans=0.09899494936611666
2024-10-08 13:01:52,076 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=16028.0, ans=0.13971999999999998
2024-10-08 13:01:56,804 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.51 vs. limit=19.521
2024-10-08 13:02:06,113 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.21 vs. limit=13.51175
2024-10-08 13:02:06,902 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=16031.333333333334, ans=0.125
2024-10-08 13:02:13,257 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.901e+01 1.073e+02 1.246e+02 1.360e+02 1.803e+02, threshold=2.492e+02, percent-clipped=0.0
2024-10-08 13:02:13,495 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=16031.333333333334, ans=0.125
2024-10-08 13:02:14,850 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=16031.333333333334, ans=0.125
2024-10-08 13:02:17,406 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=16034.666666666666, ans=0.025
2024-10-08 13:02:19,118 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.43 vs. limit=13.017333333333333
2024-10-08 13:02:22,597 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=16034.666666666666, ans=0.125
2024-10-08 13:02:22,716 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=16034.666666666666, ans=0.3387866666666667
2024-10-08 13:02:31,643 INFO [train.py:1153] Epoch 8, batch 150, loss[loss=0.1706, simple_loss=0.2238, pruned_loss=0.04046, ctc_loss=0.09114, over 4909.00 frames. ], tot_loss[loss=0.2399, simple_loss=0.2734, pruned_loss=0.07297, ctc_loss=0.1514, over 513352.13 frames. ], batch size: 19, lr: 1.18e-02,
2024-10-08 13:02:35,722 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=16038.0, ans=0.13962
2024-10-08 13:02:44,880 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=16041.333333333334, ans=0.3385533333333334
2024-10-08 13:02:54,143 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=16041.333333333334, ans=0.025
2024-10-08 13:02:55,348 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=16041.333333333334, ans=0.125
2024-10-08 13:03:08,416 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=16044.666666666666, ans=0.0
2024-10-08 13:03:22,809 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=16051.333333333334, ans=0.007380144927536232
2024-10-08 13:03:37,099 INFO [train.py:1153] Epoch 8, batch 200, loss[loss=0.259, simple_loss=0.284, pruned_loss=0.08404, ctc_loss=0.1647, over 4748.00 frames. ], tot_loss[loss=0.2388, simple_loss=0.2723, pruned_loss=0.07261, ctc_loss=0.1501, over 613762.55 frames. ], batch size: 45, lr: 1.18e-02,
2024-10-08 13:03:47,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=16054.666666666666, ans=0.125
2024-10-08 13:03:49,085 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=16058.0, ans=0.125
2024-10-08 13:03:54,391 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=16058.0, ans=0.025
2024-10-08 13:03:57,819 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=3.64 vs. limit=13.52175
2024-10-08 13:04:05,055 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=16061.333333333334, ans=0.0
2024-10-08 13:04:16,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=16064.666666666666, ans=0.13935333333333333
2024-10-08 13:04:24,769 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.114e+01 1.059e+02 1.184e+02 1.303e+02 1.969e+02, threshold=2.367e+02, percent-clipped=0.0
2024-10-08 13:04:43,158 INFO [train.py:1153] Epoch 8, batch 250, loss[loss=0.2908, simple_loss=0.2954, pruned_loss=0.1028, ctc_loss=0.2015, over 4843.00 frames. ], tot_loss[loss=0.2387, simple_loss=0.2722, pruned_loss=0.0726, ctc_loss=0.1503, over 692496.29 frames. ], batch size: 38, lr: 1.18e-02,
2024-10-08 13:04:49,817 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=16071.333333333334, ans=0.007375797101449276
2024-10-08 13:04:56,064 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.46 vs. limit=9.018666666666666
2024-10-08 13:05:00,882 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=16074.666666666666, ans=0.9107466666666666
2024-10-08 13:05:43,885 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=16084.666666666666, ans=0.125
2024-10-08 13:05:48,981 INFO [train.py:1153] Epoch 8, batch 300, loss[loss=0.2729, simple_loss=0.2908, pruned_loss=0.09072, ctc_loss=0.1841, over 4733.00 frames. ], tot_loss[loss=0.2375, simple_loss=0.2712, pruned_loss=0.07199, ctc_loss=0.1495, over 752751.90 frames. ], batch size: 32, lr: 1.18e-02,
2024-10-08 13:05:55,314 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=4.35 vs. limit=5.0
2024-10-08 13:05:59,825 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.whiten.whitening_limit, batch_count=16088.0, ans=10.4352
2024-10-08 13:06:03,492 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=16091.333333333334, ans=0.051155166666666696
2024-10-08 13:06:08,928 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=16091.333333333334, ans=0.007371449275362318
2024-10-08 13:06:36,357 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.153e+01 1.102e+02 1.273e+02 1.415e+02 1.809e+02, threshold=2.545e+02, percent-clipped=0.0
2024-10-08 13:06:38,072 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=9.251e-02
2024-10-08 13:06:54,790 INFO [train.py:1153] Epoch 8, batch 350, loss[loss=0.1852, simple_loss=0.2323, pruned_loss=0.04949, ctc_loss=0.09755, over 4883.00 frames. ], tot_loss[loss=0.2352, simple_loss=0.27, pruned_loss=0.07077, ctc_loss=0.147, over 800191.69 frames. ], batch size: 19, lr: 1.18e-02,
2024-10-08 13:07:10,700 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=16108.0, ans=0.125
2024-10-08 13:07:10,723 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=16108.0, ans=0.0
2024-10-08 13:07:46,119 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=16118.0, ans=0.125
2024-10-08 13:07:46,526 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.52 vs. limit=9.029499999999999
2024-10-08 13:08:00,523 INFO [train.py:1153] Epoch 8, batch 400, loss[loss=0.1539, simple_loss=0.2137, pruned_loss=0.03203, ctc_loss=0.07517, over 4852.00 frames. ], tot_loss[loss=0.2355, simple_loss=0.2704, pruned_loss=0.07087, ctc_loss=0.1474, over 836842.35 frames. ], batch size: 22, lr: 1.18e-02,
2024-10-08 13:08:19,635 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=5.40 vs. limit=13.54675
2024-10-08 13:08:41,975 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=2.51 vs. limit=13.54925
2024-10-08 13:08:47,872 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.055e+01 1.093e+02 1.217e+02 1.380e+02 1.725e+02, threshold=2.433e+02, percent-clipped=0.0
2024-10-08 13:08:59,765 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=16134.666666666666, ans=0.0
2024-10-08 13:09:03,847 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=16134.666666666666, ans=0.125
2024-10-08 13:09:06,275 INFO [train.py:1153] Epoch 8, batch 450, loss[loss=0.2156, simple_loss=0.2574, pruned_loss=0.0603, ctc_loss=0.1331, over 4850.00 frames. ], tot_loss[loss=0.237, simple_loss=0.2712, pruned_loss=0.07165, ctc_loss=0.1488, over 865407.73 frames. ], batch size: 23, lr: 1.18e-02,
2024-10-08 13:09:07,799 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=16138.0, ans=0.125
2024-10-08 13:09:26,300 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=16141.333333333334, ans=0.3350533333333334
2024-10-08 13:09:50,196 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=16148.0, ans=0.125
2024-10-08 13:09:55,425 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=16148.0, ans=0.007359130434782609
2024-10-08 13:10:01,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=15.96 vs. limit=19.613500000000002
2024-10-08 13:10:12,499 INFO [train.py:1153] Epoch 8, batch 500, loss[loss=0.2491, simple_loss=0.2755, pruned_loss=0.07985, ctc_loss=0.1577, over 4808.00 frames. ], tot_loss[loss=0.2357, simple_loss=0.2705, pruned_loss=0.07093, ctc_loss=0.1478, over 888095.55 frames. ], batch size: 34, lr: 1.18e-02,
2024-10-08 13:10:21,880 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=16154.666666666666, ans=0.125
2024-10-08 13:10:22,379 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.98 vs. limit=9.038666666666668
2024-10-08 13:10:23,213 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=16154.666666666666, ans=0.0
2024-10-08 13:10:45,821 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=16161.333333333334, ans=0.13838666666666666
2024-10-08 13:10:59,008 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=16164.666666666666, ans=0.007355507246376812
2024-10-08 13:11:00,123 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.026e+01 1.073e+02 1.189e+02 1.339e+02 2.090e+02, threshold=2.377e+02, percent-clipped=0.0
2024-10-08 13:11:18,708 INFO [train.py:1153] Epoch 8, batch 550, loss[loss=0.2366, simple_loss=0.2714, pruned_loss=0.07059, ctc_loss=0.1514, over 4800.00 frames. ], tot_loss[loss=0.2352, simple_loss=0.2702, pruned_loss=0.07065, ctc_loss=0.1475, over 905505.04 frames. ], batch size: 40, lr: 1.18e-02,
2024-10-08 13:11:18,845 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=16171.333333333334, ans=0.04949747468305833
2024-10-08 13:11:18,877 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=16171.333333333334, ans=10.0
2024-10-08 13:11:51,614 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=3.91 vs. limit=5.0
2024-10-08 13:11:57,200 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=16181.333333333334, ans=0.125
2024-10-08 13:11:57,643 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=11.19 vs. limit=13.568
2024-10-08 13:11:57,755 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.30 vs. limit=19.636
2024-10-08 13:12:01,907 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.92 vs. limit=13.568
2024-10-08 13:12:24,892 INFO [train.py:1153] Epoch 8, batch 600, loss[loss=0.2768, simple_loss=0.2967, pruned_loss=0.09138, ctc_loss=0.1853, over 4817.00 frames. ], tot_loss[loss=0.2339, simple_loss=0.2694, pruned_loss=0.06999, ctc_loss=0.1462, over 919389.77 frames. ], batch size: 38, lr: 1.18e-02,
2024-10-08 13:13:12,779 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.020e+01 1.088e+02 1.224e+02 1.341e+02 1.730e+02, threshold=2.448e+02, percent-clipped=0.0
2024-10-08 13:13:31,331 INFO [train.py:1153] Epoch 8, batch 650, loss[loss=0.204, simple_loss=0.2612, pruned_loss=0.05227, ctc_loss=0.1055, over 4835.00 frames. ], tot_loss[loss=0.2319, simple_loss=0.2684, pruned_loss=0.06889, ctc_loss=0.1441, over 930294.84 frames. ], batch size: 21, lr: 1.18e-02,
2024-10-08 13:13:31,533 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=16204.666666666666, ans=0.125
2024-10-08 13:13:40,632 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=16204.666666666666, ans=0.125
2024-10-08 13:13:49,806 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=16208.0, ans=0.125
2024-10-08 13:13:52,936 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=11.61 vs. limit=13.578
2024-10-08 13:13:56,520 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=16211.333333333334, ans=0.0
2024-10-08 13:14:07,781 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=12.38 vs. limit=13.57925
2024-10-08 13:14:17,730 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=16214.666666666666, ans=0.3324866666666667
2024-10-08 13:14:32,168 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=16218.0, ans=0.0
2024-10-08 13:14:36,033 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=16221.333333333334, ans=0.025
2024-10-08 13:14:37,171 INFO [train.py:1153] Epoch 8, batch 700, loss[loss=0.1982, simple_loss=0.2463, pruned_loss=0.05343, ctc_loss=0.1083, over 4749.00 frames. ], tot_loss[loss=0.2314, simple_loss=0.2678, pruned_loss=0.06869, ctc_loss=0.144, over 938204.43 frames. ], batch size: 19, lr: 1.17e-02,
2024-10-08 13:14:37,876 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.83 vs. limit=9.055333333333333
2024-10-08 13:14:49,166 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=16224.666666666666, ans=0.007342463768115942
2024-10-08 13:15:10,288 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=16228.0, ans=0.007341739130434783
2024-10-08 13:15:24,907 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.948e+01 1.069e+02 1.235e+02 1.348e+02 1.863e+02, threshold=2.470e+02, percent-clipped=0.0
2024-10-08 13:15:31,760 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=16234.666666666666, ans=0.0
2024-10-08 13:15:37,206 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=16234.666666666666, ans=0.0
2024-10-08 13:15:43,501 INFO [train.py:1153] Epoch 8, batch 750, loss[loss=0.1738, simple_loss=0.2242, pruned_loss=0.04255, ctc_loss=0.096, over 4907.00 frames. ], tot_loss[loss=0.2303, simple_loss=0.2674, pruned_loss=0.06804, ctc_loss=0.1425, over 944975.20 frames. ], batch size: 22, lr: 1.17e-02,
2024-10-08 13:15:57,017 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=16241.333333333334, ans=0.125
2024-10-08 13:16:07,570 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=16241.333333333334, ans=0.025
2024-10-08 13:16:22,327 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=16248.0, ans=0.007337391304347827
2024-10-08 13:16:45,198 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.69 vs. limit=13.594249999999999
2024-10-08 13:16:49,909 INFO [train.py:1153] Epoch 8, batch 800, loss[loss=0.2113, simple_loss=0.2515, pruned_loss=0.05918, ctc_loss=0.1321, over 4853.00 frames. ], tot_loss[loss=0.2305, simple_loss=0.2676, pruned_loss=0.06813, ctc_loss=0.1427, over 949770.00 frames. ], batch size: 19, lr: 1.17e-02,
2024-10-08 13:16:53,014 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.03 vs. limit=13.595500000000001
2024-10-08 13:17:03,904 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=16258.0, ans=0.04949747468305833
2024-10-08 13:17:14,301 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff2_skip_rate, batch_count=16258.0, ans=0.007335217391304348
2024-10-08 13:17:24,461 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=16261.333333333334, ans=0.125
2024-10-08 13:17:36,871 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=17.61 vs. limit=13.59925
2024-10-08 13:17:37,436 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.634e+01 1.031e+02 1.169e+02 1.287e+02 2.122e+02, threshold=2.339e+02, percent-clipped=0.0
2024-10-08 13:17:48,160 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=16268.0, ans=0.125
2024-10-08 13:17:48,243 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=16268.0, ans=0.0
2024-10-08 13:17:50,903 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 13:17:55,936 INFO [train.py:1153] Epoch 8, batch 850, loss[loss=0.2721, simple_loss=0.3029, pruned_loss=0.08398, ctc_loss=0.1832, over 4781.00 frames. ], tot_loss[loss=0.2314, simple_loss=0.2682, pruned_loss=0.0686, ctc_loss=0.1433, over 953984.61 frames. ], batch size: 29, lr: 1.17e-02,
2024-10-08 13:17:57,954 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.36 vs. limit=9.067833333333333
2024-10-08 13:18:06,497 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=16271.333333333334, ans=0.0
2024-10-08 13:18:10,475 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 13:18:19,612 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=16274.666666666666, ans=0.125
2024-10-08 13:18:22,164 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=16278.0, ans=0.125
2024-10-08 13:18:22,188 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=16278.0, ans=0.13721999999999998
2024-10-08 13:18:33,465 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.28 vs. limit=13.60425
2024-10-08 13:18:44,792 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=16281.333333333334, ans=0.3301533333333334
2024-10-08 13:18:46,146 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=16281.333333333334, ans=0.007330144927536232
2024-10-08 13:19:01,765 INFO [train.py:1153] Epoch 8, batch 900, loss[loss=0.1974, simple_loss=0.2482, pruned_loss=0.05066, ctc_loss=0.1131, over 4852.00 frames. ], tot_loss[loss=0.2319, simple_loss=0.2688, pruned_loss=0.06886, ctc_loss=0.1434, over 956893.61 frames. ], batch size: 19, lr: 1.17e-02,
2024-10-08 13:19:22,937 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=16291.333333333334, ans=0.025
2024-10-08 13:19:49,133 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.597e+01 1.059e+02 1.174e+02 1.332e+02 2.290e+02, threshold=2.348e+02, percent-clipped=0.0
2024-10-08 13:19:50,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=16298.0, ans=0.32957000000000014
2024-10-08 13:19:50,701 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=16298.0, ans=0.32957000000000014
2024-10-08 13:20:00,278 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=9.72 vs. limit=13.613
2024-10-08 13:20:07,700 INFO [train.py:1153] Epoch 8, batch 950, loss[loss=0.2175, simple_loss=0.2619, pruned_loss=0.0606, ctc_loss=0.1297, over 4817.00 frames. ], tot_loss[loss=0.2327, simple_loss=0.2695, pruned_loss=0.06919, ctc_loss=0.1438, over 958719.10 frames. ], batch size: 19, lr: 1.17e-02,
2024-10-08 13:20:09,136 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=16304.666666666666, ans=0.13695333333333334
2024-10-08 13:20:10,781 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=17.94 vs. limit=19.7285
2024-10-08 13:20:30,133 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=16308.0, ans=0.32922000000000007
2024-10-08 13:21:09,742 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=16318.0, ans=0.125
2024-10-08 13:21:09,770 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=16318.0, ans=0.13682
2024-10-08 13:21:13,638 INFO [train.py:1153] Epoch 8, batch 1000, loss[loss=0.2086, simple_loss=0.2556, pruned_loss=0.057, ctc_loss=0.1189, over 4931.00 frames. ], tot_loss[loss=0.2338, simple_loss=0.2703, pruned_loss=0.06967, ctc_loss=0.1451, over 960339.61 frames. ], batch size: 20, lr: 1.17e-02,
2024-10-08 13:21:16,358 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=16321.333333333334, ans=0.125
2024-10-08 13:21:39,548 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=15.62 vs. limit=13.623000000000001
2024-10-08 13:22:00,563 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.prob, batch_count=16331.333333333334, ans=0.125
2024-10-08 13:22:01,528 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.694e+01 1.086e+02 1.209e+02 1.374e+02 2.193e+02, threshold=2.417e+02, percent-clipped=0.0
2024-10-08 13:22:13,531 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=16334.666666666666, ans=0.125
2024-10-08 13:22:19,991 INFO [train.py:1153] Epoch 8, batch 1050, loss[loss=0.2226, simple_loss=0.2678, pruned_loss=0.06263, ctc_loss=0.1305, over 4803.00 frames. ], tot_loss[loss=0.2333, simple_loss=0.2699, pruned_loss=0.06947, ctc_loss=0.1445, over 962588.29 frames. ], batch size: 25, lr: 1.17e-02,
2024-10-08 13:22:20,620 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=20.32 vs. limit=19.753500000000003
2024-10-08 13:22:21,495 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=16338.0, ans=0.125
2024-10-08 13:22:28,255 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=16338.0, ans=0.125
2024-10-08 13:22:31,495 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.12 vs. limit=19.753500000000003
2024-10-08 13:22:52,044 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=16344.666666666666, ans=0.13655333333333333
2024-10-08 13:23:02,936 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=16348.0, ans=0.125
2024-10-08 13:23:09,570 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=16348.0, ans=0.025
2024-10-08 13:23:26,813 INFO [train.py:1153] Epoch 8, batch 1100, loss[loss=0.1941, simple_loss=0.2472, pruned_loss=0.04821, ctc_loss=0.1115, over 4851.00 frames. ], tot_loss[loss=0.2318, simple_loss=0.2689, pruned_loss=0.06873, ctc_loss=0.1429, over 964179.35 frames. ], batch size: 20, lr: 1.17e-02,
2024-10-08 13:23:36,234 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=16354.666666666666, ans=0.125
2024-10-08 13:23:50,703 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=16358.0, ans=0.125
2024-10-08 13:24:03,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=16361.333333333334, ans=0.125
2024-10-08 13:24:14,027 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.712e+01 1.074e+02 1.213e+02 1.349e+02 1.894e+02, threshold=2.425e+02, percent-clipped=0.0
2024-10-08 13:24:32,561 INFO [train.py:1153] Epoch 8, batch 1150, loss[loss=0.2213, simple_loss=0.259, pruned_loss=0.06539, ctc_loss=0.1321, over 4861.00 frames. ], tot_loss[loss=0.2326, simple_loss=0.2696, pruned_loss=0.06904, ctc_loss=0.1434, over 964593.63 frames. ], batch size: 20, lr: 1.17e-02,
2024-10-08 13:24:38,127 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 13:24:48,242 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=10.90 vs. limit=13.6405
2024-10-08 13:24:48,651 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=16374.666666666666, ans=0.0
2024-10-08 13:25:08,556 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=16378.0, ans=0.125
2024-10-08 13:25:11,148 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.prob, batch_count=16381.333333333334, ans=0.125
2024-10-08 13:25:17,094 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=3.68 vs. limit=10.552533333333333
2024-10-08 13:25:20,321 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=16381.333333333334, ans=0.0073084057971014495
2024-10-08 13:25:36,624 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=16384.666666666668, ans=0.125
2024-10-08 13:25:39,105 INFO [train.py:1153] Epoch 8, batch 1200, loss[loss=0.2015, simple_loss=0.2383, pruned_loss=0.05866, ctc_loss=0.1185, over 4815.00 frames. ], tot_loss[loss=0.2334, simple_loss=0.27, pruned_loss=0.0695, ctc_loss=0.1444, over 964484.39 frames. ], batch size: 25, lr: 1.17e-02,
2024-10-08 13:25:42,135 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=5.901e-02
2024-10-08 13:25:53,132 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.66 vs. limit=13.646749999999999
2024-10-08 13:26:03,591 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=16391.333333333332, ans=0.007306231884057971
2024-10-08 13:26:22,279 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=16398.0, ans=0.0
2024-10-08 13:26:27,488 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.125e+01 1.080e+02 1.176e+02 1.358e+02 1.971e+02, threshold=2.353e+02, percent-clipped=0.0
2024-10-08 13:26:38,753 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.09 vs. limit=19.801
2024-10-08 13:26:46,008 INFO [train.py:1153] Epoch 8, batch 1250, loss[loss=0.2736, simple_loss=0.2951, pruned_loss=0.08844, ctc_loss=0.1879, over 4781.00 frames. ], tot_loss[loss=0.2346, simple_loss=0.2704, pruned_loss=0.07032, ctc_loss=0.1455, over 964470.37 frames. ], batch size: 32, lr: 1.17e-02,
2024-10-08 13:27:21,550 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.29 vs. limit=13.65425
2024-10-08 13:27:53,274 INFO [train.py:1153] Epoch 8, batch 1300, loss[loss=0.2811, simple_loss=0.2861, pruned_loss=0.09423, ctc_loss=0.2189, over 4820.00 frames. ], tot_loss[loss=0.2341, simple_loss=0.2699, pruned_loss=0.0701, ctc_loss=0.1454, over 965749.97 frames. ], batch size: 43, lr: 1.17e-02,
2024-10-08 13:27:55,363 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.70 vs. limit=13.210666666666667
2024-10-08 13:28:02,797 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=16421.333333333332, ans=0.0
2024-10-08 13:28:30,451 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.91 vs. limit=13.660499999999999
2024-10-08 13:28:34,067 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=16431.333333333332, ans=0.13568666666666668
2024-10-08 13:28:40,810 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=16431.333333333332, ans=0.32490333333333343
2024-10-08 13:28:41,934 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.833e+01 1.087e+02 1.204e+02 1.339e+02 1.766e+02, threshold=2.409e+02, percent-clipped=0.0
2024-10-08 13:28:56,656 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=16434.666666666668, ans=0.125
2024-10-08 13:29:00,542 INFO [train.py:1153] Epoch 8, batch 1350, loss[loss=0.2104, simple_loss=0.2493, pruned_loss=0.06053, ctc_loss=0.1262, over 4856.00 frames. ], tot_loss[loss=0.2329, simple_loss=0.269, pruned_loss=0.06948, ctc_loss=0.1443, over 966515.36 frames. ], batch size: 21, lr: 1.17e-02,
2024-10-08 13:29:15,673 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=16441.333333333332, ans=0.32455333333333336
2024-10-08 13:29:30,372 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=16444.666666666668, ans=0.125
2024-10-08 13:29:31,663 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=16444.666666666668, ans=0.125
2024-10-08 13:29:48,529 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.32 vs. limit=13.668
2024-10-08 13:29:56,039 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=16451.333333333332, ans=0.3242033333333334
2024-10-08 13:30:05,883 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.88 vs. limit=13.669249999999998
2024-10-08 13:30:07,766 INFO [train.py:1153] Epoch 8, batch 1400, loss[loss=0.1875, simple_loss=0.2428, pruned_loss=0.04798, ctc_loss=0.09062, over 4940.00 frames. ], tot_loss[loss=0.2334, simple_loss=0.2695, pruned_loss=0.06976, ctc_loss=0.1445, over 966786.14 frames. ], batch size: 19, lr: 1.17e-02,
2024-10-08 13:30:09,687 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.80 vs. limit=10.581866666666667
2024-10-08 13:30:18,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=16454.666666666668, ans=0.0
2024-10-08 13:30:22,713 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=16458.0, ans=0.125
2024-10-08 13:30:26,752 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=16458.0, ans=0.3239700000000001
2024-10-08 13:30:36,213 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=16461.333333333332, ans=0.125
2024-10-08 13:30:38,865 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=16461.333333333332, ans=0.13538666666666668
2024-10-08 13:30:40,684 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=19.49 vs. limit=19.845999999999997
2024-10-08 13:30:48,143 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=16464.666666666668, ans=0.125
2024-10-08 13:30:56,140 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.316e+01 1.020e+02 1.195e+02 1.303e+02 1.907e+02, threshold=2.390e+02, percent-clipped=0.0
2024-10-08 13:30:56,373 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=16464.666666666668, ans=0.007290289855072464
2024-10-08 13:31:14,789 INFO [train.py:1153] Epoch 8, batch 1450, loss[loss=0.2627, simple_loss=0.2867, pruned_loss=0.08301, ctc_loss=0.1814, over 4823.00 frames. ], tot_loss[loss=0.2331, simple_loss=0.2687, pruned_loss=0.0698, ctc_loss=0.1446, over 966640.50 frames. ], batch size: 34, lr: 1.17e-02,
2024-10-08 13:31:17,538 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=16471.333333333332, ans=0.125
2024-10-08 13:31:36,452 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=16474.666666666668, ans=0.0
2024-10-08 13:31:37,800 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=16474.666666666668, ans=0.3233866666666667
2024-10-08 13:31:44,817 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.12 vs. limit=9.1195
2024-10-08 13:32:01,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=16481.333333333332, ans=0.0
2024-10-08 13:32:12,633 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 13:32:19,350 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=16484.666666666668, ans=0.13515333333333332
2024-10-08 13:32:21,954 INFO [train.py:1153] Epoch 8, batch 1500, loss[loss=0.2246, simple_loss=0.2579, pruned_loss=0.06179, ctc_loss=0.1694, over 4759.00 frames. ], tot_loss[loss=0.2341, simple_loss=0.2697, pruned_loss=0.07022, ctc_loss=0.1455, over 966424.66 frames. ], batch size: 26, lr: 1.17e-02,
2024-10-08 13:33:10,196 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.273e+01 1.102e+02 1.244e+02 1.391e+02 1.888e+02, threshold=2.488e+02, percent-clipped=0.0
2024-10-08 13:33:28,851 INFO [train.py:1153] Epoch 8, batch 1550, loss[loss=0.2999, simple_loss=0.305, pruned_loss=0.103, ctc_loss=0.2223, over 4840.00 frames. ], tot_loss[loss=0.2351, simple_loss=0.2704, pruned_loss=0.07057, ctc_loss=0.1467, over 966259.11 frames. ], batch size: 31, lr: 1.16e-02,
2024-10-08 13:33:30,850 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.99 vs. limit=19.878500000000003
2024-10-08 13:33:32,107 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.47 vs. limit=5.4757
2024-10-08 13:33:53,331 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=16508.0, ans=0.007280869565217392
2024-10-08 13:34:18,996 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=16514.666666666668, ans=0.0
2024-10-08 13:34:21,690 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=16518.0, ans=0.0
2024-10-08 13:34:36,192 INFO [train.py:1153] Epoch 8, batch 1600, loss[loss=0.2264, simple_loss=0.2722, pruned_loss=0.06395, ctc_loss=0.1317, over 4818.00 frames. ], tot_loss[loss=0.2336, simple_loss=0.2695, pruned_loss=0.06977, ctc_loss=0.1455, over 966631.49 frames. ], batch size: 25, lr: 1.16e-02,
2024-10-08 13:34:37,655 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=16521.333333333332, ans=0.0
2024-10-08 13:34:44,416 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=16521.333333333332, ans=0.0
2024-10-08 13:34:47,187 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=16521.333333333332, ans=0.007277971014492754
2024-10-08 13:35:11,305 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=16528.0, ans=0.32152000000000014
2024-10-08 13:35:16,763 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=16531.333333333332, ans=0.125
2024-10-08 13:35:24,640 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.521e+01 1.062e+02 1.180e+02 1.328e+02 1.792e+02, threshold=2.360e+02, percent-clipped=0.0
2024-10-08 13:35:26,092 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=16531.333333333332, ans=0.3214033333333334
2024-10-08 13:35:30,715 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=5.32 vs. limit=10.613866666666667
2024-10-08 13:35:43,377 INFO [train.py:1153] Epoch 8, batch 1650, loss[loss=0.2134, simple_loss=0.2571, pruned_loss=0.05766, ctc_loss=0.1361, over 4771.00 frames. ], tot_loss[loss=0.2339, simple_loss=0.2695, pruned_loss=0.06991, ctc_loss=0.1461, over 966999.92 frames. ], batch size: 29, lr: 1.16e-02,
2024-10-08 13:35:47,511 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=16538.0, ans=0.125
2024-10-08 13:35:48,879 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=16538.0, ans=0.08461999999999997
2024-10-08 13:36:02,920 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.09 vs. limit=13.703
2024-10-08 13:36:07,730 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=16541.333333333332, ans=0.0
2024-10-08 13:36:14,327 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=16544.666666666668, ans=0.035
2024-10-08 13:36:31,842 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=16548.0, ans=0.13452
2024-10-08 13:36:33,236 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=16548.0, ans=0.0
2024-10-08 13:36:34,661 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=16548.0, ans=0.007272173913043478
2024-10-08 13:36:50,593 INFO [train.py:1153] Epoch 8, batch 1700, loss[loss=0.1971, simple_loss=0.2388, pruned_loss=0.05666, ctc_loss=0.1053, over 4940.00 frames. ], tot_loss[loss=0.2332, simple_loss=0.269, pruned_loss=0.06968, ctc_loss=0.1453, over 967032.55 frames. ], batch size: 19, lr: 1.16e-02,
2024-10-08 13:37:38,901 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.931e+01 1.054e+02 1.185e+02 1.289e+02 1.928e+02, threshold=2.370e+02, percent-clipped=0.0
2024-10-08 13:37:48,489 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=16568.0, ans=0.0
2024-10-08 13:37:57,854 INFO [train.py:1153] Epoch 8, batch 1750, loss[loss=0.1933, simple_loss=0.2421, pruned_loss=0.05203, ctc_loss=0.1009, over 4959.00 frames. ], tot_loss[loss=0.2324, simple_loss=0.2686, pruned_loss=0.06926, ctc_loss=0.1442, over 967183.13 frames. ], batch size: 19, lr: 1.16e-02,
2024-10-08 13:38:25,544 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.87 vs. limit=9.1445
2024-10-08 13:38:25,751 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.33 vs. limit=13.716750000000001
2024-10-08 13:38:31,008 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.59 vs. limit=9.1445
2024-10-08 13:38:42,608 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.max_abs, batch_count=16581.333333333332, ans=10.0
2024-10-08 13:38:53,790 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.88 vs. limit=19.9385
2024-10-08 13:38:56,538 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.36 vs. limit=9.146166666666666
2024-10-08 13:39:04,086 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.73 vs. limit=13.71925
2024-10-08 13:39:04,828 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=16588.0, ans=0.125
2024-10-08 13:39:05,845 INFO [train.py:1153] Epoch 8, batch 1800, loss[loss=0.2389, simple_loss=0.2783, pruned_loss=0.07231, ctc_loss=0.1369, over 4842.00 frames. ], tot_loss[loss=0.2333, simple_loss=0.2691, pruned_loss=0.06977, ctc_loss=0.1451, over 967947.84 frames. ], batch size: 23, lr: 1.16e-02,
2024-10-08 13:39:09,845 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=16588.0, ans=0.125
2024-10-08 13:39:15,113 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=16588.0, ans=0.125
2024-10-08 13:39:31,063 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=16594.666666666668, ans=0.125
2024-10-08 13:39:53,602 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.400e+01 1.049e+02 1.194e+02 1.349e+02 1.844e+02, threshold=2.387e+02, percent-clipped=0.0
2024-10-08 13:40:05,789 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=16601.333333333332, ans=0.0
2024-10-08 13:40:06,300 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=20.70 vs. limit=19.951
2024-10-08 13:40:07,544 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.00 vs. limit=13.7255
2024-10-08 13:40:12,345 INFO [train.py:1153] Epoch 8, batch 1850, loss[loss=0.2202, simple_loss=0.2653, pruned_loss=0.06314, ctc_loss=0.122, over 4736.00 frames. ], tot_loss[loss=0.2329, simple_loss=0.2689, pruned_loss=0.06953, ctc_loss=0.1447, over 968130.10 frames. ], batch size: 26, lr: 1.16e-02,
2024-10-08 13:40:37,893 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.max_abs, batch_count=16611.333333333332, ans=10.0
2024-10-08 13:40:47,388 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.75 vs. limit=19.9585
2024-10-08 13:40:57,840 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=16614.666666666668, ans=0.125
2024-10-08 13:41:03,187 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=16614.666666666668, ans=0.3184866666666667
2024-10-08 13:41:04,520 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=16618.0, ans=0.125
2024-10-08 13:41:08,564 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=16618.0, ans=0.125
2024-10-08 13:41:14,192 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.45 vs. limit=10.6472
2024-10-08 13:41:17,029 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=12.93 vs. limit=13.73175
2024-10-08 13:41:19,045 INFO [train.py:1153] Epoch 8, batch 1900, loss[loss=0.1939, simple_loss=0.2415, pruned_loss=0.05079, ctc_loss=0.1119, over 4785.00 frames. ], tot_loss[loss=0.2349, simple_loss=0.2704, pruned_loss=0.07042, ctc_loss=0.1461, over 967789.48 frames. ], batch size: 29, lr: 1.16e-02,
2024-10-08 13:41:19,235 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=16621.333333333332, ans=0.3182533333333334
2024-10-08 13:41:24,431 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=16621.333333333332, ans=0.125
2024-10-08 13:41:42,231 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.40 vs. limit=13.312333333333335
2024-10-08 13:41:49,864 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=16628.0, ans=0.125
2024-10-08 13:41:58,303 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.36 vs. limit=13.736749999999999
2024-10-08 13:41:59,141 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=16631.333333333332, ans=0.125
2024-10-08 13:42:07,034 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.715e+01 1.114e+02 1.224e+02 1.349e+02 2.521e+02, threshold=2.448e+02, percent-clipped=1.0
2024-10-08 13:42:24,412 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=16638.0, ans=0.007252608695652174
2024-10-08 13:42:25,560 INFO [train.py:1153] Epoch 8, batch 1950, loss[loss=0.2335, simple_loss=0.2749, pruned_loss=0.06921, ctc_loss=0.1345, over 4861.00 frames. ], tot_loss[loss=0.2345, simple_loss=0.2705, pruned_loss=0.07014, ctc_loss=0.1457, over 966798.05 frames. ], batch size: 20, lr: 1.16e-02,
2024-10-08 13:42:27,413 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.98 vs. limit=19.9785
2024-10-08 13:42:53,517 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=16644.666666666668, ans=0.0
2024-10-08 13:43:07,989 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=16648.0, ans=0.125
2024-10-08 13:43:29,315 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=16651.333333333332, ans=0.025
2024-10-08 13:43:31,959 INFO [train.py:1153] Epoch 8, batch 2000, loss[loss=0.2665, simple_loss=0.3029, pruned_loss=0.0833, ctc_loss=0.1588, over 4959.00 frames. ], tot_loss[loss=0.2369, simple_loss=0.272, pruned_loss=0.07139, ctc_loss=0.1477, over 966692.38 frames. ], batch size: 19, lr: 1.16e-02,
2024-10-08 13:43:42,954 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=16654.666666666668, ans=0.025
2024-10-08 13:43:44,306 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=16658.0, ans=0.007248260869565218
2024-10-08 13:43:49,577 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=16658.0, ans=0.125
2024-10-08 13:43:58,507 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module2.whiten, num_groups=1, num_channels=192, metric=3.46 vs. limit=13.748
2024-10-08 13:44:11,181 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 13:44:20,406 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.404e+01 1.101e+02 1.203e+02 1.348e+02 1.851e+02, threshold=2.407e+02, percent-clipped=0.0
2024-10-08 13:44:24,544 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.min_abs, batch_count=16668.0, ans=0.45002
2024-10-08 13:44:35,415 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=16668.0, ans=0.025
2024-10-08 13:44:39,162 INFO [train.py:1153] Epoch 8, batch 2050, loss[loss=0.2307, simple_loss=0.2641, pruned_loss=0.07079, ctc_loss=0.139, over 4909.00 frames. ], tot_loss[loss=0.2391, simple_loss=0.2737, pruned_loss=0.07235, ctc_loss=0.1493, over 967155.75 frames. ], batch size: 19, lr: 1.16e-02,
2024-10-08 13:44:40,662 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=16671.333333333332, ans=0.31650333333333336
2024-10-08 13:44:43,386 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=16671.333333333332, ans=0.1332866666666667
2024-10-08 13:44:44,754 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=16671.333333333332, ans=0.025
2024-10-08 13:45:04,760 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=16678.0, ans=0.0
2024-10-08 13:45:27,901 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.74 vs. limit=20.011
2024-10-08 13:45:37,302 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.35 vs. limit=20.0135
2024-10-08 13:45:46,276 INFO [train.py:1153] Epoch 8, batch 2100, loss[loss=0.2336, simple_loss=0.2774, pruned_loss=0.06409, ctc_loss=0.1539, over 4853.00 frames. ], tot_loss[loss=0.2373, simple_loss=0.2732, pruned_loss=0.07115, ctc_loss=0.1476, over 967325.56 frames. ], batch size: 21, lr: 1.16e-02,
2024-10-08 13:45:54,292 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=16688.0, ans=0.13312
2024-10-08 13:45:55,772 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=16688.0, ans=0.0
2024-10-08 13:45:58,311 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=16691.333333333332, ans=0.007241014492753623
2024-10-08 13:46:09,266 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=4.31 vs. limit=10.676533333333332
2024-10-08 13:46:10,291 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 13:46:14,404 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=16694.666666666668, ans=0.025
2024-10-08 13:46:34,431 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.358e+01 1.082e+02 1.192e+02 1.310e+02 1.821e+02, threshold=2.383e+02, percent-clipped=0.0
2024-10-08 13:46:53,447 INFO [train.py:1153] Epoch 8, batch 2150, loss[loss=0.2233, simple_loss=0.273, pruned_loss=0.06185, ctc_loss=0.1245, over 4857.00 frames. ], tot_loss[loss=0.2357, simple_loss=0.272, pruned_loss=0.07058, ctc_loss=0.1457, over 968172.27 frames. ], batch size: 20, lr: 1.16e-02,
2024-10-08 13:47:01,115 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.88 vs. limit=9.176166666666667
2024-10-08 13:47:55,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=16718.0, ans=0.125
2024-10-08 13:48:00,366 INFO [train.py:1153] Epoch 8, batch 2200, loss[loss=0.2459, simple_loss=0.2859, pruned_loss=0.06876, ctc_loss=0.1707, over 4745.00 frames. ], tot_loss[loss=0.2357, simple_loss=0.2718, pruned_loss=0.07063, ctc_loss=0.1457, over 967780.87 frames. ], batch size: 26, lr: 1.16e-02,
2024-10-08 13:48:03,345 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=3.304e-01
2024-10-08 13:48:05,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=16721.333333333332, ans=0.125
2024-10-08 13:48:11,431 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=16721.333333333332, ans=0.025
2024-10-08 13:48:25,282 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.15 vs. limit=13.77175
2024-10-08 13:48:27,428 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=16728.0, ans=0.13272
2024-10-08 13:48:28,847 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=16728.0, ans=0.125
2024-10-08 13:48:49,035 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.206e+01 1.036e+02 1.160e+02 1.301e+02 2.508e+02, threshold=2.319e+02, percent-clipped=1.0
2024-10-08 13:49:07,929 INFO [train.py:1153] Epoch 8, batch 2250, loss[loss=0.2023, simple_loss=0.2522, pruned_loss=0.05201, ctc_loss=0.1209, over 4888.00 frames. ], tot_loss[loss=0.235, simple_loss=0.2715, pruned_loss=0.07023, ctc_loss=0.1451, over 967746.76 frames. ], batch size: 22, lr: 1.16e-02,
2024-10-08 13:49:14,761 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=16738.0, ans=0.13262
2024-10-08 13:49:31,794 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=16741.333333333332, ans=0.007230144927536233
2024-10-08 13:49:37,304 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=16744.666666666668, ans=0.125
2024-10-08 13:49:58,810 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=16748.0, ans=0.125
2024-10-08 13:50:12,449 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=16751.333333333332, ans=0.125
2024-10-08 13:50:14,910 INFO [train.py:1153] Epoch 8, batch 2300, loss[loss=0.208, simple_loss=0.2588, pruned_loss=0.0541, ctc_loss=0.1227, over 4883.00 frames. ], tot_loss[loss=0.2328, simple_loss=0.2697, pruned_loss=0.06928, ctc_loss=0.1431, over 968299.71 frames. ], batch size: 19, lr: 1.16e-02,
2024-10-08 13:50:28,616 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=16758.0, ans=0.125
2024-10-08 13:50:52,776 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=16761.333333333332, ans=0.0
2024-10-08 13:50:54,178 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=16764.666666666668, ans=0.13235333333333332
2024-10-08 13:51:03,318 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.174e+01 1.034e+02 1.157e+02 1.257e+02 1.735e+02, threshold=2.313e+02, percent-clipped=0.0
2024-10-08 13:51:22,303 INFO [train.py:1153] Epoch 8, batch 2350, loss[loss=0.2328, simple_loss=0.2738, pruned_loss=0.06623, ctc_loss=0.1483, over 4886.00 frames. ], tot_loss[loss=0.2326, simple_loss=0.2699, pruned_loss=0.06911, ctc_loss=0.1429, over 968433.63 frames. ], batch size: 23, lr: 1.16e-02,
2024-10-08 13:51:34,311 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=16774.666666666668, ans=0.007222898550724638
2024-10-08 13:51:43,463 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=16774.666666666668, ans=0.125
2024-10-08 13:51:44,063 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.99 vs. limit=9.193666666666667
2024-10-08 13:51:52,170 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.19 vs. limit=9.1945
2024-10-08 13:51:54,321 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=16778.0, ans=0.3127700000000001
2024-10-08 13:52:10,409 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=16781.333333333332, ans=0.007221449275362319
2024-10-08 13:52:29,016 INFO [train.py:1153] Epoch 8, batch 2400, loss[loss=0.2206, simple_loss=0.2637, pruned_loss=0.06135, ctc_loss=0.1371, over 4755.00 frames. ], tot_loss[loss=0.2329, simple_loss=0.2701, pruned_loss=0.06921, ctc_loss=0.1434, over 967624.81 frames. ], batch size: 19, lr: 1.15e-02,
2024-10-08 13:53:11,099 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=15.89 vs. limit=13.79925
2024-10-08 13:53:17,154 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.171e+01 1.131e+02 1.238e+02 1.393e+02 1.933e+02, threshold=2.476e+02, percent-clipped=0.0
2024-10-08 13:53:18,637 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=16798.0, ans=0.125
2024-10-08 13:53:19,973 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=16798.0, ans=0.0
2024-10-08 13:53:25,319 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=16801.333333333332, ans=0.3119533333333334
2024-10-08 13:53:30,734 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=16801.333333333332, ans=0.007217101449275363
2024-10-08 13:53:30,770 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=16801.333333333332, ans=0.025
2024-10-08 13:53:35,912 INFO [train.py:1153] Epoch 8, batch 2450, loss[loss=0.2352, simple_loss=0.2673, pruned_loss=0.0737, ctc_loss=0.1395, over 4869.00 frames. ], tot_loss[loss=0.2329, simple_loss=0.2701, pruned_loss=0.06918, ctc_loss=0.1433, over 966800.84 frames. ], batch size: 22, lr: 1.15e-02,
2024-10-08 13:54:00,153 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=16808.0, ans=0.125
2024-10-08 13:54:28,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=16818.0, ans=0.035
2024-10-08 13:54:42,809 INFO [train.py:1153] Epoch 8, batch 2500, loss[loss=0.2503, simple_loss=0.2798, pruned_loss=0.07909, ctc_loss=0.1564, over 4701.00 frames. ], tot_loss[loss=0.232, simple_loss=0.2694, pruned_loss=0.0688, ctc_loss=0.1425, over 966295.64 frames. ], batch size: 26, lr: 1.15e-02,
2024-10-08 13:54:47,241 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=16821.333333333332, ans=0.125
2024-10-08 13:54:52,590 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=16821.333333333332, ans=0.1317866666666667
2024-10-08 13:54:54,583 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.52 vs. limit=13.808
2024-10-08 13:55:15,318 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=16828.0, ans=0.125
2024-10-08 13:55:31,227 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.803e+01 1.045e+02 1.191e+02 1.348e+02 3.844e+02, threshold=2.383e+02, percent-clipped=1.0
2024-10-08 13:55:35,288 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=16834.666666666668, ans=0.125
2024-10-08 13:55:35,811 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.70 vs. limit=5.5252
2024-10-08 13:55:41,604 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.98 vs. limit=10.733866666666668
2024-10-08 13:55:50,239 INFO [train.py:1153] Epoch 8, batch 2550, loss[loss=0.2104, simple_loss=0.2519, pruned_loss=0.05785, ctc_loss=0.133, over 4959.00 frames. ], tot_loss[loss=0.2327, simple_loss=0.2696, pruned_loss=0.06921, ctc_loss=0.1432, over 966861.74 frames. ], batch size: 19, lr: 1.15e-02,
2024-10-08 13:55:54,384 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=16838.0, ans=0.13162
2024-10-08 13:56:01,520 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.88 vs. limit=20.128500000000003
2024-10-08 13:56:21,695 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.74 vs. limit=13.81675
2024-10-08 13:56:22,822 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.02 vs. limit=20.1335
2024-10-08 13:56:41,740 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.41 vs. limit=5.527200000000001
2024-10-08 13:56:42,593 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=16851.333333333332, ans=0.0
2024-10-08 13:56:49,162 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=16851.333333333332, ans=0.3102033333333334
2024-10-08 13:56:54,525 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=16851.333333333332, ans=0.0
2024-10-08 13:56:57,054 INFO [train.py:1153] Epoch 8, batch 2600, loss[loss=0.2062, simple_loss=0.2455, pruned_loss=0.06009, ctc_loss=0.1169, over 4861.00 frames. ], tot_loss[loss=0.2319, simple_loss=0.269, pruned_loss=0.06881, ctc_loss=0.1427, over 966461.99 frames. ], batch size: 20, lr: 1.15e-02,
2024-10-08 13:56:57,627 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.59 vs. limit=20.141000000000002
2024-10-08 13:56:58,689 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=16854.666666666668, ans=0.125
2024-10-08 13:57:04,491 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.04 vs. limit=13.820500000000001
2024-10-08 13:57:30,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=16861.333333333332, ans=0.3098533333333334
2024-10-08 13:57:33,299 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=16861.333333333332, ans=0.125
2024-10-08 13:57:38,619 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=16864.666666666668, ans=0.0
2024-10-08 13:57:45,320 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.401e+01 1.086e+02 1.252e+02 1.427e+02 1.885e+02, threshold=2.503e+02, percent-clipped=0.0
2024-10-08 13:58:03,916 INFO [train.py:1153] Epoch 8, batch 2650, loss[loss=0.2552, simple_loss=0.2872, pruned_loss=0.07774, ctc_loss=0.1693, over 4821.00 frames. ], tot_loss[loss=0.2328, simple_loss=0.2694, pruned_loss=0.06937, ctc_loss=0.1435, over 966296.42 frames. ], batch size: 38, lr: 1.15e-02,
2024-10-08 13:58:05,885 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.80 vs. limit=10.748533333333333
2024-10-08 13:58:24,391 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=16874.666666666668, ans=0.3093866666666667
2024-10-08 13:58:49,937 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=16881.333333333332, ans=0.025
2024-10-08 13:58:51,208 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=16881.333333333332, ans=0.007199710144927537
2024-10-08 13:58:51,256 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=16881.333333333332, ans=0.125
2024-10-08 13:58:59,655 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=2.84 vs. limit=13.83175
2024-10-08 13:59:10,897 INFO [train.py:1153] Epoch 8, batch 2700, loss[loss=0.2018, simple_loss=0.2499, pruned_loss=0.05274, ctc_loss=0.1207, over 4847.00 frames. ], tot_loss[loss=0.2322, simple_loss=0.2688, pruned_loss=0.06907, ctc_loss=0.1435, over 966390.23 frames. ], batch size: 28, lr: 1.15e-02,
2024-10-08 13:59:20,415 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=16888.0, ans=0.13112
2024-10-08 13:59:28,580 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=16891.333333333332, ans=0.125
2024-10-08 13:59:33,925 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=16891.333333333332, ans=0.125
2024-10-08 13:59:35,259 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=16891.333333333332, ans=0.0
2024-10-08 13:59:59,141 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.754e+01 1.062e+02 1.168e+02 1.314e+02 1.928e+02, threshold=2.337e+02, percent-clipped=0.0
2024-10-08 14:00:08,018 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.30 vs. limit=20.176
2024-10-08 14:00:18,092 INFO [train.py:1153] Epoch 8, batch 2750, loss[loss=0.1861, simple_loss=0.2322, pruned_loss=0.04781, ctc_loss=0.1111, over 4801.00 frames. ], tot_loss[loss=0.2317, simple_loss=0.2685, pruned_loss=0.06891, ctc_loss=0.1426, over 967060.32 frames. ], batch size: 19, lr: 1.15e-02,
2024-10-08 14:00:21,064 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=16904.666666666668, ans=0.125
2024-10-08 14:00:22,820 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.04 vs. limit=20.1785
2024-10-08 14:00:55,875 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=16911.333333333332, ans=0.3081033333333334
2024-10-08 14:01:09,342 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=16914.666666666668, ans=0.0
2024-10-08 14:01:22,613 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=16918.0, ans=0.125
2024-10-08 14:01:25,203 INFO [train.py:1153] Epoch 8, batch 2800, loss[loss=0.2215, simple_loss=0.2545, pruned_loss=0.06802, ctc_loss=0.131, over 4785.00 frames. ], tot_loss[loss=0.2323, simple_loss=0.2684, pruned_loss=0.06936, ctc_loss=0.1436, over 967211.92 frames. ], batch size: 53, lr: 1.15e-02,
2024-10-08 14:01:35,349 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.45 vs. limit=13.8455
2024-10-08 14:01:37,420 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=16924.666666666668, ans=0.125
2024-10-08 14:01:40,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=16924.666666666668, ans=0.30763666666666667
2024-10-08 14:01:43,541 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.09 vs. limit=5.5387
2024-10-08 14:01:45,887 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=10.18 vs. limit=13.84675
2024-10-08 14:01:50,676 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=16928.0, ans=0.13072
2024-10-08 14:01:52,359 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.73 vs. limit=13.464
2024-10-08 14:02:13,303 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.767e+01 1.090e+02 1.204e+02 1.352e+02 1.875e+02, threshold=2.409e+02, percent-clipped=0.0
2024-10-08 14:02:13,455 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=16931.333333333332, ans=0.125
2024-10-08 14:02:16,167 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=16931.333333333332, ans=0.025
2024-10-08 14:02:16,492 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.93 vs. limit=13.84925
2024-10-08 14:02:26,797 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=16934.666666666668, ans=0.125
2024-10-08 14:02:32,087 INFO [train.py:1153] Epoch 8, batch 2850, loss[loss=0.2208, simple_loss=0.2676, pruned_loss=0.06124, ctc_loss=0.1288, over 4929.00 frames. ], tot_loss[loss=0.2344, simple_loss=0.2697, pruned_loss=0.07043, ctc_loss=0.1454, over 966915.44 frames. ], batch size: 20, lr: 1.15e-02,
2024-10-08 14:02:32,607 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.62 vs. limit=13.85175
2024-10-08 14:02:46,954 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=16941.333333333332, ans=0.007186666666666668
2024-10-08 14:03:03,061 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=16944.666666666668, ans=0.007185942028985507
2024-10-08 14:03:17,849 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=16948.0, ans=0.13052
2024-10-08 14:03:35,255 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=16951.333333333332, ans=0.125
2024-10-08 14:03:39,041 INFO [train.py:1153] Epoch 8, batch 2900, loss[loss=0.2162, simple_loss=0.2725, pruned_loss=0.05606, ctc_loss=0.1196, over 4753.00 frames. ], tot_loss[loss=0.2349, simple_loss=0.2704, pruned_loss=0.07057, ctc_loss=0.1457, over 966047.38 frames. ], batch size: 20, lr: 1.15e-02,
2024-10-08 14:04:23,543 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=16964.666666666668, ans=0.05
2024-10-08 14:04:27,302 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.545e+01 1.054e+02 1.197e+02 1.319e+02 1.680e+02, threshold=2.395e+02, percent-clipped=0.0
2024-10-08 14:04:28,847 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=16964.666666666668, ans=0.125
2024-10-08 14:04:41,107 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.78 vs. limit=10.7872
2024-10-08 14:04:42,167 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=16968.0, ans=0.30612000000000006
2024-10-08 14:04:45,904 INFO [train.py:1153] Epoch 8, batch 2950, loss[loss=0.1812, simple_loss=0.2543, pruned_loss=0.0376, ctc_loss=0.08241, over 4798.00 frames. ], tot_loss[loss=0.2331, simple_loss=0.2693, pruned_loss=0.06965, ctc_loss=0.1439, over 966615.95 frames. ], batch size: 19, lr: 1.15e-02,
2024-10-08 14:04:54,168 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=16971.333333333332, ans=0.025
2024-10-08 14:04:54,756 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.66 vs. limit=13.485666666666665
2024-10-08 14:05:11,681 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=16978.0, ans=0.0
2024-10-08 14:05:18,355 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=16978.0, ans=0.007178695652173913
2024-10-08 14:05:20,056 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=9.22 vs. limit=13.86675
2024-10-08 14:05:30,327 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=16981.333333333332, ans=0.0
2024-10-08 14:05:40,278 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=14.25 vs. limit=13.869250000000001
2024-10-08 14:05:53,116 INFO [train.py:1153] Epoch 8, batch 3000, loss[loss=0.1878, simple_loss=0.2563, pruned_loss=0.04125, ctc_loss=0.09185, over 4842.00 frames. ], tot_loss[loss=0.2325, simple_loss=0.2687, pruned_loss=0.06944, ctc_loss=0.1435, over 967269.77 frames. ], batch size: 21, lr: 1.15e-02,
2024-10-08 14:05:53,116 INFO [train.py:1176] Computing validation loss
2024-10-08 14:06:01,033 INFO [train.py:1185] Epoch 8, validation: loss=0.1635, simple_loss=0.2475, pruned_loss=0.02847, ctc_loss=0.05614, over 90464.00 frames.
2024-10-08 14:06:01,034 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 14:06:02,550 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=16988.0, ans=0.125
2024-10-08 14:06:10,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=16988.0, ans=0.125
2024-10-08 14:06:49,282 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.654e+01 1.103e+02 1.223e+02 1.371e+02 2.012e+02, threshold=2.446e+02, percent-clipped=0.0
2024-10-08 14:07:07,585 INFO [train.py:1153] Epoch 8, batch 3050, loss[loss=0.2084, simple_loss=0.259, pruned_loss=0.05507, ctc_loss=0.1189, over 4751.00 frames. ], tot_loss[loss=0.2335, simple_loss=0.2693, pruned_loss=0.07008, ctc_loss=0.1439, over 966697.00 frames. ], batch size: 19, lr: 1.15e-02,
2024-10-08 14:07:18,380 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=17004.666666666668, ans=0.025
2024-10-08 14:07:52,898 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=17014.666666666668, ans=0.0
2024-10-08 14:07:55,635 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=17014.666666666668, ans=0.0
2024-10-08 14:08:14,259 INFO [train.py:1153] Epoch 8, batch 3100, loss[loss=0.2981, simple_loss=0.3214, pruned_loss=0.09865, ctc_loss=0.194, over 4824.00 frames. ], tot_loss[loss=0.2334, simple_loss=0.2694, pruned_loss=0.06988, ctc_loss=0.1438, over 966376.72 frames. ], batch size: 38, lr: 1.15e-02,
2024-10-08 14:08:22,930 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=4.36 vs. limit=13.883
2024-10-08 14:08:26,513 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.min_positive, batch_count=17024.666666666668, ans=0.05
2024-10-08 14:08:41,785 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.60 vs. limit=13.8855
2024-10-08 14:09:02,434 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.130e+01 1.048e+02 1.167e+02 1.261e+02 1.843e+02, threshold=2.334e+02, percent-clipped=0.0
2024-10-08 14:09:21,220 INFO [train.py:1153] Epoch 8, batch 3150, loss[loss=0.3017, simple_loss=0.3222, pruned_loss=0.09835, ctc_loss=0.2113, over 4785.00 frames. ], tot_loss[loss=0.2324, simple_loss=0.2689, pruned_loss=0.06931, ctc_loss=0.143, over 966803.61 frames. ], batch size: 40, lr: 1.15e-02,
2024-10-08 14:10:02,870 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.attention_skip_rate, batch_count=17048.0, ans=0.0
2024-10-08 14:10:08,175 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=17048.0, ans=0.30332000000000015
2024-10-08 14:10:17,476 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=17051.333333333332, ans=0.007162753623188406
2024-10-08 14:10:21,550 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=17051.333333333332, ans=0.0
2024-10-08 14:10:28,183 INFO [train.py:1153] Epoch 8, batch 3200, loss[loss=0.2633, simple_loss=0.2917, pruned_loss=0.08509, ctc_loss=0.162, over 4748.00 frames. ], tot_loss[loss=0.2326, simple_loss=0.2693, pruned_loss=0.06943, ctc_loss=0.1428, over 967159.54 frames. ], batch size: 20, lr: 1.15e-02,
2024-10-08 14:10:45,852 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=17058.0, ans=0.125
2024-10-08 14:10:49,272 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.64 vs. limit=20.2935
2024-10-08 14:11:16,544 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.776e+01 1.068e+02 1.157e+02 1.311e+02 1.848e+02, threshold=2.314e+02, percent-clipped=0.0
2024-10-08 14:11:16,745 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=17064.666666666668, ans=0.0
2024-10-08 14:11:30,894 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=18.42 vs. limit=20.301000000000002
2024-10-08 14:11:35,284 INFO [train.py:1153] Epoch 8, batch 3250, loss[loss=0.2314, simple_loss=0.259, pruned_loss=0.07237, ctc_loss=0.1477, over 4839.00 frames. ], tot_loss[loss=0.2328, simple_loss=0.2698, pruned_loss=0.06942, ctc_loss=0.1425, over 967152.59 frames. ], batch size: 24, lr: 1.15e-02,
2024-10-08 14:11:36,882 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=17071.333333333332, ans=0.0
2024-10-08 14:11:45,050 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=17071.333333333332, ans=0.125
2024-10-08 14:11:52,364 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=3.95 vs. limit=13.903
2024-10-08 14:11:54,371 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=17074.666666666668, ans=0.04949747468305833
2024-10-08 14:12:03,786 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=17078.0, ans=0.04949747468305833
2024-10-08 14:12:13,289 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=17078.0, ans=0.0
2024-10-08 14:12:13,305 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=17078.0, ans=0.125
2024-10-08 14:12:17,835 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.66 vs. limit=9.270333333333333
2024-10-08 14:12:25,289 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=17081.333333333332, ans=0.9208133333333333
2024-10-08 14:12:33,561 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=17084.666666666668, ans=0.125
2024-10-08 14:12:42,857 INFO [train.py:1153] Epoch 8, batch 3300, loss[loss=0.2309, simple_loss=0.2642, pruned_loss=0.06959, ctc_loss=0.1462, over 4835.00 frames. ], tot_loss[loss=0.2323, simple_loss=0.2691, pruned_loss=0.06926, ctc_loss=0.1422, over 967736.28 frames. ], batch size: 43, lr: 1.14e-02,
2024-10-08 14:12:44,375 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=17088.0, ans=0.125
2024-10-08 14:12:51,721 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=6.56 vs. limit=7.4176
2024-10-08 14:13:00,225 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=17091.333333333332, ans=0.125
2024-10-08 14:13:15,150 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.31 vs. limit=20.321
2024-10-08 14:13:19,472 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.22 vs. limit=9.273666666666667
2024-10-08 14:13:24,430 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=17098.0, ans=0.125
2024-10-08 14:13:29,602 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=17098.0, ans=0.125
2024-10-08 14:13:30,914 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.002e+01 1.051e+02 1.224e+02 1.374e+02 1.750e+02, threshold=2.448e+02, percent-clipped=0.0
2024-10-08 14:13:32,475 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=17098.0, ans=0.125
2024-10-08 14:13:43,074 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=17101.333333333332, ans=0.125
2024-10-08 14:13:49,622 INFO [train.py:1153] Epoch 8, batch 3350, loss[loss=0.2414, simple_loss=0.2798, pruned_loss=0.06993, ctc_loss=0.1577, over 4781.00 frames. ], tot_loss[loss=0.2364, simple_loss=0.2714, pruned_loss=0.07128, ctc_loss=0.1469, over 966904.06 frames. ], batch size: 40, lr: 1.14e-02,
2024-10-08 14:14:00,435 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=17104.666666666668, ans=0.0
2024-10-08 14:14:16,627 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=17111.333333333332, ans=0.125
2024-10-08 14:14:19,782 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=6.40 vs. limit=13.91675
2024-10-08 14:14:23,282 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=17111.333333333332, ans=0.12888666666666668
2024-10-08 14:14:40,850 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 14:14:56,904 INFO [train.py:1153] Epoch 8, batch 3400, loss[loss=0.1927, simple_loss=0.2455, pruned_loss=0.04943, ctc_loss=0.1025, over 4959.00 frames. ], tot_loss[loss=0.2358, simple_loss=0.2709, pruned_loss=0.07104, ctc_loss=0.1466, over 966617.19 frames. ], batch size: 19, lr: 1.14e-02,
2024-10-08 14:15:05,035 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.min_abs, batch_count=17121.333333333332, ans=0.45682
2024-10-08 14:15:06,395 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=17121.333333333332, ans=0.125
2024-10-08 14:15:14,501 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=17124.666666666668, ans=0.025
2024-10-08 14:15:45,415 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.672e+01 1.074e+02 1.162e+02 1.315e+02 1.704e+02, threshold=2.325e+02, percent-clipped=0.0
2024-10-08 14:15:53,464 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=17134.666666666668, ans=0.125
2024-10-08 14:16:03,984 INFO [train.py:1153] Epoch 8, batch 3450, loss[loss=0.2914, simple_loss=0.3125, pruned_loss=0.09556, ctc_loss=0.1977, over 4848.00 frames. ], tot_loss[loss=0.2353, simple_loss=0.2707, pruned_loss=0.07081, ctc_loss=0.146, over 966807.54 frames. ], batch size: 43, lr: 1.14e-02,
2024-10-08 14:16:04,140 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=17138.0, ans=0.125
2024-10-08 14:16:19,178 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=18.38 vs. limit=13.927999999999999
2024-10-08 14:16:29,336 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=17144.666666666668, ans=0.125
2024-10-08 14:16:55,275 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=1.532e-02
2024-10-08 14:16:59,270 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=17151.333333333332, ans=0.1284866666666667
2024-10-08 14:17:11,298 INFO [train.py:1153] Epoch 8, batch 3500, loss[loss=0.2178, simple_loss=0.267, pruned_loss=0.06143, ctc_loss=0.1141, over 4883.00 frames. ], tot_loss[loss=0.2352, simple_loss=0.2708, pruned_loss=0.07065, ctc_loss=0.1457, over 967155.82 frames. ], batch size: 19, lr: 1.14e-02,
2024-10-08 14:17:29,653 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.08 vs. limit=20.368499999999997
2024-10-08 14:18:00,115 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.682e+01 1.061e+02 1.203e+02 1.354e+02 1.802e+02, threshold=2.405e+02, percent-clipped=0.0
2024-10-08 14:18:18,940 INFO [train.py:1153] Epoch 8, batch 3550, loss[loss=0.2676, simple_loss=0.2952, pruned_loss=0.0848, ctc_loss=0.1761, over 4770.00 frames. ], tot_loss[loss=0.2339, simple_loss=0.2702, pruned_loss=0.06996, ctc_loss=0.144, over 967134.11 frames. ], batch size: 29, lr: 1.14e-02,
2024-10-08 14:18:40,924 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=17174.666666666668, ans=0.125
2024-10-08 14:18:48,985 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=17178.0, ans=0.2987700000000001
2024-10-08 14:19:11,675 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=17184.666666666668, ans=0.04949747468305833
2024-10-08 14:19:26,639 INFO [train.py:1153] Epoch 8, batch 3600, loss[loss=0.2554, simple_loss=0.2829, pruned_loss=0.08076, ctc_loss=0.1662, over 4934.00 frames. ], tot_loss[loss=0.2334, simple_loss=0.2701, pruned_loss=0.06966, ctc_loss=0.1436, over 967290.38 frames. ], batch size: 20, lr: 1.14e-02,
2024-10-08 14:19:44,601 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=17191.333333333332, ans=0.07
2024-10-08 14:19:50,007 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=17191.333333333332, ans=0.12808666666666668
2024-10-08 14:19:54,605 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.22 vs. limit=9.298666666666668
2024-10-08 14:20:03,512 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=17194.666666666668, ans=0.9219466666666667
2024-10-08 14:20:13,504 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=12.95 vs. limit=13.94925
2024-10-08 14:20:15,620 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.832e+01 1.062e+02 1.165e+02 1.309e+02 1.726e+02, threshold=2.331e+02, percent-clipped=0.0
2024-10-08 14:20:19,846 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=17201.333333333332, ans=0.125
2024-10-08 14:20:27,981 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=17201.333333333332, ans=0.2979533333333334
2024-10-08 14:20:34,438 INFO [train.py:1153] Epoch 8, batch 3650, loss[loss=0.2236, simple_loss=0.2544, pruned_loss=0.06961, ctc_loss=0.1341, over 4867.00 frames. ], tot_loss[loss=0.2334, simple_loss=0.2698, pruned_loss=0.06974, ctc_loss=0.1437, over 967856.88 frames. ], batch size: 31, lr: 1.14e-02,
2024-10-08 14:20:59,094 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=17208.0, ans=0.12792
2024-10-08 14:21:15,267 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=17214.666666666668, ans=0.125
2024-10-08 14:21:29,161 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=10.37 vs. limit=13.95675
2024-10-08 14:21:42,161 INFO [train.py:1153] Epoch 8, batch 3700, loss[loss=0.2188, simple_loss=0.2701, pruned_loss=0.0594, ctc_loss=0.1216, over 4844.00 frames. ], tot_loss[loss=0.2316, simple_loss=0.2687, pruned_loss=0.06889, ctc_loss=0.1417, over 967230.98 frames. ], batch size: 24, lr: 1.14e-02,
2024-10-08 14:21:45,030 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer2.prob, batch_count=17221.333333333332, ans=0.125
2024-10-08 14:21:48,977 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=17221.333333333332, ans=0.04070266666666669
2024-10-08 14:21:53,200 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=17221.333333333332, ans=0.09899494936611666
2024-10-08 14:22:14,828 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=17228.0, ans=0.12772
2024-10-08 14:22:17,872 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=4.01 vs. limit=9.307
2024-10-08 14:22:30,966 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.927e+01 1.068e+02 1.220e+02 1.322e+02 2.062e+02, threshold=2.441e+02, percent-clipped=0.0
2024-10-08 14:22:33,882 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=17231.333333333332, ans=0.125
2024-10-08 14:22:37,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=17234.666666666668, ans=0.025
2024-10-08 14:22:49,876 INFO [train.py:1153] Epoch 8, batch 3750, loss[loss=0.1887, simple_loss=0.2298, pruned_loss=0.05142, ctc_loss=0.1121, over 4959.00 frames. ], tot_loss[loss=0.2309, simple_loss=0.2682, pruned_loss=0.0685, ctc_loss=0.1415, over 967663.10 frames. ], batch size: 19, lr: 1.14e-02,
2024-10-08 14:22:56,956 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=17238.0, ans=0.2966700000000001
2024-10-08 14:22:59,670 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=17238.0, ans=0.2966700000000001
2024-10-08 14:23:06,583 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 14:23:18,675 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=17244.666666666668, ans=0.12755333333333332
2024-10-08 14:23:31,437 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=17248.0, ans=0.29632000000000014
2024-10-08 14:23:36,175 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=17248.0, ans=0.125
2024-10-08 14:23:48,482 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=15.74 vs. limit=13.969249999999999
2024-10-08 14:23:54,705 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=17251.333333333332, ans=0.007119275362318841
2024-10-08 14:23:57,209 INFO [train.py:1153] Epoch 8, batch 3800, loss[loss=0.2213, simple_loss=0.2529, pruned_loss=0.06885, ctc_loss=0.1299, over 4708.00 frames. ], tot_loss[loss=0.2278, simple_loss=0.266, pruned_loss=0.06706, ctc_loss=0.1389, over 967414.26 frames. ], batch size: 26, lr: 1.14e-02,
2024-10-08 14:24:01,411 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=17254.666666666668, ans=0.125
2024-10-08 14:24:38,136 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=17264.666666666668, ans=0.025
2024-10-08 14:24:46,146 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.568e+01 1.046e+02 1.144e+02 1.270e+02 1.821e+02, threshold=2.289e+02, percent-clipped=0.0
2024-10-08 14:24:56,032 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.38 vs. limit=5.590199999999999
2024-10-08 14:25:04,951 INFO [train.py:1153] Epoch 8, batch 3850, loss[loss=0.2347, simple_loss=0.2745, pruned_loss=0.07053, ctc_loss=0.1345, over 4821.00 frames. ], tot_loss[loss=0.2276, simple_loss=0.2661, pruned_loss=0.06702, ctc_loss=0.1379, over 967314.94 frames. ], batch size: 38, lr: 1.14e-02,
2024-10-08 14:25:17,736 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=5.01 vs. limit=10.909866666666666
2024-10-08 14:25:58,407 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.34 vs. limit=13.981750000000002
2024-10-08 14:26:01,629 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=17284.666666666668, ans=0.1271533333333333
2024-10-08 14:26:10,989 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=17288.0, ans=0.125
2024-10-08 14:26:12,180 INFO [train.py:1153] Epoch 8, batch 3900, loss[loss=0.2025, simple_loss=0.2562, pruned_loss=0.05399, ctc_loss=0.102, over 4747.00 frames. ], tot_loss[loss=0.2297, simple_loss=0.2676, pruned_loss=0.06794, ctc_loss=0.1397, over 966830.24 frames. ], batch size: 26, lr: 1.14e-02,
2024-10-08 14:26:28,736 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.43 vs. limit=9.322833333333332
2024-10-08 14:27:00,665 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.716e+01 1.045e+02 1.187e+02 1.348e+02 2.026e+02, threshold=2.374e+02, percent-clipped=0.0
2024-10-08 14:27:03,569 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=17298.0, ans=0.0
2024-10-08 14:27:10,517 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=17301.333333333332, ans=0.00710840579710145
2024-10-08 14:27:15,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=17301.333333333332, ans=0.00710840579710145
2024-10-08 14:27:19,665 INFO [train.py:1153] Epoch 8, batch 3950, loss[loss=0.2346, simple_loss=0.2709, pruned_loss=0.06934, ctc_loss=0.1489, over 4828.00 frames. ], tot_loss[loss=0.2283, simple_loss=0.2669, pruned_loss=0.06716, ctc_loss=0.1384, over 967033.93 frames. ], batch size: 36, lr: 1.14e-02,
2024-10-08 14:27:22,524 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=17304.666666666668, ans=0.1269533333333333
2024-10-08 14:27:37,393 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=17308.0, ans=0.125
2024-10-08 14:27:46,822 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=17311.333333333332, ans=0.125
2024-10-08 14:28:07,339 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=17314.666666666668, ans=0.29398666666666673
2024-10-08 14:28:16,582 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=17318.0, ans=0.125
2024-10-08 14:28:24,999 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.87 vs. limit=9.3295
2024-10-08 14:28:27,081 INFO [train.py:1153] Epoch 8, batch 4000, loss[loss=0.2247, simple_loss=0.2585, pruned_loss=0.067, ctc_loss=0.1423, over 4813.00 frames. ], tot_loss[loss=0.2283, simple_loss=0.2664, pruned_loss=0.06742, ctc_loss=0.1385, over 967080.77 frames. ], batch size: 19, lr: 1.14e-02,
2024-10-08 14:28:36,871 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=17321.333333333332, ans=0.125
2024-10-08 14:28:54,369 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=17328.0, ans=0.05
2024-10-08 14:29:09,419 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=17331.333333333332, ans=0.125
2024-10-08 14:29:09,434 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=17331.333333333332, ans=0.025
2024-10-08 14:29:14,861 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/checkpoint-52000.pt
2024-10-08 14:29:16,832 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.243e+01 1.055e+02 1.161e+02 1.353e+02 2.138e+02, threshold=2.321e+02, percent-clipped=0.0
2024-10-08 14:29:29,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=17334.666666666668, ans=0.125
2024-10-08 14:29:34,779 INFO [train.py:1153] Epoch 8, batch 4050, loss[loss=0.2554, simple_loss=0.2722, pruned_loss=0.08577, ctc_loss=0.1673, over 4782.00 frames. ], tot_loss[loss=0.2285, simple_loss=0.2663, pruned_loss=0.06753, ctc_loss=0.1394, over 967382.53 frames. ], batch size: 53, lr: 1.14e-02,
2024-10-08 14:29:34,936 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=17338.0, ans=0.12661999999999998
2024-10-08 14:29:35,578 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.16 vs. limit=10.9352
2024-10-08 14:29:49,488 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=17341.333333333332, ans=0.125
2024-10-08 14:30:02,669 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff2_skip_rate, batch_count=17344.666666666668, ans=0.007098985507246377
2024-10-08 14:30:41,328 INFO [train.py:1153] Epoch 8, batch 4100, loss[loss=0.2521, simple_loss=0.2827, pruned_loss=0.07932, ctc_loss=0.1569, over 4844.00 frames. ], tot_loss[loss=0.2286, simple_loss=0.2663, pruned_loss=0.06757, ctc_loss=0.1395, over 966862.27 frames. ], batch size: 31, lr: 1.14e-02,
2024-10-08 14:30:41,472 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=17354.666666666668, ans=0.1264533333333333
2024-10-08 14:30:44,240 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=17354.666666666668, ans=0.1264533333333333
2024-10-08 14:30:49,773 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=17354.666666666668, ans=0.125
2024-10-08 14:31:17,439 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=17361.333333333332, ans=0.125
2024-10-08 14:31:26,671 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=17364.666666666668, ans=0.125
2024-10-08 14:31:29,081 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.057e+01 1.080e+02 1.203e+02 1.401e+02 1.633e+02, threshold=2.406e+02, percent-clipped=0.0
2024-10-08 14:31:45,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=17368.0, ans=0.0
2024-10-08 14:31:47,996 INFO [train.py:1153] Epoch 8, batch 4150, loss[loss=0.2423, simple_loss=0.2801, pruned_loss=0.07069, ctc_loss=0.1577, over 4753.00 frames. ], tot_loss[loss=0.2283, simple_loss=0.2659, pruned_loss=0.06739, ctc_loss=0.1397, over 966987.38 frames. ], batch size: 20, lr: 1.14e-02,
2024-10-08 14:32:05,717 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=17374.666666666668, ans=0.29188666666666674
2024-10-08 14:32:17,733 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=17378.0, ans=0.125
2024-10-08 14:32:39,226 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=17381.333333333332, ans=0.007091014492753624
2024-10-08 14:32:39,316 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=17381.333333333332, ans=0.1261866666666667
2024-10-08 14:32:40,611 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=17384.666666666668, ans=0.125
2024-10-08 14:32:48,708 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=17384.666666666668, ans=0.125
2024-10-08 14:32:51,341 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=17384.666666666668, ans=0.07
2024-10-08 14:32:55,225 INFO [train.py:1153] Epoch 8, batch 4200, loss[loss=0.2622, simple_loss=0.2982, pruned_loss=0.07892, ctc_loss=0.1709, over 4857.00 frames. ], tot_loss[loss=0.2287, simple_loss=0.2665, pruned_loss=0.06743, ctc_loss=0.1401, over 967163.27 frames. ], batch size: 31, lr: 1.14e-02,
2024-10-08 14:32:59,505 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=17388.0, ans=0.0070895652173913045
2024-10-08 14:33:12,991 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.min_positive, batch_count=17391.333333333332, ans=0.07608666666666669
2024-10-08 14:33:22,346 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=17394.666666666668, ans=0.125
2024-10-08 14:33:25,175 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=17394.666666666668, ans=0.0
2024-10-08 14:33:32,770 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=16.85 vs. limit=20.546
2024-10-08 14:33:44,266 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.962e+01 1.037e+02 1.155e+02 1.293e+02 1.600e+02, threshold=2.310e+02, percent-clipped=0.0
2024-10-08 14:34:03,047 INFO [train.py:1153] Epoch 8, batch 4250, loss[loss=0.1831, simple_loss=0.2462, pruned_loss=0.04107, ctc_loss=0.09491, over 4754.00 frames. ], tot_loss[loss=0.2289, simple_loss=0.2672, pruned_loss=0.06728, ctc_loss=0.1402, over 967021.63 frames. ], batch size: 19, lr: 1.13e-02,
2024-10-08 14:34:10,345 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=3.93 vs. limit=9.351166666666668
2024-10-08 14:34:18,208 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=17408.0, ans=0.125
2024-10-08 14:34:34,389 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=17411.333333333332, ans=0.125
2024-10-08 14:34:37,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=17411.333333333332, ans=0.07
2024-10-08 14:34:49,551 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=10.58 vs. limit=14.0305
2024-10-08 14:34:59,015 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.74 vs. limit=14.031749999999999
2024-10-08 14:35:05,731 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2.whitening_limit, batch_count=17418.0, ans=13.709
2024-10-08 14:35:07,117 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.73 vs. limit=20.563499999999998
2024-10-08 14:35:10,663 INFO [train.py:1153] Epoch 8, batch 4300, loss[loss=0.2195, simple_loss=0.2595, pruned_loss=0.06356, ctc_loss=0.131, over 4840.00 frames. ], tot_loss[loss=0.2303, simple_loss=0.268, pruned_loss=0.06801, ctc_loss=0.1416, over 967356.40 frames. ], batch size: 21, lr: 1.13e-02,
2024-10-08 14:35:10,843 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=17421.333333333332, ans=0.0
2024-10-08 14:35:25,415 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=17424.666666666668, ans=0.0
2024-10-08 14:35:41,532 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 14:35:47,251 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.66 vs. limit=10.9712
2024-10-08 14:35:58,659 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.876e+01 1.030e+02 1.172e+02 1.327e+02 2.264e+02, threshold=2.343e+02, percent-clipped=0.0
2024-10-08 14:36:00,239 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=17431.333333333332, ans=0.1256866666666667
2024-10-08 14:36:13,627 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=17434.666666666668, ans=0.28978666666666675
2024-10-08 14:36:17,577 INFO [train.py:1153] Epoch 8, batch 4350, loss[loss=0.2016, simple_loss=0.2457, pruned_loss=0.05378, ctc_loss=0.1247, over 4845.00 frames. ], tot_loss[loss=0.2299, simple_loss=0.2676, pruned_loss=0.06785, ctc_loss=0.1412, over 966297.90 frames. ], batch size: 21, lr: 1.13e-02,
2024-10-08 14:36:21,706 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=17438.0, ans=0.0
2024-10-08 14:36:31,121 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=17441.333333333332, ans=0.0
2024-10-08 14:36:53,027 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=15.51 vs. limit=14.04175
2024-10-08 14:37:04,790 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=17448.0, ans=0.09899494936611666
2024-10-08 14:37:15,943 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=13.62 vs. limit=14.044249999999998
2024-10-08 14:37:20,383 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=8.23 vs. limit=14.044249999999998
2024-10-08 14:37:22,448 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 14:37:24,867 INFO [train.py:1153] Epoch 8, batch 4400, loss[loss=0.2218, simple_loss=0.2686, pruned_loss=0.06287, ctc_loss=0.1231, over 4731.00 frames. ], tot_loss[loss=0.2279, simple_loss=0.2661, pruned_loss=0.06707, ctc_loss=0.1389, over 965902.25 frames. ], batch size: 26, lr: 1.13e-02,
2024-10-08 14:37:37,069 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=17458.0, ans=0.0
2024-10-08 14:37:38,819 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=3.87 vs. limit=10.9832
2024-10-08 14:37:39,909 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=17458.0, ans=0.007074347826086957
2024-10-08 14:37:53,675 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.min_positive, batch_count=17461.333333333332, ans=0.05
2024-10-08 14:38:13,274 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.457e+01 1.117e+02 1.217e+02 1.340e+02 1.777e+02, threshold=2.435e+02, percent-clipped=0.0
2024-10-08 14:38:26,755 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=17468.0, ans=0.2886200000000001
2024-10-08 14:38:31,915 INFO [train.py:1153] Epoch 8, batch 4450, loss[loss=0.1985, simple_loss=0.2507, pruned_loss=0.04965, ctc_loss=0.1175, over 4883.00 frames. ], tot_loss[loss=0.2313, simple_loss=0.2683, pruned_loss=0.06879, ctc_loss=0.1417, over 966155.54 frames. ], batch size: 19, lr: 1.13e-02,
2024-10-08 14:38:41,502 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 14:38:50,723 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=17474.666666666668, ans=0.125
2024-10-08 14:38:53,359 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.prob, batch_count=17474.666666666668, ans=0.125
2024-10-08 14:38:54,725 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=17474.666666666668, ans=0.2883866666666667
2024-10-08 14:38:58,799 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=17478.0, ans=0.125
2024-10-08 14:39:05,382 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=17478.0, ans=0.12522
2024-10-08 14:39:22,903 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=17481.333333333332, ans=0.125
2024-10-08 14:39:38,648 INFO [train.py:1153] Epoch 8, batch 4500, loss[loss=0.242, simple_loss=0.278, pruned_loss=0.07072, ctc_loss=0.1612, over 4828.00 frames. ], tot_loss[loss=0.2316, simple_loss=0.2685, pruned_loss=0.06891, ctc_loss=0.1422, over 966178.33 frames. ], batch size: 28, lr: 1.13e-02,
2024-10-08 14:39:52,265 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=17491.333333333332, ans=0.12508666666666668
2024-10-08 14:40:01,732 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=17491.333333333332, ans=0.125
2024-10-08 14:40:12,541 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=17494.666666666668, ans=0.125
2024-10-08 14:40:14,389 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.83 vs. limit=13.747333333333334
2024-10-08 14:40:27,000 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.237e+01 1.075e+02 1.171e+02 1.326e+02 1.763e+02, threshold=2.342e+02, percent-clipped=0.0
2024-10-08 14:40:45,528 INFO [train.py:1153] Epoch 8, batch 4550, loss[loss=0.2156, simple_loss=0.2486, pruned_loss=0.06551, ctc_loss=0.1287, over 4864.00 frames. ], tot_loss[loss=0.231, simple_loss=0.2682, pruned_loss=0.06862, ctc_loss=0.1415, over 965975.56 frames. ], batch size: 20, lr: 1.13e-02,
2024-10-08 14:41:04,696 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=17508.0, ans=0.007063478260869565
2024-10-08 14:41:07,982 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=19.13 vs. limit=20.631
2024-10-08 14:41:22,153 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=17511.333333333332, ans=0.025
2024-10-08 14:41:30,159 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=17514.666666666668, ans=0.125
2024-10-08 14:41:38,347 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=17518.0, ans=0.0
2024-10-08 14:41:40,332 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=5.44 vs. limit=11.007200000000001
2024-10-08 14:41:41,295 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.47 vs. limit=5.6277
2024-10-08 14:41:52,808 INFO [train.py:1153] Epoch 8, batch 4600, loss[loss=0.2445, simple_loss=0.2702, pruned_loss=0.07682, ctc_loss=0.163, over 4750.00 frames. ], tot_loss[loss=0.2304, simple_loss=0.2678, pruned_loss=0.06834, ctc_loss=0.1409, over 966255.39 frames. ], batch size: 45, lr: 1.13e-02,
2024-10-08 14:42:01,125 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=17521.333333333332, ans=0.2867533333333334
2024-10-08 14:42:09,031 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=17524.666666666668, ans=0.125
2024-10-08 14:42:09,094 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=17524.666666666668, ans=0.125
2024-10-08 14:42:41,016 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.797e+01 1.076e+02 1.206e+02 1.383e+02 2.045e+02, threshold=2.413e+02, percent-clipped=0.0
2024-10-08 14:42:59,696 INFO [train.py:1153] Epoch 8, batch 4650, loss[loss=0.2774, simple_loss=0.3024, pruned_loss=0.09023, ctc_loss=0.1798, over 4818.00 frames. ], tot_loss[loss=0.231, simple_loss=0.2679, pruned_loss=0.06865, ctc_loss=0.1417, over 965719.96 frames. ], batch size: 36, lr: 1.13e-02,
2024-10-08 14:43:09,248 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=17538.0, ans=0.125
2024-10-08 14:43:17,311 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=17541.333333333332, ans=0.007056231884057971
2024-10-08 14:43:34,573 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=17544.666666666668, ans=0.0
2024-10-08 14:44:06,850 INFO [train.py:1153] Epoch 8, batch 4700, loss[loss=0.1968, simple_loss=0.2511, pruned_loss=0.04821, ctc_loss=0.1152, over 4940.00 frames. ], tot_loss[loss=0.2306, simple_loss=0.2679, pruned_loss=0.06836, ctc_loss=0.1414, over 965693.73 frames. ], batch size: 19, lr: 1.13e-02,
2024-10-08 14:44:46,222 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.06 vs. limit=14.08675
2024-10-08 14:44:49,689 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=17564.666666666668, ans=0.125
2024-10-08 14:44:50,562 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=3.77 vs. limit=14.08675
2024-10-08 14:44:55,027 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.828e+01 1.048e+02 1.182e+02 1.319e+02 1.795e+02, threshold=2.364e+02, percent-clipped=0.0
2024-10-08 14:45:05,934 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=17568.0, ans=0.125
2024-10-08 14:45:08,962 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.18 vs. limit=20.676000000000002
2024-10-08 14:45:09,903 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=17568.0, ans=0.125
2024-10-08 14:45:12,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=17571.333333333332, ans=0.07
2024-10-08 14:45:13,716 INFO [train.py:1153] Epoch 8, batch 4750, loss[loss=0.2299, simple_loss=0.258, pruned_loss=0.07301, ctc_loss=0.1397, over 4752.00 frames. ], tot_loss[loss=0.2315, simple_loss=0.2685, pruned_loss=0.06889, ctc_loss=0.1421, over 965607.89 frames. ], batch size: 45, lr: 1.13e-02,
2024-10-08 14:45:22,401 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.75 vs. limit=9.392833333333332
2024-10-08 14:45:28,660 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=17574.666666666668, ans=0.125
2024-10-08 14:45:34,622 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.77 vs. limit=14.090499999999999
2024-10-08 14:45:43,674 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=17578.0, ans=0.2847700000000001
2024-10-08 14:46:21,345 INFO [train.py:1153] Epoch 8, batch 4800, loss[loss=0.243, simple_loss=0.2716, pruned_loss=0.07787, ctc_loss=0.1467, over 4870.00 frames. ], tot_loss[loss=0.2308, simple_loss=0.2681, pruned_loss=0.06851, ctc_loss=0.1411, over 965950.49 frames. ], batch size: 22, lr: 1.13e-02,
2024-10-08 14:46:42,297 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.73 vs. limit=14.09675
2024-10-08 14:46:43,266 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=17591.333333333332, ans=0.125
2024-10-08 14:46:47,354 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=1.401e-01
2024-10-08 14:46:48,540 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=17594.666666666668, ans=0.04949747468305833
2024-10-08 14:46:52,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=17594.666666666668, ans=0.125
2024-10-08 14:46:54,278 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=5.00 vs. limit=11.037866666666666
2024-10-08 14:46:59,898 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.08 vs. limit=9.398666666666667
2024-10-08 14:47:00,659 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 14:47:03,336 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=17598.0, ans=0.0070439130434782615
2024-10-08 14:47:09,996 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.409e+01 1.029e+02 1.197e+02 1.372e+02 2.015e+02, threshold=2.394e+02, percent-clipped=0.0
2024-10-08 14:47:28,688 INFO [train.py:1153] Epoch 8, batch 4850, loss[loss=0.1987, simple_loss=0.2405, pruned_loss=0.0521, ctc_loss=0.1316, over 4838.00 frames. ], tot_loss[loss=0.2298, simple_loss=0.2674, pruned_loss=0.06803, ctc_loss=0.1401, over 966577.97 frames. ], batch size: 28, lr: 1.13e-02,
2024-10-08 14:47:28,877 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=17604.666666666668, ans=0.007042463768115942
2024-10-08 14:47:49,028 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=17608.0, ans=0.0
2024-10-08 14:47:53,135 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=17608.0, ans=0.125
2024-10-08 14:48:15,972 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=17614.666666666668, ans=0.007040289855072464
2024-10-08 14:48:19,982 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=17614.666666666668, ans=0.125
2024-10-08 14:48:36,026 INFO [train.py:1153] Epoch 8, batch 4900, loss[loss=0.2025, simple_loss=0.2496, pruned_loss=0.05517, ctc_loss=0.1128, over 4838.00 frames. ], tot_loss[loss=0.2293, simple_loss=0.267, pruned_loss=0.06788, ctc_loss=0.1396, over 967203.75 frames. ], batch size: 21, lr: 1.13e-02,
2024-10-08 14:48:57,604 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=17624.666666666668, ans=0.007038115942028985
2024-10-08 14:48:57,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=17624.666666666668, ans=0.125
2024-10-08 14:49:04,385 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=17628.0, ans=0.125
2024-10-08 14:49:24,336 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.028e+01 1.106e+02 1.222e+02 1.398e+02 2.466e+02, threshold=2.444e+02, percent-clipped=1.0
2024-10-08 14:49:42,865 INFO [train.py:1153] Epoch 8, batch 4950, loss[loss=0.27, simple_loss=0.2881, pruned_loss=0.08691, ctc_loss=0.1953, over 4766.00 frames. ], tot_loss[loss=0.232, simple_loss=0.269, pruned_loss=0.06911, ctc_loss=0.1422, over 966777.49 frames. ], batch size: 53, lr: 1.13e-02,
2024-10-08 14:50:03,471 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=17641.333333333332, ans=0.125
2024-10-08 14:50:16,942 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=17644.666666666668, ans=0.125
2024-10-08 14:50:23,572 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=17648.0, ans=0.125
2024-10-08 14:50:27,133 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.02 vs. limit=9.411999999999999
2024-10-08 14:50:33,580 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=17648.0, ans=0.125
2024-10-08 14:50:50,803 INFO [train.py:1153] Epoch 8, batch 5000, loss[loss=0.226, simple_loss=0.2564, pruned_loss=0.06728, ctc_loss=0.1526, over 4789.00 frames. ], tot_loss[loss=0.2295, simple_loss=0.2673, pruned_loss=0.06789, ctc_loss=0.1398, over 967647.56 frames. ], batch size: 29, lr: 1.13e-02,
2024-10-08 14:50:50,963 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=17654.666666666668, ans=0.125
2024-10-08 14:50:58,994 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=17654.666666666668, ans=0.12345333333333333
2024-10-08 14:51:11,214 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=17658.0, ans=0.025
2024-10-08 14:51:16,716 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=17661.333333333332, ans=0.2818533333333334
2024-10-08 14:51:19,283 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=17661.333333333332, ans=0.025
2024-10-08 14:51:30,030 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=17664.666666666668, ans=0.12335333333333331
2024-10-08 14:51:39,335 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.436e+01 1.053e+02 1.176e+02 1.328e+02 1.910e+02, threshold=2.353e+02, percent-clipped=0.0
2024-10-08 14:51:40,840 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=17664.666666666668, ans=0.0
2024-10-08 14:51:44,982 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=17668.0, ans=0.07
2024-10-08 14:51:58,431 INFO [train.py:1153] Epoch 8, batch 5050, loss[loss=0.2063, simple_loss=0.2478, pruned_loss=0.05735, ctc_loss=0.1252, over 4858.00 frames. ], tot_loss[loss=0.2285, simple_loss=0.2662, pruned_loss=0.06749, ctc_loss=0.1397, over 968679.60 frames. ], batch size: 19, lr: 1.13e-02,
2024-10-08 14:52:14,559 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=17674.666666666668, ans=0.12325333333333333
2024-10-08 14:52:18,892 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.28 vs. limit=20.756
2024-10-08 14:52:22,524 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=17674.666666666668, ans=0.0
2024-10-08 14:52:39,033 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.61 vs. limit=14.1305
2024-10-08 14:52:45,436 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=5.968e-02
2024-10-08 14:52:49,271 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=17681.333333333332, ans=0.1231866666666667
2024-10-08 14:53:05,169 INFO [train.py:1153] Epoch 8, batch 5100, loss[loss=0.1808, simple_loss=0.2385, pruned_loss=0.04225, ctc_loss=0.09658, over 4812.00 frames. ], tot_loss[loss=0.2306, simple_loss=0.2676, pruned_loss=0.06859, ctc_loss=0.141, over 967909.66 frames. ], batch size: 19, lr: 1.13e-02,
2024-10-08 14:53:28,380 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=17691.333333333332, ans=0.125
2024-10-08 14:53:37,783 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=17694.666666666668, ans=0.025
2024-10-08 14:53:49,958 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=17698.0, ans=0.09899494936611666
2024-10-08 14:53:53,838 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.372e+01 1.039e+02 1.175e+02 1.300e+02 2.180e+02, threshold=2.349e+02, percent-clipped=0.0
2024-10-08 14:54:12,500 INFO [train.py:1153] Epoch 8, batch 5150, loss[loss=0.2622, simple_loss=0.2918, pruned_loss=0.08396, ctc_loss=0.1617, over 4828.00 frames. ], tot_loss[loss=0.2309, simple_loss=0.268, pruned_loss=0.0687, ctc_loss=0.141, over 968032.42 frames. ], batch size: 36, lr: 1.13e-02,
2024-10-08 14:54:38,234 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer_ff2.min_abs, batch_count=17711.333333333332, ans=0.1
2024-10-08 14:54:42,141 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=17711.333333333332, ans=0.28010333333333337
2024-10-08 14:54:42,234 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=17711.333333333332, ans=0.0
2024-10-08 14:55:19,784 INFO [train.py:1153] Epoch 8, batch 5200, loss[loss=0.2362, simple_loss=0.265, pruned_loss=0.07254, ctc_loss=0.156, over 4771.00 frames. ], tot_loss[loss=0.2305, simple_loss=0.2677, pruned_loss=0.06849, ctc_loss=0.1407, over 967521.15 frames. ], batch size: 29, lr: 1.12e-02,
2024-10-08 14:55:33,450 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=17724.666666666668, ans=0.007016376811594203
2024-10-08 14:55:38,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=17724.666666666668, ans=0.27963666666666676
2024-10-08 14:55:49,718 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=7.061e-02
2024-10-08 14:55:50,971 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=17728.0, ans=0.125
2024-10-08 14:56:08,549 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.812e+01 1.084e+02 1.201e+02 1.363e+02 1.793e+02, threshold=2.402e+02, percent-clipped=0.0
2024-10-08 14:56:21,336 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=17.46 vs. limit=14.150500000000001
2024-10-08 14:56:27,739 INFO [train.py:1153] Epoch 8, batch 5250, loss[loss=0.2168, simple_loss=0.2626, pruned_loss=0.05984, ctc_loss=0.1281, over 4856.00 frames. ], tot_loss[loss=0.229, simple_loss=0.2666, pruned_loss=0.06777, ctc_loss=0.1394, over 967720.71 frames. ], batch size: 20, lr: 1.12e-02,
2024-10-08 14:56:27,900 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=17738.0, ans=0.125
2024-10-08 14:56:35,825 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=17738.0, ans=0.125
2024-10-08 14:56:46,961 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten.whitening_limit, batch_count=17741.333333333332, ans=14.152999999999999
2024-10-08 14:56:55,822 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=17744.666666666668, ans=0.12255333333333332
2024-10-08 14:56:57,098 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=17744.666666666668, ans=0.007012028985507246
2024-10-08 14:57:06,593 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=17748.0, ans=0.0
2024-10-08 14:57:09,258 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=17748.0, ans=0.27882000000000007
2024-10-08 14:57:16,079 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=17748.0, ans=0.125
2024-10-08 14:57:21,561 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=17751.333333333332, ans=0.125
2024-10-08 14:57:34,797 INFO [train.py:1153] Epoch 8, batch 5300, loss[loss=0.2863, simple_loss=0.3074, pruned_loss=0.09295, ctc_loss=0.1984, over 4810.00 frames. ], tot_loss[loss=0.2305, simple_loss=0.2678, pruned_loss=0.06844, ctc_loss=0.1409, over 967717.72 frames. ], batch size: 38, lr: 1.12e-02,
2024-10-08 14:57:37,655 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=17754.666666666668, ans=0.125
2024-10-08 14:57:43,465 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=10.13 vs. limit=14.158000000000001
2024-10-08 14:57:53,763 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=17758.0, ans=0.125
2024-10-08 14:58:07,188 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=17761.333333333332, ans=0.125
2024-10-08 14:58:16,398 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=17764.666666666668, ans=0.0070076811594202894
2024-10-08 14:58:22,872 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.619e+01 1.116e+02 1.218e+02 1.369e+02 1.708e+02, threshold=2.435e+02, percent-clipped=0.0
2024-10-08 14:58:27,263 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=17768.0, ans=0.125
2024-10-08 14:58:36,479 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=17768.0, ans=0.125
2024-10-08 14:58:41,753 INFO [train.py:1153] Epoch 8, batch 5350, loss[loss=0.2113, simple_loss=0.2525, pruned_loss=0.06153, ctc_loss=0.1178, over 4978.00 frames. ], tot_loss[loss=0.2312, simple_loss=0.2682, pruned_loss=0.06877, ctc_loss=0.1416, over 967137.95 frames. ], batch size: 19, lr: 1.12e-02,
2024-10-08 14:58:54,159 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=17774.666666666668, ans=0.0
2024-10-08 14:58:55,593 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 14:59:11,532 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=17778.0, ans=0.125
2024-10-08 14:59:14,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=17778.0, ans=0.07221999999999998
2024-10-08 14:59:18,386 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.25 vs. limit=14.16675
2024-10-08 14:59:26,247 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=17781.333333333332, ans=0.125
2024-10-08 14:59:36,103 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.50 vs. limit=13.892333333333335
2024-10-08 14:59:47,624 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=17788.0, ans=0.007002608695652174
2024-10-08 14:59:47,660 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=17788.0, ans=0.125
2024-10-08 14:59:48,740 INFO [train.py:1153] Epoch 8, batch 5400, loss[loss=0.2739, simple_loss=0.2979, pruned_loss=0.08894, ctc_loss=0.1801, over 4774.00 frames. ], tot_loss[loss=0.2331, simple_loss=0.2694, pruned_loss=0.06971, ctc_loss=0.1434, over 966489.81 frames. ], batch size: 49, lr: 1.12e-02,
2024-10-08 14:59:52,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=17788.0, ans=0.0
2024-10-08 14:59:59,704 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=17788.0, ans=0.0
2024-10-08 15:00:10,744 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.91 vs. limit=14.17175
2024-10-08 15:00:37,256 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.611e+01 1.063e+02 1.174e+02 1.311e+02 1.648e+02, threshold=2.348e+02, percent-clipped=0.0
2024-10-08 15:00:49,558 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 15:00:55,994 INFO [train.py:1153] Epoch 8, batch 5450, loss[loss=0.2054, simple_loss=0.2602, pruned_loss=0.05215, ctc_loss=0.1157, over 4940.00 frames. ], tot_loss[loss=0.2305, simple_loss=0.2681, pruned_loss=0.06824, ctc_loss=0.1408, over 967213.71 frames. ], batch size: 19, lr: 1.12e-02,
2024-10-08 15:00:57,959 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.62 vs. limit=20.8535
2024-10-08 15:01:14,770 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=17808.0, ans=0.12192
2024-10-08 15:01:18,967 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=17808.0, ans=0.125
2024-10-08 15:01:57,165 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=14.62 vs. limit=14.181750000000001
2024-10-08 15:02:03,316 INFO [train.py:1153] Epoch 8, batch 5500, loss[loss=0.2726, simple_loss=0.2875, pruned_loss=0.09039, ctc_loss=0.1923, over 4764.00 frames. ], tot_loss[loss=0.2312, simple_loss=0.2686, pruned_loss=0.06852, ctc_loss=0.1417, over 967553.88 frames. ], batch size: 49, lr: 1.12e-02,
2024-10-08 15:02:08,761 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=17821.333333333332, ans=0.27625333333333335
2024-10-08 15:02:17,392 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.43 vs. limit=20.868499999999997
2024-10-08 15:02:23,505 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=17824.666666666668, ans=0.2761366666666667
2024-10-08 15:02:28,852 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=17828.0, ans=0.0
2024-10-08 15:02:28,870 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=17828.0, ans=0.0
2024-10-08 15:02:28,876 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=17828.0, ans=0.125
2024-10-08 15:02:44,738 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=17831.333333333332, ans=0.04949747468305833
2024-10-08 15:02:51,149 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.895e+01 1.032e+02 1.230e+02 1.353e+02 1.831e+02, threshold=2.461e+02, percent-clipped=0.0
2024-10-08 15:03:01,985 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=17834.666666666668, ans=0.12165333333333331
2024-10-08 15:03:06,125 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=17834.666666666668, ans=0.125
2024-10-08 15:03:09,930 INFO [train.py:1153] Epoch 8, batch 5550, loss[loss=0.2318, simple_loss=0.265, pruned_loss=0.07007, ctc_loss=0.146, over 4800.00 frames. ], tot_loss[loss=0.2295, simple_loss=0.2674, pruned_loss=0.06779, ctc_loss=0.14, over 967224.80 frames. ], batch size: 19, lr: 1.12e-02,
2024-10-08 15:03:13,278 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.54 vs. limit=20.878500000000003
2024-10-08 15:03:20,720 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=17838.0, ans=0.12162
2024-10-08 15:03:27,723 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 15:03:30,519 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=4.06 vs. limit=9.460333333333333
2024-10-08 15:03:31,571 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=17841.333333333332, ans=0.025
2024-10-08 15:03:41,002 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=17844.666666666668, ans=0.27543666666666666
2024-10-08 15:04:04,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=17851.333333333332, ans=0.025
2024-10-08 15:04:16,843 INFO [train.py:1153] Epoch 8, batch 5600, loss[loss=0.2124, simple_loss=0.2546, pruned_loss=0.05995, ctc_loss=0.1257, over 4872.00 frames. ], tot_loss[loss=0.2297, simple_loss=0.2673, pruned_loss=0.06789, ctc_loss=0.1408, over 967154.97 frames. ], batch size: 28, lr: 1.12e-02,
2024-10-08 15:04:37,298 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=17858.0, ans=0.12142
2024-10-08 15:04:39,311 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=3.60 vs. limit=11.1432
2024-10-08 15:04:41,914 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=3.99 vs. limit=14.19675
2024-10-08 15:05:05,497 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.249e+01 1.045e+02 1.178e+02 1.317e+02 1.760e+02, threshold=2.356e+02, percent-clipped=0.0
2024-10-08 15:05:12,214 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=17868.0, ans=0.0069852173913043485
2024-10-08 15:05:21,578 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=17868.0, ans=0.125
2024-10-08 15:05:22,982 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=17871.333333333332, ans=0.27450333333333343
2024-10-08 15:05:24,077 INFO [train.py:1153] Epoch 8, batch 5650, loss[loss=0.2854, simple_loss=0.3001, pruned_loss=0.09742, ctc_loss=0.1894, over 4737.00 frames. ], tot_loss[loss=0.2294, simple_loss=0.2674, pruned_loss=0.06767, ctc_loss=0.14, over 967118.53 frames. ], batch size: 45, lr: 1.12e-02,
2024-10-08 15:05:52,178 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=17878.0, ans=0.125
2024-10-08 15:06:02,784 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=17881.333333333332, ans=0.07
2024-10-08 15:06:11,515 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.28 vs. limit=9.470333333333333
2024-10-08 15:06:20,575 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.71 vs. limit=20.9135
2024-10-08 15:06:21,551 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=17884.666666666668, ans=0.0
2024-10-08 15:06:30,694 INFO [train.py:1153] Epoch 8, batch 5700, loss[loss=0.1966, simple_loss=0.247, pruned_loss=0.05224, ctc_loss=0.1044, over 4871.00 frames. ], tot_loss[loss=0.2295, simple_loss=0.2674, pruned_loss=0.06786, ctc_loss=0.1399, over 966457.15 frames. ], batch size: 22, lr: 1.12e-02,
2024-10-08 15:06:31,641 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module2.whiten, num_groups=1, num_channels=192, metric=2.89 vs. limit=14.208
2024-10-08 15:06:34,145 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.47 vs. limit=20.916
2024-10-08 15:07:07,645 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.34 vs. limit=14.2105
2024-10-08 15:07:16,447 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=17898.0, ans=0.12101999999999999
2024-10-08 15:07:18,887 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.252e+01 1.033e+02 1.170e+02 1.347e+02 1.794e+02, threshold=2.339e+02, percent-clipped=0.0
2024-10-08 15:07:22,262 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=5.79 vs. limit=5.684699999999999
2024-10-08 15:07:26,260 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=17.16 vs. limit=14.213000000000001
2024-10-08 15:07:29,111 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.36 vs. limit=5.6852
2024-10-08 15:07:37,810 INFO [train.py:1153] Epoch 8, batch 5750, loss[loss=0.2878, simple_loss=0.2993, pruned_loss=0.099, ctc_loss=0.1959, over 4830.00 frames. ], tot_loss[loss=0.2307, simple_loss=0.2683, pruned_loss=0.06838, ctc_loss=0.1409, over 966828.07 frames. ], batch size: 43, lr: 1.12e-02,
2024-10-08 15:07:51,268 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=17908.0, ans=0.125
2024-10-08 15:08:30,879 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=17918.0, ans=0.0
2024-10-08 15:08:43,951 INFO [train.py:1153] Epoch 8, batch 5800, loss[loss=0.1995, simple_loss=0.2383, pruned_loss=0.0541, ctc_loss=0.1313, over 4835.00 frames. ], tot_loss[loss=0.2327, simple_loss=0.2697, pruned_loss=0.06931, ctc_loss=0.1429, over 966220.21 frames. ], batch size: 43, lr: 1.12e-02,
2024-10-08 15:08:48,529 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.34 vs. limit=13.960666666666667
2024-10-08 15:08:50,814 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=17921.333333333332, ans=0.006973623188405797
2024-10-08 15:08:52,212 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=17921.333333333332, ans=0.12078666666666668
2024-10-08 15:08:56,244 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=17924.666666666668, ans=0.27263666666666675
2024-10-08 15:09:01,550 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 15:09:21,453 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=17928.0, ans=0.125
2024-10-08 15:09:32,329 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.780e+01 1.078e+02 1.194e+02 1.349e+02 1.850e+02, threshold=2.389e+02, percent-clipped=0.0
2024-10-08 15:09:50,886 INFO [train.py:1153] Epoch 8, batch 5850, loss[loss=0.2851, simple_loss=0.2903, pruned_loss=0.09809, ctc_loss=0.209, over 4739.00 frames. ], tot_loss[loss=0.2329, simple_loss=0.2698, pruned_loss=0.06944, ctc_loss=0.1429, over 966480.21 frames. ], batch size: 45, lr: 1.12e-02,
2024-10-08 15:09:51,601 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.67 vs. limit=13.969
2024-10-08 15:10:07,137 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=17941.333333333332, ans=0.9294133333333333
2024-10-08 15:10:25,833 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=17944.666666666668, ans=0.125
2024-10-08 15:10:53,784 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=17951.333333333332, ans=0.125
2024-10-08 15:10:57,623 INFO [train.py:1153] Epoch 8, batch 5900, loss[loss=0.2393, simple_loss=0.2687, pruned_loss=0.07464, ctc_loss=0.1514, over 4832.00 frames. ], tot_loss[loss=0.2325, simple_loss=0.2694, pruned_loss=0.06928, ctc_loss=0.1428, over 966648.01 frames. ], batch size: 34, lr: 1.12e-02,
2024-10-08 15:11:09,918 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=17958.0, ans=0.006965652173913043
2024-10-08 15:11:11,977 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.70 vs. limit=14.23425
2024-10-08 15:11:22,730 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.77 vs. limit=5.6937
2024-10-08 15:11:24,668 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=17961.333333333332, ans=0.125
2024-10-08 15:11:45,986 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.439e+01 1.056e+02 1.206e+02 1.307e+02 1.771e+02, threshold=2.412e+02, percent-clipped=0.0
2024-10-08 15:12:04,926 INFO [train.py:1153] Epoch 8, batch 5950, loss[loss=0.2815, simple_loss=0.2944, pruned_loss=0.09485, ctc_loss=0.1974, over 4823.00 frames. ], tot_loss[loss=0.231, simple_loss=0.2681, pruned_loss=0.06869, ctc_loss=0.1411, over 966041.53 frames. ], batch size: 34, lr: 1.12e-02,
2024-10-08 15:12:25,246 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=17974.666666666668, ans=0.125
2024-10-08 15:12:26,628 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=17974.666666666668, ans=0.12025333333333332
2024-10-08 15:12:30,662 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=17978.0, ans=0.0
2024-10-08 15:12:42,609 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=17978.0, ans=0.125
2024-10-08 15:13:12,211 INFO [train.py:1153] Epoch 8, batch 6000, loss[loss=0.2788, simple_loss=0.2914, pruned_loss=0.09004, ctc_loss=0.2152, over 4781.00 frames. ], tot_loss[loss=0.231, simple_loss=0.2678, pruned_loss=0.06871, ctc_loss=0.1422, over 966730.15 frames. ], batch size: 49, lr: 1.12e-02,
2024-10-08 15:13:12,212 INFO [train.py:1176] Computing validation loss
2024-10-08 15:13:20,226 INFO [train.py:1185] Epoch 8, validation: loss=0.1597, simple_loss=0.2439, pruned_loss=0.02711, ctc_loss=0.05317, over 90464.00 frames.
2024-10-08 15:13:20,226 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 15:13:21,092 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.15 vs. limit=14.2455
2024-10-08 15:13:24,402 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=17988.0, ans=0.006959130434782608
2024-10-08 15:13:39,552 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=22.50 vs. limit=20.993499999999997
2024-10-08 15:13:52,659 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer_na.min_abs, batch_count=17994.666666666668, ans=0.02
2024-10-08 15:13:58,600 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.21 vs. limit=5.699199999999999
2024-10-08 15:14:08,737 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.482e+01 1.018e+02 1.196e+02 1.338e+02 1.860e+02, threshold=2.392e+02, percent-clipped=0.0
2024-10-08 15:14:15,619 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=18001.333333333332, ans=0.125
2024-10-08 15:14:27,465 INFO [train.py:1153] Epoch 8, batch 6050, loss[loss=0.1778, simple_loss=0.2327, pruned_loss=0.04356, ctc_loss=0.08977, over 4813.00 frames. ], tot_loss[loss=0.2295, simple_loss=0.2669, pruned_loss=0.06802, ctc_loss=0.1401, over 966688.31 frames. ], batch size: 19, lr: 1.12e-02,
2024-10-08 15:14:34,184 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=18004.666666666668, ans=0.125
2024-10-08 15:14:57,275 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=18011.333333333332, ans=0.2696033333333334
2024-10-08 15:15:08,022 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=18014.666666666668, ans=0.125
2024-10-08 15:15:15,956 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=18014.666666666668, ans=0.125
2024-10-08 15:15:18,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=18014.666666666668, ans=0.125
2024-10-08 15:15:26,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=18018.0, ans=0.035
2024-10-08 15:15:34,832 INFO [train.py:1153] Epoch 8, batch 6100, loss[loss=0.2478, simple_loss=0.2667, pruned_loss=0.082, ctc_loss=0.1621, over 4789.00 frames. ], tot_loss[loss=0.2293, simple_loss=0.267, pruned_loss=0.06782, ctc_loss=0.1399, over 966192.52 frames. ], batch size: 34, lr: 1.12e-02,
2024-10-08 15:15:44,339 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=18021.333333333332, ans=0.125
2024-10-08 15:16:23,367 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.107e+01 1.030e+02 1.162e+02 1.326e+02 1.729e+02, threshold=2.323e+02, percent-clipped=0.0
2024-10-08 15:16:26,125 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=18031.333333333332, ans=0.11968666666666669
2024-10-08 15:16:26,831 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=19.87 vs. limit=21.0235
2024-10-08 15:16:42,241 INFO [train.py:1153] Epoch 8, batch 6150, loss[loss=0.1964, simple_loss=0.2476, pruned_loss=0.0523, ctc_loss=0.1017, over 4835.00 frames. ], tot_loss[loss=0.2285, simple_loss=0.2665, pruned_loss=0.06742, ctc_loss=0.139, over 966485.80 frames. ], batch size: 43, lr: 1.12e-02,
2024-10-08 15:16:49,291 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=18038.0, ans=0.2686700000000001
2024-10-08 15:17:01,357 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=18041.333333333332, ans=0.2685533333333334
2024-10-08 15:17:04,867 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=10.65 vs. limit=14.020666666666665
2024-10-08 15:17:06,790 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=18041.333333333332, ans=0.125
2024-10-08 15:17:45,823 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=18051.333333333332, ans=0.125
2024-10-08 15:17:49,712 INFO [train.py:1153] Epoch 8, batch 6200, loss[loss=0.2454, simple_loss=0.2889, pruned_loss=0.07206, ctc_loss=0.1444, over 4766.00 frames. ], tot_loss[loss=0.2289, simple_loss=0.2666, pruned_loss=0.06767, ctc_loss=0.1396, over 966563.44 frames. ], batch size: 29, lr: 1.11e-02,
2024-10-08 15:17:53,988 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=18054.666666666668, ans=10.0
2024-10-08 15:17:57,217 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.45 vs. limit=21.041
2024-10-08 15:17:58,036 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=18054.666666666668, ans=0.0
2024-10-08 15:18:06,408 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=14.42 vs. limit=14.27175
2024-10-08 15:18:18,115 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=18061.333333333332, ans=0.025
2024-10-08 15:18:38,243 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.622e+01 1.023e+02 1.174e+02 1.296e+02 1.874e+02, threshold=2.347e+02, percent-clipped=0.0
2024-10-08 15:18:39,687 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=18064.666666666668, ans=0.9306466666666666
2024-10-08 15:18:57,069 INFO [train.py:1153] Epoch 8, batch 6250, loss[loss=0.2448, simple_loss=0.2799, pruned_loss=0.07362, ctc_loss=0.1563, over 4734.00 frames. ], tot_loss[loss=0.2281, simple_loss=0.2665, pruned_loss=0.06719, ctc_loss=0.1385, over 966827.17 frames. ], batch size: 26, lr: 1.11e-02,
2024-10-08 15:19:02,669 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=18071.333333333332, ans=0.125
2024-10-08 15:19:32,117 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=18078.0, ans=0.0
2024-10-08 15:19:48,273 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=18081.333333333332, ans=0.11918666666666669
2024-10-08 15:19:52,605 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=4.77 vs. limit=11.233866666666668
2024-10-08 15:19:53,020 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=4.39 vs. limit=14.281749999999999
2024-10-08 15:20:04,160 INFO [train.py:1153] Epoch 8, batch 6300, loss[loss=0.1941, simple_loss=0.2359, pruned_loss=0.05192, ctc_loss=0.1208, over 4978.00 frames. ], tot_loss[loss=0.2276, simple_loss=0.2662, pruned_loss=0.06688, ctc_loss=0.1379, over 966564.77 frames. ], batch size: 19, lr: 1.11e-02,
2024-10-08 15:20:06,958 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer1.prob, batch_count=18088.0, ans=0.125
2024-10-08 15:20:15,095 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=18088.0, ans=0.125
2024-10-08 15:20:36,815 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=18094.666666666668, ans=0.26668666666666674
2024-10-08 15:20:36,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=18094.666666666668, ans=0.9309466666666666
2024-10-08 15:20:52,775 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.404e+01 1.071e+02 1.197e+02 1.304e+02 1.611e+02, threshold=2.393e+02, percent-clipped=0.0
2024-10-08 15:20:54,896 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.04 vs. limit=14.28675
2024-10-08 15:20:57,236 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten.whitening_limit, batch_count=18101.333333333332, ans=14.288
2024-10-08 15:21:11,551 INFO [train.py:1153] Epoch 8, batch 6350, loss[loss=0.2404, simple_loss=0.2776, pruned_loss=0.07228, ctc_loss=0.1465, over 4846.00 frames. ], tot_loss[loss=0.2268, simple_loss=0.2656, pruned_loss=0.06654, ctc_loss=0.1373, over 966212.40 frames. ], batch size: 36, lr: 1.11e-02,
2024-10-08 15:21:29,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=18108.0, ans=0.125
2024-10-08 15:21:32,166 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.30 vs. limit=21.081
2024-10-08 15:21:35,843 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=18108.0, ans=0.0
2024-10-08 15:21:37,157 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=18111.333333333332, ans=0.125
2024-10-08 15:21:40,258 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.23 vs. limit=14.29175
2024-10-08 15:21:45,963 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=2.82 vs. limit=14.29175
2024-10-08 15:21:50,850 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.max_abs, batch_count=18114.666666666668, ans=10.0
2024-10-08 15:22:12,522 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=18118.0, ans=0.0
2024-10-08 15:22:19,027 INFO [train.py:1153] Epoch 8, batch 6400, loss[loss=0.2177, simple_loss=0.262, pruned_loss=0.06261, ctc_loss=0.1208, over 4879.00 frames. ], tot_loss[loss=0.2253, simple_loss=0.2645, pruned_loss=0.0659, ctc_loss=0.1359, over 965949.26 frames. ], batch size: 23, lr: 1.11e-02,
2024-10-08 15:22:23,221 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=18121.333333333332, ans=0.2657533333333334
2024-10-08 15:23:07,408 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.232e+01 1.021e+02 1.169e+02 1.327e+02 1.904e+02, threshold=2.339e+02, percent-clipped=0.0
2024-10-08 15:23:08,781 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=18131.333333333332, ans=0.125
2024-10-08 15:23:12,956 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=18134.666666666668, ans=0.07
2024-10-08 15:23:26,353 INFO [train.py:1153] Epoch 8, batch 6450, loss[loss=0.2191, simple_loss=0.2712, pruned_loss=0.06024, ctc_loss=0.1165, over 4721.00 frames. ], tot_loss[loss=0.2258, simple_loss=0.2648, pruned_loss=0.06613, ctc_loss=0.1362, over 965392.76 frames. ], batch size: 26, lr: 1.11e-02,
2024-10-08 15:23:27,913 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.prob, batch_count=18138.0, ans=0.125
2024-10-08 15:23:46,897 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=18141.333333333332, ans=0.1185866666666667
2024-10-08 15:23:48,170 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=18141.333333333332, ans=0.125
2024-10-08 15:23:54,884 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=18144.666666666668, ans=0.125
2024-10-08 15:23:56,124 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=18144.666666666668, ans=0.11855333333333332
2024-10-08 15:24:24,997 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=13.05 vs. limit=14.306750000000001
2024-10-08 15:24:25,841 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=18151.333333333332, ans=0.125
2024-10-08 15:24:33,853 INFO [train.py:1153] Epoch 8, batch 6500, loss[loss=0.1912, simple_loss=0.2522, pruned_loss=0.04296, ctc_loss=0.1108, over 4736.00 frames. ], tot_loss[loss=0.2253, simple_loss=0.2649, pruned_loss=0.06562, ctc_loss=0.136, over 965134.71 frames. ], batch size: 26, lr: 1.11e-02,
2024-10-08 15:24:40,713 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=18154.666666666668, ans=0.9315466666666666
2024-10-08 15:25:06,250 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=18161.333333333332, ans=0.0
2024-10-08 15:25:22,208 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.097e+01 1.032e+02 1.121e+02 1.307e+02 2.623e+02, threshold=2.242e+02, percent-clipped=1.0
2024-10-08 15:25:25,045 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=18164.666666666668, ans=0.125
2024-10-08 15:25:26,588 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=18168.0, ans=0.05
2024-10-08 15:25:31,863 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.min_positive, batch_count=18168.0, ans=0.025
2024-10-08 15:25:39,972 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=18171.333333333332, ans=0.006919275362318841
2024-10-08 15:25:40,559 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=20.94 vs. limit=21.128500000000003
2024-10-08 15:25:41,181 INFO [train.py:1153] Epoch 8, batch 6550, loss[loss=0.2197, simple_loss=0.2601, pruned_loss=0.06582, ctc_loss=0.1188, over 4978.00 frames. ], tot_loss[loss=0.2229, simple_loss=0.2633, pruned_loss=0.06454, ctc_loss=0.1336, over 964787.21 frames. ], batch size: 19, lr: 1.11e-02,
2024-10-08 15:25:52,093 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=18171.333333333332, ans=0.0
2024-10-08 15:26:13,889 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=18178.0, ans=0.006917826086956522
2024-10-08 15:26:17,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=18178.0, ans=0.125
2024-10-08 15:26:20,600 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=18181.333333333332, ans=0.125
2024-10-08 15:26:23,287 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=18181.333333333332, ans=0.006917101449275363
2024-10-08 15:26:26,094 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=18181.333333333332, ans=0.125
2024-10-08 15:26:31,557 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=18181.333333333332, ans=0.125
2024-10-08 15:26:37,056 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=18184.666666666668, ans=0.0069163768115942026
2024-10-08 15:26:44,529 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=13.43 vs. limit=21.1385
2024-10-08 15:26:45,055 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=18184.666666666668, ans=0.0
2024-10-08 15:26:49,044 INFO [train.py:1153] Epoch 8, batch 6600, loss[loss=0.2523, simple_loss=0.2855, pruned_loss=0.07813, ctc_loss=0.1569, over 4873.00 frames. ], tot_loss[loss=0.2235, simple_loss=0.2637, pruned_loss=0.06485, ctc_loss=0.134, over 965619.50 frames. ], batch size: 22, lr: 1.11e-02,
2024-10-08 15:26:49,237 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=18188.0, ans=0.125
2024-10-08 15:27:09,851 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=18191.333333333332, ans=0.125
2024-10-08 15:27:20,637 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=18194.666666666668, ans=0.0
2024-10-08 15:27:27,568 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=18194.666666666668, ans=0.2631866666666667
2024-10-08 15:27:33,159 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=18198.0, ans=0.26307000000000014
2024-10-08 15:27:38,506 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.059e+01 1.040e+02 1.155e+02 1.324e+02 1.832e+02, threshold=2.310e+02, percent-clipped=0.0
2024-10-08 15:27:48,260 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=18201.333333333332, ans=0.125
2024-10-08 15:27:56,450 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=18204.666666666668, ans=0.006912028985507246
2024-10-08 15:27:57,641 INFO [train.py:1153] Epoch 8, batch 6650, loss[loss=0.2039, simple_loss=0.2608, pruned_loss=0.05137, ctc_loss=0.1103, over 4747.00 frames. ], tot_loss[loss=0.222, simple_loss=0.2628, pruned_loss=0.06413, ctc_loss=0.1326, over 967275.30 frames. ], batch size: 20, lr: 1.11e-02,
2024-10-08 15:28:22,454 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=18208.0, ans=0.125
2024-10-08 15:28:27,874 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 15:28:45,651 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=18214.666666666668, ans=0.11785333333333331
2024-10-08 15:29:06,194 INFO [train.py:1153] Epoch 8, batch 6700, loss[loss=0.1873, simple_loss=0.2365, pruned_loss=0.04945, ctc_loss=0.09796, over 4934.00 frames. ], tot_loss[loss=0.2199, simple_loss=0.2615, pruned_loss=0.06305, ctc_loss=0.1304, over 969518.57 frames. ], batch size: 20, lr: 1.11e-02,
2024-10-08 15:29:17,382 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=18221.333333333332, ans=0.125
2024-10-08 15:29:21,470 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=18224.666666666668, ans=0.125
2024-10-08 15:29:29,211 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.46 vs. limit=14.33425
2024-10-08 15:29:56,062 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.639e+01 1.022e+02 1.133e+02 1.258e+02 1.681e+02, threshold=2.267e+02, percent-clipped=0.0
2024-10-08 15:30:15,523 INFO [train.py:1153] Epoch 8, batch 6750, loss[loss=0.2164, simple_loss=0.2613, pruned_loss=0.06076, ctc_loss=0.1248, over 4909.00 frames. ], tot_loss[loss=0.2181, simple_loss=0.2603, pruned_loss=0.06221, ctc_loss=0.1286, over 972498.45 frames. ], batch size: 19, lr: 1.11e-02,
2024-10-08 15:30:17,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=18238.0, ans=0.125
2024-10-08 15:30:25,447 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=18238.0, ans=0.26167000000000007
2024-10-08 15:30:30,511 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=8.78 vs. limit=9.560333333333332
2024-10-08 15:31:11,042 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=18251.333333333332, ans=0.11748666666666668
2024-10-08 15:31:22,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=5.71 vs. limit=5.7377
2024-10-08 15:31:24,800 INFO [train.py:1153] Epoch 8, batch 6800, loss[loss=0.206, simple_loss=0.242, pruned_loss=0.06325, ctc_loss=0.1086, over 4909.00 frames. ], tot_loss[loss=0.2157, simple_loss=0.2583, pruned_loss=0.0613, ctc_loss=0.1263, over 974782.69 frames. ], batch size: 19, lr: 1.11e-02,
2024-10-08 15:32:15,043 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.125e+01 9.283e+01 1.070e+02 1.234e+02 1.652e+02, threshold=2.140e+02, percent-clipped=0.0
2024-10-08 15:32:17,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=18264.666666666668, ans=0.125
2024-10-08 15:32:33,346 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=18271.333333333332, ans=0.11728666666666668
2024-10-08 15:32:33,418 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=18271.333333333332, ans=0.125
2024-10-08 15:32:34,553 INFO [train.py:1153] Epoch 8, batch 6850, loss[loss=0.196, simple_loss=0.252, pruned_loss=0.04826, ctc_loss=0.1088, over 4978.00 frames. ], tot_loss[loss=0.2131, simple_loss=0.2562, pruned_loss=0.06018, ctc_loss=0.1241, over 979128.19 frames. ], batch size: 19, lr: 1.11e-02,
2024-10-08 15:32:34,654 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/epoch-8.pt
2024-10-08 15:32:57,366 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=2.387e-01
2024-10-08 15:32:58,024 INFO [train.py:1153] Epoch 9, batch 0, loss[loss=0.1976, simple_loss=0.2502, pruned_loss=0.051, ctc_loss=0.1078, over 4855.00 frames. ], tot_loss[loss=0.1976, simple_loss=0.2502, pruned_loss=0.051, ctc_loss=0.1078, over 4855.00 frames. ], batch size: 19, lr: 1.05e-02,
2024-10-08 15:32:58,024 INFO [train.py:1176] Computing validation loss
2024-10-08 15:33:04,364 INFO [train.py:1185] Epoch 9, validation: loss=0.1586, simple_loss=0.2433, pruned_loss=0.02696, ctc_loss=0.05018, over 90464.00 frames.
2024-10-08 15:33:04,365 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 15:33:07,918 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=18271.666666666668, ans=0.2604916666666667
2024-10-08 15:33:10,798 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=6.00 vs. limit=14.351875
2024-10-08 15:33:21,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=18275.0, ans=0.11725000000000002
2024-10-08 15:33:36,634 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=18278.333333333332, ans=0.125
2024-10-08 15:34:01,618 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=18285.0, ans=0.125
2024-10-08 15:34:06,842 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 15:34:08,044 INFO [train.py:1153] Epoch 9, batch 50, loss[loss=0.2456, simple_loss=0.2652, pruned_loss=0.08197, ctc_loss=0.1552, over 4911.00 frames. ], tot_loss[loss=0.2264, simple_loss=0.2655, pruned_loss=0.06583, ctc_loss=0.1391, over 217761.27 frames. ], batch size: 19, lr: 1.05e-02,
2024-10-08 15:34:16,311 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=18288.333333333332, ans=0.0
2024-10-08 15:34:47,177 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=18298.333333333332, ans=0.0
2024-10-08 15:34:54,943 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.305e+01 9.944e+01 1.160e+02 1.340e+02 1.972e+02, threshold=2.319e+02, percent-clipped=0.0
2024-10-08 15:35:15,036 INFO [train.py:1153] Epoch 9, batch 100, loss[loss=0.1867, simple_loss=0.2487, pruned_loss=0.04265, ctc_loss=0.09826, over 4757.00 frames. ], tot_loss[loss=0.2296, simple_loss=0.2678, pruned_loss=0.06734, ctc_loss=0.1418, over 383367.36 frames. ], batch size: 19, lr: 1.05e-02,
2024-10-08 15:35:42,050 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=18311.666666666668, ans=0.0
2024-10-08 15:35:44,714 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=18311.666666666668, ans=0.2590916666666667
2024-10-08 15:35:46,082 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=18311.666666666668, ans=0.0
2024-10-08 15:35:51,570 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=18311.666666666668, ans=0.0068887681159420285
2024-10-08 15:36:05,061 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=18315.0, ans=0.125
2024-10-08 15:36:22,470 INFO [train.py:1153] Epoch 9, batch 150, loss[loss=0.1801, simple_loss=0.2441, pruned_loss=0.04038, ctc_loss=0.08832, over 4909.00 frames. ], tot_loss[loss=0.2248, simple_loss=0.264, pruned_loss=0.06547, ctc_loss=0.1367, over 513297.71 frames. ], batch size: 19, lr: 1.05e-02,
2024-10-08 15:36:22,715 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=18321.666666666668, ans=0.11678333333333332
2024-10-08 15:36:32,084 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer_ff2.min_abs, batch_count=18321.666666666668, ans=0.1
2024-10-08 15:36:44,128 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=18325.0, ans=0.474875
2024-10-08 15:37:09,591 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.830e+01 9.913e+01 1.154e+02 1.296e+02 1.742e+02, threshold=2.307e+02, percent-clipped=0.0
2024-10-08 15:37:15,089 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=18335.0, ans=0.125
2024-10-08 15:37:30,215 INFO [train.py:1153] Epoch 9, batch 200, loss[loss=0.2811, simple_loss=0.3046, pruned_loss=0.09165, ctc_loss=0.1859, over 4742.00 frames. ], tot_loss[loss=0.2239, simple_loss=0.2634, pruned_loss=0.06505, ctc_loss=0.1354, over 613832.69 frames. ], batch size: 45, lr: 1.05e-02,
2024-10-08 15:37:40,754 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=18338.333333333332, ans=0.025
2024-10-08 15:37:50,494 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=20.87 vs. limit=21.25625
2024-10-08 15:37:53,957 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=18341.666666666668, ans=0.125
2024-10-08 15:38:04,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=18345.0, ans=0.125
2024-10-08 15:38:04,760 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=18345.0, ans=0.125
2024-10-08 15:38:08,701 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=18348.333333333332, ans=0.025
2024-10-08 15:38:15,713 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=19.59 vs. limit=21.26125
2024-10-08 15:38:22,711 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.77 vs. limit=14.175833333333335
2024-10-08 15:38:36,669 INFO [train.py:1153] Epoch 9, batch 250, loss[loss=0.2632, simple_loss=0.2835, pruned_loss=0.08693, ctc_loss=0.1723, over 4830.00 frames. ], tot_loss[loss=0.2239, simple_loss=0.2638, pruned_loss=0.06505, ctc_loss=0.1351, over 692479.25 frames. ], batch size: 38, lr: 1.05e-02,
2024-10-08 15:38:43,487 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=18355.0, ans=0.257575
2024-10-08 15:38:55,696 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=18358.333333333332, ans=0.25745833333333346
2024-10-08 15:39:15,862 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=18365.0, ans=0.09899494936611666
2024-10-08 15:39:18,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=18365.0, ans=0.125
2024-10-08 15:39:23,715 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.225e+01 1.024e+02 1.128e+02 1.247e+02 1.542e+02, threshold=2.255e+02, percent-clipped=0.0
2024-10-08 15:39:33,346 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=18368.333333333332, ans=0.0
2024-10-08 15:39:43,821 INFO [train.py:1153] Epoch 9, batch 300, loss[loss=0.2966, simple_loss=0.3138, pruned_loss=0.1032, ctc_loss=0.1824, over 4776.00 frames. ], tot_loss[loss=0.2253, simple_loss=0.2648, pruned_loss=0.06567, ctc_loss=0.136, over 752820.13 frames. ], batch size: 32, lr: 1.05e-02,
2024-10-08 15:39:53,409 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.min_abs, batch_count=18371.666666666668, ans=0.475575
2024-10-08 15:40:00,204 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=18375.0, ans=0.025
2024-10-08 15:40:06,830 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=18375.0, ans=0.125
2024-10-08 15:40:17,536 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=18378.333333333332, ans=0.125
2024-10-08 15:40:27,076 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=18381.666666666668, ans=0.025
2024-10-08 15:40:27,092 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=18381.666666666668, ans=0.125
2024-10-08 15:40:27,171 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=18381.666666666668, ans=0.0068735507246376806
2024-10-08 15:40:28,460 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=18381.666666666668, ans=0.025
2024-10-08 15:40:28,463 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=18381.666666666668, ans=0.1161833333333333
2024-10-08 15:40:51,372 INFO [train.py:1153] Epoch 9, batch 350, loss[loss=0.2088, simple_loss=0.2636, pruned_loss=0.05329, ctc_loss=0.1186, over 4883.00 frames. ], tot_loss[loss=0.2257, simple_loss=0.2655, pruned_loss=0.0657, ctc_loss=0.1364, over 800332.48 frames. ], batch size: 19, lr: 1.05e-02,
2024-10-08 15:41:00,813 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=18388.333333333332, ans=0.0
2024-10-08 15:41:11,701 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=18391.666666666668, ans=0.11608333333333332
2024-10-08 15:41:19,734 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=18395.0, ans=0.025
2024-10-08 15:41:38,494 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.306e+01 1.039e+02 1.182e+02 1.306e+02 1.737e+02, threshold=2.364e+02, percent-clipped=0.0
2024-10-08 15:41:46,722 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=18401.666666666668, ans=0.0
2024-10-08 15:41:49,340 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=18401.666666666668, ans=0.0
2024-10-08 15:41:58,662 INFO [train.py:1153] Epoch 9, batch 400, loss[loss=0.1834, simple_loss=0.2183, pruned_loss=0.05406, ctc_loss=0.1012, over 4877.00 frames. ], tot_loss[loss=0.2237, simple_loss=0.2637, pruned_loss=0.06488, ctc_loss=0.1348, over 837051.28 frames. ], batch size: 22, lr: 1.05e-02,
2024-10-08 15:41:58,896 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=18405.0, ans=0.11595000000000003
2024-10-08 15:42:05,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=18405.0, ans=0.0
2024-10-08 15:42:07,119 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.21 vs. limit=21.30375
2024-10-08 15:42:13,619 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=18408.333333333332, ans=0.006867753623188406
2024-10-08 15:42:18,028 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=14.85 vs. limit=14.204166666666666
2024-10-08 15:42:33,634 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=18411.666666666668, ans=0.125
2024-10-08 15:42:51,248 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=18418.333333333332, ans=0.025
2024-10-08 15:42:58,382 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=14.21 vs. limit=14.406875
2024-10-08 15:42:59,739 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.80 vs. limit=14.406875
2024-10-08 15:43:04,553 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=18421.666666666668, ans=0.125
2024-10-08 15:43:05,708 INFO [train.py:1153] Epoch 9, batch 450, loss[loss=0.1964, simple_loss=0.2478, pruned_loss=0.05147, ctc_loss=0.1051, over 4848.00 frames. ], tot_loss[loss=0.2239, simple_loss=0.264, pruned_loss=0.06494, ctc_loss=0.1349, over 865510.72 frames. ], batch size: 23, lr: 1.04e-02,
2024-10-08 15:43:28,891 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer_ff3.min_abs, batch_count=18425.0, ans=0.2
2024-10-08 15:43:47,859 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=18431.666666666668, ans=0.125
2024-10-08 15:43:53,123 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.273e+01 1.014e+02 1.112e+02 1.259e+02 1.814e+02, threshold=2.225e+02, percent-clipped=0.0
2024-10-08 15:44:01,424 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=18435.0, ans=0.025
2024-10-08 15:44:02,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=18435.0, ans=0.125
2024-10-08 15:44:13,430 INFO [train.py:1153] Epoch 9, batch 500, loss[loss=0.1924, simple_loss=0.2418, pruned_loss=0.04933, ctc_loss=0.1109, over 4809.00 frames. ], tot_loss[loss=0.2225, simple_loss=0.2634, pruned_loss=0.06406, ctc_loss=0.1337, over 888194.48 frames. ], batch size: 34, lr: 1.04e-02,
2024-10-08 15:44:16,781 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=21.24 vs. limit=21.32875
2024-10-08 15:44:20,963 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.00 vs. limit=9.609583333333333
2024-10-08 15:44:25,679 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.min_positive, batch_count=18441.666666666668, ans=0.05
2024-10-08 15:44:57,939 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=18448.333333333332, ans=0.25430833333333347
2024-10-08 15:45:01,173 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=14.29 vs. limit=14.418125
2024-10-08 15:45:07,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=18451.666666666668, ans=0.0068583333333333335
2024-10-08 15:45:09,830 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=18451.666666666668, ans=0.125
2024-10-08 15:45:20,588 INFO [train.py:1153] Epoch 9, batch 550, loss[loss=0.2352, simple_loss=0.2663, pruned_loss=0.07336, ctc_loss=0.1436, over 4789.00 frames. ], tot_loss[loss=0.2227, simple_loss=0.263, pruned_loss=0.06437, ctc_loss=0.1341, over 905704.86 frames. ], batch size: 40, lr: 1.04e-02,
2024-10-08 15:46:01,202 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=18465.0, ans=0.125
2024-10-08 15:46:07,900 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.175e+01 9.952e+01 1.178e+02 1.326e+02 1.903e+02, threshold=2.356e+02, percent-clipped=0.0
2024-10-08 15:46:28,223 INFO [train.py:1153] Epoch 9, batch 600, loss[loss=0.2581, simple_loss=0.2858, pruned_loss=0.08235, ctc_loss=0.1641, over 4829.00 frames. ], tot_loss[loss=0.2222, simple_loss=0.2628, pruned_loss=0.06415, ctc_loss=0.1334, over 919484.37 frames. ], batch size: 38, lr: 1.04e-02,
2024-10-08 15:46:37,834 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=18471.666666666668, ans=0.006853985507246377
2024-10-08 15:46:54,096 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=18478.333333333332, ans=0.04949747468305833
2024-10-08 15:47:02,444 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=18478.333333333332, ans=0.0
2024-10-08 15:47:16,068 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=18481.666666666668, ans=0.125
2024-10-08 15:47:36,116 INFO [train.py:1153] Epoch 9, batch 650, loss[loss=0.1868, simple_loss=0.2438, pruned_loss=0.04553, ctc_loss=0.09658, over 4848.00 frames. ], tot_loss[loss=0.2193, simple_loss=0.2605, pruned_loss=0.06286, ctc_loss=0.1309, over 930350.43 frames. ], batch size: 21, lr: 1.04e-02,
2024-10-08 15:47:56,918 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=5.55 vs. limit=5.77375
2024-10-08 15:48:00,903 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten.whitening_limit, batch_count=18491.666666666668, ans=14.434375
2024-10-08 15:48:23,212 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.424e+01 9.952e+01 1.095e+02 1.241e+02 1.534e+02, threshold=2.191e+02, percent-clipped=0.0
2024-10-08 15:48:29,316 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.30 vs. limit=21.37625
2024-10-08 15:48:35,521 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=18501.666666666668, ans=0.0
2024-10-08 15:48:43,315 INFO [train.py:1153] Epoch 9, batch 700, loss[loss=0.2117, simple_loss=0.2523, pruned_loss=0.06097, ctc_loss=0.1226, over 4745.00 frames. ], tot_loss[loss=0.2207, simple_loss=0.2618, pruned_loss=0.06334, ctc_loss=0.1322, over 938128.92 frames. ], batch size: 19, lr: 1.04e-02,
2024-10-08 15:48:56,764 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=18508.333333333332, ans=0.125
2024-10-08 15:49:30,690 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=18515.0, ans=0.11485000000000001
2024-10-08 15:49:32,205 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 15:49:44,340 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=18518.333333333332, ans=0.025
2024-10-08 15:49:51,641 INFO [train.py:1153] Epoch 9, batch 750, loss[loss=0.1735, simple_loss=0.2349, pruned_loss=0.03834, ctc_loss=0.08856, over 4875.00 frames. ], tot_loss[loss=0.2209, simple_loss=0.2623, pruned_loss=0.06341, ctc_loss=0.1318, over 944947.21 frames. ], batch size: 22, lr: 1.04e-02,
2024-10-08 15:49:58,378 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=18521.666666666668, ans=0.125
2024-10-08 15:50:04,967 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=18525.0, ans=0.006842391304347826
2024-10-08 15:50:11,530 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=18525.0, ans=0.07
2024-10-08 15:50:38,130 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.082e+01 9.996e+01 1.131e+02 1.290e+02 1.731e+02, threshold=2.263e+02, percent-clipped=0.0
2024-10-08 15:50:46,383 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=18535.0, ans=0.125
2024-10-08 15:50:47,094 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=5.95 vs. limit=14.450624999999999
2024-10-08 15:50:47,954 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.92 vs. limit=14.450624999999999
2024-10-08 15:50:58,249 INFO [train.py:1153] Epoch 9, batch 800, loss[loss=0.2411, simple_loss=0.275, pruned_loss=0.07351, ctc_loss=0.1508, over 4854.00 frames. ], tot_loss[loss=0.2205, simple_loss=0.2618, pruned_loss=0.06318, ctc_loss=0.1319, over 949745.66 frames. ], batch size: 19, lr: 1.04e-02,
2024-10-08 15:51:17,975 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.70 vs. limit=14.453125
2024-10-08 15:51:53,814 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=18551.666666666668, ans=0.125
2024-10-08 15:51:57,810 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 15:52:05,747 INFO [train.py:1153] Epoch 9, batch 850, loss[loss=0.1989, simple_loss=0.2516, pruned_loss=0.04889, ctc_loss=0.1209, over 4793.00 frames. ], tot_loss[loss=0.2205, simple_loss=0.2619, pruned_loss=0.06326, ctc_loss=0.1315, over 953951.54 frames. ], batch size: 29, lr: 1.04e-02,
2024-10-08 15:52:14,508 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.20 vs. limit=5.78325
2024-10-08 15:52:27,464 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=18558.333333333332, ans=0.006835144927536232
2024-10-08 15:52:41,285 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=13.24 vs. limit=14.460625
2024-10-08 15:52:52,665 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.926e+01 1.011e+02 1.149e+02 1.291e+02 2.055e+02, threshold=2.298e+02, percent-clipped=0.0
2024-10-08 15:53:13,163 INFO [train.py:1153] Epoch 9, batch 900, loss[loss=0.2011, simple_loss=0.2497, pruned_loss=0.05297, ctc_loss=0.1165, over 4850.00 frames. ], tot_loss[loss=0.2208, simple_loss=0.2619, pruned_loss=0.06344, ctc_loss=0.1319, over 956983.82 frames. ], batch size: 19, lr: 1.04e-02,
2024-10-08 15:54:20,343 INFO [train.py:1153] Epoch 9, batch 950, loss[loss=0.2439, simple_loss=0.2789, pruned_loss=0.07285, ctc_loss=0.1582, over 4814.00 frames. ], tot_loss[loss=0.2211, simple_loss=0.2622, pruned_loss=0.0635, ctc_loss=0.1322, over 958749.80 frames. ], batch size: 19, lr: 1.04e-02,
2024-10-08 15:54:20,463 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=18588.333333333332, ans=0.025
2024-10-08 15:54:23,243 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=18588.333333333332, ans=0.0
2024-10-08 15:54:23,261 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=18588.333333333332, ans=0.0
2024-10-08 15:54:24,678 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 15:54:40,651 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=18591.666666666668, ans=0.11408333333333331
2024-10-08 15:55:02,183 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=18598.333333333332, ans=0.125
2024-10-08 15:55:07,704 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.349e+01 1.004e+02 1.135e+02 1.277e+02 2.008e+02, threshold=2.270e+02, percent-clipped=0.0
2024-10-08 15:55:16,455 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.35 vs. limit=11.440666666666667
2024-10-08 15:55:24,462 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=16.72 vs. limit=14.475625
2024-10-08 15:55:27,976 INFO [train.py:1153] Epoch 9, batch 1000, loss[loss=0.1568, simple_loss=0.2213, pruned_loss=0.03197, ctc_loss=0.07109, over 4925.00 frames. ], tot_loss[loss=0.2214, simple_loss=0.2624, pruned_loss=0.06375, ctc_loss=0.1323, over 960621.97 frames. ], batch size: 20, lr: 1.04e-02,
2024-10-08 15:55:30,749 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=18605.0, ans=0.11395000000000002
2024-10-08 15:56:10,778 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 15:56:31,774 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.28 vs. limit=21.463749999999997
2024-10-08 15:56:36,343 INFO [train.py:1153] Epoch 9, batch 1050, loss[loss=0.1833, simple_loss=0.227, pruned_loss=0.04704, ctc_loss=0.1138, over 4837.00 frames. ], tot_loss[loss=0.2222, simple_loss=0.2628, pruned_loss=0.06414, ctc_loss=0.1332, over 962746.37 frames. ], batch size: 25, lr: 1.04e-02,
2024-10-08 15:56:45,982 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.prob, batch_count=18621.666666666668, ans=0.125
2024-10-08 15:56:54,261 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=18625.0, ans=0.125
2024-10-08 15:57:07,675 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=18628.333333333332, ans=0.11371666666666669
2024-10-08 15:57:09,270 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.02 vs. limit=14.485624999999999
2024-10-08 15:57:23,769 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.707e+01 1.036e+02 1.172e+02 1.249e+02 1.844e+02, threshold=2.344e+02, percent-clipped=0.0
2024-10-08 15:57:25,355 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=18631.666666666668, ans=0.125
2024-10-08 15:57:40,164 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=18635.0, ans=0.11365
2024-10-08 15:57:44,230 INFO [train.py:1153] Epoch 9, batch 1100, loss[loss=0.246, simple_loss=0.2761, pruned_loss=0.0759, ctc_loss=0.1602, over 4867.00 frames. ], tot_loss[loss=0.2222, simple_loss=0.2625, pruned_loss=0.06426, ctc_loss=0.1334, over 964180.86 frames. ], batch size: 20, lr: 1.04e-02,
2024-10-08 15:57:45,054 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.65 vs. limit=14.489374999999999
2024-10-08 15:58:03,014 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=18641.666666666668, ans=0.11358333333333331
2024-10-08 15:58:06,751 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=9.04 vs. limit=9.660416666666666
2024-10-08 15:58:10,573 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=13.69 vs. limit=14.491875
2024-10-08 15:58:34,240 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=18648.333333333332, ans=0.125
2024-10-08 15:58:51,629 INFO [train.py:1153] Epoch 9, batch 1150, loss[loss=0.1762, simple_loss=0.2295, pruned_loss=0.04299, ctc_loss=0.09235, over 4852.00 frames. ], tot_loss[loss=0.2227, simple_loss=0.2628, pruned_loss=0.06446, ctc_loss=0.1341, over 964448.33 frames. ], batch size: 20, lr: 1.04e-02,
2024-10-08 15:58:55,827 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.attention_skip_rate, batch_count=18655.0, ans=0.0
2024-10-08 15:58:57,286 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff2_skip_rate, batch_count=18655.0, ans=0.006814130434782609
2024-10-08 15:58:57,823 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.43 vs. limit=14.3275
2024-10-08 15:59:06,012 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.93 vs. limit=14.496875
2024-10-08 15:59:07,929 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=18658.333333333332, ans=0.0
2024-10-08 15:59:17,286 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=18661.666666666668, ans=0.125
2024-10-08 15:59:32,216 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=18665.0, ans=0.00681195652173913
2024-10-08 15:59:33,561 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=18665.0, ans=0.00681195652173913
2024-10-08 15:59:37,477 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/checkpoint-56000.pt
2024-10-08 15:59:39,466 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.687e+01 1.035e+02 1.161e+02 1.300e+02 2.143e+02, threshold=2.322e+02, percent-clipped=0.0
2024-10-08 15:59:58,875 INFO [train.py:1153] Epoch 9, batch 1200, loss[loss=0.2266, simple_loss=0.2758, pruned_loss=0.0629, ctc_loss=0.1291, over 4816.00 frames. ], tot_loss[loss=0.2237, simple_loss=0.2636, pruned_loss=0.06489, ctc_loss=0.1349, over 964378.79 frames. ], batch size: 25, lr: 1.04e-02,
2024-10-08 16:00:04,365 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=18671.666666666668, ans=0.24649166666666666
2024-10-08 16:00:07,507 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.95 vs. limit=9.667916666666667
2024-10-08 16:00:11,017 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=18675.0, ans=0.0
2024-10-08 16:00:12,320 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=18675.0, ans=0.05
2024-10-08 16:00:33,816 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=18678.333333333332, ans=0.11321666666666669
2024-10-08 16:00:36,498 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=18678.333333333332, ans=0.24625833333333347
2024-10-08 16:00:56,395 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=18685.0, ans=0.025
2024-10-08 16:01:06,076 INFO [train.py:1153] Epoch 9, batch 1250, loss[loss=0.2271, simple_loss=0.2581, pruned_loss=0.06983, ctc_loss=0.1409, over 4751.00 frames. ], tot_loss[loss=0.2246, simple_loss=0.2638, pruned_loss=0.06549, ctc_loss=0.136, over 964356.24 frames. ], batch size: 32, lr: 1.04e-02,
2024-10-08 16:01:06,244 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=18688.333333333332, ans=0.2459083333333334
2024-10-08 16:01:31,911 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.02 vs. limit=14.510625000000001
2024-10-08 16:01:40,842 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=18695.0, ans=0.04949747468305833
2024-10-08 16:01:44,658 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=18698.333333333332, ans=0.11301666666666668
2024-10-08 16:01:49,156 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.07 vs. limit=14.511875
2024-10-08 16:01:50,133 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=18698.333333333332, ans=0.24555833333333343
2024-10-08 16:01:52,563 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.678e+01 1.030e+02 1.153e+02 1.283e+02 1.686e+02, threshold=2.306e+02, percent-clipped=0.0
2024-10-08 16:02:09,003 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=18701.666666666668, ans=0.125
2024-10-08 16:02:12,719 INFO [train.py:1153] Epoch 9, batch 1300, loss[loss=0.2729, simple_loss=0.3006, pruned_loss=0.08623, ctc_loss=0.1818, over 4827.00 frames. ], tot_loss[loss=0.2242, simple_loss=0.2637, pruned_loss=0.06514, ctc_loss=0.136, over 965670.37 frames. ], batch size: 43, lr: 1.04e-02,
2024-10-08 16:02:14,197 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=18705.0, ans=0.025
2024-10-08 16:02:30,429 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=18708.333333333332, ans=0.125
2024-10-08 16:02:40,611 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten.whitening_limit, batch_count=18711.666666666668, ans=14.516874999999999
2024-10-08 16:02:46,794 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=18711.666666666668, ans=0.07
2024-10-08 16:02:49,957 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=21.45 vs. limit=21.533749999999998
2024-10-08 16:02:54,832 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=18715.0, ans=0.125
2024-10-08 16:03:20,543 INFO [train.py:1153] Epoch 9, batch 1350, loss[loss=0.2083, simple_loss=0.2607, pruned_loss=0.05349, ctc_loss=0.1224, over 4835.00 frames. ], tot_loss[loss=0.2228, simple_loss=0.263, pruned_loss=0.06438, ctc_loss=0.1344, over 966366.69 frames. ], batch size: 21, lr: 1.04e-02,
2024-10-08 16:03:27,508 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=18721.666666666668, ans=0.125
2024-10-08 16:04:01,410 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=18731.666666666668, ans=0.006797463768115942
2024-10-08 16:04:08,044 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.515e+01 1.028e+02 1.184e+02 1.307e+02 1.807e+02, threshold=2.369e+02, percent-clipped=0.0
2024-10-08 16:04:16,354 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=18735.0, ans=0.05
2024-10-08 16:04:18,923 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=18735.0, ans=0.125
2024-10-08 16:04:24,372 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=18735.0, ans=0.006796739130434782
2024-10-08 16:04:28,314 INFO [train.py:1153] Epoch 9, batch 1400, loss[loss=0.2252, simple_loss=0.2765, pruned_loss=0.06068, ctc_loss=0.1314, over 4940.00 frames. ], tot_loss[loss=0.2225, simple_loss=0.2629, pruned_loss=0.06428, ctc_loss=0.1336, over 966661.79 frames. ], batch size: 19, lr: 1.04e-02,
2024-10-08 16:04:36,650 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=18738.333333333332, ans=0.025
2024-10-08 16:05:19,565 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=18748.333333333332, ans=0.11251666666666668
2024-10-08 16:05:35,569 INFO [train.py:1153] Epoch 9, batch 1450, loss[loss=0.2273, simple_loss=0.26, pruned_loss=0.06701, ctc_loss=0.1515, over 4794.00 frames. ], tot_loss[loss=0.2237, simple_loss=0.2636, pruned_loss=0.06492, ctc_loss=0.1346, over 966637.92 frames. ], batch size: 34, lr: 1.04e-02,
2024-10-08 16:05:37,523 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.57 vs. limit=21.56625
2024-10-08 16:05:42,357 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=18755.0, ans=0.125
2024-10-08 16:06:00,045 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=18758.333333333332, ans=0.125
2024-10-08 16:06:03,040 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.56 vs. limit=14.535625
2024-10-08 16:06:22,612 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.680e+01 1.032e+02 1.161e+02 1.282e+02 1.702e+02, threshold=2.321e+02, percent-clipped=0.0
2024-10-08 16:06:28,157 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=18768.333333333332, ans=0.125
2024-10-08 16:06:39,070 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=18768.333333333332, ans=0.125
2024-10-08 16:06:42,900 INFO [train.py:1153] Epoch 9, batch 1500, loss[loss=0.2655, simple_loss=0.3016, pruned_loss=0.08103, ctc_loss=0.1684, over 4734.00 frames. ], tot_loss[loss=0.2225, simple_loss=0.2632, pruned_loss=0.06425, ctc_loss=0.1331, over 966203.16 frames. ], batch size: 26, lr: 1.04e-02,
2024-10-08 16:07:08,573 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=18778.333333333332, ans=0.07
2024-10-08 16:07:28,776 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=18781.666666666668, ans=0.0
2024-10-08 16:07:34,158 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=18781.666666666668, ans=0.00678659420289855
2024-10-08 16:07:34,190 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=18781.666666666668, ans=0.0
2024-10-08 16:07:50,196 INFO [train.py:1153] Epoch 9, batch 1550, loss[loss=0.2664, simple_loss=0.2883, pruned_loss=0.08381, ctc_loss=0.1922, over 4851.00 frames. ], tot_loss[loss=0.221, simple_loss=0.262, pruned_loss=0.06366, ctc_loss=0.1318, over 966048.15 frames. ], batch size: 31, lr: 1.03e-02,
2024-10-08 16:07:53,530 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.65 vs. limit=14.394166666666665
2024-10-08 16:08:09,231 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=18791.666666666668, ans=0.035
2024-10-08 16:08:10,552 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=18791.666666666668, ans=0.125
2024-10-08 16:08:14,784 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=18791.666666666668, ans=0.025
2024-10-08 16:08:37,745 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.844e+01 1.010e+02 1.159e+02 1.275e+02 1.637e+02, threshold=2.319e+02, percent-clipped=0.0
2024-10-08 16:08:57,928 INFO [train.py:1153] Epoch 9, batch 1600, loss[loss=0.193, simple_loss=0.2461, pruned_loss=0.04833, ctc_loss=0.1084, over 4807.00 frames. ], tot_loss[loss=0.2211, simple_loss=0.2623, pruned_loss=0.06362, ctc_loss=0.1317, over 966327.90 frames. ], batch size: 25, lr: 1.03e-02,
2024-10-08 16:09:15,380 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.min_positive, batch_count=18808.333333333332, ans=0.061916666666666675
2024-10-08 16:09:23,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=18811.666666666668, ans=0.125
2024-10-08 16:09:34,246 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=18811.666666666668, ans=0.0
2024-10-08 16:10:05,391 INFO [train.py:1153] Epoch 9, batch 1650, loss[loss=0.2043, simple_loss=0.2569, pruned_loss=0.05013, ctc_loss=0.1284, over 4765.00 frames. ], tot_loss[loss=0.2223, simple_loss=0.2633, pruned_loss=0.06407, ctc_loss=0.133, over 966715.55 frames. ], batch size: 29, lr: 1.03e-02,
2024-10-08 16:10:33,229 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.20 vs. limit=14.560625
2024-10-08 16:10:38,353 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=1.327e-02
2024-10-08 16:10:45,073 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=18831.666666666668, ans=0.025
2024-10-08 16:10:49,526 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=24.11 vs. limit=21.62375
2024-10-08 16:10:52,881 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.251e+01 9.661e+01 1.102e+02 1.298e+02 1.651e+02, threshold=2.204e+02, percent-clipped=0.0
2024-10-08 16:10:58,614 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=18835.0, ans=0.11165
2024-10-08 16:11:13,303 INFO [train.py:1153] Epoch 9, batch 1700, loss[loss=0.1954, simple_loss=0.2467, pruned_loss=0.05059, ctc_loss=0.1074, over 4940.00 frames. ], tot_loss[loss=0.2201, simple_loss=0.2619, pruned_loss=0.06297, ctc_loss=0.1309, over 966810.73 frames. ], batch size: 19, lr: 1.03e-02,
2024-10-08 16:11:36,646 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=18841.666666666668, ans=0.125
2024-10-08 16:11:37,842 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=18841.666666666668, ans=0.24054166666666676
2024-10-08 16:11:37,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=18841.666666666668, ans=0.0
2024-10-08 16:12:13,972 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=18851.666666666668, ans=0.125
2024-10-08 16:12:20,521 INFO [train.py:1153] Epoch 9, batch 1750, loss[loss=0.1725, simple_loss=0.2324, pruned_loss=0.03936, ctc_loss=0.08473, over 4959.00 frames. ], tot_loss[loss=0.2201, simple_loss=0.2615, pruned_loss=0.06307, ctc_loss=0.1313, over 966925.89 frames. ], batch size: 19, lr: 1.03e-02,
2024-10-08 16:12:22,035 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=18855.0, ans=0.125
2024-10-08 16:12:44,990 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=18858.333333333332, ans=0.125
2024-10-08 16:12:48,039 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.10 vs. limit=11.544666666666668
2024-10-08 16:12:54,303 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=18861.666666666668, ans=0.23984166666666673
2024-10-08 16:13:07,435 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.55 vs. limit=14.574375
2024-10-08 16:13:07,851 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.725e+01 1.018e+02 1.136e+02 1.286e+02 1.937e+02, threshold=2.272e+02, percent-clipped=0.0
2024-10-08 16:13:08,561 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.62 vs. limit=14.4325
2024-10-08 16:13:12,662 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.63 vs. limit=14.574375
2024-10-08 16:13:26,931 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 16:13:28,101 INFO [train.py:1153] Epoch 9, batch 1800, loss[loss=0.1803, simple_loss=0.2408, pruned_loss=0.04054, ctc_loss=0.0966, over 4846.00 frames. ], tot_loss[loss=0.2205, simple_loss=0.2618, pruned_loss=0.06319, ctc_loss=0.1319, over 967868.79 frames. ], batch size: 23, lr: 1.03e-02,
2024-10-08 16:13:42,987 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer2.prob, batch_count=18875.0, ans=0.125
2024-10-08 16:13:48,312 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=18875.0, ans=0.0
2024-10-08 16:13:56,483 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.prob, batch_count=18878.333333333332, ans=0.125
2024-10-08 16:14:01,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=18878.333333333332, ans=0.0
2024-10-08 16:14:12,229 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=7.39 vs. limit=14.580625000000001
2024-10-08 16:14:16,712 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=18881.666666666668, ans=0.11118333333333333
2024-10-08 16:14:23,415 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=18885.0, ans=0.0
2024-10-08 16:14:35,331 INFO [train.py:1153] Epoch 9, batch 1850, loss[loss=0.2296, simple_loss=0.2841, pruned_loss=0.06138, ctc_loss=0.131, over 4716.00 frames. ], tot_loss[loss=0.2209, simple_loss=0.262, pruned_loss=0.06344, ctc_loss=0.1325, over 968063.88 frames. ], batch size: 26, lr: 1.03e-02,
2024-10-08 16:14:47,708 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=18891.666666666668, ans=0.483375
2024-10-08 16:15:03,695 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff2_skip_rate, batch_count=18895.0, ans=0.006761956521739131
2024-10-08 16:15:11,709 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=18895.0, ans=0.125
2024-10-08 16:15:15,768 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=18898.333333333332, ans=0.0
2024-10-08 16:15:22,191 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.185e+01 1.038e+02 1.165e+02 1.285e+02 1.762e+02, threshold=2.330e+02, percent-clipped=0.0
2024-10-08 16:15:27,893 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=18901.666666666668, ans=0.125
2024-10-08 16:15:35,860 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=18901.666666666668, ans=0.0
2024-10-08 16:15:42,455 INFO [train.py:1153] Epoch 9, batch 1900, loss[loss=0.2505, simple_loss=0.2867, pruned_loss=0.07497, ctc_loss=0.1609, over 4781.00 frames. ], tot_loss[loss=0.2225, simple_loss=0.2628, pruned_loss=0.0643, ctc_loss=0.1338, over 968031.08 frames. ], batch size: 29, lr: 1.03e-02,
2024-10-08 16:15:45,239 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=18905.0, ans=0.125
2024-10-08 16:16:49,620 INFO [train.py:1153] Epoch 9, batch 1950, loss[loss=0.1967, simple_loss=0.2426, pruned_loss=0.0518, ctc_loss=0.1178, over 4859.00 frames. ], tot_loss[loss=0.2233, simple_loss=0.2632, pruned_loss=0.06482, ctc_loss=0.1345, over 966945.67 frames. ], batch size: 20, lr: 1.03e-02,
2024-10-08 16:17:10,424 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten.whitening_limit, batch_count=18925.0, ans=14.596875
2024-10-08 16:17:36,218 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.754e+01 1.045e+02 1.190e+02 1.328e+02 1.740e+02, threshold=2.380e+02, percent-clipped=0.0
2024-10-08 16:17:41,848 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=18935.0, ans=0.125
2024-10-08 16:17:43,717 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.65 vs. limit=9.73375
2024-10-08 16:17:56,633 INFO [train.py:1153] Epoch 9, batch 2000, loss[loss=0.1761, simple_loss=0.2312, pruned_loss=0.04244, ctc_loss=0.09036, over 4959.00 frames. ], tot_loss[loss=0.2244, simple_loss=0.2639, pruned_loss=0.0653, ctc_loss=0.1356, over 966652.64 frames. ], batch size: 19, lr: 1.03e-02,
2024-10-08 16:18:14,356 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=18941.666666666668, ans=0.11058333333333331
2024-10-08 16:18:14,379 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=18941.666666666668, ans=0.2370416666666667
2024-10-08 16:18:22,476 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=18945.0, ans=10.0
2024-10-08 16:18:30,568 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=18945.0, ans=0.11055000000000001
2024-10-08 16:18:47,969 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=18948.333333333332, ans=0.125
2024-10-08 16:19:03,960 INFO [train.py:1153] Epoch 9, batch 2050, loss[loss=0.1956, simple_loss=0.2293, pruned_loss=0.05787, ctc_loss=0.1154, over 4909.00 frames. ], tot_loss[loss=0.2245, simple_loss=0.2638, pruned_loss=0.06545, ctc_loss=0.1356, over 967075.72 frames. ], batch size: 19, lr: 1.03e-02,
2024-10-08 16:19:06,885 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=18955.0, ans=0.125
2024-10-08 16:19:51,526 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.029e+01 1.001e+02 1.110e+02 1.278e+02 1.693e+02, threshold=2.220e+02, percent-clipped=0.0
2024-10-08 16:20:11,987 INFO [train.py:1153] Epoch 9, batch 2100, loss[loss=0.2097, simple_loss=0.2619, pruned_loss=0.05392, ctc_loss=0.1242, over 4851.00 frames. ], tot_loss[loss=0.2219, simple_loss=0.2623, pruned_loss=0.06412, ctc_loss=0.1333, over 967155.33 frames. ], batch size: 21, lr: 1.03e-02,
2024-10-08 16:20:33,781 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=18975.0, ans=0.0
2024-10-08 16:20:44,782 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=18978.333333333332, ans=0.11021666666666668
2024-10-08 16:20:51,641 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=18981.666666666668, ans=0.125
2024-10-08 16:21:18,688 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=18988.333333333332, ans=0.1101166666666667
2024-10-08 16:21:19,937 INFO [train.py:1153] Epoch 9, batch 2150, loss[loss=0.1639, simple_loss=0.2289, pruned_loss=0.03369, ctc_loss=0.07881, over 4855.00 frames. ], tot_loss[loss=0.2198, simple_loss=0.2609, pruned_loss=0.0631, ctc_loss=0.1314, over 967939.41 frames. ], batch size: 20, lr: 1.03e-02,
2024-10-08 16:21:29,646 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=1.856e-02
2024-10-08 16:21:38,806 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 16:21:41,581 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=18991.666666666668, ans=0.0
2024-10-08 16:21:44,331 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=18991.666666666668, ans=0.125
2024-10-08 16:22:07,535 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.681e+01 9.665e+01 1.122e+02 1.249e+02 1.685e+02, threshold=2.244e+02, percent-clipped=0.0
2024-10-08 16:22:13,045 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=19001.666666666668, ans=0.10998333333333332
2024-10-08 16:22:27,310 INFO [train.py:1153] Epoch 9, batch 2200, loss[loss=0.1976, simple_loss=0.2425, pruned_loss=0.05466, ctc_loss=0.1085, over 4746.00 frames. ], tot_loss[loss=0.2197, simple_loss=0.2607, pruned_loss=0.06313, ctc_loss=0.1312, over 967522.30 frames. ], batch size: 26, lr: 1.03e-02,
2024-10-08 16:22:31,602 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=19005.0, ans=0.125
2024-10-08 16:22:32,828 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=19005.0, ans=0.23482500000000006
2024-10-08 16:23:09,349 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer_ff2.min_abs, batch_count=19015.0, ans=0.1
2024-10-08 16:23:18,740 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=19015.0, ans=0.125
2024-10-08 16:23:34,714 INFO [train.py:1153] Epoch 9, batch 2250, loss[loss=0.2028, simple_loss=0.2598, pruned_loss=0.05227, ctc_loss=0.1033, over 4883.00 frames. ], tot_loss[loss=0.2196, simple_loss=0.2607, pruned_loss=0.06312, ctc_loss=0.131, over 967521.02 frames. ], batch size: 22, lr: 1.03e-02,
2024-10-08 16:23:48,506 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=19025.0, ans=0.125
2024-10-08 16:24:22,234 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.527e+01 1.021e+02 1.114e+02 1.253e+02 1.663e+02, threshold=2.227e+02, percent-clipped=0.0
2024-10-08 16:24:23,783 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=19031.666666666668, ans=0.125
2024-10-08 16:24:42,523 INFO [train.py:1153] Epoch 9, batch 2300, loss[loss=0.2063, simple_loss=0.2564, pruned_loss=0.05133, ctc_loss=0.1338, over 4883.00 frames. ], tot_loss[loss=0.2191, simple_loss=0.2605, pruned_loss=0.06279, ctc_loss=0.1302, over 968325.25 frames. ], batch size: 19, lr: 1.03e-02,
2024-10-08 16:24:45,340 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=19038.333333333332, ans=0.2336583333333334
2024-10-08 16:25:11,373 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=19045.0, ans=0.125
2024-10-08 16:25:13,989 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=19045.0, ans=0.0
2024-10-08 16:25:27,516 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=19048.333333333332, ans=0.10951666666666668
2024-10-08 16:25:38,250 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=19051.666666666668, ans=0.125
2024-10-08 16:25:39,563 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=19051.666666666668, ans=0.025
2024-10-08 16:25:50,298 INFO [train.py:1153] Epoch 9, batch 2350, loss[loss=0.2132, simple_loss=0.2709, pruned_loss=0.05522, ctc_loss=0.1128, over 4840.00 frames. ], tot_loss[loss=0.2193, simple_loss=0.2606, pruned_loss=0.06297, ctc_loss=0.1302, over 968326.73 frames. ], batch size: 23, lr: 1.03e-02,
2024-10-08 16:25:57,049 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=19055.0, ans=0.006727173913043479
2024-10-08 16:25:57,088 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=19055.0, ans=0.025
2024-10-08 16:25:57,092 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=19055.0, ans=0.006727173913043479
2024-10-08 16:26:11,033 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.67 vs. limit=14.529166666666665
2024-10-08 16:26:22,448 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer1.prob, batch_count=19061.666666666668, ans=0.125
2024-10-08 16:26:22,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=19061.666666666668, ans=0.1093833333333333
2024-10-08 16:26:31,891 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=19065.0, ans=0.125
2024-10-08 16:26:37,094 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.536e+01 1.040e+02 1.153e+02 1.347e+02 1.877e+02, threshold=2.306e+02, percent-clipped=0.0
2024-10-08 16:26:37,713 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.72 vs. limit=5.85975
2024-10-08 16:26:42,970 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.19 vs. limit=21.80125
2024-10-08 16:26:57,286 INFO [train.py:1153] Epoch 9, batch 2400, loss[loss=0.2186, simple_loss=0.2606, pruned_loss=0.06195, ctc_loss=0.1317, over 4754.00 frames. ], tot_loss[loss=0.2197, simple_loss=0.2607, pruned_loss=0.0631, ctc_loss=0.1311, over 967582.63 frames. ], batch size: 19, lr: 1.03e-02,
2024-10-08 16:27:00,026 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=19071.666666666668, ans=0.0
2024-10-08 16:27:11,968 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=19075.0, ans=0.125
2024-10-08 16:27:14,570 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=19075.0, ans=0.125
2024-10-08 16:27:35,114 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.25 vs. limit=14.654375
2024-10-08 16:27:56,043 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=19085.0, ans=0.07
2024-10-08 16:28:03,913 INFO [train.py:1153] Epoch 9, batch 2450, loss[loss=0.217, simple_loss=0.263, pruned_loss=0.06052, ctc_loss=0.1248, over 4860.00 frames. ], tot_loss[loss=0.2198, simple_loss=0.2611, pruned_loss=0.063, ctc_loss=0.1312, over 966813.18 frames. ], batch size: 22, lr: 1.03e-02,
2024-10-08 16:28:25,727 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=19091.666666666668, ans=0.0
2024-10-08 16:28:32,407 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=19095.0, ans=0.0
2024-10-08 16:28:35,059 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=19095.0, ans=0.125
2024-10-08 16:28:40,432 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=19095.0, ans=0.125
2024-10-08 16:28:50,879 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.657e+01 1.091e+02 1.206e+02 1.336e+02 1.597e+02, threshold=2.413e+02, percent-clipped=0.0
2024-10-08 16:29:00,622 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=19101.666666666668, ans=0.125
2024-10-08 16:29:04,578 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=19101.666666666668, ans=0.0
2024-10-08 16:29:11,230 INFO [train.py:1153] Epoch 9, batch 2500, loss[loss=0.2497, simple_loss=0.2798, pruned_loss=0.07672, ctc_loss=0.1656, over 4719.00 frames. ], tot_loss[loss=0.2203, simple_loss=0.2613, pruned_loss=0.06319, ctc_loss=0.1323, over 966401.49 frames. ], batch size: 26, lr: 1.03e-02,
2024-10-08 16:29:33,149 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=19108.333333333332, ans=0.125
2024-10-08 16:29:43,743 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=19111.666666666668, ans=0.006714855072463768
2024-10-08 16:29:54,630 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=19115.0, ans=0.0
2024-10-08 16:30:12,191 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=19118.333333333332, ans=0.125
2024-10-08 16:30:18,673 INFO [train.py:1153] Epoch 9, batch 2550, loss[loss=0.1865, simple_loss=0.2237, pruned_loss=0.05412, ctc_loss=0.1025, over 4959.00 frames. ], tot_loss[loss=0.2194, simple_loss=0.2604, pruned_loss=0.06293, ctc_loss=0.1314, over 966825.27 frames. ], batch size: 19, lr: 1.03e-02,
2024-10-08 16:30:21,539 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=19121.666666666668, ans=0.10878333333333334
2024-10-08 16:30:51,136 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=19128.333333333332, ans=0.23050833333333343
2024-10-08 16:31:05,862 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.319e+01 1.041e+02 1.160e+02 1.319e+02 1.850e+02, threshold=2.319e+02, percent-clipped=0.0
2024-10-08 16:31:11,393 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=19135.0, ans=0.0
2024-10-08 16:31:25,768 INFO [train.py:1153] Epoch 9, batch 2600, loss[loss=0.1808, simple_loss=0.2364, pruned_loss=0.04485, ctc_loss=0.0888, over 4860.00 frames. ], tot_loss[loss=0.2196, simple_loss=0.2604, pruned_loss=0.06316, ctc_loss=0.1312, over 966328.87 frames. ], batch size: 20, lr: 1.03e-02,
2024-10-08 16:31:28,570 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=19138.333333333332, ans=0.0
2024-10-08 16:31:55,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=19145.0, ans=0.10855000000000001
2024-10-08 16:32:06,193 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=19148.333333333332, ans=0.125
2024-10-08 16:32:06,720 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=16.53 vs. limit=14.680625
2024-10-08 16:32:18,065 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer1.prob, batch_count=19151.666666666668, ans=0.125
2024-10-08 16:32:19,437 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer2.prob, batch_count=19151.666666666668, ans=0.125
2024-10-08 16:32:32,553 INFO [train.py:1153] Epoch 9, batch 2650, loss[loss=0.2611, simple_loss=0.2911, pruned_loss=0.08168, ctc_loss=0.1695, over 4837.00 frames. ], tot_loss[loss=0.2215, simple_loss=0.2617, pruned_loss=0.06406, ctc_loss=0.133, over 966048.73 frames. ], batch size: 38, lr: 1.02e-02,
2024-10-08 16:32:50,818 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=6.55 vs. limit=14.684375
2024-10-08 16:32:58,150 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=19161.666666666668, ans=0.006703985507246377
2024-10-08 16:33:00,795 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=19161.666666666668, ans=0.125
2024-10-08 16:33:09,230 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=11.43 vs. limit=14.685625
2024-10-08 16:33:19,362 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.988e+01 9.962e+01 1.133e+02 1.299e+02 1.806e+02, threshold=2.266e+02, percent-clipped=0.0
2024-10-08 16:33:20,925 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=19165.0, ans=0.10835
2024-10-08 16:33:24,902 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=19168.333333333332, ans=0.125
2024-10-08 16:33:34,135 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=19168.333333333332, ans=0.125
2024-10-08 16:33:39,305 INFO [train.py:1153] Epoch 9, batch 2700, loss[loss=0.1944, simple_loss=0.2527, pruned_loss=0.04737, ctc_loss=0.1031, over 4847.00 frames. ], tot_loss[loss=0.2201, simple_loss=0.2609, pruned_loss=0.06332, ctc_loss=0.1317, over 966230.34 frames. ], batch size: 28, lr: 1.02e-02,
2024-10-08 16:33:52,911 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=19175.0, ans=0.0
2024-10-08 16:34:10,542 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=19178.333333333332, ans=0.125
2024-10-08 16:34:19,372 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module1.whiten, num_groups=1, num_channels=192, metric=5.44 vs. limit=14.693125
2024-10-08 16:34:34,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=19185.0, ans=0.22852499999999998
2024-10-08 16:34:46,718 INFO [train.py:1153] Epoch 9, batch 2750, loss[loss=0.2103, simple_loss=0.2544, pruned_loss=0.05997, ctc_loss=0.1156, over 4799.00 frames. ], tot_loss[loss=0.2183, simple_loss=0.26, pruned_loss=0.06237, ctc_loss=0.1297, over 967000.74 frames. ], batch size: 19, lr: 1.02e-02,
2024-10-08 16:34:54,033 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.62 vs. limit=14.594166666666666
2024-10-08 16:35:00,552 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=19191.666666666668, ans=0.006697463768115941
2024-10-08 16:35:04,467 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=19191.666666666668, ans=0.0
2024-10-08 16:35:14,667 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten.whitening_limit, batch_count=19195.0, ans=14.698125000000001
2024-10-08 16:35:33,954 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.22 vs. limit=5.87975
2024-10-08 16:35:34,190 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.869e+01 1.020e+02 1.174e+02 1.306e+02 2.032e+02, threshold=2.348e+02, percent-clipped=0.0
2024-10-08 16:35:34,782 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.03 vs. limit=14.699375
2024-10-08 16:35:41,508 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.76 vs. limit=11.680666666666667
2024-10-08 16:35:43,745 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=19201.666666666668, ans=0.125
2024-10-08 16:35:51,667 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=19201.666666666668, ans=0.0
2024-10-08 16:35:54,131 INFO [train.py:1153] Epoch 9, batch 2800, loss[loss=0.2427, simple_loss=0.2705, pruned_loss=0.07643, ctc_loss=0.1551, over 4785.00 frames. ], tot_loss[loss=0.2203, simple_loss=0.2613, pruned_loss=0.06335, ctc_loss=0.1316, over 967130.97 frames. ], batch size: 53, lr: 1.02e-02,
2024-10-08 16:36:05,423 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=14.99 vs. limit=14.701875000000001
2024-10-08 16:36:14,351 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=19208.333333333332, ans=0.2277083333333334
2024-10-08 16:36:47,739 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=19218.333333333332, ans=0.025
2024-10-08 16:37:01,218 INFO [train.py:1153] Epoch 9, batch 2850, loss[loss=0.1658, simple_loss=0.2282, pruned_loss=0.03647, ctc_loss=0.0761, over 4928.00 frames. ], tot_loss[loss=0.2216, simple_loss=0.2621, pruned_loss=0.06394, ctc_loss=0.1331, over 966900.58 frames. ], batch size: 20, lr: 1.02e-02,
2024-10-08 16:37:03,022 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=8.91 vs. limit=14.708124999999999
2024-10-08 16:37:16,050 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=19225.0, ans=0.0
2024-10-08 16:37:27,346 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.38 vs. limit=9.807083333333333
2024-10-08 16:37:32,270 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=19228.333333333332, ans=0.125
2024-10-08 16:37:36,287 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=19228.333333333332, ans=0.22700833333333337
2024-10-08 16:37:48,272 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.532e+01 1.051e+02 1.168e+02 1.348e+02 2.211e+02, threshold=2.335e+02, percent-clipped=0.0
2024-10-08 16:38:06,967 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=19238.333333333332, ans=0.125
2024-10-08 16:38:08,196 INFO [train.py:1153] Epoch 9, batch 2900, loss[loss=0.1706, simple_loss=0.2181, pruned_loss=0.04179, ctc_loss=0.0986, over 4761.00 frames. ], tot_loss[loss=0.2224, simple_loss=0.2623, pruned_loss=0.06452, ctc_loss=0.1337, over 966092.37 frames. ], batch size: 20, lr: 1.02e-02,
2024-10-08 16:38:23,419 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=19241.666666666668, ans=0.125
2024-10-08 16:38:32,769 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=19241.666666666668, ans=0.125
2024-10-08 16:38:36,905 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=19245.0, ans=0.226425
2024-10-08 16:38:42,176 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=19245.0, ans=0.10755
2024-10-08 16:38:53,517 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.52 vs. limit=14.718125
2024-10-08 16:39:15,352 INFO [train.py:1153] Epoch 9, batch 2950, loss[loss=0.1717, simple_loss=0.2353, pruned_loss=0.03697, ctc_loss=0.08572, over 4800.00 frames. ], tot_loss[loss=0.222, simple_loss=0.2621, pruned_loss=0.06443, ctc_loss=0.1329, over 966636.28 frames. ], batch size: 19, lr: 1.02e-02,
2024-10-08 16:39:19,924 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.58 vs. limit=14.720625
2024-10-08 16:39:20,805 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=19255.0, ans=0.0
2024-10-08 16:39:22,233 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=19255.0, ans=0.125
2024-10-08 16:39:36,943 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=19258.333333333332, ans=0.10741666666666669
2024-10-08 16:39:42,434 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=19261.666666666668, ans=0.22584166666666672
2024-10-08 16:39:45,145 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=19261.666666666668, ans=0.125
2024-10-08 16:39:56,013 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=19265.0, ans=0.125
2024-10-08 16:39:58,691 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 16:40:02,570 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.476e+01 1.002e+02 1.182e+02 1.332e+02 1.709e+02, threshold=2.364e+02, percent-clipped=0.0
2024-10-08 16:40:05,442 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=19265.0, ans=0.125
2024-10-08 16:40:22,922 INFO [train.py:1153] Epoch 9, batch 3000, loss[loss=0.2168, simple_loss=0.2668, pruned_loss=0.05764, ctc_loss=0.1287, over 4848.00 frames. ], tot_loss[loss=0.2218, simple_loss=0.2619, pruned_loss=0.06435, ctc_loss=0.1325, over 967260.56 frames. ], batch size: 21, lr: 1.02e-02,
2024-10-08 16:40:22,923 INFO [train.py:1176] Computing validation loss
2024-10-08 16:40:29,681 INFO [zipformer.py:1858] name=encoder.encoders.4.encoder.layers.1.self_attn_weights, attn_weights_entropy = tensor([3.4044, 4.1503, 3.4628, 4.0413], device='cuda:0')
2024-10-08 16:40:30,293 INFO [zipformer.py:1858] name=encoder.encoders.2.encoder.layers.0.self_attn_weights, attn_weights_entropy = tensor([2.5941, 3.7627, 3.2239, 3.6783], device='cuda:0')
2024-10-08 16:40:30,935 INFO [train.py:1185] Epoch 9, validation: loss=0.1558, simple_loss=0.2403, pruned_loss=0.02596, ctc_loss=0.04853, over 90464.00 frames.
2024-10-08 16:40:30,935 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 16:40:42,262 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=19271.666666666668, ans=0.125
2024-10-08 16:40:46,999 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=19275.0, ans=0.125
2024-10-08 16:40:48,735 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=13.09 vs. limit=14.728125
2024-10-08 16:40:52,196 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=19275.0, ans=0.0
2024-10-08 16:40:52,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=19275.0, ans=0.125
2024-10-08 16:41:17,837 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=7.53 vs. limit=11.712666666666667
2024-10-08 16:41:22,608 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=19285.0, ans=0.125
2024-10-08 16:41:36,853 INFO [train.py:1153] Epoch 9, batch 3050, loss[loss=0.1985, simple_loss=0.2577, pruned_loss=0.04966, ctc_loss=0.09984, over 4753.00 frames. ], tot_loss[loss=0.2214, simple_loss=0.2617, pruned_loss=0.06403, ctc_loss=0.1326, over 966749.04 frames. ], batch size: 19, lr: 1.02e-02,
2024-10-08 16:41:44,922 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=19288.333333333332, ans=0.0
2024-10-08 16:41:51,643 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=19291.666666666668, ans=0.22479166666666672
2024-10-08 16:41:55,618 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=19291.666666666668, ans=0.025
2024-10-08 16:42:23,488 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.165e+01 1.025e+02 1.132e+02 1.242e+02 1.905e+02, threshold=2.263e+02, percent-clipped=0.0
2024-10-08 16:42:25,036 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=19298.333333333332, ans=0.10701666666666668
2024-10-08 16:42:30,554 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=19301.666666666668, ans=0.22444166666666676
2024-10-08 16:42:41,562 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.08 vs. limit=14.738125
2024-10-08 16:42:43,803 INFO [train.py:1153] Epoch 9, batch 3100, loss[loss=0.2548, simple_loss=0.2866, pruned_loss=0.0784, ctc_loss=0.1656, over 4833.00 frames. ], tot_loss[loss=0.2208, simple_loss=0.2615, pruned_loss=0.06369, ctc_loss=0.1319, over 966377.85 frames. ], batch size: 38, lr: 1.02e-02,
2024-10-08 16:43:13,462 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.min_abs, batch_count=19311.666666666668, ans=0.489675
2024-10-08 16:43:18,736 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=19311.666666666668, ans=0.125
2024-10-08 16:43:19,094 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=3.95 vs. limit=9.827916666666667
2024-10-08 16:43:46,510 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=6.07 vs. limit=14.744375
2024-10-08 16:43:47,049 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=19318.333333333332, ans=0.125
2024-10-08 16:43:47,137 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=19318.333333333332, ans=0.125
2024-10-08 16:43:50,797 INFO [train.py:1153] Epoch 9, batch 3150, loss[loss=0.2292, simple_loss=0.261, pruned_loss=0.0702, ctc_loss=0.1424, over 4784.00 frames. ], tot_loss[loss=0.2201, simple_loss=0.261, pruned_loss=0.06327, ctc_loss=0.1315, over 966711.16 frames. ], batch size: 40, lr: 1.02e-02,
2024-10-08 16:43:56,399 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=19321.666666666668, ans=0.10678333333333334
2024-10-08 16:44:23,608 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.max_abs, batch_count=19328.333333333332, ans=10.0
2024-10-08 16:44:26,246 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=19328.333333333332, ans=0.10671666666666668
2024-10-08 16:44:26,278 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=19328.333333333332, ans=0.006667753623188406
2024-10-08 16:44:38,372 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.079e+01 1.019e+02 1.146e+02 1.283e+02 1.814e+02, threshold=2.292e+02, percent-clipped=0.0
2024-10-08 16:44:43,912 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=19335.0, ans=0.10665000000000002
2024-10-08 16:44:43,917 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=19335.0, ans=0.10665000000000002
2024-10-08 16:44:54,587 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=19335.0, ans=0.125
2024-10-08 16:44:55,882 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=19335.0, ans=0.223275
2024-10-08 16:44:58,444 INFO [train.py:1153] Epoch 9, batch 3200, loss[loss=0.2516, simple_loss=0.2811, pruned_loss=0.07773, ctc_loss=0.1664, over 4745.00 frames. ], tot_loss[loss=0.2193, simple_loss=0.2607, pruned_loss=0.06286, ctc_loss=0.1306, over 967145.97 frames. ], batch size: 20, lr: 1.02e-02,
2024-10-08 16:45:26,926 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 16:45:45,427 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=19348.333333333332, ans=0.22280833333333339
2024-10-08 16:46:00,105 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=19351.666666666668, ans=0.025
2024-10-08 16:46:05,433 INFO [train.py:1153] Epoch 9, batch 3250, loss[loss=0.2685, simple_loss=0.293, pruned_loss=0.08779, ctc_loss=0.171, over 4853.00 frames. ], tot_loss[loss=0.2205, simple_loss=0.2616, pruned_loss=0.06345, ctc_loss=0.131, over 967294.26 frames. ], batch size: 24, lr: 1.02e-02,
2024-10-08 16:46:41,032 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.40 vs. limit=14.760625000000001
2024-10-08 16:46:42,030 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=19361.666666666668, ans=0.125
2024-10-08 16:46:52,420 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.001e+01 9.926e+01 1.154e+02 1.272e+02 1.849e+02, threshold=2.309e+02, percent-clipped=0.0
2024-10-08 16:47:12,805 INFO [train.py:1153] Epoch 9, batch 3300, loss[loss=0.2691, simple_loss=0.3147, pruned_loss=0.07952, ctc_loss=0.1614, over 4812.00 frames. ], tot_loss[loss=0.2204, simple_loss=0.2618, pruned_loss=0.06335, ctc_loss=0.131, over 967852.76 frames. ], batch size: 43, lr: 1.02e-02,
2024-10-08 16:47:15,083 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=4.47 vs. limit=14.764375000000001
2024-10-08 16:47:20,864 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=19371.666666666668, ans=0.125
2024-10-08 16:47:39,635 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=19378.333333333332, ans=0.125
2024-10-08 16:47:41,407 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.77 vs. limit=11.751333333333333
2024-10-08 16:48:07,740 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=19385.0, ans=0.10615000000000002
2024-10-08 16:48:10,424 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=19385.0, ans=0.125
2024-10-08 16:48:19,554 INFO [train.py:1153] Epoch 9, batch 3350, loss[loss=0.2171, simple_loss=0.2632, pruned_loss=0.06158, ctc_loss=0.1196, over 4791.00 frames. ], tot_loss[loss=0.2226, simple_loss=0.2629, pruned_loss=0.06453, ctc_loss=0.1329, over 967056.66 frames. ], batch size: 40, lr: 1.02e-02,
2024-10-08 16:48:24,971 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=19388.333333333332, ans=0.22140833333333343
2024-10-08 16:48:26,193 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=19388.333333333332, ans=0.125
2024-10-08 16:48:38,353 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=19391.666666666668, ans=0.22129166666666666
2024-10-08 16:48:57,041 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=19395.0, ans=0.10605
2024-10-08 16:49:07,030 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.132e+01 1.070e+02 1.185e+02 1.358e+02 1.852e+02, threshold=2.370e+02, percent-clipped=0.0
2024-10-08 16:49:11,895 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=19401.666666666668, ans=0.125
2024-10-08 16:49:22,772 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=19401.666666666668, ans=0.0
2024-10-08 16:49:26,538 INFO [train.py:1153] Epoch 9, batch 3400, loss[loss=0.1862, simple_loss=0.236, pruned_loss=0.04786, ctc_loss=0.1014, over 4959.00 frames. ], tot_loss[loss=0.2225, simple_loss=0.2625, pruned_loss=0.0646, ctc_loss=0.1333, over 966809.56 frames. ], batch size: 19, lr: 1.02e-02,
2024-10-08 16:49:31,900 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=19405.0, ans=0.0
2024-10-08 16:49:32,723 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=19.16 vs. limit=22.05375
2024-10-08 16:49:35,750 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=19405.0, ans=0.10595000000000002
2024-10-08 16:49:50,702 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=19408.333333333332, ans=0.2207083333333334
2024-10-08 16:49:51,990 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=19411.666666666668, ans=0.0066496376811594204
2024-10-08 16:50:08,089 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=19415.0, ans=0.10585
2024-10-08 16:50:33,449 INFO [train.py:1153] Epoch 9, batch 3450, loss[loss=0.2309, simple_loss=0.2656, pruned_loss=0.06682, ctc_loss=0.1566, over 4835.00 frames. ], tot_loss[loss=0.2214, simple_loss=0.2618, pruned_loss=0.06405, ctc_loss=0.1321, over 967051.99 frames. ], batch size: 43, lr: 1.02e-02,
2024-10-08 16:50:36,287 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=19421.666666666668, ans=0.125
2024-10-08 16:51:03,632 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=6.12 vs. limit=14.785625
2024-10-08 16:51:20,338 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.789e+01 9.878e+01 1.143e+02 1.267e+02 2.315e+02, threshold=2.287e+02, percent-clipped=0.0
2024-10-08 16:51:38,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=19435.0, ans=0.21977500000000005
2024-10-08 16:51:40,644 INFO [train.py:1153] Epoch 9, batch 3500, loss[loss=0.1989, simple_loss=0.2452, pruned_loss=0.05368, ctc_loss=0.1131, over 4883.00 frames. ], tot_loss[loss=0.2205, simple_loss=0.261, pruned_loss=0.06369, ctc_loss=0.1318, over 967383.18 frames. ], batch size: 19, lr: 1.02e-02,
2024-10-08 16:51:42,151 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=19438.333333333332, ans=0.0
2024-10-08 16:51:50,263 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=19438.333333333332, ans=0.10561666666666669
2024-10-08 16:51:50,268 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=19438.333333333332, ans=0.10561666666666669
2024-10-08 16:51:51,656 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=19438.333333333332, ans=0.2196583333333334
2024-10-08 16:51:54,354 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=19441.666666666668, ans=0.125
2024-10-08 16:52:04,424 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.79 vs. limit=14.790625
2024-10-08 16:52:10,489 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=19445.0, ans=0.21942499999999998
2024-10-08 16:52:42,078 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.12 vs. limit=14.794374999999999
2024-10-08 16:52:46,847 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=19455.0, ans=0.10545000000000002
2024-10-08 16:52:47,999 INFO [train.py:1153] Epoch 9, batch 3550, loss[loss=0.2256, simple_loss=0.2543, pruned_loss=0.07163, ctc_loss=0.134, over 4790.00 frames. ], tot_loss[loss=0.2197, simple_loss=0.2603, pruned_loss=0.06327, ctc_loss=0.1311, over 967396.29 frames. ], batch size: 29, lr: 1.02e-02,
2024-10-08 16:52:59,408 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=5.36 vs. limit=11.782
2024-10-08 16:53:06,085 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.15 vs. limit=14.729166666666666
2024-10-08 16:53:35,128 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.986e+01 1.001e+02 1.144e+02 1.275e+02 2.287e+02, threshold=2.289e+02, percent-clipped=1.0
2024-10-08 16:53:49,055 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=19468.333333333332, ans=0.2186083333333334
2024-10-08 16:53:52,900 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 16:53:55,499 INFO [train.py:1153] Epoch 9, batch 3600, loss[loss=0.2112, simple_loss=0.2669, pruned_loss=0.05621, ctc_loss=0.1073, over 4933.00 frames. ], tot_loss[loss=0.2192, simple_loss=0.26, pruned_loss=0.06308, ctc_loss=0.1307, over 967638.32 frames. ], batch size: 20, lr: 1.02e-02,
2024-10-08 16:53:55,716 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=19471.666666666668, ans=0.025
2024-10-08 16:54:05,592 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.95 vs. limit=11.788666666666668
2024-10-08 16:55:02,988 INFO [train.py:1153] Epoch 9, batch 3650, loss[loss=0.2807, simple_loss=0.3069, pruned_loss=0.09253, ctc_loss=0.1735, over 4864.00 frames. ], tot_loss[loss=0.2193, simple_loss=0.26, pruned_loss=0.06321, ctc_loss=0.1303, over 968053.18 frames. ], batch size: 31, lr: 1.02e-02,
2024-10-08 16:55:08,494 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=19488.333333333332, ans=0.125
2024-10-08 16:55:12,582 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=19488.333333333332, ans=0.125
2024-10-08 16:55:14,549 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.63 vs. limit=14.808125
2024-10-08 16:55:15,270 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=19491.666666666668, ans=0.125
2024-10-08 16:55:16,508 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=19491.666666666668, ans=0.125
2024-10-08 16:55:18,480 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=5.48 vs. limit=5.92375
2024-10-08 16:55:25,979 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 16:55:38,058 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=19495.0, ans=0.21767500000000006
2024-10-08 16:55:49,890 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.290e+01 9.591e+01 1.088e+02 1.277e+02 1.773e+02, threshold=2.175e+02, percent-clipped=0.0
2024-10-08 16:55:55,845 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.09 vs. limit=22.12625
2024-10-08 16:55:58,652 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.72 vs. limit=14.813125
2024-10-08 16:56:02,967 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=3.55 vs. limit=11.800666666666668
2024-10-08 16:56:10,223 INFO [train.py:1153] Epoch 9, batch 3700, loss[loss=0.1848, simple_loss=0.229, pruned_loss=0.0483, ctc_loss=0.1098, over 4845.00 frames. ], tot_loss[loss=0.2189, simple_loss=0.2598, pruned_loss=0.06298, ctc_loss=0.13, over 967400.63 frames. ], batch size: 24, lr: 1.02e-02,
2024-10-08 16:56:10,379 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=19505.0, ans=0.217325
2024-10-08 16:57:05,894 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=19518.333333333332, ans=0.1048166666666667
2024-10-08 16:57:07,202 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff3_skip_rate, batch_count=19518.333333333332, ans=0.006626449275362319
2024-10-08 16:57:09,469 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.80 vs. limit=14.819375
2024-10-08 16:57:17,892 INFO [train.py:1153] Epoch 9, batch 3750, loss[loss=0.1978, simple_loss=0.2432, pruned_loss=0.0532, ctc_loss=0.1152, over 4959.00 frames. ], tot_loss[loss=0.2191, simple_loss=0.2599, pruned_loss=0.0632, ctc_loss=0.1299, over 967700.79 frames. ], batch size: 19, lr: 1.02e-02,
2024-10-08 16:57:18,871 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.88 vs. limit=14.820625
2024-10-08 16:57:19,468 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 16:58:04,513 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.134e+01 9.900e+01 1.133e+02 1.283e+02 1.764e+02, threshold=2.267e+02, percent-clipped=0.0
2024-10-08 16:58:19,397 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=19535.0, ans=0.125
2024-10-08 16:58:19,886 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.65 vs. limit=11.814
2024-10-08 16:58:24,596 INFO [train.py:1153] Epoch 9, batch 3800, loss[loss=0.2606, simple_loss=0.2917, pruned_loss=0.08447, ctc_loss=0.1512, over 4734.00 frames. ], tot_loss[loss=0.2186, simple_loss=0.2602, pruned_loss=0.06272, ctc_loss=0.1292, over 967532.86 frames. ], batch size: 26, lr: 1.01e-02,
2024-10-08 16:58:27,789 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.77 vs. limit=14.826875000000001
2024-10-08 16:58:28,669 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=19538.333333333332, ans=0.125
2024-10-08 16:58:31,934 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.58 vs. limit=14.826875000000001
2024-10-08 16:58:39,650 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=5.473e-03
2024-10-08 16:59:01,097 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=19545.0, ans=0.125
2024-10-08 16:59:11,030 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.20 vs. limit=14.830625000000001
2024-10-08 16:59:29,611 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=19551.666666666668, ans=0.025
2024-10-08 16:59:32,077 INFO [train.py:1153] Epoch 9, batch 3850, loss[loss=0.2296, simple_loss=0.2723, pruned_loss=0.06408, ctc_loss=0.1468, over 4827.00 frames. ], tot_loss[loss=0.2176, simple_loss=0.2592, pruned_loss=0.06229, ctc_loss=0.1286, over 967580.68 frames. ], batch size: 38, lr: 1.01e-02,
2024-10-08 16:59:32,313 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=19555.0, ans=0.21557500000000007
2024-10-08 16:59:37,592 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=19555.0, ans=0.21557500000000007
2024-10-08 16:59:37,635 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=19555.0, ans=0.0
2024-10-08 16:59:41,604 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=19555.0, ans=0.10445000000000002
2024-10-08 16:59:46,161 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.25 vs. limit=14.834375
2024-10-08 16:59:47,138 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=19558.333333333332, ans=0.006617753623188406
2024-10-08 17:00:00,600 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=19561.666666666668, ans=0.025
2024-10-08 17:00:12,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=19565.0, ans=0.493475
2024-10-08 17:00:19,376 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.455e+01 1.025e+02 1.163e+02 1.306e+02 1.858e+02, threshold=2.325e+02, percent-clipped=0.0
2024-10-08 17:00:19,585 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=19565.0, ans=0.10435
2024-10-08 17:00:23,632 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=19565.0, ans=0.006616304347826087
2024-10-08 17:00:39,543 INFO [train.py:1153] Epoch 9, batch 3900, loss[loss=0.1845, simple_loss=0.2414, pruned_loss=0.04301, ctc_loss=0.1037, over 4734.00 frames. ], tot_loss[loss=0.2181, simple_loss=0.2593, pruned_loss=0.0625, ctc_loss=0.1296, over 967061.49 frames. ], batch size: 26, lr: 1.01e-02,
2024-10-08 17:00:59,784 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=19575.0, ans=0.21487500000000004
2024-10-08 17:01:05,340 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=19578.333333333332, ans=0.0
2024-10-08 17:01:14,763 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=19578.333333333332, ans=0.0
2024-10-08 17:01:29,938 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.prob, batch_count=19581.666666666668, ans=0.125
2024-10-08 17:01:47,183 INFO [train.py:1153] Epoch 9, batch 3950, loss[loss=0.279, simple_loss=0.3039, pruned_loss=0.08818, ctc_loss=0.1944, over 4817.00 frames. ], tot_loss[loss=0.2171, simple_loss=0.259, pruned_loss=0.06192, ctc_loss=0.1284, over 967255.30 frames. ], batch size: 36, lr: 1.01e-02,
2024-10-08 17:01:51,718 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten.whitening_limit, batch_count=19588.333333333332, ans=14.845625
2024-10-08 17:02:03,507 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=19591.666666666668, ans=0.10408333333333333
2024-10-08 17:02:21,520 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.84 vs. limit=5.9392499999999995
2024-10-08 17:02:23,955 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=19595.0, ans=0.0
2024-10-08 17:02:34,751 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.029e+01 9.593e+01 1.115e+02 1.236e+02 1.996e+02, threshold=2.231e+02, percent-clipped=0.0
2024-10-08 17:02:41,914 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.33 vs. limit=22.20125
2024-10-08 17:02:52,272 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=19601.666666666668, ans=0.006608333333333333
2024-10-08 17:02:52,770 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=23.23 vs. limit=22.20125
2024-10-08 17:02:53,566 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=19605.0, ans=0.125
2024-10-08 17:02:54,771 INFO [train.py:1153] Epoch 9, batch 4000, loss[loss=0.1889, simple_loss=0.2261, pruned_loss=0.0547, ctc_loss=0.1059, over 4815.00 frames. ], tot_loss[loss=0.2164, simple_loss=0.2582, pruned_loss=0.06164, ctc_loss=0.1284, over 967328.26 frames. ], batch size: 19, lr: 1.01e-02,
2024-10-08 17:03:12,537 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=19608.333333333332, ans=0.025
2024-10-08 17:03:14,259 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.93 vs. limit=14.853125
2024-10-08 17:03:25,907 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=19611.666666666668, ans=0.10388333333333333
2024-10-08 17:04:02,254 INFO [train.py:1153] Epoch 9, batch 4050, loss[loss=0.2688, simple_loss=0.297, pruned_loss=0.08681, ctc_loss=0.1672, over 4806.00 frames. ], tot_loss[loss=0.2172, simple_loss=0.2585, pruned_loss=0.06211, ctc_loss=0.129, over 967744.95 frames. ], batch size: 53, lr: 1.01e-02,
2024-10-08 17:04:02,912 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=15.93 vs. limit=14.858125000000001
2024-10-08 17:04:20,399 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=7.93 vs. limit=14.859375
2024-10-08 17:04:21,215 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=19625.0, ans=0.0
2024-10-08 17:04:25,224 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=19625.0, ans=0.125
2024-10-08 17:04:49,019 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.035e+01 1.001e+02 1.143e+02 1.277e+02 1.837e+02, threshold=2.285e+02, percent-clipped=0.0
2024-10-08 17:04:53,259 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=19631.666666666668, ans=0.125
2024-10-08 17:05:01,352 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=19635.0, ans=0.125
2024-10-08 17:05:09,014 INFO [train.py:1153] Epoch 9, batch 4100, loss[loss=0.2298, simple_loss=0.2623, pruned_loss=0.06703, ctc_loss=0.1583, over 4868.00 frames. ], tot_loss[loss=0.219, simple_loss=0.2598, pruned_loss=0.06294, ctc_loss=0.1309, over 967192.42 frames. ], batch size: 31, lr: 1.01e-02,
2024-10-08 17:05:29,224 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=19641.666666666668, ans=10.0
2024-10-08 17:05:29,232 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=19641.666666666668, ans=0.025
2024-10-08 17:05:30,492 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=19641.666666666668, ans=0.10358333333333333
2024-10-08 17:05:30,563 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=19641.666666666668, ans=0.0
2024-10-08 17:05:55,903 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=19648.333333333332, ans=0.006598188405797102
2024-10-08 17:05:57,407 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=19648.333333333332, ans=0.125
2024-10-08 17:05:57,809 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.78 vs. limit=9.912083333333332
2024-10-08 17:06:03,603 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=7.89 vs. limit=14.869375
2024-10-08 17:06:04,214 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=19651.666666666668, ans=0.125
2024-10-08 17:06:16,127 INFO [train.py:1153] Epoch 9, batch 4150, loss[loss=0.2149, simple_loss=0.2524, pruned_loss=0.06184, ctc_loss=0.1344, over 4741.00 frames. ], tot_loss[loss=0.2194, simple_loss=0.2603, pruned_loss=0.06306, ctc_loss=0.1306, over 967330.74 frames. ], batch size: 20, lr: 1.01e-02,
2024-10-08 17:06:18,116 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.77 vs. limit=14.8275
2024-10-08 17:06:28,483 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=19658.333333333332, ans=0.10341666666666668
2024-10-08 17:06:33,961 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=19658.333333333332, ans=0.006596014492753623
2024-10-08 17:06:49,472 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.04 vs. limit=9.915416666666667
2024-10-08 17:06:51,778 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.46 vs. limit=14.873125
2024-10-08 17:07:03,487 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.743e+01 1.044e+02 1.127e+02 1.260e+02 1.760e+02, threshold=2.255e+02, percent-clipped=0.0
2024-10-08 17:07:23,589 INFO [train.py:1153] Epoch 9, batch 4200, loss[loss=0.2573, simple_loss=0.2875, pruned_loss=0.08115, ctc_loss=0.1618, over 4845.00 frames. ], tot_loss[loss=0.2183, simple_loss=0.2597, pruned_loss=0.06251, ctc_loss=0.1299, over 967348.10 frames. ], batch size: 31, lr: 1.01e-02,
2024-10-08 17:07:23,714 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=19671.666666666668, ans=0.025
2024-10-08 17:07:33,212 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=19671.666666666668, ans=0.125
2024-10-08 17:07:35,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=19675.0, ans=0.025
2024-10-08 17:07:37,369 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=19675.0, ans=0.125
2024-10-08 17:07:53,520 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=19678.333333333332, ans=0.025
2024-10-08 17:08:20,735 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=19685.0, ans=0.09899494936611666
2024-10-08 17:08:23,368 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=19685.0, ans=0.21102500000000002
2024-10-08 17:08:31,328 INFO [train.py:1153] Epoch 9, batch 4250, loss[loss=0.177, simple_loss=0.2331, pruned_loss=0.04318, ctc_loss=0.08628, over 4749.00 frames. ], tot_loss[loss=0.219, simple_loss=0.2601, pruned_loss=0.06285, ctc_loss=0.1308, over 967204.05 frames. ], batch size: 19, lr: 1.01e-02,
2024-10-08 17:08:38,539 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 17:08:50,504 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=19691.666666666668, ans=0.125
2024-10-08 17:08:56,516 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=13.01 vs. limit=14.884375
2024-10-08 17:08:57,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.82 vs. limit=22.271250000000002
2024-10-08 17:09:01,905 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.51 vs. limit=14.885625000000001
2024-10-08 17:09:18,958 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.183e+01 9.987e+01 1.120e+02 1.274e+02 1.869e+02, threshold=2.241e+02, percent-clipped=0.0
2024-10-08 17:09:39,455 INFO [train.py:1153] Epoch 9, batch 4300, loss[loss=0.2086, simple_loss=0.2536, pruned_loss=0.05731, ctc_loss=0.1227, over 4834.00 frames. ], tot_loss[loss=0.2194, simple_loss=0.2606, pruned_loss=0.06288, ctc_loss=0.1311, over 967377.99 frames. ], batch size: 21, lr: 1.01e-02,
2024-10-08 17:09:40,443 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.79 vs. limit=5.95575
2024-10-08 17:10:01,178 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=19708.333333333332, ans=0.125
2024-10-08 17:10:46,709 INFO [train.py:1153] Epoch 9, batch 4350, loss[loss=0.2, simple_loss=0.2434, pruned_loss=0.05566, ctc_loss=0.1133, over 4825.00 frames. ], tot_loss[loss=0.2188, simple_loss=0.2601, pruned_loss=0.06265, ctc_loss=0.1304, over 966306.68 frames. ], batch size: 21, lr: 1.01e-02,
2024-10-08 17:10:49,428 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=19721.666666666668, ans=0.006582246376811594
2024-10-08 17:10:53,549 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=19721.666666666668, ans=0.125
2024-10-08 17:10:57,582 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=19721.666666666668, ans=0.20974166666666672
2024-10-08 17:11:00,209 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=19725.0, ans=0.10275000000000001
2024-10-08 17:11:06,837 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=19725.0, ans=0.20962500000000006
2024-10-08 17:11:10,422 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module1.whiten, num_groups=1, num_channels=192, metric=6.22 vs. limit=14.896875
2024-10-08 17:11:23,016 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=19728.333333333332, ans=0.2095083333333334
2024-10-08 17:11:27,438 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.35 vs. limit=22.29875
2024-10-08 17:11:28,404 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=19731.666666666668, ans=0.04949747468305833
2024-10-08 17:11:33,730 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.167e+01 1.012e+02 1.153e+02 1.295e+02 1.677e+02, threshold=2.306e+02, percent-clipped=0.0
2024-10-08 17:11:33,982 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=19731.666666666668, ans=0.9473166666666666
2024-10-08 17:11:37,981 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=0.000e+00
2024-10-08 17:11:40,102 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module1.whiten, num_groups=1, num_channels=192, metric=5.37 vs. limit=14.900625
2024-10-08 17:11:44,570 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=19735.0, ans=0.006579347826086957
2024-10-08 17:11:53,723 INFO [train.py:1153] Epoch 9, batch 4400, loss[loss=0.2041, simple_loss=0.2524, pruned_loss=0.0554, ctc_loss=0.1123, over 4742.00 frames. ], tot_loss[loss=0.219, simple_loss=0.2603, pruned_loss=0.06279, ctc_loss=0.1301, over 965859.98 frames. ], batch size: 26, lr: 1.01e-02,
2024-10-08 17:12:17,764 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=10.77 vs. limit=14.903125
2024-10-08 17:12:33,134 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=19748.333333333332, ans=0.125
2024-10-08 17:12:44,238 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=19748.333333333332, ans=0.125
2024-10-08 17:13:01,746 INFO [train.py:1153] Epoch 9, batch 4450, loss[loss=0.181, simple_loss=0.243, pruned_loss=0.0416, ctc_loss=0.08986, over 4883.00 frames. ], tot_loss[loss=0.2203, simple_loss=0.2609, pruned_loss=0.06354, ctc_loss=0.1313, over 966125.23 frames. ], batch size: 19, lr: 1.01e-02,
2024-10-08 17:13:06,438 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.59 vs. limit=14.908125
2024-10-08 17:13:10,147 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=19755.0, ans=0.20857500000000007
2024-10-08 17:13:48,049 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=19765.0, ans=0.10235000000000002
2024-10-08 17:13:49,152 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.395e+01 1.086e+02 1.207e+02 1.348e+02 2.296e+02, threshold=2.414e+02, percent-clipped=0.0
2024-10-08 17:13:57,383 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=19768.333333333332, ans=0.125
2024-10-08 17:14:09,462 INFO [train.py:1153] Epoch 9, batch 4500, loss[loss=0.2232, simple_loss=0.2571, pruned_loss=0.06971, ctc_loss=0.1249, over 4857.00 frames. ], tot_loss[loss=0.22, simple_loss=0.2611, pruned_loss=0.06331, ctc_loss=0.1305, over 966114.41 frames. ], batch size: 28, lr: 1.01e-02,
2024-10-08 17:14:12,305 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=19771.666666666668, ans=0.125
2024-10-08 17:14:19,063 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=19771.666666666668, ans=0.0
2024-10-08 17:14:24,368 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=19775.0, ans=0.125
2024-10-08 17:14:27,125 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=19775.0, ans=0.125
2024-10-08 17:14:50,613 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.23 vs. limit=14.918125
2024-10-08 17:15:02,323 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=19785.0, ans=0.125
2024-10-08 17:15:10,440 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=19785.0, ans=0.20752500000000007
2024-10-08 17:15:16,989 INFO [train.py:1153] Epoch 9, batch 4550, loss[loss=0.2079, simple_loss=0.2584, pruned_loss=0.05521, ctc_loss=0.1172, over 4856.00 frames. ], tot_loss[loss=0.2197, simple_loss=0.2612, pruned_loss=0.06308, ctc_loss=0.13, over 965863.29 frames. ], batch size: 20, lr: 1.01e-02,
2024-10-08 17:15:29,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=19791.666666666668, ans=0.20729166666666676
2024-10-08 17:15:34,991 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=19791.666666666668, ans=0.125
2024-10-08 17:15:51,081 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=19795.0, ans=0.035
2024-10-08 17:16:03,674 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=19798.333333333332, ans=0.025
2024-10-08 17:16:04,705 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.944e+01 9.937e+01 1.120e+02 1.291e+02 1.620e+02, threshold=2.240e+02, percent-clipped=0.0
2024-10-08 17:16:07,672 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 17:16:22,267 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=19801.666666666668, ans=0.125
2024-10-08 17:16:24,807 INFO [train.py:1153] Epoch 9, batch 4600, loss[loss=0.2705, simple_loss=0.2938, pruned_loss=0.08856, ctc_loss=0.175, over 4727.00 frames. ], tot_loss[loss=0.2194, simple_loss=0.2612, pruned_loss=0.06287, ctc_loss=0.1296, over 966124.08 frames. ], batch size: 45, lr: 1.01e-02,
2024-10-08 17:16:29,072 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=19805.0, ans=0.20682500000000004
2024-10-08 17:16:32,271 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=4.78 vs. limit=11.922
2024-10-08 17:16:42,506 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=19808.333333333332, ans=0.125
2024-10-08 17:16:49,740 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.45 vs. limit=5.9712499999999995
2024-10-08 17:17:09,891 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.23 vs. limit=14.9075
2024-10-08 17:17:28,452 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=19818.333333333332, ans=0.1018166666666667
2024-10-08 17:17:32,343 INFO [train.py:1153] Epoch 9, batch 4650, loss[loss=0.2044, simple_loss=0.2527, pruned_loss=0.05568, ctc_loss=0.1119, over 4819.00 frames. ], tot_loss[loss=0.2187, simple_loss=0.2605, pruned_loss=0.06262, ctc_loss=0.1292, over 965357.56 frames. ], batch size: 36, lr: 1.01e-02,
2024-10-08 17:17:58,200 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=19828.333333333332, ans=0.20600833333333346
2024-10-08 17:17:59,528 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 17:17:59,541 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=19828.333333333332, ans=0.20600833333333346
2024-10-08 17:18:08,096 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=25.13 vs. limit=22.37125
2024-10-08 17:18:13,100 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=19831.666666666668, ans=0.10168333333333335
2024-10-08 17:18:19,539 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.928e+01 1.022e+02 1.139e+02 1.237e+02 1.770e+02, threshold=2.278e+02, percent-clipped=0.0
2024-10-08 17:18:20,353 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.00 vs. limit=14.936875
2024-10-08 17:18:27,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=19835.0, ans=0.125
2024-10-08 17:18:30,191 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=8.09 vs. limit=14.938125
2024-10-08 17:18:39,524 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten.whitening_limit, batch_count=19838.333333333332, ans=14.939375
2024-10-08 17:18:40,107 INFO [train.py:1153] Epoch 9, batch 4700, loss[loss=0.191, simple_loss=0.2396, pruned_loss=0.05015, ctc_loss=0.1052, over 4940.00 frames. ], tot_loss[loss=0.2193, simple_loss=0.261, pruned_loss=0.06282, ctc_loss=0.1298, over 965358.09 frames. ], batch size: 19, lr: 1.01e-02,
2024-10-08 17:18:46,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=19838.333333333332, ans=0.0
2024-10-08 17:19:00,237 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer_na.min_abs, batch_count=19841.666666666668, ans=0.02
2024-10-08 17:19:16,146 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.prob, batch_count=19845.0, ans=0.125
2024-10-08 17:19:39,218 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=19851.666666666668, ans=0.125
2024-10-08 17:19:46,833 INFO [train.py:1153] Epoch 9, batch 4750, loss[loss=0.2079, simple_loss=0.2516, pruned_loss=0.05634, ctc_loss=0.1287, over 4765.00 frames. ], tot_loss[loss=0.2196, simple_loss=0.261, pruned_loss=0.06301, ctc_loss=0.1303, over 965328.68 frames. ], batch size: 45, lr: 1.01e-02,
2024-10-08 17:20:02,448 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=6.33 vs. limit=14.946875
2024-10-08 17:20:08,831 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=1.233e-01
2024-10-08 17:20:10,457 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=10.48 vs. limit=14.946875
2024-10-08 17:20:32,344 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.44 vs. limit=22.39875
2024-10-08 17:20:34,415 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.720e+01 9.968e+01 1.123e+02 1.253e+02 1.650e+02, threshold=2.245e+02, percent-clipped=0.0
2024-10-08 17:20:54,584 INFO [train.py:1153] Epoch 9, batch 4800, loss[loss=0.2289, simple_loss=0.268, pruned_loss=0.06783, ctc_loss=0.1355, over 4860.00 frames. ], tot_loss[loss=0.2172, simple_loss=0.2596, pruned_loss=0.06176, ctc_loss=0.128, over 965801.39 frames. ], batch size: 22, lr: 1.01e-02,
2024-10-08 17:21:04,086 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=19871.666666666668, ans=0.025
2024-10-08 17:21:25,225 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.47 vs. limit=5.98175
2024-10-08 17:21:28,606 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=19878.333333333332, ans=0.10121666666666668
2024-10-08 17:21:47,484 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=19885.0, ans=0.10115000000000002
2024-10-08 17:21:53,292 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.whiten.whitening_limit, batch_count=19885.0, ans=11.954
2024-10-08 17:21:59,750 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=19885.0, ans=0.006546739130434783
2024-10-08 17:22:02,174 INFO [train.py:1153] Epoch 9, batch 4850, loss[loss=0.2384, simple_loss=0.2557, pruned_loss=0.07778, ctc_loss=0.1638, over 4844.00 frames. ], tot_loss[loss=0.2169, simple_loss=0.2593, pruned_loss=0.06166, ctc_loss=0.1278, over 966543.37 frames. ], batch size: 28, lr: 1.01e-02,
2024-10-08 17:22:21,374 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 17:22:49,460 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.151e+01 1.017e+02 1.166e+02 1.283e+02 1.713e+02, threshold=2.332e+02, percent-clipped=0.0
2024-10-08 17:22:59,534 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=2.63 vs. limit=14.963125
2024-10-08 17:23:00,540 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=19901.666666666668, ans=0.20344166666666674
2024-10-08 17:23:09,943 INFO [train.py:1153] Epoch 9, batch 4900, loss[loss=0.2189, simple_loss=0.2571, pruned_loss=0.06284, ctc_loss=0.1375, over 4836.00 frames. ], tot_loss[loss=0.2186, simple_loss=0.2603, pruned_loss=0.06258, ctc_loss=0.1293, over 967009.63 frames. ], batch size: 21, lr: 1.01e-02,
2024-10-08 17:23:16,833 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=19905.0, ans=0.006542391304347826
2024-10-08 17:23:19,351 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=19905.0, ans=0.0
2024-10-08 17:23:26,049 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=19908.333333333332, ans=0.10091666666666668
2024-10-08 17:23:32,846 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=19908.333333333332, ans=0.10091666666666668
2024-10-08 17:23:46,405 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=19911.666666666668, ans=0.20309166666666667
2024-10-08 17:23:51,651 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=19915.0, ans=0.125
2024-10-08 17:23:57,193 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=19915.0, ans=0.20297500000000002
2024-10-08 17:24:17,067 INFO [train.py:1153] Epoch 9, batch 4950, loss[loss=0.2305, simple_loss=0.2689, pruned_loss=0.06733, ctc_loss=0.1435, over 4779.00 frames. ], tot_loss[loss=0.2202, simple_loss=0.2613, pruned_loss=0.06343, ctc_loss=0.1307, over 966723.81 frames. ], batch size: 53, lr: 1.01e-02,
2024-10-08 17:24:28,770 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.83 vs. limit=9.980416666666667
2024-10-08 17:24:43,661 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.95 vs. limit=14.973125
2024-10-08 17:25:04,952 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.649e+01 1.001e+02 1.171e+02 1.298e+02 1.614e+02, threshold=2.342e+02, percent-clipped=0.0
2024-10-08 17:25:25,295 INFO [train.py:1153] Epoch 9, batch 5000, loss[loss=0.2043, simple_loss=0.2623, pruned_loss=0.0525, ctc_loss=0.1032, over 4783.00 frames. ], tot_loss[loss=0.2177, simple_loss=0.2597, pruned_loss=0.06224, ctc_loss=0.1283, over 967660.80 frames. ], batch size: 29, lr: 1.00e-02,
2024-10-08 17:25:25,512 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 17:25:39,016 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=19941.666666666668, ans=0.006534420289855072
2024-10-08 17:25:40,438 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=19941.666666666668, ans=0.10058333333333333
2024-10-08 17:25:40,490 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=19941.666666666668, ans=0.125
2024-10-08 17:25:46,064 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=19941.666666666668, ans=0.006534420289855072
2024-10-08 17:25:50,795 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=2.72 vs. limit=14.978125
2024-10-08 17:25:56,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=19945.0, ans=0.125
2024-10-08 17:26:02,464 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=19945.0, ans=0.125
2024-10-08 17:26:06,876 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.06 vs. limit=14.980625
2024-10-08 17:26:33,778 INFO [train.py:1153] Epoch 9, batch 5050, loss[loss=0.2047, simple_loss=0.2469, pruned_loss=0.0561, ctc_loss=0.1258, over 4854.00 frames. ], tot_loss[loss=0.2173, simple_loss=0.2592, pruned_loss=0.06203, ctc_loss=0.1282, over 968621.51 frames. ], batch size: 19, lr: 1.00e-02,
2024-10-08 17:26:47,359 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=19958.333333333332, ans=0.125
2024-10-08 17:27:10,821 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=18.81 vs. limit=22.471249999999998
2024-10-08 17:27:15,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.min_positive, batch_count=19965.0, ans=0.050350000000000006
2024-10-08 17:27:20,747 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.017e+01 1.007e+02 1.116e+02 1.246e+02 2.240e+02, threshold=2.231e+02, percent-clipped=0.0
2024-10-08 17:27:28,957 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=19968.333333333332, ans=0.006528623188405797
2024-10-08 17:27:40,913 INFO [train.py:1153] Epoch 9, batch 5100, loss[loss=0.2178, simple_loss=0.2654, pruned_loss=0.06076, ctc_loss=0.1216, over 4812.00 frames. ], tot_loss[loss=0.2183, simple_loss=0.2595, pruned_loss=0.06271, ctc_loss=0.1293, over 967953.49 frames. ], batch size: 19, lr: 1.00e-02,
2024-10-08 17:27:42,360 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=19971.666666666668, ans=0.20099166666666668
2024-10-08 17:27:46,503 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=19971.666666666668, ans=0.125
2024-10-08 17:27:49,304 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=19971.666666666668, ans=0.125
2024-10-08 17:28:06,839 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=19978.333333333332, ans=0.025
2024-10-08 17:28:06,840 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=19978.333333333332, ans=0.125
2024-10-08 17:28:06,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=19978.333333333332, ans=0.10021666666666668
2024-10-08 17:28:09,494 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=19978.333333333332, ans=0.20075833333333337
2024-10-08 17:28:21,704 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 17:28:23,649 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.53 vs. limit=9.995416666666667
2024-10-08 17:28:48,275 INFO [train.py:1153] Epoch 9, batch 5150, loss[loss=0.2362, simple_loss=0.2738, pruned_loss=0.06778, ctc_loss=0.1574, over 4813.00 frames. ], tot_loss[loss=0.2187, simple_loss=0.2599, pruned_loss=0.06282, ctc_loss=0.1297, over 968033.28 frames. ], batch size: 36, lr: 1.00e-02,
2024-10-08 17:29:17,934 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=19995.0, ans=0.10005
2024-10-08 17:29:21,029 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.27 vs. limit=5.99925
2024-10-08 17:29:21,056 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=22.04 vs. limit=22.49625
2024-10-08 17:29:21,108 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=21.33 vs. limit=22.49625
2024-10-08 17:29:34,087 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/checkpoint-60000.pt
2024-10-08 17:29:36,047 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.741e+01 1.019e+02 1.134e+02 1.303e+02 1.996e+02, threshold=2.269e+02, percent-clipped=0.0
2024-10-08 17:29:36,191 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=19998.333333333332, ans=0.125
2024-10-08 17:29:38,507 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=19998.333333333332, ans=0.125
2024-10-08 17:29:55,436 INFO [train.py:1153] Epoch 9, batch 5200, loss[loss=0.2022, simple_loss=0.2445, pruned_loss=0.05582, ctc_loss=0.1205, over 4795.00 frames. ], tot_loss[loss=0.2188, simple_loss=0.2604, pruned_loss=0.06268, ctc_loss=0.1295, over 967621.11 frames. ], batch size: 29, lr: 1.00e-02,
2024-10-08 17:29:56,910 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=20005.0, ans=0.125
2024-10-08 17:29:58,763 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=5.68 vs. limit=6.0
2024-10-08 17:30:00,870 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=20005.0, ans=0.025
2024-10-08 17:30:02,130 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 17:30:02,275 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 17:30:11,908 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=23.21 vs. limit=22.5
2024-10-08 17:30:15,379 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=20008.333333333332, ans=0.1
2024-10-08 17:30:48,037 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=22.87 vs. limit=22.5
2024-10-08 17:30:49,963 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=20018.333333333332, ans=0.125
2024-10-08 17:30:56,747 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=20018.333333333332, ans=0.125
2024-10-08 17:30:58,028 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 17:30:59,414 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=20018.333333333332, ans=0.0065177536231884065
2024-10-08 17:30:59,487 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=20018.333333333332, ans=0.1
2024-10-08 17:30:59,989 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=14.25 vs. limit=15.0
2024-10-08 17:31:01,981 INFO [train.py:1153] Epoch 9, batch 5250, loss[loss=0.188, simple_loss=0.2458, pruned_loss=0.04559, ctc_loss=0.09763, over 4858.00 frames. ], tot_loss[loss=0.2185, simple_loss=0.2602, pruned_loss=0.06263, ctc_loss=0.1291, over 967714.29 frames. ], batch size: 20, lr: 1.00e-02,
2024-10-08 17:31:05,531 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=4.67 vs. limit=12.0
2024-10-08 17:31:09,924 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=20021.666666666668, ans=0.1
2024-10-08 17:31:14,087 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=20025.0, ans=0.125
2024-10-08 17:31:15,418 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=20025.0, ans=0.125
2024-10-08 17:31:27,568 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=20028.333333333332, ans=0.125
2024-10-08 17:31:38,530 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=5.73 vs. limit=15.0
2024-10-08 17:31:38,594 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=11.75 vs. limit=15.0
2024-10-08 17:31:43,454 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=20031.666666666668, ans=0.125
2024-10-08 17:31:48,521 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.315e+01 1.008e+02 1.122e+02 1.276e+02 2.023e+02, threshold=2.245e+02, percent-clipped=0.0
2024-10-08 17:31:56,873 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=20035.0, ans=0.125
2024-10-08 17:31:58,232 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=20035.0, ans=0.035
2024-10-08 17:32:08,811 INFO [train.py:1153] Epoch 9, batch 5300, loss[loss=0.2511, simple_loss=0.2853, pruned_loss=0.07557, ctc_loss=0.1643, over 4805.00 frames. ], tot_loss[loss=0.2184, simple_loss=0.2597, pruned_loss=0.06268, ctc_loss=0.1291, over 967762.46 frames. ], batch size: 38, lr: 1.00e-02,
2024-10-08 17:32:11,578 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=20038.333333333332, ans=0.0
2024-10-08 17:32:33,261 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=20041.666666666668, ans=0.0
2024-10-08 17:32:34,674 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=20045.0, ans=0.125
2024-10-08 17:32:41,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=20045.0, ans=0.125
2024-10-08 17:32:42,795 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=20045.0, ans=0.2
2024-10-08 17:33:05,671 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=20051.666666666668, ans=0.0
2024-10-08 17:33:15,034 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.hidden_balancer.prob, batch_count=20055.0, ans=0.125
2024-10-08 17:33:16,207 INFO [train.py:1153] Epoch 9, batch 5350, loss[loss=0.2116, simple_loss=0.2498, pruned_loss=0.06166, ctc_loss=0.1251, over 4978.00 frames. ], tot_loss[loss=0.2199, simple_loss=0.2606, pruned_loss=0.06353, ctc_loss=0.1305, over 967154.52 frames. ], batch size: 19, lr: 1.00e-02,
2024-10-08 17:33:40,572 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=20058.333333333332, ans=0.125
2024-10-08 17:33:44,582 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=20061.666666666668, ans=0.125
2024-10-08 17:33:59,395 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=20065.0, ans=0.125
2024-10-08 17:34:03,300 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.930e+01 1.013e+02 1.159e+02 1.332e+02 1.818e+02, threshold=2.318e+02, percent-clipped=0.0
2024-10-08 17:34:23,364 INFO [train.py:1153] Epoch 9, batch 5400, loss[loss=0.281, simple_loss=0.3113, pruned_loss=0.08745, ctc_loss=0.1893, over 4794.00 frames. ], tot_loss[loss=0.2212, simple_loss=0.2614, pruned_loss=0.06412, ctc_loss=0.1321, over 966422.18 frames. ], batch size: 49, lr: 1.00e-02,
2024-10-08 17:34:30,351 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=20071.666666666668, ans=0.125
2024-10-08 17:34:46,618 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=20075.0, ans=0.0
2024-10-08 17:34:46,696 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=20075.0, ans=0.0
2024-10-08 17:35:01,400 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=20078.333333333332, ans=0.0
2024-10-08 17:35:26,767 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=20085.0, ans=0.0
2024-10-08 17:35:28,243 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=20085.0, ans=0.006503260869565218
2024-10-08 17:35:30,789 INFO [train.py:1153] Epoch 9, batch 5450, loss[loss=0.2222, simple_loss=0.2631, pruned_loss=0.0658, ctc_loss=0.1243, over 4940.00 frames. ], tot_loss[loss=0.2191, simple_loss=0.2601, pruned_loss=0.06306, ctc_loss=0.1299, over 967038.88 frames. ], batch size: 19, lr: 1.00e-02,
2024-10-08 17:36:15,561 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 17:36:18,094 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.267e+01 1.022e+02 1.151e+02 1.264e+02 1.776e+02, threshold=2.302e+02, percent-clipped=0.0
2024-10-08 17:36:19,653 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 17:36:33,178 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=20101.666666666668, ans=0.0
2024-10-08 17:36:38,474 INFO [train.py:1153] Epoch 9, batch 5500, loss[loss=0.2732, simple_loss=0.3026, pruned_loss=0.08576, ctc_loss=0.1809, over 4774.00 frames. ], tot_loss[loss=0.2207, simple_loss=0.2615, pruned_loss=0.06371, ctc_loss=0.1312, over 967371.60 frames. ], batch size: 49, lr: 1.00e-02,
2024-10-08 17:36:46,502 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=20105.0, ans=0.125
2024-10-08 17:36:46,978 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=21.84 vs. limit=22.5
2024-10-08 17:37:00,300 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=20108.333333333332, ans=0.1
2024-10-08 17:37:04,240 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.skip_rate, batch_count=20111.666666666668, ans=0.035
2024-10-08 17:37:23,564 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=7.38 vs. limit=15.0
2024-10-08 17:37:30,372 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=5.03 vs. limit=12.0
2024-10-08 17:37:37,647 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=20118.333333333332, ans=0.125
2024-10-08 17:37:45,577 INFO [train.py:1153] Epoch 9, batch 5550, loss[loss=0.2049, simple_loss=0.247, pruned_loss=0.05978, ctc_loss=0.108, over 4802.00 frames. ], tot_loss[loss=0.2201, simple_loss=0.2611, pruned_loss=0.06345, ctc_loss=0.1304, over 967199.00 frames. ], batch size: 19, lr: 1.00e-02,
2024-10-08 17:38:15,331 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.min_positive, batch_count=20128.333333333332, ans=0.05
2024-10-08 17:38:19,238 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=20128.333333333332, ans=0.1
2024-10-08 17:38:32,864 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.004e+01 1.049e+02 1.163e+02 1.331e+02 1.871e+02, threshold=2.326e+02, percent-clipped=0.0
2024-10-08 17:38:53,047 INFO [train.py:1153] Epoch 9, batch 5600, loss[loss=0.2163, simple_loss=0.2646, pruned_loss=0.059, ctc_loss=0.1249, over 4827.00 frames. ], tot_loss[loss=0.2211, simple_loss=0.2618, pruned_loss=0.064, ctc_loss=0.131, over 967006.82 frames. ], batch size: 28, lr: 1.00e-02,
2024-10-08 17:39:12,155 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=20141.666666666668, ans=0.025
2024-10-08 17:39:46,148 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.skip_rate, batch_count=20151.666666666668, ans=0.07
2024-10-08 17:39:52,865 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.prob, batch_count=20151.666666666668, ans=0.125
2024-10-08 17:40:00,692 INFO [train.py:1153] Epoch 9, batch 5650, loss[loss=0.2918, simple_loss=0.3089, pruned_loss=0.09745, ctc_loss=0.1993, over 4733.00 frames. ], tot_loss[loss=0.2204, simple_loss=0.2616, pruned_loss=0.06355, ctc_loss=0.1302, over 967069.04 frames. ], batch size: 45, lr: 1.00e-02,
2024-10-08 17:40:05,305 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=4.80 vs. limit=10.0
2024-10-08 17:40:10,210 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=20155.0, ans=0.125
2024-10-08 17:40:15,661 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=20158.333333333332, ans=0.025
2024-10-08 17:40:37,140 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass_mid.scale_min, batch_count=20161.666666666668, ans=0.2
2024-10-08 17:40:47,630 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.849e+01 9.673e+01 1.122e+02 1.245e+02 1.853e+02, threshold=2.243e+02, percent-clipped=0.0
2024-10-08 17:41:05,806 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.63 vs. limit=15.0
2024-10-08 17:41:07,887 INFO [train.py:1153] Epoch 9, batch 5700, loss[loss=0.2067, simple_loss=0.2547, pruned_loss=0.05334, ctc_loss=0.1302, over 4848.00 frames. ], tot_loss[loss=0.22, simple_loss=0.2615, pruned_loss=0.06327, ctc_loss=0.1297, over 966434.09 frames. ], batch size: 22, lr: 9.99e-03,
2024-10-08 17:41:30,899 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=20175.0, ans=0.125
2024-10-08 17:42:14,075 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=20188.333333333332, ans=0.125
2024-10-08 17:42:15,345 INFO [train.py:1153] Epoch 9, batch 5750, loss[loss=0.2425, simple_loss=0.267, pruned_loss=0.07421, ctc_loss=0.1741, over 4850.00 frames. ], tot_loss[loss=0.2198, simple_loss=0.2612, pruned_loss=0.06317, ctc_loss=0.1302, over 966779.06 frames. ], batch size: 43, lr: 9.99e-03,
2024-10-08 17:42:27,669 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=20191.666666666668, ans=0.125
2024-10-08 17:42:33,213 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=0.000e+00
2024-10-08 17:42:41,179 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=20195.0, ans=0.2
2024-10-08 17:42:41,218 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=20195.0, ans=0.0
2024-10-08 17:42:47,950 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=20195.0, ans=0.125
2024-10-08 17:43:02,575 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.543e+01 9.863e+01 1.111e+02 1.256e+02 1.939e+02, threshold=2.222e+02, percent-clipped=0.0
2024-10-08 17:43:14,965 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=20201.666666666668, ans=0.125
2024-10-08 17:43:22,752 INFO [train.py:1153] Epoch 9, batch 5800, loss[loss=0.2543, simple_loss=0.295, pruned_loss=0.07548, ctc_loss=0.1569, over 4840.00 frames. ], tot_loss[loss=0.2208, simple_loss=0.2622, pruned_loss=0.06356, ctc_loss=0.1306, over 966295.10 frames. ], batch size: 43, lr: 9.98e-03,
2024-10-08 17:43:24,264 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=20205.0, ans=0.125
2024-10-08 17:43:26,202 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=23.00 vs. limit=22.5
2024-10-08 17:43:49,814 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=20211.666666666668, ans=0.1
2024-10-08 17:44:06,072 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.const_attention_rate, batch_count=20215.0, ans=0.025
2024-10-08 17:44:30,447 INFO [train.py:1153] Epoch 9, batch 5850, loss[loss=0.3198, simple_loss=0.316, pruned_loss=0.1167, ctc_loss=0.2252, over 4745.00 frames. ], tot_loss[loss=0.2214, simple_loss=0.2627, pruned_loss=0.06389, ctc_loss=0.1308, over 966566.41 frames. ], batch size: 45, lr: 9.98e-03,
2024-10-08 17:44:34,741 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=20221.666666666668, ans=0.125
2024-10-08 17:44:48,412 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=20225.0, ans=0.2
2024-10-08 17:45:18,024 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.285e+01 1.014e+02 1.127e+02 1.335e+02 1.712e+02, threshold=2.255e+02, percent-clipped=0.0
2024-10-08 17:45:33,489 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.44 vs. limit=15.0
2024-10-08 17:45:38,291 INFO [train.py:1153] Epoch 9, batch 5900, loss[loss=0.2932, simple_loss=0.3098, pruned_loss=0.09886, ctc_loss=0.1969, over 4782.00 frames. ], tot_loss[loss=0.222, simple_loss=0.2632, pruned_loss=0.06414, ctc_loss=0.1314, over 966662.46 frames. ], batch size: 34, lr: 9.98e-03,
2024-10-08 17:45:54,972 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=20241.666666666668, ans=0.125
2024-10-08 17:46:33,896 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 17:46:45,801 INFO [train.py:1153] Epoch 9, batch 5950, loss[loss=0.225, simple_loss=0.2626, pruned_loss=0.06806, ctc_loss=0.128, over 4792.00 frames. ], tot_loss[loss=0.2194, simple_loss=0.2611, pruned_loss=0.06297, ctc_loss=0.1294, over 965993.28 frames. ], batch size: 34, lr: 9.97e-03,
2024-10-08 17:47:19,018 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.35 vs. limit=15.0
2024-10-08 17:47:33,261 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.077e+01 9.615e+01 1.127e+02 1.292e+02 2.003e+02, threshold=2.253e+02, percent-clipped=0.0
2024-10-08 17:47:53,537 INFO [train.py:1153] Epoch 9, batch 6000, loss[loss=0.2812, simple_loss=0.298, pruned_loss=0.09456, ctc_loss=0.1884, over 4785.00 frames. ], tot_loss[loss=0.219, simple_loss=0.2611, pruned_loss=0.06262, ctc_loss=0.1291, over 966571.77 frames. ], batch size: 49, lr: 9.97e-03,
2024-10-08 17:47:53,537 INFO [train.py:1176] Computing validation loss
2024-10-08 17:48:01,538 INFO [train.py:1185] Epoch 9, validation: loss=0.1577, simple_loss=0.2401, pruned_loss=0.02737, ctc_loss=0.0516, over 90464.00 frames.
2024-10-08 17:48:01,539 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 17:48:01,725 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=20271.666666666668, ans=0.0
2024-10-08 17:48:38,071 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=20278.333333333332, ans=0.0
2024-10-08 17:49:08,879 INFO [train.py:1153] Epoch 9, batch 6050, loss[loss=0.1574, simple_loss=0.2216, pruned_loss=0.03335, ctc_loss=0.0666, over 4816.00 frames. ], tot_loss[loss=0.2182, simple_loss=0.2604, pruned_loss=0.06236, ctc_loss=0.1284, over 966629.22 frames. ], batch size: 19, lr: 9.96e-03,
2024-10-08 17:49:27,926 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=20291.666666666668, ans=0.025
2024-10-08 17:49:33,954 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.78 vs. limit=15.0
2024-10-08 17:49:34,714 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=20295.0, ans=0.0
2024-10-08 17:49:44,802 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.18 vs. limit=15.0
2024-10-08 17:49:55,911 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.949e+01 1.048e+02 1.146e+02 1.282e+02 1.774e+02, threshold=2.293e+02, percent-clipped=0.0
2024-10-08 17:50:16,038 INFO [train.py:1153] Epoch 9, batch 6100, loss[loss=0.2441, simple_loss=0.2716, pruned_loss=0.07829, ctc_loss=0.1499, over 4814.00 frames. ], tot_loss[loss=0.2183, simple_loss=0.2602, pruned_loss=0.06247, ctc_loss=0.1286, over 966255.62 frames. ], batch size: 34, lr: 9.96e-03,
2024-10-08 17:50:20,642 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.62 vs. limit=15.0
2024-10-08 17:50:21,541 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=20305.0, ans=0.125
2024-10-08 17:50:25,642 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=20305.0, ans=0.0
2024-10-08 17:50:35,609 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.98 vs. limit=15.0
2024-10-08 17:51:23,714 INFO [train.py:1153] Epoch 9, batch 6150, loss[loss=0.2672, simple_loss=0.292, pruned_loss=0.08607, ctc_loss=0.1759, over 4806.00 frames. ], tot_loss[loss=0.2188, simple_loss=0.2608, pruned_loss=0.06265, ctc_loss=0.1287, over 966360.75 frames. ], batch size: 43, lr: 9.96e-03,
2024-10-08 17:51:50,685 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=20328.333333333332, ans=0.125
2024-10-08 17:52:10,920 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.708e+01 9.956e+01 1.152e+02 1.277e+02 1.713e+02, threshold=2.303e+02, percent-clipped=0.0
2024-10-08 17:52:15,169 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=20331.666666666668, ans=0.1
2024-10-08 17:52:31,135 INFO [train.py:1153] Epoch 9, batch 6200, loss[loss=0.2209, simple_loss=0.2634, pruned_loss=0.06473, ctc_loss=0.1222, over 4764.00 frames. ], tot_loss[loss=0.2189, simple_loss=0.2605, pruned_loss=0.06276, ctc_loss=0.1294, over 966665.52 frames. ], batch size: 29, lr: 9.95e-03,
2024-10-08 17:52:32,580 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=20338.333333333332, ans=0.0
2024-10-08 17:52:50,136 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=20341.666666666668, ans=0.125
2024-10-08 17:52:52,871 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=20341.666666666668, ans=0.0
2024-10-08 17:52:59,597 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=20345.0, ans=0.125
2024-10-08 17:53:14,520 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=20348.333333333332, ans=0.2
2024-10-08 17:53:24,519 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.17 vs. limit=15.0
2024-10-08 17:53:24,527 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.02 vs. limit=10.0
2024-10-08 17:53:38,603 INFO [train.py:1153] Epoch 9, batch 6250, loss[loss=0.2234, simple_loss=0.2626, pruned_loss=0.06733, ctc_loss=0.124, over 4716.00 frames. ], tot_loss[loss=0.2179, simple_loss=0.2602, pruned_loss=0.06217, ctc_loss=0.1281, over 966880.04 frames. ], batch size: 26, lr: 9.95e-03,
2024-10-08 17:53:50,780 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=20358.333333333332, ans=0.125
2024-10-08 17:53:59,335 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys.whitening_limit, batch_count=20358.333333333332, ans=6.0
2024-10-08 17:54:05,116 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=13.77 vs. limit=15.0
2024-10-08 17:54:25,580 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.258e+01 1.028e+02 1.155e+02 1.311e+02 1.756e+02, threshold=2.310e+02, percent-clipped=0.0
2024-10-08 17:54:46,034 INFO [train.py:1153] Epoch 9, batch 6300, loss[loss=0.1834, simple_loss=0.2264, pruned_loss=0.04935, ctc_loss=0.1043, over 4978.00 frames. ], tot_loss[loss=0.2192, simple_loss=0.2604, pruned_loss=0.06312, ctc_loss=0.1293, over 966635.29 frames. ], batch size: 19, lr: 9.94e-03,
2024-10-08 17:54:49,683 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.79 vs. limit=10.0
2024-10-08 17:54:51,208 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=4.37 vs. limit=5.0
2024-10-08 17:55:11,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=20378.333333333332, ans=0.1
2024-10-08 17:55:14,921 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=20378.333333333332, ans=0.0
2024-10-08 17:55:29,620 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=20381.666666666668, ans=0.125
2024-10-08 17:55:43,106 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=20385.0, ans=0.1
2024-10-08 17:55:53,722 INFO [train.py:1153] Epoch 9, batch 6350, loss[loss=0.2632, simple_loss=0.2931, pruned_loss=0.08365, ctc_loss=0.1647, over 4820.00 frames. ], tot_loss[loss=0.2172, simple_loss=0.2591, pruned_loss=0.06215, ctc_loss=0.1275, over 966142.10 frames. ], batch size: 36, lr: 9.94e-03,
2024-10-08 17:56:00,496 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=20388.333333333332, ans=0.125
2024-10-08 17:56:02,704 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.02 vs. limit=15.0
2024-10-08 17:56:19,536 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=20395.0, ans=0.125
2024-10-08 17:56:34,375 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=20398.333333333332, ans=0.125
2024-10-08 17:56:41,083 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.726e+01 1.002e+02 1.128e+02 1.243e+02 1.428e+02, threshold=2.256e+02, percent-clipped=0.0
2024-10-08 17:57:01,408 INFO [train.py:1153] Epoch 9, batch 6400, loss[loss=0.1847, simple_loss=0.2407, pruned_loss=0.04493, ctc_loss=0.09723, over 4884.00 frames. ], tot_loss[loss=0.2154, simple_loss=0.2581, pruned_loss=0.06126, ctc_loss=0.1254, over 965941.35 frames. ], batch size: 23, lr: 9.93e-03,
2024-10-08 17:57:28,669 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=20411.666666666668, ans=0.125
2024-10-08 17:57:29,972 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=20411.666666666668, ans=0.1
2024-10-08 17:57:48,417 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=11.58 vs. limit=15.0
2024-10-08 17:57:57,256 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.63 vs. limit=22.5
2024-10-08 17:58:03,057 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.83 vs. limit=22.5
2024-10-08 17:58:08,720 INFO [train.py:1153] Epoch 9, batch 6450, loss[loss=0.1868, simple_loss=0.2497, pruned_loss=0.04334, ctc_loss=0.09286, over 4750.00 frames. ], tot_loss[loss=0.2153, simple_loss=0.2583, pruned_loss=0.06104, ctc_loss=0.1254, over 965469.36 frames. ], batch size: 26, lr: 9.93e-03,
2024-10-08 17:58:08,880 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=20421.666666666668, ans=0.125
2024-10-08 17:58:56,165 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.561e+01 1.016e+02 1.181e+02 1.279e+02 1.709e+02, threshold=2.363e+02, percent-clipped=0.0
2024-10-08 17:58:57,784 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=20431.666666666668, ans=0.125
2024-10-08 17:59:16,809 INFO [train.py:1153] Epoch 9, batch 6500, loss[loss=0.2003, simple_loss=0.2517, pruned_loss=0.05315, ctc_loss=0.1064, over 4718.00 frames. ], tot_loss[loss=0.2134, simple_loss=0.2571, pruned_loss=0.06009, ctc_loss=0.1236, over 965088.65 frames. ], batch size: 26, lr: 9.93e-03,
2024-10-08 17:59:27,708 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=20438.333333333332, ans=0.125
2024-10-08 17:59:38,459 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass_mid.scale_min, batch_count=20441.666666666668, ans=0.2
2024-10-08 17:59:47,832 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=20445.0, ans=0.125
2024-10-08 18:00:03,992 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=20448.333333333332, ans=0.0
2024-10-08 18:00:04,052 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=20448.333333333332, ans=0.0
2024-10-08 18:00:24,366 INFO [train.py:1153] Epoch 9, batch 6550, loss[loss=0.23, simple_loss=0.2648, pruned_loss=0.06697, ctc_loss=0.153, over 4978.00 frames. ], tot_loss[loss=0.2121, simple_loss=0.2563, pruned_loss=0.05942, ctc_loss=0.1225, over 964720.17 frames. ], batch size: 19, lr: 9.92e-03,
2024-10-08 18:00:27,703 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=6.14 vs. limit=6.0
2024-10-08 18:00:28,627 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=20455.0, ans=0.04949747468305833
2024-10-08 18:00:32,751 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=20455.0, ans=0.0
2024-10-08 18:01:09,603 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer_ff3.min_abs, batch_count=20465.0, ans=0.2
2024-10-08 18:01:12,181 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.067e+01 1.017e+02 1.105e+02 1.227e+02 2.149e+02, threshold=2.210e+02, percent-clipped=0.0
2024-10-08 18:01:14,927 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=20465.0, ans=0.125
2024-10-08 18:01:25,964 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=20468.333333333332, ans=0.006419927536231884
2024-10-08 18:01:32,554 INFO [train.py:1153] Epoch 9, batch 6600, loss[loss=0.2272, simple_loss=0.2761, pruned_loss=0.06231, ctc_loss=0.1342, over 4868.00 frames. ], tot_loss[loss=0.2112, simple_loss=0.2565, pruned_loss=0.05873, ctc_loss=0.1211, over 965365.16 frames. ], batch size: 23, lr: 9.92e-03,
2024-10-08 18:01:45,410 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=5.10 vs. limit=12.0
2024-10-08 18:02:19,382 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=20481.666666666668, ans=0.006417028985507246
2024-10-08 18:02:23,799 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=4.02 vs. limit=10.0
2024-10-08 18:02:29,018 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=20485.0, ans=0.125
2024-10-08 18:02:39,960 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=20488.333333333332, ans=0.025
2024-10-08 18:02:41,195 INFO [train.py:1153] Epoch 9, batch 6650, loss[loss=0.2206, simple_loss=0.2734, pruned_loss=0.06, ctc_loss=0.1195, over 4734.00 frames. ], tot_loss[loss=0.2099, simple_loss=0.2558, pruned_loss=0.05806, ctc_loss=0.1194, over 967176.46 frames. ], batch size: 20, lr: 9.92e-03,
2024-10-08 18:02:48,106 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=20488.333333333332, ans=0.0
2024-10-08 18:02:59,285 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=20491.666666666668, ans=0.125
2024-10-08 18:03:04,667 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=20491.666666666668, ans=0.125
2024-10-08 18:03:08,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=20495.0, ans=0.1
2024-10-08 18:03:25,063 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=20498.333333333332, ans=0.1
2024-10-08 18:03:28,902 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.552e+01 1.037e+02 1.151e+02 1.276e+02 1.793e+02, threshold=2.301e+02, percent-clipped=0.0
2024-10-08 18:03:37,474 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=20501.666666666668, ans=0.125
2024-10-08 18:03:49,956 INFO [train.py:1153] Epoch 9, batch 6700, loss[loss=0.2273, simple_loss=0.277, pruned_loss=0.06297, ctc_loss=0.1292, over 4929.00 frames. ], tot_loss[loss=0.2091, simple_loss=0.2554, pruned_loss=0.05776, ctc_loss=0.1182, over 969240.29 frames. ], batch size: 20, lr: 9.91e-03,
2024-10-08 18:04:03,847 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=20508.333333333332, ans=0.2
2024-10-08 18:04:12,207 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=20508.333333333332, ans=0.2
2024-10-08 18:04:33,076 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=20515.0, ans=0.2
2024-10-08 18:04:56,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=20518.333333333332, ans=0.2
2024-10-08 18:04:59,216 INFO [train.py:1153] Epoch 9, batch 6750, loss[loss=0.2155, simple_loss=0.2522, pruned_loss=0.06319, ctc_loss=0.1308, over 4910.00 frames. ], tot_loss[loss=0.2058, simple_loss=0.2526, pruned_loss=0.0564, ctc_loss=0.1157, over 972301.90 frames. ], batch size: 19, lr: 9.91e-03,
2024-10-08 18:05:13,297 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=20525.0, ans=0.0
2024-10-08 18:05:15,991 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=20525.0, ans=0.0
2024-10-08 18:05:18,800 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=20525.0, ans=0.0
2024-10-08 18:05:32,748 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=20528.333333333332, ans=0.125
2024-10-08 18:05:36,112 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.86 vs. limit=15.0
2024-10-08 18:05:48,032 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.908e+01 9.815e+01 1.082e+02 1.213e+02 1.690e+02, threshold=2.165e+02, percent-clipped=0.0
2024-10-08 18:05:49,624 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=20531.666666666668, ans=0.0
2024-10-08 18:06:08,903 INFO [train.py:1153] Epoch 9, batch 6800, loss[loss=0.174, simple_loss=0.2299, pruned_loss=0.0426, ctc_loss=0.08208, over 4908.00 frames. ], tot_loss[loss=0.2049, simple_loss=0.2513, pruned_loss=0.05619, ctc_loss=0.1151, over 974607.54 frames. ], batch size: 19, lr: 9.90e-03,
2024-10-08 18:06:20,218 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=20538.333333333332, ans=0.1
2024-10-08 18:06:25,941 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=20541.666666666668, ans=0.125
2024-10-08 18:06:30,052 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=20541.666666666668, ans=0.1
2024-10-08 18:06:41,220 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=20545.0, ans=0.0
2024-10-08 18:06:44,685 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=22.03 vs. limit=22.5
2024-10-08 18:06:46,871 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=20545.0, ans=0.025
2024-10-08 18:07:19,014 INFO [train.py:1153] Epoch 9, batch 6850, loss[loss=0.2226, simple_loss=0.2634, pruned_loss=0.06532, ctc_loss=0.1276, over 4978.00 frames. ], tot_loss[loss=0.2034, simple_loss=0.2497, pruned_loss=0.05576, ctc_loss=0.1141, over 978943.69 frames. ], batch size: 19, lr: 9.90e-03,
2024-10-08 18:07:20,527 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/epoch-9.pt
2024-10-08 18:07:43,607 INFO [train.py:1153] Epoch 10, batch 0, loss[loss=0.2338, simple_loss=0.2812, pruned_loss=0.06791, ctc_loss=0.1263, over 4854.00 frames. ], tot_loss[loss=0.2338, simple_loss=0.2812, pruned_loss=0.06791, ctc_loss=0.1263, over 4854.00 frames. ], batch size: 19, lr: 9.41e-03,
2024-10-08 18:07:43,608 INFO [train.py:1176] Computing validation loss
2024-10-08 18:07:49,923 INFO [train.py:1185] Epoch 10, validation: loss=0.1587, simple_loss=0.2409, pruned_loss=0.02789, ctc_loss=0.05142, over 90464.00 frames.
2024-10-08 18:07:49,923 INFO [train.py:1186] Maximum memory allocated so far is 5609MB
2024-10-08 18:08:02,989 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=20559.0, ans=0.125
2024-10-08 18:08:19,550 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=20562.333333333332, ans=0.09899494936611666
2024-10-08 18:08:31,119 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.429e+01 9.524e+01 1.086e+02 1.252e+02 1.695e+02, threshold=2.171e+02, percent-clipped=0.0
2024-10-08 18:08:53,706 INFO [train.py:1153] Epoch 10, batch 50, loss[loss=0.2572, simple_loss=0.2699, pruned_loss=0.08858, ctc_loss=0.1686, over 4914.00 frames. ], tot_loss[loss=0.2233, simple_loss=0.2633, pruned_loss=0.06484, ctc_loss=0.134, over 217733.27 frames. ], batch size: 19, lr: 9.41e-03,
2024-10-08 18:09:32,822 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=20582.333333333332, ans=0.0
2024-10-08 18:10:00,791 INFO [train.py:1153] Epoch 10, batch 100, loss[loss=0.1612, simple_loss=0.2196, pruned_loss=0.03587, ctc_loss=0.07787, over 4753.00 frames. ], tot_loss[loss=0.2193, simple_loss=0.2604, pruned_loss=0.06271, ctc_loss=0.132, over 383156.06 frames. ], batch size: 19, lr: 9.41e-03,
2024-10-08 18:10:05,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.91 vs. limit=22.5
2024-10-08 18:10:08,105 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.29 vs. limit=10.0
2024-10-08 18:10:24,134 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.60 vs. limit=22.5
2024-10-08 18:10:42,539 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=20599.0, ans=0.04949747468305833
2024-10-08 18:10:45,161 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.470e+01 9.628e+01 1.100e+02 1.220e+02 1.940e+02, threshold=2.200e+02, percent-clipped=0.0
2024-10-08 18:10:46,636 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=20599.0, ans=0.2
2024-10-08 18:11:08,119 INFO [train.py:1153] Epoch 10, batch 150, loss[loss=0.1863, simple_loss=0.2461, pruned_loss=0.04491, ctc_loss=0.09183, over 4910.00 frames. ], tot_loss[loss=0.2152, simple_loss=0.258, pruned_loss=0.06077, ctc_loss=0.127, over 513179.35 frames. ], batch size: 19, lr: 9.40e-03,
2024-10-08 18:11:15,061 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=20605.666666666668, ans=0.125
2024-10-08 18:11:15,144 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=20605.666666666668, ans=0.2
2024-10-08 18:11:16,422 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=20605.666666666668, ans=0.0
2024-10-08 18:11:32,650 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=20609.0, ans=0.0
2024-10-08 18:11:47,372 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=20615.666666666668, ans=0.015
2024-10-08 18:12:06,041 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.91 vs. limit=10.0
2024-10-08 18:12:10,582 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=20619.0, ans=0.1
2024-10-08 18:12:11,882 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=20619.0, ans=0.2
2024-10-08 18:12:13,825 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=14.74 vs. limit=15.0
2024-10-08 18:12:15,798 INFO [train.py:1153] Epoch 10, batch 200, loss[loss=0.236, simple_loss=0.2682, pruned_loss=0.07575, ctc_loss=0.1306, over 4752.00 frames. ], tot_loss[loss=0.2152, simple_loss=0.2575, pruned_loss=0.06102, ctc_loss=0.1273, over 613746.97 frames. ], batch size: 45, lr: 9.40e-03,
2024-10-08 18:12:22,398 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=20622.333333333332, ans=0.0
2024-10-08 18:12:23,714 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=20622.333333333332, ans=0.2
2024-10-08 18:12:42,780 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=20629.0, ans=0.0
2024-10-08 18:12:48,016 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=20629.0, ans=0.0
2024-10-08 18:12:59,856 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.330e+01 1.005e+02 1.170e+02 1.306e+02 2.630e+02, threshold=2.340e+02, percent-clipped=1.0
2024-10-08 18:13:06,719 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=20632.333333333332, ans=0.006384275362318842
2024-10-08 18:13:10,673 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.scale_min, batch_count=20635.666666666668, ans=0.2
2024-10-08 18:13:18,790 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=20635.666666666668, ans=0.2
2024-10-08 18:13:22,618 INFO [train.py:1153] Epoch 10, batch 250, loss[loss=0.2927, simple_loss=0.2935, pruned_loss=0.1054, ctc_loss=0.2029, over 4831.00 frames. ], tot_loss[loss=0.2151, simple_loss=0.2573, pruned_loss=0.06109, ctc_loss=0.1269, over 692462.84 frames. ], batch size: 38, lr: 9.39e-03,
2024-10-08 18:13:39,995 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=20642.333333333332, ans=0.1
2024-10-08 18:13:50,801 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=20645.666666666668, ans=0.125
2024-10-08 18:13:56,207 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=20645.666666666668, ans=0.125
2024-10-08 18:13:57,976 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.71 vs. limit=15.0
2024-10-08 18:14:12,349 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=20649.0, ans=0.2
2024-10-08 18:14:21,667 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=20652.333333333332, ans=0.0
2024-10-08 18:14:29,576 INFO [train.py:1153] Epoch 10, batch 300, loss[loss=0.2498, simple_loss=0.2825, pruned_loss=0.07626, ctc_loss=0.1616, over 4776.00 frames. ], tot_loss[loss=0.2142, simple_loss=0.2564, pruned_loss=0.06072, ctc_loss=0.1264, over 752797.56 frames. ], batch size: 32, lr: 9.39e-03,
2024-10-08 18:14:44,542 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=20659.0, ans=0.125
2024-10-08 18:14:52,537 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=20659.0, ans=0.2
2024-10-08 18:15:04,086 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=16.81 vs. limit=22.5
2024-10-08 18:15:06,052 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=20662.333333333332, ans=0.125
2024-10-08 18:15:08,766 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=20665.666666666668, ans=0.1
2024-10-08 18:15:14,061 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.913e+01 1.023e+02 1.164e+02 1.281e+02 2.016e+02, threshold=2.327e+02, percent-clipped=0.0
2024-10-08 18:15:15,568 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=20665.666666666668, ans=0.1
2024-10-08 18:15:25,487 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=21.33 vs. limit=22.5
2024-10-08 18:15:33,027 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=20669.0, ans=0.125
2024-10-08 18:15:36,925 INFO [train.py:1153] Epoch 10, batch 350, loss[loss=0.1844, simple_loss=0.2386, pruned_loss=0.04698, ctc_loss=0.09046, over 4883.00 frames. ], tot_loss[loss=0.214, simple_loss=0.2569, pruned_loss=0.06045, ctc_loss=0.1254, over 800246.22 frames. ], batch size: 19, lr: 9.39e-03,
2024-10-08 18:15:39,130 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.89 vs. limit=22.5
2024-10-08 18:15:42,448 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=20672.333333333332, ans=0.125
2024-10-08 18:16:27,711 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.11 vs. limit=15.0
2024-10-08 18:16:44,499 INFO [train.py:1153] Epoch 10, batch 400, loss[loss=0.1681, simple_loss=0.2302, pruned_loss=0.03856, ctc_loss=0.07201, over 4870.00 frames. ], tot_loss[loss=0.2129, simple_loss=0.2558, pruned_loss=0.0601, ctc_loss=0.1244, over 836963.14 frames. ], batch size: 22, lr: 9.38e-03,
2024-10-08 18:16:49,992 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=20689.0, ans=0.0
2024-10-08 18:16:53,057 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=11.73 vs. limit=15.0
2024-10-08 18:16:56,653 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=20692.333333333332, ans=0.2
2024-10-08 18:17:03,231 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=20692.333333333332, ans=0.035
2024-10-08 18:17:11,337 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=20695.666666666668, ans=0.1
2024-10-08 18:17:28,870 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.898e+01 1.022e+02 1.129e+02 1.282e+02 1.860e+02, threshold=2.258e+02, percent-clipped=0.0
2024-10-08 18:17:35,677 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=20699.0, ans=0.125
2024-10-08 18:17:47,939 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=20702.333333333332, ans=0.125
2024-10-08 18:17:51,780 INFO [train.py:1153] Epoch 10, batch 450, loss[loss=0.2252, simple_loss=0.2691, pruned_loss=0.06493, ctc_loss=0.1286, over 4855.00 frames. ], tot_loss[loss=0.2138, simple_loss=0.2565, pruned_loss=0.06059, ctc_loss=0.1249, over 865611.46 frames. ], batch size: 23, lr: 9.38e-03,
2024-10-08 18:17:53,315 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=20705.666666666668, ans=0.125
2024-10-08 18:18:21,771 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=20712.333333333332, ans=0.125
2024-10-08 18:18:40,581 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=20715.666666666668, ans=0.1
2024-10-08 18:18:54,867 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.33 vs. limit=15.0
2024-10-08 18:18:59,427 INFO [train.py:1153] Epoch 10, batch 500, loss[loss=0.2176, simple_loss=0.2581, pruned_loss=0.06239, ctc_loss=0.1309, over 4808.00 frames. ], tot_loss[loss=0.2123, simple_loss=0.2559, pruned_loss=0.05968, ctc_loss=0.1233, over 888160.89 frames. ], batch size: 34, lr: 9.38e-03,
2024-10-08 18:18:59,585 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=20722.333333333332, ans=0.1
2024-10-08 18:19:06,958 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=5.92 vs. limit=6.0
2024-10-08 18:19:10,378 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=20722.333333333332, ans=0.125
2024-10-08 18:19:19,209 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=5.80 vs. limit=15.0
2024-10-08 18:19:29,402 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.18 vs. limit=22.5
2024-10-08 18:19:30,461 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=20729.0, ans=0.125
2024-10-08 18:19:43,946 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.111e+01 9.900e+01 1.132e+02 1.279e+02 1.758e+02, threshold=2.264e+02, percent-clipped=0.0
2024-10-08 18:19:50,816 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=20732.333333333332, ans=0.2
2024-10-08 18:20:00,247 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.max_abs, batch_count=20735.666666666668, ans=10.0
2024-10-08 18:20:04,258 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=20735.666666666668, ans=0.125
2024-10-08 18:20:06,828 INFO [train.py:1153] Epoch 10, batch 550, loss[loss=0.2489, simple_loss=0.2819, pruned_loss=0.0767, ctc_loss=0.156, over 4808.00 frames. ], tot_loss[loss=0.2115, simple_loss=0.2555, pruned_loss=0.05914, ctc_loss=0.1231, over 905650.33 frames. ], batch size: 40, lr: 9.37e-03,
2024-10-08 18:20:21,158 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=21.08 vs. limit=22.5
2024-10-08 18:20:24,605 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=20742.333333333332, ans=0.125
2024-10-08 18:20:36,820 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=20745.666666666668, ans=0.125
2024-10-08 18:20:57,041 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=20749.0, ans=0.5
2024-10-08 18:21:14,508 INFO [train.py:1153] Epoch 10, batch 600, loss[loss=0.232, simple_loss=0.2706, pruned_loss=0.06686, ctc_loss=0.1488, over 4834.00 frames. ], tot_loss[loss=0.2105, simple_loss=0.2549, pruned_loss=0.05868, ctc_loss=0.1221, over 919352.53 frames. ], batch size: 38, lr: 9.37e-03,
2024-10-08 18:21:44,991 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=11.70 vs. limit=15.0
2024-10-08 18:21:59,145 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.407e+01 9.735e+01 1.086e+02 1.223e+02 1.902e+02, threshold=2.171e+02, percent-clipped=0.0
2024-10-08 18:22:08,019 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.32 vs. limit=15.0
2024-10-08 18:22:16,867 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=20769.0, ans=0.2
2024-10-08 18:22:18,308 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=20769.0, ans=0.125
2024-10-08 18:22:22,111 INFO [train.py:1153] Epoch 10, batch 650, loss[loss=0.2245, simple_loss=0.2721, pruned_loss=0.06317, ctc_loss=0.1264, over 4833.00 frames. ], tot_loss[loss=0.2091, simple_loss=0.254, pruned_loss=0.05798, ctc_loss=0.1204, over 930280.42 frames. ], batch size: 21, lr: 9.36e-03,
2024-10-08 18:22:46,579 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=20775.666666666668, ans=0.0063531159420289855
2024-10-08 18:22:58,196 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=13.83 vs. limit=15.0
2024-10-08 18:23:24,370 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=20785.666666666668, ans=0.04949747468305833
2024-10-08 18:23:29,465 INFO [train.py:1153] Epoch 10, batch 700, loss[loss=0.2543, simple_loss=0.2863, pruned_loss=0.0811, ctc_loss=0.1505, over 4745.00 frames. ], tot_loss[loss=0.2117, simple_loss=0.2558, pruned_loss=0.05925, ctc_loss=0.1227, over 938073.92 frames. ], batch size: 19, lr: 9.36e-03,
2024-10-08 18:23:40,231 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff2_skip_rate, batch_count=20789.0, ans=0.006350217391304348
2024-10-08 18:23:53,748 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=20792.333333333332, ans=0.125
2024-10-08 18:23:55,357 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=13.96 vs. limit=15.0
2024-10-08 18:23:59,149 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=2.100e-02
2024-10-08 18:24:13,999 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.704e+01 9.624e+01 1.092e+02 1.236e+02 1.681e+02, threshold=2.185e+02, percent-clipped=0.0
2024-10-08 18:24:16,953 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=20799.0, ans=0.0
2024-10-08 18:24:18,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=20799.0, ans=0.125
2024-10-08 18:24:31,324 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.58 vs. limit=22.5
2024-10-08 18:24:37,291 INFO [train.py:1153] Epoch 10, batch 750, loss[loss=0.2094, simple_loss=0.2715, pruned_loss=0.05241, ctc_loss=0.1062, over 4901.00 frames. ], tot_loss[loss=0.2099, simple_loss=0.255, pruned_loss=0.05826, ctc_loss=0.1207, over 944949.98 frames. ], batch size: 22, lr: 9.36e-03,
2024-10-08 18:24:55,824 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=20809.0, ans=0.125
2024-10-08 18:25:13,209 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=20812.333333333332, ans=0.025
2024-10-08 18:25:25,546 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.17 vs. limit=15.0
2024-10-08 18:25:43,752 INFO [train.py:1153] Epoch 10, batch 800, loss[loss=0.2001, simple_loss=0.2359, pruned_loss=0.06, ctc_loss=0.1109, over 4856.00 frames. ], tot_loss[loss=0.21, simple_loss=0.2549, pruned_loss=0.05842, ctc_loss=0.1208, over 949685.30 frames. ], batch size: 19, lr: 9.35e-03,
2024-10-08 18:26:00,276 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=20825.666666666668, ans=0.125
2024-10-08 18:26:13,727 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=20829.0, ans=0.125
2024-10-08 18:26:17,628 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=20829.0, ans=0.0
2024-10-08 18:26:28,344 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.671e+01 9.654e+01 1.120e+02 1.256e+02 1.937e+02, threshold=2.241e+02, percent-clipped=0.0
2024-10-08 18:26:29,953 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.attention_skip_rate, batch_count=20832.333333333332, ans=0.0
2024-10-08 18:26:44,756 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=20835.666666666668, ans=0.1
2024-10-08 18:26:51,319 INFO [train.py:1153] Epoch 10, batch 850, loss[loss=0.2434, simple_loss=0.2784, pruned_loss=0.07454, ctc_loss=0.1484, over 4779.00 frames. ], tot_loss[loss=0.2118, simple_loss=0.2562, pruned_loss=0.05922, ctc_loss=0.1222, over 953928.98 frames. ], batch size: 29, lr: 9.35e-03,
2024-10-08 18:27:07,971 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.35 vs. limit=15.0
2024-10-08 18:27:13,019 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=20842.333333333332, ans=0.125
2024-10-08 18:27:15,593 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=20842.333333333332, ans=0.006338623188405798
2024-10-08 18:27:22,434 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=20845.666666666668, ans=0.0063378985507246376
2024-10-08 18:27:28,903 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=20845.666666666668, ans=0.125
2024-10-08 18:27:31,780 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=20849.0, ans=0.125
2024-10-08 18:27:39,934 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=20849.0, ans=0.0
2024-10-08 18:27:58,537 INFO [train.py:1153] Epoch 10, batch 900, loss[loss=0.1798, simple_loss=0.2349, pruned_loss=0.04392, ctc_loss=0.09198, over 4855.00 frames. ], tot_loss[loss=0.2116, simple_loss=0.2562, pruned_loss=0.05905, ctc_loss=0.1223, over 956885.94 frames. ], batch size: 19, lr: 9.35e-03,
2024-10-08 18:28:01,359 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=20855.666666666668, ans=0.125
2024-10-08 18:28:12,247 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=20859.0, ans=0.125
2024-10-08 18:28:13,518 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer_ff2.min_abs, batch_count=20859.0, ans=0.1
2024-10-08 18:28:24,534 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.07 vs. limit=15.0
2024-10-08 18:28:34,755 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=20862.333333333332, ans=0.1
2024-10-08 18:28:42,934 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.390e+01 9.987e+01 1.122e+02 1.282e+02 2.010e+02, threshold=2.244e+02, percent-clipped=0.0
2024-10-08 18:28:53,462 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=20869.0, ans=0.025
2024-10-08 18:29:05,351 INFO [train.py:1153] Epoch 10, batch 950, loss[loss=0.2375, simple_loss=0.2664, pruned_loss=0.07659, ctc_loss=0.1386, over 4818.00 frames. ], tot_loss[loss=0.2116, simple_loss=0.2559, pruned_loss=0.05907, ctc_loss=0.1227, over 958852.14 frames. ], batch size: 19, lr: 9.34e-03,
2024-10-08 18:29:05,469 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=20872.333333333332, ans=0.1
2024-10-08 18:29:08,652 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=20.98 vs. limit=22.5
2024-10-08 18:29:09,443 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=20872.333333333332, ans=0.125
2024-10-08 18:29:35,737 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=16.00 vs. limit=22.5
2024-10-08 18:29:52,163 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=20882.333333333332, ans=0.0
2024-10-08 18:29:54,874 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=20882.333333333332, ans=0.04949747468305833
2024-10-08 18:30:11,047 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=20889.0, ans=0.125
2024-10-08 18:30:12,303 INFO [train.py:1153] Epoch 10, batch 1000, loss[loss=0.2054, simple_loss=0.2485, pruned_loss=0.05623, ctc_loss=0.1246, over 4934.00 frames. ], tot_loss[loss=0.2117, simple_loss=0.256, pruned_loss=0.05906, ctc_loss=0.1234, over 960582.37 frames. ], batch size: 20, lr: 9.34e-03,
2024-10-08 18:30:13,717 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=20889.0, ans=0.125
2024-10-08 18:30:28,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=20892.333333333332, ans=0.0063277536231884065
2024-10-08 18:30:34,712 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.whiten.whitening_limit, batch_count=20892.333333333332, ans=12.0
2024-10-08 18:30:57,082 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.744e+01 1.013e+02 1.090e+02 1.252e+02 1.629e+02, threshold=2.181e+02, percent-clipped=0.0
2024-10-08 18:30:58,982 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.38 vs. limit=15.0
2024-10-08 18:31:10,285 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=8.68 vs. limit=15.0
2024-10-08 18:31:20,323 INFO [train.py:1153] Epoch 10, batch 1050, loss[loss=0.2424, simple_loss=0.2673, pruned_loss=0.07718, ctc_loss=0.1578, over 4835.00 frames. ], tot_loss[loss=0.211, simple_loss=0.2556, pruned_loss=0.05864, ctc_loss=0.1226, over 962750.63 frames. ], batch size: 25, lr: 9.34e-03,
2024-10-08 18:31:27,179 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=20905.666666666668, ans=0.025
2024-10-08 18:31:31,898 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=2.80 vs. limit=15.0
2024-10-08 18:31:34,752 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.81 vs. limit=15.0
2024-10-08 18:31:48,131 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=2.99 vs. limit=15.0
2024-10-08 18:32:00,527 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.95 vs. limit=22.5
2024-10-08 18:32:11,965 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=20915.666666666668, ans=0.125
2024-10-08 18:32:19,275 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.46 vs. limit=15.0
2024-10-08 18:32:21,591 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=20919.0, ans=0.025
2024-10-08 18:32:28,172 INFO [train.py:1153] Epoch 10, batch 1100, loss[loss=0.2151, simple_loss=0.2566, pruned_loss=0.06188, ctc_loss=0.1247, over 4855.00 frames. ], tot_loss[loss=0.2114, simple_loss=0.2558, pruned_loss=0.05894, ctc_loss=0.1231, over 964090.44 frames. ], batch size: 20, lr: 9.33e-03,
2024-10-08 18:32:30,973 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=20922.333333333332, ans=0.006321231884057971
2024-10-08 18:32:42,986 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=20925.666666666668, ans=0.125
2024-10-08 18:32:53,795 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=20929.0, ans=0.5
2024-10-08 18:33:05,138 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.98 vs. limit=22.5
2024-10-08 18:33:12,553 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.161e+01 1.007e+02 1.102e+02 1.208e+02 1.812e+02, threshold=2.203e+02, percent-clipped=0.0
2024-10-08 18:33:28,638 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=0.000e+00
2024-10-08 18:33:35,214 INFO [train.py:1153] Epoch 10, batch 1150, loss[loss=0.2237, simple_loss=0.2586, pruned_loss=0.06712, ctc_loss=0.1363, over 4860.00 frames. ], tot_loss[loss=0.2117, simple_loss=0.2554, pruned_loss=0.05931, ctc_loss=0.1235, over 964376.57 frames. ], batch size: 20, lr: 9.33e-03,
2024-10-08 18:33:46,296 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=20939.0, ans=0.006317608695652174
2024-10-08 18:34:08,101 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=14.74 vs. limit=15.0
2024-10-08 18:34:42,293 INFO [train.py:1153] Epoch 10, batch 1200, loss[loss=0.2498, simple_loss=0.287, pruned_loss=0.07678, ctc_loss=0.1476, over 4802.00 frames. ], tot_loss[loss=0.2117, simple_loss=0.256, pruned_loss=0.05895, ctc_loss=0.1235, over 964377.78 frames. ], batch size: 25, lr: 9.32e-03,
2024-10-08 18:34:52,441 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=5.47 vs. limit=6.0
2024-10-08 18:35:04,494 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.65 vs. limit=15.0
2024-10-08 18:35:09,404 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=20962.333333333332, ans=0.125
2024-10-08 18:35:26,594 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.797e+01 1.007e+02 1.142e+02 1.257e+02 2.737e+02, threshold=2.284e+02, percent-clipped=1.0
2024-10-08 18:35:44,378 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=20969.0, ans=0.1
2024-10-08 18:35:49,521 INFO [train.py:1153] Epoch 10, batch 1250, loss[loss=0.2405, simple_loss=0.2635, pruned_loss=0.07611, ctc_loss=0.1631, over 4790.00 frames. ], tot_loss[loss=0.2121, simple_loss=0.2559, pruned_loss=0.05934, ctc_loss=0.124, over 964514.42 frames. ], batch size: 32, lr: 9.32e-03,
2024-10-08 18:36:02,616 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=20975.666666666668, ans=0.025
2024-10-08 18:36:19,615 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=20979.0, ans=0.125
2024-10-08 18:36:24,053 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.04 vs. limit=15.0
2024-10-08 18:36:35,275 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=20982.333333333332, ans=0.125
2024-10-08 18:36:51,518 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer1.prob, batch_count=20985.666666666668, ans=0.125
2024-10-08 18:36:55,448 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=20989.0, ans=0.125
2024-10-08 18:36:56,663 INFO [train.py:1153] Epoch 10, batch 1300, loss[loss=0.2362, simple_loss=0.2767, pruned_loss=0.06771, ctc_loss=0.1506, over 4825.00 frames. ], tot_loss[loss=0.2122, simple_loss=0.2562, pruned_loss=0.05939, ctc_loss=0.1236, over 965760.53 frames. ], batch size: 43, lr: 9.32e-03,
2024-10-08 18:37:14,196 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=20992.333333333332, ans=0.125
2024-10-08 18:37:24,295 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=14.84 vs. limit=15.0
2024-10-08 18:37:28,542 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=6.06 vs. limit=6.0
2024-10-08 18:37:37,743 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.83 vs. limit=15.0
2024-10-08 18:37:41,263 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.611e+01 1.012e+02 1.118e+02 1.262e+02 1.653e+02, threshold=2.235e+02, percent-clipped=0.0
2024-10-08 18:38:04,209 INFO [train.py:1153] Epoch 10, batch 1350, loss[loss=0.2077, simple_loss=0.2596, pruned_loss=0.05417, ctc_loss=0.1189, over 4844.00 frames. ], tot_loss[loss=0.2119, simple_loss=0.256, pruned_loss=0.05931, ctc_loss=0.123, over 966580.97 frames. ], batch size: 21, lr: 9.31e-03,
2024-10-08 18:38:09,035 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=512, metric=3.70 vs. limit=15.0
2024-10-08 18:38:22,025 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=21009.0, ans=0.1
2024-10-08 18:38:25,974 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=21009.0, ans=0.2
2024-10-08 18:38:38,095 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=21012.333333333332, ans=0.2
2024-10-08 18:38:45,712 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=14.58 vs. limit=15.0
2024-10-08 18:38:57,482 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.54 vs. limit=15.0
2024-10-08 18:38:59,852 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=21019.0, ans=0.1
2024-10-08 18:39:01,126 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=21019.0, ans=0.0
2024-10-08 18:39:11,805 INFO [train.py:1153] Epoch 10, batch 1400, loss[loss=0.1799, simple_loss=0.2342, pruned_loss=0.04444, ctc_loss=0.09189, over 4940.00 frames. ], tot_loss[loss=0.2122, simple_loss=0.2565, pruned_loss=0.05934, ctc_loss=0.1229, over 966961.23 frames. ], batch size: 19, lr: 9.31e-03,
2024-10-08 18:39:20,145 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=21022.333333333332, ans=0.07
2024-10-08 18:39:26,074 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=15.71 vs. limit=15.0
2024-10-08 18:39:30,135 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=14.92 vs. limit=15.0
2024-10-08 18:39:30,898 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=21025.666666666668, ans=0.125
2024-10-08 18:39:39,388 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=12.81 vs. limit=15.0
2024-10-08 18:39:52,316 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=21032.333333333332, ans=0.2
2024-10-08 18:39:56,004 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.407e+01 1.050e+02 1.181e+02 1.333e+02 1.897e+02, threshold=2.362e+02, percent-clipped=0.0
2024-10-08 18:40:04,221 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=21035.666666666668, ans=0.1
2024-10-08 18:40:15,042 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=21035.666666666668, ans=0.2
2024-10-08 18:40:15,044 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=21035.666666666668, ans=0.125
2024-10-08 18:40:18,847 INFO [train.py:1153] Epoch 10, batch 1450, loss[loss=0.2706, simple_loss=0.3025, pruned_loss=0.08317, ctc_loss=0.1808, over 4822.00 frames. ], tot_loss[loss=0.2143, simple_loss=0.2582, pruned_loss=0.06027, ctc_loss=0.1246, over 966725.28 frames. ], batch size: 34, lr: 9.31e-03,
2024-10-08 18:40:24,227 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=21039.0, ans=0.125
2024-10-08 18:40:45,607 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=21045.666666666668, ans=0.125
2024-10-08 18:40:57,619 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=21049.0, ans=0.2
2024-10-08 18:41:01,585 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=21049.0, ans=0.125
2024-10-08 18:41:12,227 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=21052.333333333332, ans=0.1
2024-10-08 18:41:15,360 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=3.89 vs. limit=12.0
2024-10-08 18:41:21,727 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff3_skip_rate, batch_count=21052.333333333332, ans=0.006292971014492754
2024-10-08 18:41:25,554 INFO [train.py:1153] Epoch 10, batch 1500, loss[loss=0.1915, simple_loss=0.2434, pruned_loss=0.0494, ctc_loss=0.1021, over 4747.00 frames. ], tot_loss[loss=0.2144, simple_loss=0.2583, pruned_loss=0.06036, ctc_loss=0.1244, over 966388.34 frames. ], batch size: 26, lr: 9.30e-03,
2024-10-08 18:41:27,430 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys.whitening_limit, batch_count=21055.666666666668, ans=6.0
2024-10-08 18:41:35,555 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=25.97 vs. limit=22.5
2024-10-08 18:41:50,004 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=21059.0, ans=0.95
2024-10-08 18:42:09,888 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.387e+01 1.047e+02 1.161e+02 1.299e+02 1.794e+02, threshold=2.321e+02, percent-clipped=0.0
2024-10-08 18:42:14,071 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=21065.666666666668, ans=0.1
2024-10-08 18:42:20,080 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=8.77 vs. limit=15.0
2024-10-08 18:42:32,708 INFO [train.py:1153] Epoch 10, batch 1550, loss[loss=0.2169, simple_loss=0.2563, pruned_loss=0.06158, ctc_loss=0.1355, over 4853.00 frames. ], tot_loss[loss=0.214, simple_loss=0.2579, pruned_loss=0.06022, ctc_loss=0.1244, over 966265.99 frames. ], batch size: 31, lr: 9.30e-03,
2024-10-08 18:42:39,493 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=21072.333333333332, ans=0.2
2024-10-08 18:42:47,513 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=21075.666666666668, ans=0.125
2024-10-08 18:42:51,515 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff2_skip_rate, batch_count=21075.666666666668, ans=0.006287898550724638
2024-10-08 18:43:09,134 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=21079.0, ans=0.07
2024-10-08 18:43:30,579 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=21085.666666666668, ans=0.125
2024-10-08 18:43:40,013 INFO [train.py:1153] Epoch 10, batch 1600, loss[loss=0.2377, simple_loss=0.2802, pruned_loss=0.0689, ctc_loss=0.1434, over 4835.00 frames. ], tot_loss[loss=0.2135, simple_loss=0.2573, pruned_loss=0.05999, ctc_loss=0.1242, over 966475.40 frames. ], batch size: 25, lr: 9.30e-03,
2024-10-08 18:43:41,945 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.32 vs. limit=15.0
2024-10-08 18:43:46,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=21089.0, ans=0.0
2024-10-08 18:43:56,147 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=21092.333333333332, ans=0.125
2024-10-08 18:43:57,886 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.73 vs. limit=15.0
2024-10-08 18:44:10,912 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=21095.666666666668, ans=0.0
2024-10-08 18:44:14,973 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=21095.666666666668, ans=0.09899494936611666
2024-10-08 18:44:17,567 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=21095.666666666668, ans=0.006283550724637681
2024-10-08 18:44:20,221 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.scale_min, batch_count=21099.0, ans=0.2
2024-10-08 18:44:23,948 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.336e+01 1.020e+02 1.138e+02 1.293e+02 1.732e+02, threshold=2.276e+02, percent-clipped=0.0
2024-10-08 18:44:36,876 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=19.31 vs. limit=15.0
2024-10-08 18:44:46,995 INFO [train.py:1153] Epoch 10, batch 1650, loss[loss=0.2123, simple_loss=0.261, pruned_loss=0.05498, ctc_loss=0.1341, over 4764.00 frames. ], tot_loss[loss=0.214, simple_loss=0.2574, pruned_loss=0.06025, ctc_loss=0.125, over 966863.16 frames. ], batch size: 29, lr: 9.29e-03,
2024-10-08 18:44:58,013 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=21105.666666666668, ans=0.125
2024-10-08 18:45:02,677 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.12 vs. limit=15.0
2024-10-08 18:45:24,019 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=21112.333333333332, ans=0.125
2024-10-08 18:45:32,559 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.23 vs. limit=15.0
2024-10-08 18:45:34,717 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=21115.666666666668, ans=0.125
2024-10-08 18:45:52,245 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.prob, batch_count=21119.0, ans=0.125
2024-10-08 18:45:54,808 INFO [train.py:1153] Epoch 10, batch 1700, loss[loss=0.1651, simple_loss=0.2203, pruned_loss=0.03788, ctc_loss=0.08517, over 4940.00 frames. ], tot_loss[loss=0.2121, simple_loss=0.256, pruned_loss=0.05946, ctc_loss=0.1232, over 966860.76 frames. ], batch size: 19, lr: 9.29e-03,
2024-10-08 18:46:05,940 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=21122.333333333332, ans=0.125
2024-10-08 18:46:20,730 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=21129.0, ans=0.0
2024-10-08 18:46:22,096 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=21129.0, ans=0.125
2024-10-08 18:46:31,402 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=1.155e-02
2024-10-08 18:46:39,085 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.732e+01 9.587e+01 1.078e+02 1.249e+02 1.922e+02, threshold=2.155e+02, percent-clipped=0.0
2024-10-08 18:46:44,574 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=21132.333333333332, ans=0.1
2024-10-08 18:46:50,906 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=4.17 vs. limit=15.0
2024-10-08 18:46:56,899 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=21135.666666666668, ans=0.125
2024-10-08 18:47:02,113 INFO [train.py:1153] Epoch 10, batch 1750, loss[loss=0.2019, simple_loss=0.2607, pruned_loss=0.05209, ctc_loss=0.09725, over 4959.00 frames. ], tot_loss[loss=0.2104, simple_loss=0.2548, pruned_loss=0.05871, ctc_loss=0.1216, over 966913.36 frames. ], batch size: 19, lr: 9.28e-03,
2024-10-08 18:47:32,766 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=21145.666666666668, ans=0.125
2024-10-08 18:47:36,171 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module1.whiten, num_groups=1, num_channels=192, metric=3.51 vs. limit=15.0
2024-10-08 18:47:51,429 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=21149.0, ans=0.006271956521739131
2024-10-08 18:48:08,911 INFO [train.py:1153] Epoch 10, batch 1800, loss[loss=0.2114, simple_loss=0.2725, pruned_loss=0.05379, ctc_loss=0.1069, over 4873.00 frames. ], tot_loss[loss=0.2094, simple_loss=0.2541, pruned_loss=0.05817, ctc_loss=0.1206, over 967772.70 frames. ], batch size: 23, lr: 9.28e-03,
2024-10-08 18:48:16,644 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.82 vs. limit=6.0
2024-10-08 18:48:39,849 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=21162.333333333332, ans=0.125
2024-10-08 18:48:52,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=21165.666666666668, ans=0.125
2024-10-08 18:48:53,286 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.312e+01 9.370e+01 1.059e+02 1.210e+02 1.492e+02, threshold=2.117e+02, percent-clipped=0.0
2024-10-08 18:49:16,126 INFO [train.py:1153] Epoch 10, batch 1850, loss[loss=0.1835, simple_loss=0.2371, pruned_loss=0.04587, ctc_loss=0.09546, over 4743.00 frames. ], tot_loss[loss=0.21, simple_loss=0.2546, pruned_loss=0.05852, ctc_loss=0.1208, over 967869.32 frames. ], batch size: 26, lr: 9.28e-03,
2024-10-08 18:49:18,138 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.44 vs. limit=15.0
2024-10-08 18:49:35,209 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=21175.666666666668, ans=0.1
2024-10-08 18:49:40,801 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.01 vs. limit=6.0
2024-10-08 18:49:51,244 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=21179.0, ans=0.125
2024-10-08 18:49:52,544 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer_na.min_abs, batch_count=21179.0, ans=0.02
2024-10-08 18:49:53,872 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=21179.0, ans=0.0
2024-10-08 18:50:23,165 INFO [train.py:1153] Epoch 10, batch 1900, loss[loss=0.1848, simple_loss=0.2281, pruned_loss=0.04902, ctc_loss=0.1086, over 4770.00 frames. ], tot_loss[loss=0.2105, simple_loss=0.2549, pruned_loss=0.05877, ctc_loss=0.1212, over 967739.49 frames. ], batch size: 29, lr: 9.27e-03,
2024-10-08 18:50:23,341 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=21189.0, ans=0.125
2024-10-08 18:50:29,961 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=21189.0, ans=0.125
2024-10-08 18:51:07,159 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.678e+01 1.003e+02 1.087e+02 1.215e+02 1.591e+02, threshold=2.174e+02, percent-clipped=0.0
2024-10-08 18:51:11,352 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=21199.0, ans=0.125
2024-10-08 18:51:13,091 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=3.76 vs. limit=12.0
2024-10-08 18:51:16,761 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=21202.333333333332, ans=0.1
2024-10-08 18:51:28,634 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=21205.666666666668, ans=0.125
2024-10-08 18:51:29,816 INFO [train.py:1153] Epoch 10, batch 1950, loss[loss=0.1849, simple_loss=0.2408, pruned_loss=0.04532, ctc_loss=0.09574, over 4858.00 frames. ], tot_loss[loss=0.2109, simple_loss=0.2553, pruned_loss=0.05894, ctc_loss=0.1216, over 966719.34 frames. ], batch size: 20, lr: 9.27e-03,
2024-10-08 18:51:37,959 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=21205.666666666668, ans=0.125
2024-10-08 18:51:47,150 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=21209.0, ans=0.0
2024-10-08 18:52:06,243 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=21.68 vs. limit=22.5
2024-10-08 18:52:09,772 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=21215.666666666668, ans=0.125
2024-10-08 18:52:11,071 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=21215.666666666668, ans=0.125
2024-10-08 18:52:11,096 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=21215.666666666668, ans=0.0
2024-10-08 18:52:33,265 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.34 vs. limit=15.0
2024-10-08 18:52:36,511 INFO [train.py:1153] Epoch 10, batch 2000, loss[loss=0.185, simple_loss=0.2388, pruned_loss=0.04669, ctc_loss=0.09448, over 4959.00 frames. ], tot_loss[loss=0.2119, simple_loss=0.2561, pruned_loss=0.05935, ctc_loss=0.1228, over 966570.66 frames. ], batch size: 19, lr: 9.27e-03,
2024-10-08 18:52:46,186 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=21222.333333333332, ans=0.125
2024-10-08 18:52:55,427 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=21225.666666666668, ans=0.125
2024-10-08 18:53:06,820 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.62 vs. limit=6.0
2024-10-08 18:53:20,794 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.441e+01 1.028e+02 1.141e+02 1.281e+02 2.686e+02, threshold=2.283e+02, percent-clipped=2.0
2024-10-08 18:53:38,697 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.84 vs. limit=15.0
2024-10-08 18:53:43,512 INFO [train.py:1153] Epoch 10, batch 2050, loss[loss=0.2161, simple_loss=0.2582, pruned_loss=0.06163, ctc_loss=0.1269, over 4910.00 frames. ], tot_loss[loss=0.2119, simple_loss=0.2557, pruned_loss=0.05945, ctc_loss=0.1229, over 966961.19 frames. ], batch size: 19, lr: 9.26e-03,
2024-10-08 18:53:53,001 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.prob, batch_count=21239.0, ans=0.125
2024-10-08 18:54:03,986 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=21242.333333333332, ans=0.125
2024-10-08 18:54:12,741 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.22 vs. limit=15.0
2024-10-08 18:54:28,163 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=21249.0, ans=0.1
2024-10-08 18:54:50,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.attention_skip_rate, batch_count=21255.666666666668, ans=0.0
2024-10-08 18:54:50,117 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=21255.666666666668, ans=0.1
2024-10-08 18:54:51,262 INFO [train.py:1153] Epoch 10, batch 2100, loss[loss=0.2167, simple_loss=0.2646, pruned_loss=0.05991, ctc_loss=0.1223, over 4857.00 frames. ], tot_loss[loss=0.2114, simple_loss=0.2561, pruned_loss=0.05902, ctc_loss=0.122, over 967086.73 frames. ], batch size: 21, lr: 9.26e-03,
2024-10-08 18:55:04,821 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=21259.0, ans=0.1
2024-10-08 18:55:25,720 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.10 vs. limit=15.0
2024-10-08 18:55:29,162 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=21262.333333333332, ans=0.2
2024-10-08 18:55:35,831 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.562e+01 9.573e+01 1.146e+02 1.297e+02 1.924e+02, threshold=2.292e+02, percent-clipped=0.0
2024-10-08 18:55:36,023 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.prob, batch_count=21265.666666666668, ans=0.125
2024-10-08 18:55:42,659 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=21265.666666666668, ans=0.125
2024-10-08 18:55:45,431 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=21269.0, ans=0.1
2024-10-08 18:55:52,856 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.81 vs. limit=15.0
2024-10-08 18:55:57,610 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=21272.333333333332, ans=0.125
2024-10-08 18:55:58,769 INFO [train.py:1153] Epoch 10, batch 2150, loss[loss=0.147, simple_loss=0.202, pruned_loss=0.03228, ctc_loss=0.06881, over 4858.00 frames. ], tot_loss[loss=0.2099, simple_loss=0.255, pruned_loss=0.0583, ctc_loss=0.1207, over 967836.24 frames. ], batch size: 20, lr: 9.26e-03,
2024-10-08 18:56:10,109 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=11.71 vs. limit=15.0
2024-10-08 18:56:33,874 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=21279.0, ans=0.125
2024-10-08 18:56:43,159 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=21282.333333333332, ans=0.125
2024-10-08 18:56:57,207 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=21285.666666666668, ans=0.07
2024-10-08 18:57:06,099 INFO [train.py:1153] Epoch 10, batch 2200, loss[loss=0.1959, simple_loss=0.2422, pruned_loss=0.05231, ctc_loss=0.1126, over 4712.00 frames. ], tot_loss[loss=0.2112, simple_loss=0.2557, pruned_loss=0.05892, ctc_loss=0.1221, over 967488.82 frames. ], batch size: 26, lr: 9.25e-03,
2024-10-08 18:57:42,121 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=21295.666666666668, ans=0.125
2024-10-08 18:57:49,938 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.680e+01 9.822e+01 1.126e+02 1.286e+02 1.927e+02, threshold=2.251e+02, percent-clipped=0.0
2024-10-08 18:57:58,084 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=21302.333333333332, ans=0.2
2024-10-08 18:57:58,118 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=21302.333333333332, ans=0.05
2024-10-08 18:58:01,339 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.79 vs. limit=15.0
2024-10-08 18:58:05,443 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=6.05 vs. limit=6.0
2024-10-08 18:58:09,415 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=5.90 vs. limit=6.0
2024-10-08 18:58:11,692 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=21305.666666666668, ans=0.006237898550724638
2024-10-08 18:58:12,883 INFO [train.py:1153] Epoch 10, batch 2250, loss[loss=0.1735, simple_loss=0.2303, pruned_loss=0.0405, ctc_loss=0.08935, over 4870.00 frames. ], tot_loss[loss=0.2114, simple_loss=0.256, pruned_loss=0.05895, ctc_loss=0.1221, over 967619.45 frames. ], batch size: 22, lr: 9.25e-03,
2024-10-08 18:58:22,499 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=21305.666666666668, ans=0.125
2024-10-08 18:58:29,286 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=21309.0, ans=0.125
2024-10-08 18:59:10,013 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=17.57 vs. limit=22.5
2024-10-08 18:59:20,295 INFO [train.py:1153] Epoch 10, batch 2300, loss[loss=0.1886, simple_loss=0.2497, pruned_loss=0.046, ctc_loss=0.08891, over 4883.00 frames. ], tot_loss[loss=0.2095, simple_loss=0.2546, pruned_loss=0.05814, ctc_loss=0.1204, over 968276.26 frames. ], batch size: 19, lr: 9.24e-03,
2024-10-08 18:59:37,897 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=21325.666666666668, ans=0.125
2024-10-08 18:59:59,706 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=21332.333333333332, ans=0.2
2024-10-08 19:00:03,616 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp1/checkpoint-64000.pt
2024-10-08 19:00:05,630 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.183e+01 9.535e+01 1.078e+02 1.180e+02 1.637e+02, threshold=2.157e+02, percent-clipped=0.0
2024-10-08 19:00:05,824 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=21332.333333333332, ans=0.125
2024-10-08 19:00:21,188 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=21335.666666666668, ans=0.2
2024-10-08 19:00:27,630 INFO [train.py:1153] Epoch 10, batch 2350, loss[loss=0.1959, simple_loss=0.2447, pruned_loss=0.05178, ctc_loss=0.1088, over 4849.00 frames. ], tot_loss[loss=0.2092, simple_loss=0.2546, pruned_loss=0.05785, ctc_loss=0.1204, over 968331.13 frames. ], batch size: 23, lr: 9.24e-03,
2024-10-08 19:00:31,727 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=21339.0, ans=0.1
2024-10-08 19:00:34,399 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=21339.0, ans=0.09899494936611666
2024-10-08 19:00:34,895 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.39 vs. limit=15.0
2024-10-08 19:00:38,424 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=21339.0, ans=0.0
2024-10-08 19:00:45,479 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten.whitening_limit, batch_count=21342.333333333332, ans=15.0
2024-10-08 19:01:00,771 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=21345.666666666668, ans=0.07
