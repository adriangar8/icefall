2024-10-08 19:29:55,447 INFO [train.py:1232] Training started
2024-10-08 19:29:55,449 INFO [train.py:1242] Device: cuda:0
2024-10-08 19:29:55,450 INFO [train.py:1273] Using dtype=torch.float32
2024-10-08 19:29:55,450 INFO [train.py:1274] Use AMP=False
2024-10-08 19:29:55,450 INFO [train.py:1276] {'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 50, 'reset_interval': 200, 'valid_interval': 3000, 'feature_dim': 80, 'subsampling_factor': 4, 'ignore_id': -1, 'label_smoothing': 0.1, 'warm_step': 2000, 'env_info': {'k2-version': '1.24.3', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': 'e400fa3b456faf8afe0ee5bfe572946b4921a3db', 'k2-git-date': 'Sat Jul 15 04:21:50 2023', 'lhotse-version': '1.25.0', 'torch-version': '2.0.1+cu118', 'torch-cuda-available': True, 'torch-cuda-version': '11.8', 'python-version': '3.9', 'icefall-git-branch': 'master', 'icefall-git-sha1': 'cabeaf7f-dirty', 'icefall-git-date': 'Thu Oct 3 12:53:52 2024', 'icefall-path': '/mnt/ahogpu_ldisk2/adriang/icefall', 'k2-path': '/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/k2/__init__.py', 'lhotse-path': '/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/lhotse/__init__.py', 'hostname': 'ahogpu', 'IP address': '192.168.1.130'}, 'world_size': 1, 'master_port': 12354, 'tensorboard': True, 'num_epochs': 30, 'start_epoch': 1, 'start_batch': 0, 'exp_dir': PosixPath('zipformer/exp_transducer_True_ctc_True_attdecoder_False_streaming_True'), 'bpe_model': '/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/data/lang_bpe_256/bpe.model', 'base_lr': 0.045, 'lr_batches': 7500, 'lr_epochs': 3.5, 'ref_duration': 600, 'context_size': 2, 'prune_range': 5, 'lm_scale': 0.25, 'am_scale': 0.0, 'simple_loss_scale': 0.5, 'ctc_loss_scale': 0.2, 'attention_decoder_loss_scale': 0.8, 'seed': 42, 'print_diagnostics': False, 'inf_check': False, 'save_every_n': 4000, 'keep_last_k': 30, 'average_period': 200, 'use_bf16': False, 'use_fp16': False, 'num_encoder_layers': '2,2,3,4,3,2', 'downsampling_factor': '1,2,4,8,4,2', 'feedforward_dim': '512,768,1024,1536,1024,768', 'num_heads': '4,4,4,8,4,4', 'encoder_dim': '192,256,384,512,384,256', 'query_head_dim': '32', 'value_head_dim': '12', 'pos_head_dim': '4', 'pos_dim': 48, 'encoder_unmasked_dim': '192,192,256,256,256,192', 'cnn_module_kernel': '31,31,15,15,15,31', 'decoder_dim': 512, 'joiner_dim': 512, 'attention_decoder_dim': 512, 'attention_decoder_num_layers': 6, 'attention_decoder_attention_dim': 512, 'attention_decoder_num_heads': 8, 'attention_decoder_feedforward_dim': 2048, 'causal': True, 'chunk_size': '16,32,64,-1', 'left_context_frames': '64,128,256,-1', 'use_transducer': True, 'use_ctc': True, 'use_attention_decoder': False, 'manifest_dir': PosixPath('/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/data/fbank'), 'max_duration': 200.0, 'bucketing_sampler': True, 'num_buckets': 30, 'concatenate_cuts': False, 'duration_factor': 1.0, 'gap': 1.0, 'on_the_fly_feats': False, 'shuffle': True, 'drop_last': True, 'return_cuts': True, 'num_workers': 2, 'enable_spec_aug': True, 'spec_aug_time_warp_factor': 80, 'enable_musan': True, 'input_strategy': 'PrecomputedFeatures', 'blank_id': 0, 'sos_id': 1, 'eos_id': 1, 'vocab_size': 256, 'dtype': torch.float32, 'use_autocast': False}
2024-10-08 19:29:55,450 INFO [train.py:1278] About to create model
2024-10-08 19:29:55,851 INFO [train.py:1282] Number of model parameters: 65741815
2024-10-08 19:29:57,418 INFO [train.py:1300] Using single GPU
2024-10-08 19:29:57,434 INFO [custom_asr_data_module.py:394] About to load train cuts
2024-10-08 19:29:57,435 INFO [custom_asr_data_module.py:204] Enable MUSAN
2024-10-08 19:29:57,435 INFO [custom_asr_data_module.py:205] About to get Musan cuts
2024-10-08 19:29:58,958 INFO [custom_asr_data_module.py:234] Enable SpecAugment
2024-10-08 19:29:58,959 INFO [custom_asr_data_module.py:235] Time warp factor: 80
2024-10-08 19:29:58,959 INFO [custom_asr_data_module.py:245] Num frame mask: 10
2024-10-08 19:29:58,959 INFO [custom_asr_data_module.py:260] About to create train dataset
2024-10-08 19:29:58,959 INFO [custom_asr_data_module.py:287] Using DynamicBucketingSampler.
2024-10-08 19:29:59,365 INFO [custom_asr_data_module.py:304] About to create train dataloader
2024-10-08 19:29:59,365 INFO [custom_asr_data_module.py:402] About to load valid cuts
2024-10-08 19:29:59,366 INFO [custom_asr_data_module.py:338] About to create dev dataset
2024-10-08 19:29:59,389 INFO [custom_asr_data_module.py:355] About to create dev dataloader
2024-10-08 19:29:59,389 INFO [train.py:1492] Sanity check -- see if any of the batches in epoch 1 would cause OOM.
/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
2024-10-08 19:30:33,578 INFO [train.py:1520] Maximum memory allocated so far is 4791MB
2024-10-08 19:30:34,461 INFO [train.py:1520] Maximum memory allocated so far is 4804MB
2024-10-08 19:30:35,323 INFO [train.py:1520] Maximum memory allocated so far is 4804MB
2024-10-08 19:30:36,287 INFO [train.py:1520] Maximum memory allocated so far is 4871MB
2024-10-08 19:30:37,174 INFO [train.py:1520] Maximum memory allocated so far is 4871MB
2024-10-08 19:30:38,107 INFO [train.py:1520] Maximum memory allocated so far is 4871MB
2024-10-08 19:30:51,216 INFO [train.py:1154] Epoch 1, batch 0, loss[loss=7.803, simple_loss=6.28, pruned_loss=6.321, ctc_loss=4.448, over 4853.00 frames. ], tot_loss[loss=7.803, simple_loss=6.28, pruned_loss=6.321, ctc_loss=4.448, over 4853.00 frames. ], batch size: 19, lr: 2.25e-02,
2024-10-08 19:30:51,217 INFO [train.py:1177] Computing validation loss
2024-10-08 19:30:56,036 INFO [train.py:1186] Epoch 1, validation: loss=7.652, simple_loss=6.139, pruned_loss=6.254, ctc_loss=4.431, over 90464.00 frames.
2024-10-08 19:30:56,037 INFO [train.py:1187] Maximum memory allocated so far is 4871MB
2024-10-08 19:31:05,933 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.551e+02 9.163e+02 1.246e+03 1.352e+03 1.545e+03, threshold=4.983e+03, percent-clipped=0.0
2024-10-08 19:31:09,370 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.min_positive, batch_count=3.3333333333333335, ans=0.24996666666666667
2024-10-08 19:31:10,620 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=19.08 vs. limit=7.5025
2024-10-08 19:31:10,941 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=18.23 vs. limit=7.5025
2024-10-08 19:31:13,646 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=246.96 vs. limit=7.50125
2024-10-08 19:31:16,227 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.045e+02 4.186e+02 8.551e+02 1.327e+03 1.636e+03, threshold=3.420e+03, percent-clipped=0.0
2024-10-08 19:31:19,120 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=18.04 vs. limit=5.003333333333333
2024-10-08 19:31:20,848 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=132.73 vs. limit=7.5025
2024-10-08 19:31:23,699 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=14.02 vs. limit=4.002666666666666
2024-10-08 19:31:33,069 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=200.02 vs. limit=7.50375
2024-10-08 19:31:35,323 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=27.48 vs. limit=5.0025
2024-10-08 19:31:37,081 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.737e+02 2.500e+02 3.832e+02 8.551e+02 1.636e+03, threshold=1.533e+03, percent-clipped=0.0
2024-10-08 19:31:37,385 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 19:31:39,017 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=38.75 vs. limit=7.505
2024-10-08 19:31:39,604 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.min_positive, batch_count=13.333333333333334, ans=0.049958333333333334
2024-10-08 19:31:42,116 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=85.95 vs. limit=7.505
2024-10-08 19:31:43,340 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=22.67 vs. limit=4.005333333333334
2024-10-08 19:31:48,761 INFO [train.py:1154] Epoch 1, batch 50, loss[loss=1.464, simple_loss=1.08, pruned_loss=1.188, ctc_loss=1.261, over 4912.00 frames. ], tot_loss[loss=3.033, simple_loss=2.404, pruned_loss=2.067, ctc_loss=2.085, over 217782.93 frames. ], batch size: 19, lr: 2.48e-02,
2024-10-08 19:31:53,675 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=16.666666666666668, ans=0.8994166666666666
2024-10-08 19:32:07,365 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=0.000e+00
2024-10-08 19:32:10,543 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=23.333333333333332, ans=0.09947500000000001
2024-10-08 19:32:22,177 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=26.666666666666668, ans=0.2997333333333333
2024-10-08 19:32:30,707 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=38.38 vs. limit=5.013333333333334
2024-10-08 19:32:33,781 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=30.0, ans=0.19887500000000002
2024-10-08 19:32:46,274 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.822e+01 6.106e+01 1.296e+02 2.855e+02 1.636e+03, threshold=2.593e+02, percent-clipped=0.0
2024-10-08 19:32:46,317 INFO [train.py:1154] Epoch 1, batch 100, loss[loss=1.212, simple_loss=0.8674, pruned_loss=1.075, ctc_loss=1.052, over 4752.00 frames. ], tot_loss[loss=2.125, simple_loss=1.637, pruned_loss=1.577, ctc_loss=1.58, over 383572.91 frames. ], batch size: 19, lr: 2.70e-02,
2024-10-08 19:32:53,057 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=22.25 vs. limit=7.525
2024-10-08 19:32:54,012 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=29.06 vs. limit=7.5125
2024-10-08 19:32:56,374 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=78.08 vs. limit=7.525
2024-10-08 19:32:56,880 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=36.666666666666664, ans=0.2996333333333333
2024-10-08 19:33:02,303 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=43.25 vs. limit=7.51375
2024-10-08 19:33:06,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=36.666666666666664, ans=0.49828125
2024-10-08 19:33:07,808 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=273.60 vs. limit=7.51375
2024-10-08 19:33:13,510 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=38.66 vs. limit=7.53
2024-10-08 19:33:15,769 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=67.27 vs. limit=7.515
2024-10-08 19:33:18,565 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=27.71 vs. limit=7.53
2024-10-08 19:33:26,158 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=41.23 vs. limit=7.51625
2024-10-08 19:33:29,554 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=43.333333333333336, ans=0.19837500000000002
2024-10-08 19:33:30,683 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=43.333333333333336, ans=0.04986458333333334
2024-10-08 19:33:38,884 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=46.666666666666664, ans=0.4978125
2024-10-08 19:33:44,439 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=42.07 vs. limit=7.51875
2024-10-08 19:33:44,824 INFO [train.py:1154] Epoch 1, batch 150, loss[loss=1.134, simple_loss=0.7914, pruned_loss=0.9468, ctc_loss=1.069, over 4910.00 frames. ], tot_loss[loss=1.739, simple_loss=1.305, pruned_loss=1.348, ctc_loss=1.383, over 513480.80 frames. ], batch size: 19, lr: 2.93e-02,
2024-10-08 19:33:54,510 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=19.43 vs. limit=5.0125
2024-10-08 19:33:58,246 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=29.10 vs. limit=5.013333333333334
2024-10-08 19:33:59,236 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=72.47 vs. limit=7.52
2024-10-08 19:34:03,206 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=22.16 vs. limit=4.021333333333334
2024-10-08 19:34:08,397 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=56.666666666666664, ans=0.04982291666666667
2024-10-08 19:34:24,544 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=60.0, ans=0.2994
2024-10-08 19:34:25,719 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=60.0, ans=0.4971875
2024-10-08 19:34:27,234 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=17.63 vs. limit=5.015
2024-10-08 19:34:30,611 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=63.333333333333336, ans=0.49703125
2024-10-08 19:34:32,459 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten.whitening_limit, batch_count=63.333333333333336, ans=7.52375
2024-10-08 19:34:33,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=512, metric=45.19 vs. limit=7.52375
2024-10-08 19:34:36,612 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=63.333333333333336, ans=0.49703125
2024-10-08 19:34:41,433 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=37.32 vs. limit=5.031666666666666
2024-10-08 19:34:43,709 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.306e+01 5.034e+01 5.999e+01 7.542e+01 1.552e+02, threshold=1.200e+02, percent-clipped=0.0
2024-10-08 19:34:43,751 INFO [train.py:1154] Epoch 1, batch 200, loss[loss=1.138, simple_loss=0.7729, pruned_loss=0.9004, ctc_loss=1.162, over 4760.00 frames. ], tot_loss[loss=1.523, simple_loss=1.118, pruned_loss=1.198, ctc_loss=1.288, over 613838.26 frames. ], batch size: 45, lr: 3.15e-02,
2024-10-08 19:34:43,909 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.skip_rate, batch_count=66.66666666666667, ans=0.5
2024-10-08 19:34:45,638 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=37.78 vs. limit=7.525
2024-10-08 19:34:54,274 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=70.0, ans=0.09956250000000001
2024-10-08 19:34:57,635 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=70.0, ans=0.49671875
2024-10-08 19:34:59,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=162.78 vs. limit=7.52625
2024-10-08 19:35:01,370 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=173.56 vs. limit=7.52625
2024-10-08 19:35:04,151 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=63.97 vs. limit=7.52625
2024-10-08 19:35:06,490 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=49.29 vs. limit=7.555
2024-10-08 19:35:09,649 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=58.15 vs. limit=7.5275
2024-10-08 19:35:13,137 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=49.71 vs. limit=7.5275
2024-10-08 19:35:15,650 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=60.17 vs. limit=7.555
2024-10-08 19:35:19,417 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=136.39 vs. limit=7.52875
2024-10-08 19:35:23,921 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=30.69 vs. limit=7.52875
2024-10-08 19:35:26,743 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=90.22 vs. limit=5.0
2024-10-08 19:35:29,963 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=38.47 vs. limit=7.56
2024-10-08 19:35:33,227 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=27.09 vs. limit=5.02
2024-10-08 19:35:36,467 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=80.0, ans=0.197
2024-10-08 19:35:36,783 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=32.28 vs. limit=7.53
2024-10-08 19:35:37,631 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.min_positive, batch_count=80.0, ans=0.04975
2024-10-08 19:35:42,029 INFO [train.py:1154] Epoch 1, batch 250, loss[loss=1.145, simple_loss=0.7694, pruned_loss=0.8858, ctc_loss=1.178, over 4819.00 frames. ], tot_loss[loss=1.382, simple_loss=0.9942, pruned_loss=1.09, ctc_loss=1.227, over 692578.36 frames. ], batch size: 38, lr: 3.38e-02,
2024-10-08 19:35:44,972 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=69.20 vs. limit=7.53125
2024-10-08 19:35:45,599 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=5.995e-02
2024-10-08 19:35:47,250 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=168.84 vs. limit=7.53125
2024-10-08 19:35:51,351 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=83.33333333333333, ans=0.49609375
2024-10-08 19:35:53,793 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=86.66666666666667, ans=0.09805
2024-10-08 19:35:54,473 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=186.13 vs. limit=7.5325
2024-10-08 19:35:58,765 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=23.93 vs. limit=4.034666666666666
2024-10-08 19:36:04,720 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=90.0, ans=0.49578125
2024-10-08 19:36:08,873 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=33.20 vs. limit=7.53375
2024-10-08 19:36:14,602 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=41.81 vs. limit=4.036
2024-10-08 19:36:28,942 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=16.69 vs. limit=5.024166666666667
2024-10-08 19:36:30,626 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.min_positive, batch_count=96.66666666666667, ans=0.04969791666666667
2024-10-08 19:36:31,734 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=96.66666666666667, ans=5.060416666666667
2024-10-08 19:36:35,217 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=96.66666666666667, ans=0.49546875
2024-10-08 19:36:40,764 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.041e+01 6.488e+01 7.481e+01 9.030e+01 1.290e+02, threshold=1.496e+02, percent-clipped=6.0
2024-10-08 19:36:40,804 INFO [train.py:1154] Epoch 1, batch 300, loss[loss=1.155, simple_loss=0.7697, pruned_loss=0.8896, ctc_loss=1.171, over 4773.00 frames. ], tot_loss[loss=1.291, simple_loss=0.912, pruned_loss=1.016, ctc_loss=1.188, over 753007.62 frames. ], batch size: 32, lr: 3.60e-02,
2024-10-08 19:36:49,015 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=100.0, ans=0.8965000000000001
2024-10-08 19:36:53,771 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=103.33333333333333, ans=0.09767500000000001
2024-10-08 19:36:55,290 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=34.87 vs. limit=4.041333333333333
2024-10-08 19:36:58,030 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=188.94 vs. limit=7.53875
2024-10-08 19:37:03,510 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=68.48 vs. limit=7.58
2024-10-08 19:37:03,813 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=88.50 vs. limit=7.54
2024-10-08 19:37:04,445 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=106.66666666666667, ans=0.09933333333333334
2024-10-08 19:37:08,383 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=512, metric=41.03 vs. limit=7.54
2024-10-08 19:37:09,496 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=54.41 vs. limit=7.54
2024-10-08 19:37:21,208 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=69.63 vs. limit=5.055
2024-10-08 19:37:25,919 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=25.18 vs. limit=4.044
2024-10-08 19:37:26,951 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=38.48 vs. limit=7.5425
2024-10-08 19:37:29,053 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=113.33333333333333, ans=0.4946875
2024-10-08 19:37:32,684 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=73.76 vs. limit=7.585
2024-10-08 19:37:33,615 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=113.33333333333333, ans=0.4946875
2024-10-08 19:37:37,741 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=18.14 vs. limit=5.028333333333333
2024-10-08 19:37:39,403 INFO [train.py:1154] Epoch 1, batch 350, loss[loss=1.069, simple_loss=0.7069, pruned_loss=0.8249, ctc_loss=1.055, over 4883.00 frames. ], tot_loss[loss=1.226, simple_loss=0.8525, pruned_loss=0.9593, ctc_loss=1.16, over 800374.59 frames. ], batch size: 19, lr: 3.83e-02,
2024-10-08 19:37:39,497 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=116.66666666666667, ans=0.49453125
2024-10-08 19:37:43,476 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=27.87 vs. limit=4.046666666666667
2024-10-08 19:37:58,553 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=120.0, ans=0.494375
2024-10-08 19:38:01,202 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=82.51 vs. limit=7.545
2024-10-08 19:38:02,677 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=105.13 vs. limit=7.54625
2024-10-08 19:38:03,869 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=43.72 vs. limit=7.5925
2024-10-08 19:38:06,313 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=14.69 vs. limit=5.030833333333334
2024-10-08 19:38:08,198 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=19.67 vs. limit=4.049333333333333
2024-10-08 19:38:09,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=33.40 vs. limit=7.5925
2024-10-08 19:38:09,495 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys.whitening_limit, batch_count=123.33333333333333, ans=3.0185
2024-10-08 19:38:09,536 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=99.97 vs. limit=7.54625
2024-10-08 19:38:14,457 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=14.77 vs. limit=5.030833333333334
2024-10-08 19:38:15,362 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=126.66666666666667, ans=0.2987333333333333
2024-10-08 19:38:24,087 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=48.32 vs. limit=7.5475
2024-10-08 19:38:26,375 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=126.66666666666667, ans=0.8955666666666667
2024-10-08 19:38:37,441 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=43.91 vs. limit=7.54875
2024-10-08 19:38:40,597 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.496e+01 6.538e+01 7.149e+01 7.734e+01 1.069e+02, threshold=1.430e+02, percent-clipped=0.0
2024-10-08 19:38:40,637 INFO [train.py:1154] Epoch 1, batch 400, loss[loss=1.055, simple_loss=0.679, pruned_loss=0.8016, ctc_loss=1.094, over 4874.00 frames. ], tot_loss[loss=1.182, simple_loss=0.809, pruned_loss=0.9182, ctc_loss=1.143, over 837078.96 frames. ], batch size: 22, lr: 4.05e-02,
2024-10-08 19:38:44,199 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=133.33333333333334, ans=0.49375
2024-10-08 19:38:46,899 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=25.86 vs. limit=7.55
2024-10-08 19:38:47,916 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=49.63 vs. limit=7.6
2024-10-08 19:38:57,980 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=136.66666666666666, ans=0.49359375
2024-10-08 19:38:57,995 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=136.66666666666666, ans=0.49359375
2024-10-08 19:39:05,727 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=151.91 vs. limit=7.5525
2024-10-08 19:39:07,572 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=140.0, ans=0.4934375
2024-10-08 19:39:16,877 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=143.33333333333334, ans=0.096775
2024-10-08 19:39:17,080 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=31.07 vs. limit=7.55375
2024-10-08 19:39:22,129 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module1.whiten, num_groups=1, num_channels=192, metric=25.93 vs. limit=7.55375
2024-10-08 19:39:26,648 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=29.33 vs. limit=7.555
2024-10-08 19:39:27,626 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=24.42 vs. limit=7.555
2024-10-08 19:39:28,884 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=40.07 vs. limit=5.073333333333333
2024-10-08 19:39:29,631 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=9.95 vs. limit=4.058666666666666
2024-10-08 19:39:33,797 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=146.66666666666666, ans=0.8948666666666667
2024-10-08 19:39:35,240 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=33.20 vs. limit=5.073333333333333
2024-10-08 19:39:36,579 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=39.71 vs. limit=7.555
2024-10-08 19:39:38,242 INFO [train.py:1154] Epoch 1, batch 450, loss[loss=1.074, simple_loss=0.677, pruned_loss=0.8124, ctc_loss=1.134, over 4873.00 frames. ], tot_loss[loss=1.146, simple_loss=0.7742, pruned_loss=0.8833, ctc_loss=1.127, over 865679.54 frames. ], batch size: 23, lr: 4.28e-02,
2024-10-08 19:39:38,650 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=216.82 vs. limit=7.55625
2024-10-08 19:39:41,484 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=90.47 vs. limit=5.075
2024-10-08 19:39:42,717 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=20.43 vs. limit=7.6125
2024-10-08 19:39:44,870 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=25.48 vs. limit=5.075
2024-10-08 19:39:45,975 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=512, metric=27.48 vs. limit=7.55625
2024-10-08 19:39:50,382 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=153.33333333333334, ans=0.8946333333333334
2024-10-08 19:39:51,877 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=112.25 vs. limit=7.5575
2024-10-08 19:39:52,684 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=153.33333333333334, ans=0.19425
2024-10-08 19:39:54,008 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=70.24 vs. limit=7.5575
2024-10-08 19:40:01,348 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=20.46 vs. limit=5.039166666666667
2024-10-08 19:40:01,776 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=81.39 vs. limit=7.55875
2024-10-08 19:40:02,778 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=7.40 vs. limit=4.062666666666667
2024-10-08 19:40:03,304 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=156.66666666666666, ans=0.49265625
2024-10-08 19:40:06,969 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=156.66666666666666, ans=0.49265625
2024-10-08 19:40:08,475 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=12.57 vs. limit=5.039166666666667
2024-10-08 19:40:13,575 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=101.41 vs. limit=7.56
2024-10-08 19:40:15,116 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=160.0, ans=0.2984
2024-10-08 19:40:18,177 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=9.69 vs. limit=4.032
2024-10-08 19:40:20,735 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=160.0, ans=0.48
2024-10-08 19:40:30,336 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=163.33333333333334, ans=0.29836666666666667
2024-10-08 19:40:31,559 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=163.33333333333334, ans=0.19387500000000002
2024-10-08 19:40:34,593 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=24.24 vs. limit=5.081666666666667
2024-10-08 19:40:35,653 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=10.51 vs. limit=5.0408333333333335
2024-10-08 19:40:35,762 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=22.74 vs. limit=5.081666666666667
2024-10-08 19:40:37,154 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.105e+01 6.771e+01 7.378e+01 8.221e+01 1.016e+02, threshold=1.476e+02, percent-clipped=0.0
2024-10-08 19:40:37,194 INFO [train.py:1154] Epoch 1, batch 500, loss[loss=1.051, simple_loss=0.6695, pruned_loss=0.7739, ctc_loss=1.065, over 4806.00 frames. ], tot_loss[loss=1.126, simple_loss=0.7501, pruned_loss=0.8603, ctc_loss=1.118, over 888220.59 frames. ], batch size: 34, lr: 4.49e-02,
2024-10-08 19:40:37,692 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=12.05 vs. limit=5.041666666666667
2024-10-08 19:40:48,385 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=23.29 vs. limit=7.56375
2024-10-08 19:40:52,521 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=170.0, ans=0.49203125
2024-10-08 19:40:55,306 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=14.68 vs. limit=5.0425
2024-10-08 19:40:56,320 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=106.63 vs. limit=7.56375
2024-10-08 19:40:57,670 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.37 vs. limit=5.0425
2024-10-08 19:41:03,526 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=28.67 vs. limit=7.63
2024-10-08 19:41:06,819 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=173.33333333333334, ans=0.1935
2024-10-08 19:41:22,249 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=8.62 vs. limit=5.0441666666666665
2024-10-08 19:41:25,195 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=22.85 vs. limit=7.5675
2024-10-08 19:41:25,583 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.whiten.whitening_limit, batch_count=180.0, ans=4.072
2024-10-08 19:41:32,903 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=40.13 vs. limit=7.5675
2024-10-08 19:41:33,548 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=180.0, ans=0.4915625
2024-10-08 19:41:36,853 INFO [train.py:1154] Epoch 1, batch 550, loss[loss=1.105, simple_loss=0.6992, pruned_loss=0.7829, ctc_loss=1.146, over 4821.00 frames. ], tot_loss[loss=1.109, simple_loss=0.7304, pruned_loss=0.84, ctc_loss=1.109, over 905711.34 frames. ], batch size: 40, lr: 4.49e-02,
2024-10-08 19:41:36,943 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=183.33333333333334, ans=0.29816666666666664
2024-10-08 19:41:38,500 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=72.21 vs. limit=7.6375
2024-10-08 19:41:41,360 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=192, metric=57.58 vs. limit=7.56875
2024-10-08 19:41:48,237 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.70 vs. limit=5.093333333333334
2024-10-08 19:41:49,226 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=10.98 vs. limit=4.074666666666666
2024-10-08 19:41:51,201 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=186.66666666666666, ans=0.2395
2024-10-08 19:41:53,764 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=31.77 vs. limit=7.57
2024-10-08 19:41:57,210 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=59.38 vs. limit=5.093333333333334
2024-10-08 19:42:00,932 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.87 vs. limit=7.6425
2024-10-08 19:42:09,233 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=24.31 vs. limit=7.6425
2024-10-08 19:42:12,224 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=26.78 vs. limit=7.5725
2024-10-08 19:42:12,426 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=10.14 vs. limit=4.077333333333334
2024-10-08 19:42:13,207 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=193.33333333333334, ans=0.19275
2024-10-08 19:42:17,326 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=41.60 vs. limit=7.5725
2024-10-08 19:42:17,470 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=23.42 vs. limit=7.645
2024-10-08 19:42:30,389 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=23.57 vs. limit=7.6475
2024-10-08 19:42:35,772 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.429e+01 7.142e+01 7.575e+01 8.373e+01 1.339e+02, threshold=1.515e+02, percent-clipped=0.0
2024-10-08 19:42:35,812 INFO [train.py:1154] Epoch 1, batch 600, loss[loss=1.117, simple_loss=0.7006, pruned_loss=0.7882, ctc_loss=1.15, over 4834.00 frames. ], tot_loss[loss=1.097, simple_loss=0.7143, pruned_loss=0.8223, ctc_loss=1.101, over 919511.50 frames. ], batch size: 38, lr: 4.49e-02,
2024-10-08 19:42:36,557 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=25.69 vs. limit=7.65
2024-10-08 19:42:41,660 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=200.0, ans=0.490625
2024-10-08 19:42:57,426 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=11.08 vs. limit=7.57625
2024-10-08 19:42:59,150 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=206.66666666666666, ans=0.29793333333333333
2024-10-08 19:43:02,238 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=19.98 vs. limit=7.655
2024-10-08 19:43:07,599 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=206.66666666666666, ans=0.29793333333333333
2024-10-08 19:43:10,434 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=38.28 vs. limit=7.57875
2024-10-08 19:43:17,492 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=17.25 vs. limit=7.57875
2024-10-08 19:43:20,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=210.0, ans=0.19212500000000002
2024-10-08 19:43:21,739 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=213.33333333333334, ans=0.0952
2024-10-08 19:43:34,377 INFO [train.py:1154] Epoch 1, batch 650, loss[loss=1.054, simple_loss=0.6499, pruned_loss=0.7652, ctc_loss=1.046, over 4844.00 frames. ], tot_loss[loss=1.091, simple_loss=0.7029, pruned_loss=0.8102, ctc_loss=1.097, over 930350.51 frames. ], batch size: 21, lr: 4.49e-02,
2024-10-08 19:43:37,657 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=45.00 vs. limit=7.58125
2024-10-08 19:43:39,098 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=216.66666666666666, ans=0.8924166666666666
2024-10-08 19:43:59,014 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=105.52 vs. limit=7.58375
2024-10-08 19:44:01,541 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.83 vs. limit=5.111666666666666
2024-10-08 19:44:08,345 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=32.95 vs. limit=7.585
2024-10-08 19:44:08,571 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=57.03 vs. limit=7.585
2024-10-08 19:44:11,672 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=226.66666666666666, ans=0.29773333333333335
2024-10-08 19:44:20,064 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.37 vs. limit=5.0575
2024-10-08 19:44:23,900 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=7.98 vs. limit=4.092
2024-10-08 19:44:28,438 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=27.19 vs. limit=7.58625
2024-10-08 19:44:32,626 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.549e+01 7.782e+01 8.600e+01 9.639e+01 1.587e+02, threshold=1.720e+02, percent-clipped=1.0
2024-10-08 19:44:32,666 INFO [train.py:1154] Epoch 1, batch 700, loss[loss=1.015, simple_loss=0.6297, pruned_loss=0.7144, ctc_loss=0.996, over 4758.00 frames. ], tot_loss[loss=1.088, simple_loss=0.6942, pruned_loss=0.7994, ctc_loss=1.096, over 938206.07 frames. ], batch size: 19, lr: 4.49e-02,
2024-10-08 19:44:36,706 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=116.87 vs. limit=7.5875
2024-10-08 19:44:38,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=233.33333333333334, ans=0.04927083333333333
2024-10-08 19:44:38,591 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=233.33333333333334, ans=0.4890625
2024-10-08 19:44:48,213 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=14.04 vs. limit=7.58875
2024-10-08 19:44:50,893 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.81 vs. limit=5.059166666666667
2024-10-08 19:45:01,757 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=240.0, ans=0.0946
2024-10-08 19:45:06,328 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=243.33333333333334, ans=0.48859375
2024-10-08 19:45:11,154 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=98.59 vs. limit=7.59125
2024-10-08 19:45:20,341 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=246.66666666666666, ans=0.236125
2024-10-08 19:45:20,432 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=246.66666666666666, ans=0.20370000000000002
2024-10-08 19:45:20,592 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=8.87 vs. limit=5.0616666666666665
2024-10-08 19:45:20,724 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=32.00 vs. limit=7.685
2024-10-08 19:45:26,775 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=10.71 vs. limit=5.0616666666666665
2024-10-08 19:45:28,936 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=108.33 vs. limit=7.5925
2024-10-08 19:45:30,743 INFO [train.py:1154] Epoch 1, batch 750, loss[loss=0.9664, simple_loss=0.5803, pruned_loss=0.6653, ctc_loss=1.018, over 4880.00 frames. ], tot_loss[loss=1.081, simple_loss=0.683, pruned_loss=0.7853, ctc_loss=1.089, over 944974.19 frames. ], batch size: 22, lr: 4.49e-02,
2024-10-08 19:45:36,291 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=10.89 vs. limit=7.59375
2024-10-08 19:45:42,283 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=253.33333333333334, ans=0.488125
2024-10-08 19:45:45,593 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=7.06 vs. limit=5.0
2024-10-08 19:45:52,823 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=256.6666666666667, ans=0.2974333333333333
2024-10-08 19:46:02,650 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=11.26 vs. limit=7.59625
2024-10-08 19:46:09,395 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=82.92 vs. limit=7.5975
2024-10-08 19:46:15,916 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=263.3333333333333, ans=0.48765625
2024-10-08 19:46:17,321 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=8.01 vs. limit=5.065833333333333
2024-10-08 19:46:17,674 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=26.41 vs. limit=7.59875
2024-10-08 19:46:24,274 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.62 vs. limit=5.131666666666667
2024-10-08 19:46:27,900 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=266.6666666666667, ans=0.29733333333333334
2024-10-08 19:46:28,954 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.558e+01 8.346e+01 9.862e+01 1.248e+02 4.171e+02, threshold=1.972e+02, percent-clipped=10.0
2024-10-08 19:46:28,994 INFO [train.py:1154] Epoch 1, batch 800, loss[loss=1.02, simple_loss=0.6234, pruned_loss=0.7006, ctc_loss=0.9938, over 4855.00 frames. ], tot_loss[loss=1.079, simple_loss=0.6758, pruned_loss=0.7751, ctc_loss=1.086, over 949819.77 frames. ], batch size: 19, lr: 4.49e-02,
2024-10-08 19:46:33,208 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.93 vs. limit=7.7
2024-10-08 19:46:40,588 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=270.0, ans=0.48734375
2024-10-08 19:46:44,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=91.46 vs. limit=7.60125
2024-10-08 19:46:46,565 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=104.52 vs. limit=7.60125
2024-10-08 19:46:51,641 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=5.46 vs. limit=4.054666666666667
2024-10-08 19:46:52,440 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=13.17 vs. limit=7.6025
2024-10-08 19:46:53,462 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=13.13 vs. limit=7.6025
2024-10-08 19:46:59,952 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=273.3333333333333, ans=0.4658333333333333
2024-10-08 19:47:03,487 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=276.6666666666667, ans=0.29723333333333335
2024-10-08 19:47:06,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=33.93 vs. limit=7.7075
2024-10-08 19:47:13,761 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=31.79 vs. limit=7.7075
2024-10-08 19:47:16,685 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=280.0, ans=0.29719999999999996
2024-10-08 19:47:18,592 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=5.96 vs. limit=5.0
2024-10-08 19:47:19,326 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=12.99 vs. limit=7.605
2024-10-08 19:47:22,724 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=14.26 vs. limit=5.14
2024-10-08 19:47:28,016 INFO [train.py:1154] Epoch 1, batch 850, loss[loss=0.9904, simple_loss=0.5907, pruned_loss=0.6757, ctc_loss=0.9953, over 4793.00 frames. ], tot_loss[loss=1.08, simple_loss=0.6707, pruned_loss=0.7675, ctc_loss=1.084, over 954090.52 frames. ], batch size: 29, lr: 4.49e-02,
2024-10-08 19:47:32,972 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=11.26 vs. limit=7.60625
2024-10-08 19:47:35,097 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=283.3333333333333, ans=0.46458333333333335
2024-10-08 19:47:43,692 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=8.74 vs. limit=7.6075
2024-10-08 19:47:52,932 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=9.35 vs. limit=7.60875
2024-10-08 19:47:54,884 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=35.02 vs. limit=7.7175
2024-10-08 19:47:59,406 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=290.0, ans=0.48640625
2024-10-08 19:47:59,890 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1.whitening_limit, batch_count=290.0, ans=5.0725
2024-10-08 19:48:05,118 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer2.prob, batch_count=293.3333333333333, ans=0.48625
2024-10-08 19:48:09,987 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=10.25 vs. limit=7.61
2024-10-08 19:48:10,864 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.0.self_attn_weights, loss-sum=1.618e+01
2024-10-08 19:48:17,237 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=11.40 vs. limit=7.7225
2024-10-08 19:48:19,139 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=11.61 vs. limit=7.61125
2024-10-08 19:48:26,080 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.041e+01 1.114e+02 1.578e+02 2.540e+02 1.821e+03, threshold=3.155e+02, percent-clipped=35.0
2024-10-08 19:48:26,120 INFO [train.py:1154] Epoch 1, batch 900, loss[loss=1.053, simple_loss=0.6268, pruned_loss=0.7183, ctc_loss=1.019, over 4854.00 frames. ], tot_loss[loss=1.082, simple_loss=0.6664, pruned_loss=0.7619, ctc_loss=1.082, over 956897.83 frames. ], batch size: 19, lr: 4.48e-02,
2024-10-08 19:48:29,772 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=94.87 vs. limit=7.6125
2024-10-08 19:48:34,338 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=35.29 vs. limit=7.6125
2024-10-08 19:48:36,692 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=18.15 vs. limit=7.61375
2024-10-08 19:48:37,612 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=303.3333333333333, ans=0.18862500000000001
2024-10-08 19:48:42,390 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=13.03 vs. limit=7.61375
2024-10-08 19:48:46,142 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=7.77 vs. limit=7.61375
2024-10-08 19:48:54,840 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=306.6666666666667, ans=0.485625
2024-10-08 19:49:01,263 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=33.84 vs. limit=7.61625
2024-10-08 19:49:04,142 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=310.0, ans=0.46125
2024-10-08 19:49:04,700 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=94.33 vs. limit=7.61625
2024-10-08 19:49:07,141 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.14 vs. limit=7.7325
2024-10-08 19:49:08,777 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.prob, batch_count=310.0, ans=0.48546875
2024-10-08 19:49:11,359 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=101.77 vs. limit=7.6175
2024-10-08 19:49:17,522 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=3.92 vs. limit=4.125333333333334
2024-10-08 19:49:23,307 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=29.01 vs. limit=7.7375
2024-10-08 19:49:23,943 INFO [train.py:1154] Epoch 1, batch 950, loss[loss=1.151, simple_loss=0.6762, pruned_loss=0.7872, ctc_loss=1.101, over 4817.00 frames. ], tot_loss[loss=1.087, simple_loss=0.6634, pruned_loss=0.7584, ctc_loss=1.082, over 958663.64 frames. ], batch size: 19, lr: 4.48e-02,
2024-10-08 19:49:29,078 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.40 vs. limit=5.079166666666667
2024-10-08 19:49:44,615 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=320.0, ans=0.485
2024-10-08 19:49:44,987 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.96 vs. limit=4.128
2024-10-08 19:49:55,339 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=8.43 vs. limit=7.62125
2024-10-08 19:49:56,330 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=31.19 vs. limit=7.7425
2024-10-08 19:50:02,011 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.72 vs. limit=5.163333333333333
2024-10-08 19:50:02,138 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=25.00 vs. limit=7.745
2024-10-08 19:50:09,588 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=3.92 vs. limit=5.165
2024-10-08 19:50:11,068 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.attention_skip_rate, batch_count=330.0, ans=0.18762500000000001
2024-10-08 19:50:21,338 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.937e+01 1.286e+02 1.993e+02 2.806e+02 1.237e+03, threshold=3.986e+02, percent-clipped=20.0
2024-10-08 19:50:21,379 INFO [train.py:1154] Epoch 1, batch 1000, loss[loss=1.035, simple_loss=0.5992, pruned_loss=0.6896, ctc_loss=1.029, over 4940.00 frames. ], tot_loss[loss=1.097, simple_loss=0.664, pruned_loss=0.7589, ctc_loss=1.083, over 960466.08 frames. ], batch size: 20, lr: 4.48e-02,
2024-10-08 19:50:21,945 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=31.01 vs. limit=7.75
2024-10-08 19:50:22,973 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=31.33 vs. limit=7.75
2024-10-08 19:50:23,856 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=333.3333333333333, ans=0.1875
2024-10-08 19:50:26,511 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=27.78 vs. limit=7.75
2024-10-08 19:50:36,942 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=50.93 vs. limit=7.62625
2024-10-08 19:50:36,983 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=53.10 vs. limit=7.62625
2024-10-08 19:50:46,246 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.60 vs. limit=5.085
2024-10-08 19:50:49,799 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=13.34 vs. limit=7.6275
2024-10-08 19:50:55,635 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=8.71 vs. limit=7.7575
2024-10-08 19:51:04,654 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=45.77 vs. limit=7.62875
2024-10-08 19:51:05,297 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.hidden_balancer.prob, batch_count=343.3333333333333, ans=0.48390625
2024-10-08 19:51:10,126 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=40.57 vs. limit=7.76
2024-10-08 19:51:15,706 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=346.6666666666667, ans=0.09783333333333334
2024-10-08 19:51:19,013 INFO [train.py:1154] Epoch 1, batch 1050, loss[loss=1.087, simple_loss=0.6341, pruned_loss=0.715, ctc_loss=1.048, over 4792.00 frames. ], tot_loss[loss=1.1, simple_loss=0.66, pruned_loss=0.7544, ctc_loss=1.078, over 962579.73 frames. ], batch size: 25, lr: 4.48e-02,
2024-10-08 19:51:22,800 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=41.51 vs. limit=7.7625
2024-10-08 19:51:27,669 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.54 vs. limit=7.7625
2024-10-08 19:51:29,556 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=353.3333333333333, ans=0.8876333333333334
2024-10-08 19:51:34,261 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.78 vs. limit=5.088333333333333
2024-10-08 19:51:42,300 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=356.6666666666667, ans=0.48328125
2024-10-08 19:51:44,656 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=356.6666666666667, ans=0.2964333333333333
2024-10-08 19:51:48,482 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=39.14 vs. limit=7.7675
2024-10-08 19:51:50,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=9.05 vs. limit=7.7675
2024-10-08 19:51:51,708 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=70.25 vs. limit=7.63375
2024-10-08 19:51:52,544 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=360.0, ans=0.8874000000000001
2024-10-08 19:51:52,873 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=35.04 vs. limit=7.635
2024-10-08 19:51:53,211 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=11.18 vs. limit=7.77
2024-10-08 19:51:57,909 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.53 vs. limit=5.09
2024-10-08 19:51:58,632 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.27 vs. limit=5.09
2024-10-08 19:52:01,365 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.12 vs. limit=7.635
2024-10-08 19:52:04,433 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten.whitening_limit, batch_count=363.3333333333333, ans=7.63625
2024-10-08 19:52:16,682 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.763e+01 1.331e+02 2.012e+02 3.242e+02 1.642e+03, threshold=4.023e+02, percent-clipped=14.0
2024-10-08 19:52:16,722 INFO [train.py:1154] Epoch 1, batch 1100, loss[loss=1.076, simple_loss=0.6184, pruned_loss=0.7207, ctc_loss=0.9918, over 4863.00 frames. ], tot_loss[loss=1.105, simple_loss=0.6578, pruned_loss=0.7526, ctc_loss=1.075, over 964036.58 frames. ], batch size: 20, lr: 4.48e-02,
2024-10-08 19:52:18,520 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.84 vs. limit=7.6375
2024-10-08 19:52:22,448 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=366.6666666666667, ans=0.18625
2024-10-08 19:52:26,968 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=370.0, ans=0.186125
2024-10-08 19:52:29,614 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=58.11 vs. limit=7.63875
2024-10-08 19:52:33,721 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=370.0, ans=0.091675
2024-10-08 19:52:38,597 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=373.3333333333333, ans=0.229
2024-10-08 19:52:43,797 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.67 vs. limit=7.78
2024-10-08 19:52:48,836 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=33.68 vs. limit=7.78
2024-10-08 19:53:07,476 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=380.0, ans=0.4821875
2024-10-08 19:53:12,496 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=104.40 vs. limit=7.64375
2024-10-08 19:53:13,091 INFO [train.py:1154] Epoch 1, batch 1150, loss[loss=1.185, simple_loss=0.6636, pruned_loss=0.7921, ctc_loss=1.116, over 4850.00 frames. ], tot_loss[loss=1.112, simple_loss=0.657, pruned_loss=0.7517, ctc_loss=1.074, over 964294.11 frames. ], batch size: 20, lr: 4.47e-02,
2024-10-08 19:53:30,196 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=34.44 vs. limit=7.79
2024-10-08 19:53:31,613 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.69 vs. limit=7.79
2024-10-08 19:53:37,582 WARNING [optim.py:503] Scaling gradients by 0.05643734335899353, model_norm_threshold=402.3010559082031
2024-10-08 19:53:37,720 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.036e+07, grad_sumsq=7.978e+08, orig_rms_sq=2.553e-02
2024-10-08 19:53:37,841 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=390.0, ans=0.48171875
2024-10-08 19:53:38,183 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.39 vs. limit=5.195
2024-10-08 19:53:39,699 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=19.90 vs. limit=7.64625
2024-10-08 19:53:43,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=390.0, ans=0.185375
2024-10-08 19:53:46,860 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=393.3333333333333, ans=0.5
2024-10-08 19:53:47,244 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=33.52 vs. limit=7.795
2024-10-08 19:53:49,572 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=59.27 vs. limit=7.6475
2024-10-08 19:53:51,718 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.07 vs. limit=5.196666666666666
2024-10-08 19:53:54,664 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.dropout.p, batch_count=393.3333333333333, ans=0.29606666666666664
2024-10-08 19:53:56,385 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=2.75 vs. limit=3.059
2024-10-08 19:53:57,230 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=396.6666666666667, ans=0.185125
2024-10-08 19:54:00,904 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=60.93 vs. limit=7.64875
2024-10-08 19:54:09,638 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.105e+01 1.929e+02 2.709e+02 4.724e+02 7.128e+03, threshold=5.418e+02, percent-clipped=30.0
2024-10-08 19:54:09,678 INFO [train.py:1154] Epoch 1, batch 1200, loss[loss=1.153, simple_loss=0.639, pruned_loss=0.7583, ctc_loss=1.101, over 4810.00 frames. ], tot_loss[loss=1.119, simple_loss=0.6558, pruned_loss=0.7507, ctc_loss=1.072, over 964164.94 frames. ], batch size: 25, lr: 4.47e-02,
2024-10-08 19:54:12,096 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=400.0, ans=0.886
2024-10-08 19:54:14,384 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.min_positive, batch_count=400.0, ans=0.0975
2024-10-08 19:54:14,784 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=57.52 vs. limit=7.65
2024-10-08 19:54:17,233 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys.whitening_limit, batch_count=400.0, ans=3.06
2024-10-08 19:54:23,128 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.45 vs. limit=5.100833333333333
2024-10-08 19:54:32,320 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=9.36 vs. limit=7.805
2024-10-08 19:54:34,383 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.73 vs. limit=4.1626666666666665
2024-10-08 19:54:37,468 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=406.6666666666667, ans=0.4809375
2024-10-08 19:54:38,490 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=406.6666666666667, ans=0.4809375
2024-10-08 19:54:38,504 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=406.6666666666667, ans=0.18475
2024-10-08 19:54:42,199 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=5.65 vs. limit=5.203333333333333
2024-10-08 19:54:46,305 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=410.0, ans=0.48078125
2024-10-08 19:54:46,332 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=410.0, ans=0.48078125
2024-10-08 19:54:46,345 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=410.0, ans=0.48078125
2024-10-08 19:55:03,463 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=7.97 vs. limit=7.655
2024-10-08 19:55:04,445 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=413.3333333333333, ans=0.44833333333333336
2024-10-08 19:55:04,812 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=30.28 vs. limit=7.81
2024-10-08 19:55:06,622 INFO [train.py:1154] Epoch 1, batch 1250, loss[loss=1.11, simple_loss=0.6265, pruned_loss=0.7158, ctc_loss=1.026, over 4754.00 frames. ], tot_loss[loss=1.128, simple_loss=0.6548, pruned_loss=0.7516, ctc_loss=1.072, over 964201.34 frames. ], batch size: 32, lr: 4.47e-02,
2024-10-08 19:55:08,069 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=49.56 vs. limit=7.65625
2024-10-08 19:55:12,725 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=2.565e+01
2024-10-08 19:55:12,926 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=12.64 vs. limit=7.65625
2024-10-08 19:55:15,546 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=59.45 vs. limit=7.65625
2024-10-08 19:55:17,939 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.87 vs. limit=5.208333333333333
2024-10-08 19:55:19,914 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=9.15 vs. limit=7.6575
2024-10-08 19:55:22,333 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=24.09 vs. limit=7.6575
2024-10-08 19:55:24,347 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1.whitening_limit, batch_count=420.0, ans=5.105
2024-10-08 19:55:36,836 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=13.47 vs. limit=7.65875
2024-10-08 19:55:37,864 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.10 vs. limit=5.105833333333333
2024-10-08 19:55:38,936 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=38.68 vs. limit=7.65875
2024-10-08 19:55:44,214 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=426.6666666666667, ans=0.44666666666666666
2024-10-08 19:55:44,689 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=29.21 vs. limit=7.66
2024-10-08 19:55:45,470 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=426.6666666666667, ans=0.48
2024-10-08 19:55:46,589 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=426.6666666666667, ans=0.04866666666666667
2024-10-08 19:55:47,782 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=426.6666666666667, ans=0.2064
2024-10-08 19:55:51,274 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=426.6666666666667, ans=0.29573333333333335
2024-10-08 19:55:51,812 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.68 vs. limit=7.66
2024-10-08 19:55:52,744 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=31.91 vs. limit=7.8225
2024-10-08 19:55:58,036 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=430.0, ans=0.183875
2024-10-08 19:56:02,661 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=430.0, ans=0.29569999999999996
2024-10-08 19:56:04,748 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.611e+01 1.800e+02 2.729e+02 4.723e+02 2.444e+03, threshold=5.457e+02, percent-clipped=21.0
2024-10-08 19:56:04,790 INFO [train.py:1154] Epoch 1, batch 1300, loss[loss=1.216, simple_loss=0.691, pruned_loss=0.7714, ctc_loss=1.107, over 4845.00 frames. ], tot_loss[loss=1.132, simple_loss=0.6519, pruned_loss=0.7492, ctc_loss=1.068, over 965501.64 frames. ], batch size: 43, lr: 4.47e-02,
2024-10-08 19:56:06,434 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=24.04 vs. limit=7.825
2024-10-08 19:56:09,315 WARNING [optim.py:503] Scaling gradients by 0.043562378734350204, model_norm_threshold=545.709716796875
2024-10-08 19:56:09,452 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.472e+07, grad_sumsq=8.284e+08, orig_rms_sq=4.192e-02
2024-10-08 19:56:18,961 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=436.6666666666667, ans=0.090175
2024-10-08 19:56:24,798 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=436.6666666666667, ans=0.8847166666666667
2024-10-08 19:56:25,836 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=436.6666666666667, ans=0.47953125
2024-10-08 19:56:29,719 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=4.50 vs. limit=7.665
2024-10-08 19:56:41,018 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.04 vs. limit=5.110833333333333
2024-10-08 19:56:41,041 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=66.43 vs. limit=7.66625
2024-10-08 19:56:45,669 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.92 vs. limit=5.110833333333333
2024-10-08 19:56:48,589 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=443.3333333333333, ans=0.47921875
2024-10-08 19:56:56,764 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=85.21 vs. limit=7.6675
2024-10-08 19:56:59,308 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.45 vs. limit=7.6675
2024-10-08 19:57:01,798 INFO [train.py:1154] Epoch 1, batch 1350, loss[loss=1.192, simple_loss=0.6626, pruned_loss=0.771, ctc_loss=1.038, over 4829.00 frames. ], tot_loss[loss=1.138, simple_loss=0.6495, pruned_loss=0.7483, ctc_loss=1.064, over 966236.50 frames. ], batch size: 21, lr: 4.46e-02,
2024-10-08 19:57:06,498 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=450.0, ans=0.048593750000000005
2024-10-08 19:57:09,105 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=70.67 vs. limit=7.66875
2024-10-08 19:57:10,021 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=450.0, ans=0.09718750000000001
2024-10-08 19:57:14,927 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.36 vs. limit=5.226666666666667
2024-10-08 19:57:23,881 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=70.35 vs. limit=7.67125
2024-10-08 19:57:24,720 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=456.6666666666667, ans=0.47859375
2024-10-08 19:57:30,635 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=456.6666666666667, ans=0.182875
2024-10-08 19:57:38,475 WARNING [optim.py:503] Scaling gradients by 0.09369029849767685, model_norm_threshold=545.709716796875
2024-10-08 19:57:38,613 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.109e+06, grad_sumsq=6.637e+09, orig_rms_sq=9.205e-04
2024-10-08 19:57:44,729 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=40.15 vs. limit=7.845
2024-10-08 19:57:47,199 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=22.01 vs. limit=7.8475
2024-10-08 19:57:52,194 WARNING [optim.py:503] Scaling gradients by 0.09394394606351852, model_norm_threshold=545.709716796875
2024-10-08 19:57:52,332 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.65, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.186e+07, grad_sumsq=1.003e+09, orig_rms_sq=2.179e-02
2024-10-08 19:57:56,098 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=10.76 vs. limit=7.67375
2024-10-08 19:57:56,207 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=51.95 vs. limit=7.8475
2024-10-08 19:57:59,313 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.295e+01 1.815e+02 3.769e+02 8.574e+02 1.253e+04, threshold=7.538e+02, percent-clipped=36.0
2024-10-08 19:57:59,353 INFO [train.py:1154] Epoch 1, batch 1400, loss[loss=1.172, simple_loss=0.6389, pruned_loss=0.7461, ctc_loss=1.06, over 4940.00 frames. ], tot_loss[loss=1.148, simple_loss=0.65, pruned_loss=0.7499, ctc_loss=1.063, over 966729.80 frames. ], batch size: 19, lr: 4.46e-02,
2024-10-08 19:58:00,952 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=35.05 vs. limit=7.675
2024-10-08 19:58:02,996 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=466.6666666666667, ans=0.04854166666666667
2024-10-08 19:58:09,573 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.min_positive, batch_count=470.0, ans=0.09706250000000001
2024-10-08 19:58:25,658 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=9.28 vs. limit=7.6775
2024-10-08 19:58:25,865 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.48 vs. limit=4.189333333333333
2024-10-08 19:58:26,626 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=473.3333333333333, ans=0.08935000000000001
2024-10-08 19:58:40,956 WARNING [optim.py:503] Scaling gradients by 0.012980664148926735, model_norm_threshold=753.7721557617188
2024-10-08 19:58:41,095 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.449e+09, grad_sumsq=6.888e+10, orig_rms_sq=2.104e-02
2024-10-08 19:58:44,137 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=16.37 vs. limit=7.86
2024-10-08 19:58:45,998 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.max_abs, batch_count=480.0, ans=5.3
2024-10-08 19:58:48,713 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=43.75 vs. limit=7.86
2024-10-08 19:58:56,161 WARNING [optim.py:503] Scaling gradients by 0.04706341028213501, model_norm_threshold=753.7721557617188
2024-10-08 19:58:56,300 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.89, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.278e+08, grad_sumsq=1.092e+10, orig_rms_sq=2.085e-02
2024-10-08 19:58:56,346 INFO [train.py:1154] Epoch 1, batch 1450, loss[loss=1.164, simple_loss=0.6258, pruned_loss=0.7358, ctc_loss=1.055, over 4798.00 frames. ], tot_loss[loss=1.156, simple_loss=0.6496, pruned_loss=0.7495, ctc_loss=1.061, over 966641.08 frames. ], batch size: 34, lr: 4.46e-02,
2024-10-08 19:58:58,586 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=483.3333333333333, ans=0.20725000000000002
2024-10-08 19:59:03,610 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.55 vs. limit=5.120833333333334
2024-10-08 19:59:03,929 WARNING [optim.py:503] Scaling gradients by 0.03631134331226349, model_norm_threshold=753.7721557617188
2024-10-08 19:59:04,068 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.133e+08, grad_sumsq=5.494e+09, orig_rms_sq=2.062e-02
2024-10-08 19:59:06,665 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=486.6666666666667, ans=0.09695833333333334
2024-10-08 19:59:14,865 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.73 vs. limit=5.243333333333333
2024-10-08 19:59:26,040 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.26 vs. limit=5.245
2024-10-08 19:59:34,009 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.85 vs. limit=5.123333333333333
2024-10-08 19:59:34,165 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.42 vs. limit=5.246666666666667
2024-10-08 19:59:37,292 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=25.57 vs. limit=7.87
2024-10-08 19:59:51,993 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=500.0, ans=0.221875
2024-10-08 19:59:52,873 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.317e+01 3.912e+02 7.012e+02 1.245e+03 5.807e+04, threshold=1.402e+03, percent-clipped=45.0
2024-10-08 19:59:52,913 INFO [train.py:1154] Epoch 1, batch 1500, loss[loss=1.146, simple_loss=0.6142, pruned_loss=0.714, ctc_loss=1.045, over 4743.00 frames. ], tot_loss[loss=1.168, simple_loss=0.6506, pruned_loss=0.7528, ctc_loss=1.063, over 966360.75 frames. ], batch size: 26, lr: 4.46e-02,
2024-10-08 20:00:04,626 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=34.78 vs. limit=7.68875
2024-10-08 20:00:05,774 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=34.19 vs. limit=7.68875
2024-10-08 20:00:12,551 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=15.64 vs. limit=7.8775
2024-10-08 20:00:27,235 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten.whitening_limit, batch_count=510.0, ans=7.8825
2024-10-08 20:00:39,190 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=513.3333333333334, ans=0.221125
2024-10-08 20:00:39,800 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.17 vs. limit=7.885
2024-10-08 20:00:48,440 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=4.92 vs. limit=4.206666666666667
2024-10-08 20:00:48,963 INFO [train.py:1154] Epoch 1, batch 1550, loss[loss=1.168, simple_loss=0.631, pruned_loss=0.7228, ctc_loss=1.023, over 4857.00 frames. ], tot_loss[loss=1.176, simple_loss=0.6502, pruned_loss=0.7528, ctc_loss=1.06, over 966180.64 frames. ], batch size: 31, lr: 4.45e-02,
2024-10-08 20:00:58,285 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=516.6666666666666, ans=0.24483333333333335
2024-10-08 20:01:09,406 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=15.62 vs. limit=7.89
2024-10-08 20:01:12,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=9.31 vs. limit=7.69625
2024-10-08 20:01:24,190 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.46 vs. limit=5.131666666666667
2024-10-08 20:01:29,782 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=72.03 vs. limit=7.6975
2024-10-08 20:01:33,853 WARNING [optim.py:503] Scaling gradients by 0.055217303335666656, model_norm_threshold=1402.4974365234375
2024-10-08 20:01:33,995 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.042e+08, grad_sumsq=1.005e+10, orig_rms_sq=2.032e-02
2024-10-08 20:01:35,528 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=13.66 vs. limit=4.212
2024-10-08 20:01:39,289 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=44.28 vs. limit=7.69875
2024-10-08 20:01:42,455 WARNING [optim.py:503] Scaling gradients by 0.020914334803819656, model_norm_threshold=1402.4974365234375
2024-10-08 20:01:42,596 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.143e+09, grad_sumsq=1.047e+11, orig_rms_sq=2.047e-02
2024-10-08 20:01:47,356 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.125e+02 2.829e+02 4.444e+02 1.117e+03 6.706e+04, threshold=8.888e+02, percent-clipped=21.0
2024-10-08 20:01:47,396 INFO [train.py:1154] Epoch 1, batch 1600, loss[loss=1.28, simple_loss=0.6825, pruned_loss=0.7981, ctc_loss=1.08, over 4810.00 frames. ], tot_loss[loss=1.182, simple_loss=0.6488, pruned_loss=0.752, ctc_loss=1.057, over 966378.52 frames. ], batch size: 25, lr: 4.45e-02,
2024-10-08 20:01:51,049 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=533.3333333333334, ans=0.08800000000000001
2024-10-08 20:01:51,264 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=10.33 vs. limit=7.7
2024-10-08 20:01:51,652 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.15 vs. limit=7.9
2024-10-08 20:01:58,182 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=10.66 vs. limit=7.70125
2024-10-08 20:02:11,772 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=540.0, ans=0.4746875
2024-10-08 20:02:12,822 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_skip_rate, batch_count=540.0, ans=0.17975000000000002
2024-10-08 20:02:12,846 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=540.0, ans=0.7554
2024-10-08 20:02:17,373 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=540.0, ans=0.4746875
2024-10-08 20:02:19,163 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.31 vs. limit=7.905
2024-10-08 20:02:19,227 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn2.whiten.whitening_limit, batch_count=540.0, ans=7.905
2024-10-08 20:02:20,485 WARNING [optim.py:503] Scaling gradients by 0.06609396636486053, model_norm_threshold=888.773681640625
2024-10-08 20:02:20,624 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.56, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.019e+08, grad_sumsq=5.063e+09, orig_rms_sq=2.013e-02
2024-10-08 20:02:25,454 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=543.3333333333334, ans=0.087775
2024-10-08 20:02:31,260 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=543.3333333333334, ans=0.09660416666666667
2024-10-08 20:02:32,661 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=3.56 vs. limit=4.218666666666667
2024-10-08 20:02:32,686 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.60 vs. limit=7.91
2024-10-08 20:02:34,177 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=7.73 vs. limit=7.705
2024-10-08 20:02:37,070 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=546.6666666666666, ans=0.474375
2024-10-08 20:02:38,699 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=37.41 vs. limit=7.91
2024-10-08 20:02:44,402 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=546.6666666666666, ans=0.21925
2024-10-08 20:02:45,150 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=8.77 vs. limit=7.91
2024-10-08 20:02:45,361 WARNING [optim.py:503] Scaling gradients by 0.09729386866092682, model_norm_threshold=888.773681640625
2024-10-08 20:02:45,498 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.73, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.127e+07, grad_sumsq=3.055e+09, orig_rms_sq=2.006e-02
2024-10-08 20:02:46,160 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.07 vs. limit=5.275
2024-10-08 20:02:46,735 INFO [train.py:1154] Epoch 1, batch 1650, loss[loss=1.294, simple_loss=0.6873, pruned_loss=0.7891, ctc_loss=1.126, over 4792.00 frames. ], tot_loss[loss=1.189, simple_loss=0.648, pruned_loss=0.7507, ctc_loss=1.055, over 966732.12 frames. ], batch size: 29, lr: 4.45e-02,
2024-10-08 20:02:46,812 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=550.0, ans=0.08762500000000001
2024-10-08 20:02:53,575 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=550.0, ans=0.88075
2024-10-08 20:02:59,989 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.07 vs. limit=5.138333333333334
2024-10-08 20:03:09,608 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=556.6666666666666, ans=0.2944333333333333
2024-10-08 20:03:10,794 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=556.6666666666666, ans=0.08747500000000001
2024-10-08 20:03:12,528 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=41.25 vs. limit=7.70875
2024-10-08 20:03:15,051 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.97 vs. limit=7.9175
2024-10-08 20:03:15,917 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=29.34 vs. limit=7.70875
2024-10-08 20:03:19,099 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=40.66 vs. limit=5.278333333333333
2024-10-08 20:03:23,745 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=30.32 vs. limit=7.71
2024-10-08 20:03:27,111 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.47 vs. limit=5.28
2024-10-08 20:03:29,391 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=21.08 vs. limit=7.92
2024-10-08 20:03:34,421 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=2.89 vs. limit=3.0845
2024-10-08 20:03:35,250 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.16 vs. limit=7.9225
2024-10-08 20:03:37,515 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.03 vs. limit=5.140833333333333
2024-10-08 20:03:38,142 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=563.3333333333334, ans=5.352083333333334
2024-10-08 20:03:39,036 WARNING [optim.py:503] Scaling gradients by 0.05017505958676338, model_norm_threshold=888.773681640625
2024-10-08 20:03:39,173 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.598e+07, grad_sumsq=1.032e+09, orig_rms_sq=5.422e-02
2024-10-08 20:03:43,701 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.143e+02 4.092e+02 7.735e+02 1.960e+03 1.771e+04, threshold=1.547e+03, percent-clipped=47.0
2024-10-08 20:03:43,742 INFO [train.py:1154] Epoch 1, batch 1700, loss[loss=1.207, simple_loss=0.6179, pruned_loss=0.751, ctc_loss=1.008, over 4940.00 frames. ], tot_loss[loss=1.201, simple_loss=0.6482, pruned_loss=0.7533, ctc_loss=1.055, over 966952.97 frames. ], batch size: 19, lr: 4.44e-02,
2024-10-08 20:03:47,672 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.04 vs. limit=3.085
2024-10-08 20:04:05,293 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=23.53 vs. limit=7.71375
2024-10-08 20:04:05,920 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=573.3333333333334, ans=0.473125
2024-10-08 20:04:11,818 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=52.15 vs. limit=7.715
2024-10-08 20:04:12,730 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=573.3333333333334, ans=0.473125
2024-10-08 20:04:16,643 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=27.60 vs. limit=7.93
2024-10-08 20:04:17,390 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=576.6666666666666, ans=0.47296875
2024-10-08 20:04:18,344 WARNING [optim.py:503] Scaling gradients by 0.027370495721697807, model_norm_threshold=1547.01513671875
2024-10-08 20:04:18,482 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.190e+09, grad_sumsq=6.601e+10, orig_rms_sq=1.803e-02
2024-10-08 20:04:18,698 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=576.6666666666666, ans=0.47296875
2024-10-08 20:04:21,292 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.20 vs. limit=5.288333333333333
2024-10-08 20:04:25,340 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=576.6666666666666, ans=0.087025
2024-10-08 20:04:25,354 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=576.6666666666666, ans=0.47296875
2024-10-08 20:04:29,455 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.36 vs. limit=5.145
2024-10-08 20:04:32,151 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=580.0, ans=0.8797
2024-10-08 20:04:41,279 INFO [train.py:1154] Epoch 1, batch 1750, loss[loss=1.234, simple_loss=0.6385, pruned_loss=0.7717, ctc_loss=0.9506, over 4959.00 frames. ], tot_loss[loss=1.208, simple_loss=0.6466, pruned_loss=0.7535, ctc_loss=1.051, over 967116.99 frames. ], batch size: 19, lr: 4.44e-02,
2024-10-08 20:04:46,335 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=4.79 vs. limit=4.233333333333333
2024-10-08 20:04:46,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten.whitening_limit, batch_count=583.3333333333334, ans=7.71875
2024-10-08 20:04:49,565 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=37.69 vs. limit=7.71875
2024-10-08 20:04:59,504 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward3.hidden_balancer.prob, batch_count=586.6666666666666, ans=0.4725
2024-10-08 20:05:01,062 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=7.30 vs. limit=7.72
2024-10-08 20:05:04,105 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=590.0, ans=0.2941
2024-10-08 20:05:08,235 WARNING [optim.py:503] Scaling gradients by 0.040530234575271606, model_norm_threshold=1547.01513671875
2024-10-08 20:05:08,371 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.82, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.200e+09, grad_sumsq=5.891e+10, orig_rms_sq=2.037e-02
2024-10-08 20:05:09,972 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=31.02 vs. limit=7.9425
2024-10-08 20:05:16,840 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.66 vs. limit=7.7225
2024-10-08 20:05:17,306 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=593.3333333333334, ans=5.370833333333334
2024-10-08 20:05:24,698 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=43.00 vs. limit=7.7225
2024-10-08 20:05:27,061 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=23.46 vs. limit=7.9475
2024-10-08 20:05:30,566 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=38.00 vs. limit=7.72375
2024-10-08 20:05:32,446 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=596.6666666666666, ans=0.7559666666666667
2024-10-08 20:05:35,856 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=596.6666666666666, ans=0.177625
2024-10-08 20:05:37,404 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=600.0, ans=0.294
2024-10-08 20:05:37,881 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=59.94 vs. limit=7.725
2024-10-08 20:05:38,329 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.702e+02 6.095e+02 9.954e+02 2.152e+03 5.652e+04, threshold=1.991e+03, percent-clipped=38.0
2024-10-08 20:05:38,370 INFO [train.py:1154] Epoch 1, batch 1800, loss[loss=1.288, simple_loss=0.6476, pruned_loss=0.7819, ctc_loss=1.102, over 4864.00 frames. ], tot_loss[loss=1.221, simple_loss=0.6481, pruned_loss=0.7569, ctc_loss=1.051, over 967907.35 frames. ], batch size: 23, lr: 4.44e-02,
2024-10-08 20:05:46,499 WARNING [optim.py:503] Scaling gradients by 0.051135942339897156, model_norm_threshold=1990.7139892578125
2024-10-08 20:05:46,636 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.507e+08, grad_sumsq=2.187e+10, orig_rms_sq=2.976e-02
2024-10-08 20:05:49,564 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=33.87 vs. limit=7.9525
2024-10-08 20:05:54,221 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=42.92 vs. limit=7.72625
2024-10-08 20:06:07,666 WARNING [optim.py:503] Scaling gradients by 0.09948857128620148, model_norm_threshold=1990.7139892578125
2024-10-08 20:06:07,803 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.688e+08, grad_sumsq=9.842e+09, orig_rms_sq=1.715e-02
2024-10-08 20:06:09,511 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=52.83 vs. limit=7.955
2024-10-08 20:06:12,693 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=15.29 vs. limit=7.9575
2024-10-08 20:06:15,793 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=610.0, ans=0.086275
2024-10-08 20:06:16,710 WARNING [optim.py:503] Scaling gradients by 0.09638001024723053, model_norm_threshold=1990.7139892578125
2024-10-08 20:06:16,848 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.047e+07, grad_sumsq=1.305e+09, orig_rms_sq=5.399e-02
2024-10-08 20:06:17,938 WARNING [optim.py:503] Scaling gradients by 0.05624746158719063, model_norm_threshold=1990.7139892578125
2024-10-08 20:06:18,074 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.825e+08, grad_sumsq=2.189e+10, orig_rms_sq=1.747e-02
2024-10-08 20:06:20,581 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass_mid.scale_min, batch_count=610.0, ans=0.87865
2024-10-08 20:06:29,166 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2.whitening_limit, batch_count=613.3333333333334, ans=5.306666666666667
2024-10-08 20:06:30,103 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=51.84 vs. limit=7.73
2024-10-08 20:06:35,860 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=14.78 vs. limit=7.73125
2024-10-08 20:06:36,541 INFO [train.py:1154] Epoch 1, batch 1850, loss[loss=1.215, simple_loss=0.6234, pruned_loss=0.7191, ctc_loss=1.046, over 4737.00 frames. ], tot_loss[loss=1.228, simple_loss=0.6471, pruned_loss=0.7562, ctc_loss=1.049, over 968162.53 frames. ], batch size: 26, lr: 4.43e-02,
2024-10-08 20:06:37,711 WARNING [optim.py:503] Scaling gradients by 0.06701167672872543, model_norm_threshold=1990.7139892578125
2024-10-08 20:06:37,887 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.736e+08, grad_sumsq=1.968e+08, orig_rms_sq=8.820e-01
2024-10-08 20:06:54,399 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.59 vs. limit=5.155
2024-10-08 20:06:59,038 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=2.98 vs. limit=4.249333333333333
2024-10-08 20:07:09,755 WARNING [optim.py:503] Scaling gradients by 0.0853281244635582, model_norm_threshold=1990.7139892578125
2024-10-08 20:07:09,893 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.61, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.344e+08, grad_sumsq=1.825e+10, orig_rms_sq=1.832e-02
2024-10-08 20:07:12,509 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=15.80 vs. limit=4.250666666666667
2024-10-08 20:07:19,535 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=28.26 vs. limit=7.735
2024-10-08 20:07:20,011 WARNING [optim.py:503] Scaling gradients by 0.06405812501907349, model_norm_threshold=1990.7139892578125
2024-10-08 20:07:20,150 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.91, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.769e+08, grad_sumsq=4.897e+10, orig_rms_sq=1.791e-02
2024-10-08 20:07:23,223 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=19.41 vs. limit=7.73625
2024-10-08 20:07:31,071 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=20.32 vs. limit=5.315
2024-10-08 20:07:33,341 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=27.94 vs. limit=7.975
2024-10-08 20:07:33,701 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.129e+02 1.172e+03 2.406e+03 6.879e+03 3.893e+04, threshold=4.811e+03, percent-clipped=56.0
2024-10-08 20:07:33,742 INFO [train.py:1154] Epoch 1, batch 1900, loss[loss=1.299, simple_loss=0.6593, pruned_loss=0.7708, ctc_loss=1.085, over 4773.00 frames. ], tot_loss[loss=1.24, simple_loss=0.6487, pruned_loss=0.7583, ctc_loss=1.053, over 967894.55 frames. ], batch size: 29, lr: 4.43e-02,
2024-10-08 20:07:38,147 WARNING [optim.py:503] Scaling gradients by 0.04079078137874603, model_norm_threshold=4811.15283203125
2024-10-08 20:07:38,286 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.426e+09, grad_sumsq=4.303e+10, orig_rms_sq=5.638e-02
2024-10-08 20:07:43,596 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.23 vs. limit=5.316666666666666
2024-10-08 20:07:44,713 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=17.00 vs. limit=7.73875
2024-10-08 20:07:44,861 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=6.31 vs. limit=4.1273333333333335
2024-10-08 20:07:49,949 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=51.32 vs. limit=7.73875
2024-10-08 20:07:50,190 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=21.37 vs. limit=7.9775
2024-10-08 20:07:50,194 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.73 vs. limit=5.159166666666667
2024-10-08 20:07:50,849 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=636.6666666666666, ans=0.2936333333333333
2024-10-08 20:07:52,987 WARNING [optim.py:503] Scaling gradients by 0.07260563969612122, model_norm_threshold=4811.15283203125
2024-10-08 20:07:53,126 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.045e+09, grad_sumsq=1.083e+09, orig_rms_sq=9.653e-01
2024-10-08 20:07:53,273 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=636.6666666666666, ans=0.048010416666666667
2024-10-08 20:07:54,757 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=36.26 vs. limit=7.9775
2024-10-08 20:07:57,710 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=640.0, ans=0.42
2024-10-08 20:07:57,860 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=22.54 vs. limit=5.32
2024-10-08 20:08:18,088 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=10.31 vs. limit=5.161666666666667
2024-10-08 20:08:19,454 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.54 vs. limit=5.161666666666667
2024-10-08 20:08:21,555 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.72 vs. limit=5.161666666666667
2024-10-08 20:08:22,064 WARNING [optim.py:503] Scaling gradients by 0.013464885763823986, model_norm_threshold=4811.15283203125
2024-10-08 20:08:22,202 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.092e+10, grad_sumsq=1.213e+12, orig_rms_sq=1.725e-02
2024-10-08 20:08:25,897 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=646.6666666666666, ans=0.4191666666666667
2024-10-08 20:08:28,553 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=8.59 vs. limit=7.7425
2024-10-08 20:08:30,511 INFO [train.py:1154] Epoch 1, batch 1950, loss[loss=1.216, simple_loss=0.6121, pruned_loss=0.7219, ctc_loss=0.981, over 4863.00 frames. ], tot_loss[loss=1.252, simple_loss=0.6503, pruned_loss=0.7598, ctc_loss=1.055, over 966894.31 frames. ], batch size: 20, lr: 4.43e-02,
2024-10-08 20:08:32,921 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=650.0, ans=0.46953125
2024-10-08 20:08:37,216 WARNING [optim.py:503] Scaling gradients by 0.09325802326202393, model_norm_threshold=4811.15283203125
2024-10-08 20:08:37,353 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.86, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.280e+09, grad_sumsq=1.399e+11, orig_rms_sq=1.630e-02
2024-10-08 20:08:37,592 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=650.0, ans=5.40625
2024-10-08 20:08:39,847 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=650.0, ans=0.46953125
2024-10-08 20:08:39,973 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=650.0, ans=0.46953125
2024-10-08 20:08:45,342 WARNING [optim.py:503] Scaling gradients by 0.003168768249452114, model_norm_threshold=4811.15283203125
2024-10-08 20:08:45,481 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.88, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.022e+12, grad_sumsq=1.245e+14, orig_rms_sq=1.624e-02
2024-10-08 20:09:04,327 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.97 vs. limit=5.165
2024-10-08 20:09:11,961 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=660.0, ans=0.4690625
2024-10-08 20:09:12,369 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=43.86 vs. limit=7.7475
2024-10-08 20:09:13,929 WARNING [optim.py:503] Scaling gradients by 0.03686146065592766, model_norm_threshold=4811.15283203125
2024-10-08 20:09:14,067 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.794e+09, grad_sumsq=1.806e+11, orig_rms_sq=1.547e-02
2024-10-08 20:09:20,395 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.23 vs. limit=7.9975
2024-10-08 20:09:22,178 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=663.3333333333334, ans=0.46890625
2024-10-08 20:09:27,833 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.912e+02 2.994e+03 7.178e+03 1.861e+04 1.518e+06, threshold=1.436e+04, percent-clipped=57.0
2024-10-08 20:09:27,873 INFO [train.py:1154] Epoch 1, batch 2000, loss[loss=1.182, simple_loss=0.5937, pruned_loss=0.6956, ctc_loss=0.9481, over 4959.00 frames. ], tot_loss[loss=1.261, simple_loss=0.6513, pruned_loss=0.7595, ctc_loss=1.055, over 966732.32 frames. ], batch size: 19, lr: 4.42e-02,
2024-10-08 20:09:31,042 WARNING [optim.py:503] Scaling gradients by 0.09575143456459045, model_norm_threshold=14356.251953125
2024-10-08 20:09:31,180 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.329e+09, grad_sumsq=4.789e+11, orig_rms_sq=1.531e-02
2024-10-08 20:09:31,449 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=666.6666666666666, ans=0.5
2024-10-08 20:09:32,852 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=28.49 vs. limit=7.75
2024-10-08 20:09:33,979 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=31.48 vs. limit=8.0
2024-10-08 20:09:34,127 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.89 vs. limit=8.0
2024-10-08 20:09:45,078 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=670.0, ans=0.2933
2024-10-08 20:09:48,959 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=22.34 vs. limit=7.75125
2024-10-08 20:09:52,854 WARNING [optim.py:503] Scaling gradients by 0.026567665860056877, model_norm_threshold=14356.251953125
2024-10-08 20:09:52,993 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.339e+10, grad_sumsq=5.444e+12, orig_rms_sq=1.532e-02
2024-10-08 20:09:58,032 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=673.3333333333334, ans=0.17475000000000002
2024-10-08 20:09:59,140 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=673.3333333333334, ans=0.8764333333333334
2024-10-08 20:10:11,630 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=676.6666666666666, ans=0.29323333333333335
2024-10-08 20:10:12,071 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=676.6666666666666, ans=8.0075
2024-10-08 20:10:18,065 WARNING [optim.py:503] Scaling gradients by 0.09658067673444748, model_norm_threshold=14356.251953125
2024-10-08 20:10:18,204 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.63, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.389e+10, grad_sumsq=8.818e+11, orig_rms_sq=1.576e-02
2024-10-08 20:10:23,555 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=28.14 vs. limit=7.755
2024-10-08 20:10:24,768 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.34 vs. limit=5.17
2024-10-08 20:10:26,342 INFO [train.py:1154] Epoch 1, batch 2050, loss[loss=1.181, simple_loss=0.5876, pruned_loss=0.6937, ctc_loss=0.9663, over 4914.00 frames. ], tot_loss[loss=1.267, simple_loss=0.651, pruned_loss=0.7584, ctc_loss=1.052, over 967008.64 frames. ], batch size: 19, lr: 4.42e-02,
2024-10-08 20:10:26,468 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=683.3333333333334, ans=0.29316666666666663
2024-10-08 20:10:28,934 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=683.3333333333334, ans=0.7568333333333334
2024-10-08 20:10:43,154 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=24.50 vs. limit=7.7575
2024-10-08 20:10:43,339 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=51.67 vs. limit=7.7575
2024-10-08 20:10:48,919 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=37.45 vs. limit=8.0175
2024-10-08 20:10:49,958 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=19.51 vs. limit=7.75875
2024-10-08 20:10:50,014 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.21 vs. limit=3.1035
2024-10-08 20:11:01,556 WARNING [optim.py:503] Scaling gradients by 0.06801281124353409, model_norm_threshold=14356.251953125
2024-10-08 20:11:01,695 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.115e+09, grad_sumsq=3.078e+12, orig_rms_sq=2.961e-03
2024-10-08 20:11:02,240 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=10.09 vs. limit=7.76
2024-10-08 20:11:02,475 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=23.79 vs. limit=7.76
2024-10-08 20:11:04,035 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=693.3333333333334, ans=0.4675
2024-10-08 20:11:07,165 WARNING [optim.py:503] Scaling gradients by 0.086933434009552, model_norm_threshold=14356.251953125
2024-10-08 20:11:07,304 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.383e+09, grad_sumsq=4.132e+09, orig_rms_sq=1.061e+00
2024-10-08 20:11:11,411 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=21.97 vs. limit=7.76125
2024-10-08 20:11:12,418 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.65 vs. limit=8.0225
2024-10-08 20:11:14,714 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.00 vs. limit=5.174166666666666
2024-10-08 20:11:14,875 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.01 vs. limit=5.348333333333334
2024-10-08 20:11:15,694 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=31.96 vs. limit=7.76125
2024-10-08 20:11:16,759 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=32.46 vs. limit=7.76125
2024-10-08 20:11:17,648 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=696.6666666666666, ans=0.2930333333333333
2024-10-08 20:11:23,303 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.682e+02 3.740e+03 9.321e+03 1.830e+04 5.404e+05, threshold=1.864e+04, percent-clipped=32.0
2024-10-08 20:11:23,343 INFO [train.py:1154] Epoch 1, batch 2100, loss[loss=1.348, simple_loss=0.6794, pruned_loss=0.7962, ctc_loss=1.061, over 4841.00 frames. ], tot_loss[loss=1.269, simple_loss=0.6491, pruned_loss=0.7566, ctc_loss=1.048, over 967171.22 frames. ], batch size: 21, lr: 4.42e-02,
2024-10-08 20:11:26,739 WARNING [optim.py:503] Scaling gradients by 0.03288925066590309, model_norm_threshold=18642.4296875
2024-10-08 20:11:26,877 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.62, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.985e+11, grad_sumsq=1.855e+11, orig_rms_sq=1.070e+00
2024-10-08 20:11:34,938 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=703.3333333333334, ans=0.173625
2024-10-08 20:11:35,351 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=24.33 vs. limit=8.0275
2024-10-08 20:11:37,350 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=703.3333333333334, ans=0.24296666666666666
2024-10-08 20:11:44,257 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=0.000e+00
2024-10-08 20:11:45,399 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=706.6666666666666, ans=0.5
2024-10-08 20:11:45,416 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=706.6666666666666, ans=0.2929333333333333
2024-10-08 20:11:48,067 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.24 vs. limit=3.106
2024-10-08 20:11:52,961 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.40 vs. limit=7.765
2024-10-08 20:11:58,426 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=25.59 vs. limit=7.76625
2024-10-08 20:12:02,049 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.63 vs. limit=8.0325
2024-10-08 20:12:04,706 WARNING [optim.py:503] Scaling gradients by 0.0765635296702385, model_norm_threshold=18642.4296875
2024-10-08 20:12:04,843 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.409e+10, grad_sumsq=2.295e+10, orig_rms_sq=1.050e+00
2024-10-08 20:12:09,913 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=34.70 vs. limit=7.7675
2024-10-08 20:12:13,314 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=713.3333333333334, ans=0.4665625
2024-10-08 20:12:21,195 INFO [train.py:1154] Epoch 1, batch 2150, loss[loss=1.216, simple_loss=0.5918, pruned_loss=0.7152, ctc_loss=1.027, over 4862.00 frames. ], tot_loss[loss=1.272, simple_loss=0.6483, pruned_loss=0.7555, ctc_loss=1.047, over 968006.36 frames. ], batch size: 20, lr: 4.41e-02,
2024-10-08 20:12:21,591 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=28.58 vs. limit=7.76875
2024-10-08 20:12:24,072 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=24.44 vs. limit=8.0375
2024-10-08 20:12:27,289 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=85.90 vs. limit=7.76875
2024-10-08 20:12:29,114 WARNING [optim.py:503] Scaling gradients by 0.018781157210469246, model_norm_threshold=18642.4296875
2024-10-08 20:12:29,252 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.283e+11, grad_sumsq=3.113e+11, orig_rms_sq=1.054e+00
2024-10-08 20:12:31,915 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=53.07 vs. limit=7.77
2024-10-08 20:12:34,076 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten.whitening_limit, batch_count=720.0, ans=7.77
2024-10-08 20:12:35,404 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=25.80 vs. limit=7.77
2024-10-08 20:12:36,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.71 vs. limit=3.108
2024-10-08 20:12:40,380 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=720.0, ans=0.21080000000000002
2024-10-08 20:12:44,137 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=14.80 vs. limit=7.77125
2024-10-08 20:12:48,120 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten.whitening_limit, batch_count=723.3333333333334, ans=7.77125
2024-10-08 20:12:48,213 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=30.79 vs. limit=8.0425
2024-10-08 20:12:52,184 WARNING [optim.py:503] Scaling gradients by 0.06744751334190369, model_norm_threshold=18642.4296875
2024-10-08 20:12:52,321 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.400e+10, grad_sumsq=1.630e+12, orig_rms_sq=1.473e-02
2024-10-08 20:12:57,344 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.56 vs. limit=5.181666666666667
2024-10-08 20:13:05,419 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.98 vs. limit=7.77375
2024-10-08 20:13:05,918 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=730.0, ans=0.46578125
2024-10-08 20:13:07,460 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=12.76 vs. limit=5.365
2024-10-08 20:13:09,218 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=730.0, ans=0.46578125
2024-10-08 20:13:09,281 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.const_attention_rate, batch_count=730.0, ans=0.2089375
2024-10-08 20:13:17,432 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.652e+02 5.290e+03 1.197e+04 2.806e+04 9.926e+05, threshold=2.394e+04, percent-clipped=39.0
2024-10-08 20:13:17,472 INFO [train.py:1154] Epoch 1, batch 2200, loss[loss=1.319, simple_loss=0.6705, pruned_loss=0.7662, ctc_loss=1.09, over 4741.00 frames. ], tot_loss[loss=1.273, simple_loss=0.647, pruned_loss=0.7535, ctc_loss=1.046, over 967775.49 frames. ], batch size: 26, lr: 4.41e-02,
2024-10-08 20:13:25,992 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=36.30 vs. limit=7.775
2024-10-08 20:13:28,923 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=736.6666666666666, ans=0.46546875
2024-10-08 20:13:28,956 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=736.6666666666666, ans=0.2926333333333333
2024-10-08 20:13:32,900 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1.whitening_limit, batch_count=736.6666666666666, ans=5.184166666666667
2024-10-08 20:13:35,270 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=4.49 vs. limit=4.294666666666666
2024-10-08 20:13:35,300 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.87 vs. limit=5.184166666666667
2024-10-08 20:13:39,775 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=14.33 vs. limit=5.37
2024-10-08 20:13:40,508 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=740.0, ans=0.4653125
2024-10-08 20:13:42,588 WARNING [optim.py:503] Scaling gradients by 0.004829186014831066, model_norm_threshold=23936.01953125
2024-10-08 20:13:42,726 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.369e+12, grad_sumsq=4.438e+14, orig_rms_sq=1.435e-02
2024-10-08 20:13:43,612 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.67 vs. limit=8.055
2024-10-08 20:13:50,942 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=743.3333333333334, ans=0.083275
2024-10-08 20:13:54,055 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.55 vs. limit=8.0575
2024-10-08 20:14:00,752 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=23.37 vs. limit=8.0575
2024-10-08 20:14:01,686 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=30.90 vs. limit=7.77875
2024-10-08 20:14:01,891 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=17.28 vs. limit=7.77875
2024-10-08 20:14:05,281 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.95 vs. limit=5.1866666666666665
2024-10-08 20:14:06,947 WARNING [optim.py:503] Scaling gradients by 0.03474948927760124, model_norm_threshold=23936.01953125
2024-10-08 20:14:07,084 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.13, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.302e+10, grad_sumsq=4.358e+12, orig_rms_sq=1.446e-02
2024-10-08 20:14:07,572 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=14.38 vs. limit=5.373333333333333
2024-10-08 20:14:14,193 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=750.0, ans=0.46484375
2024-10-08 20:14:15,288 INFO [train.py:1154] Epoch 1, batch 2250, loss[loss=1.418, simple_loss=0.7081, pruned_loss=0.841, ctc_loss=1.112, over 4870.00 frames. ], tot_loss[loss=1.276, simple_loss=0.647, pruned_loss=0.7531, ctc_loss=1.046, over 967682.50 frames. ], batch size: 22, lr: 4.40e-02,
2024-10-08 20:14:17,831 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=750.0, ans=0.46484375
2024-10-08 20:14:21,463 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=1.105e-01
2024-10-08 20:14:25,238 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=48.71 vs. limit=7.78125
2024-10-08 20:14:26,081 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=753.3333333333334, ans=0.29246666666666665
2024-10-08 20:14:29,788 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.62 vs. limit=3.113
2024-10-08 20:14:31,767 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=753.3333333333334, ans=0.46468750000000003
2024-10-08 20:14:32,840 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=753.3333333333334, ans=0.29246666666666665
2024-10-08 20:14:35,966 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=753.3333333333334, ans=0.8736333333333334
2024-10-08 20:14:40,515 WARNING [optim.py:503] Scaling gradients by 0.04352883622050285, model_norm_threshold=23936.01953125
2024-10-08 20:14:40,650 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.137e+11, grad_sumsq=8.353e+12, orig_rms_sq=1.361e-02
2024-10-08 20:14:58,158 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=760.0, ans=0.1715
2024-10-08 20:15:12,305 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.744e+02 7.370e+03 1.830e+04 5.893e+04 4.957e+06, threshold=3.659e+04, percent-clipped=43.0
2024-10-08 20:15:12,348 INFO [train.py:1154] Epoch 1, batch 2300, loss[loss=1.353, simple_loss=0.6794, pruned_loss=0.803, ctc_loss=1.05, over 4883.00 frames. ], tot_loss[loss=1.276, simple_loss=0.6457, pruned_loss=0.7521, ctc_loss=1.042, over 968174.42 frames. ], batch size: 19, lr: 4.40e-02,
2024-10-08 20:15:17,706 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.27 vs. limit=8.075
2024-10-08 20:15:21,721 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=766.6666666666666, ans=0.4640625
2024-10-08 20:15:26,220 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=770.0, ans=0.46390625
2024-10-08 20:15:27,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=770.0, ans=8.0775
2024-10-08 20:15:36,477 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.balancer1.prob, batch_count=773.3333333333334, ans=0.46375
2024-10-08 20:15:41,370 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.04 vs. limit=8.08
2024-10-08 20:15:42,222 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=59.26 vs. limit=5.386666666666667
2024-10-08 20:15:50,470 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.43 vs. limit=7.79125
2024-10-08 20:15:50,904 WARNING [optim.py:503] Scaling gradients by 0.07683920860290527, model_norm_threshold=36593.2109375
2024-10-08 20:15:51,040 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.14, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.193e+10, grad_sumsq=2.556e+12, orig_rms_sq=1.249e-02
2024-10-08 20:15:51,851 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.98 vs. limit=8.0825
2024-10-08 20:15:52,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=776.6666666666666, ans=0.46359375
2024-10-08 20:15:59,184 WARNING [optim.py:503] Scaling gradients by 0.08977339416742325, model_norm_threshold=36593.2109375
2024-10-08 20:15:59,323 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.139e+10, grad_sumsq=3.403e+12, orig_rms_sq=1.216e-02
2024-10-08 20:16:01,062 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.12 vs. limit=8.085
2024-10-08 20:16:09,644 INFO [train.py:1154] Epoch 1, batch 2350, loss[loss=1.257, simple_loss=0.6196, pruned_loss=0.7352, ctc_loss=1.061, over 4865.00 frames. ], tot_loss[loss=1.276, simple_loss=0.6452, pruned_loss=0.7509, ctc_loss=1.043, over 968126.48 frames. ], batch size: 23, lr: 4.40e-02,
2024-10-08 20:16:16,464 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=783.3333333333334, ans=0.40208333333333335
2024-10-08 20:16:25,622 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=786.6666666666666, ans=0.463125
2024-10-08 20:16:27,748 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=786.6666666666666, ans=0.2921333333333333
2024-10-08 20:16:34,190 WARNING [optim.py:503] Scaling gradients by 0.053707242012023926, model_norm_threshold=36593.2109375
2024-10-08 20:16:34,329 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.666e+10, grad_sumsq=6.298e+10, orig_rms_sq=1.217e+00
2024-10-08 20:16:38,107 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=790.0, ans=0.8723500000000001
2024-10-08 20:16:38,971 WARNING [optim.py:503] Scaling gradients by 0.046117253601551056, model_norm_threshold=36593.2109375
2024-10-08 20:16:39,110 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.554e+11, grad_sumsq=1.318e+13, orig_rms_sq=1.179e-02
2024-10-08 20:16:39,343 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=790.0, ans=0.46296875
2024-10-08 20:16:40,854 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=7.56 vs. limit=7.79625
2024-10-08 20:16:41,369 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=790.0, ans=0.29209999999999997
2024-10-08 20:16:46,172 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=38.27 vs. limit=8.095
2024-10-08 20:16:50,175 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=793.3333333333334, ans=0.29206666666666664
2024-10-08 20:16:53,290 WARNING [optim.py:503] Scaling gradients by 0.01846461370587349, model_norm_threshold=36593.2109375
2024-10-08 20:16:53,429 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.271e+12, grad_sumsq=1.040e+12, orig_rms_sq=1.221e+00
2024-10-08 20:16:55,603 WARNING [optim.py:503] Scaling gradients by 0.08784615248441696, model_norm_threshold=36593.2109375
2024-10-08 20:16:55,742 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.495e+10, grad_sumsq=5.318e+10, orig_rms_sq=1.221e+00
2024-10-08 20:17:08,023 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.488e+03 1.129e+04 2.779e+04 6.550e+04 1.982e+06, threshold=5.559e+04, percent-clipped=45.0
2024-10-08 20:17:08,062 INFO [train.py:1154] Epoch 1, batch 2400, loss[loss=1.418, simple_loss=0.7156, pruned_loss=0.8407, ctc_loss=1.1, over 4749.00 frames. ], tot_loss[loss=1.277, simple_loss=0.6455, pruned_loss=0.7504, ctc_loss=1.043, over 967387.13 frames. ], batch size: 19, lr: 4.39e-02,
2024-10-08 20:17:10,366 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=800.0, ans=0.4625
2024-10-08 20:17:14,486 WARNING [optim.py:503] Scaling gradients by 0.06410311907529831, model_norm_threshold=55587.69921875
2024-10-08 20:17:14,623 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.356e+11, grad_sumsq=2.813e+11, orig_rms_sq=1.193e+00
2024-10-08 20:17:15,689 WARNING [optim.py:503] Scaling gradients by 0.02485491894185543, model_norm_threshold=55587.69921875
2024-10-08 20:17:15,826 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.84, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.198e+12, grad_sumsq=3.406e+14, orig_rms_sq=1.232e-02
2024-10-08 20:17:20,422 WARNING [optim.py:503] Scaling gradients by 0.006924910005182028, model_norm_threshold=55587.69921875
2024-10-08 20:17:20,559 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.690e+13, grad_sumsq=1.445e+13, orig_rms_sq=1.170e+00
2024-10-08 20:17:20,711 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=803.3333333333334, ans=0.29196666666666665
2024-10-08 20:17:21,168 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=5.41 vs. limit=5.200833333333334
2024-10-08 20:17:22,862 WARNING [optim.py:503] Scaling gradients by 0.07094988971948624, model_norm_threshold=55587.69921875
2024-10-08 20:17:22,999 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.409e+11, grad_sumsq=1.243e+11, orig_rms_sq=1.133e+00
2024-10-08 20:17:25,205 WARNING [optim.py:503] Scaling gradients by 0.09776932746171951, model_norm_threshold=55587.69921875
2024-10-08 20:17:25,342 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.220e+10, grad_sumsq=5.491e+10, orig_rms_sq=1.133e+00
2024-10-08 20:17:30,233 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=11.04 vs. limit=7.8025
2024-10-08 20:17:31,560 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=24.23 vs. limit=7.8025
2024-10-08 20:17:41,348 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=4.260e+02
2024-10-08 20:17:42,013 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=15.00 vs. limit=8.1075
2024-10-08 20:17:44,802 WARNING [optim.py:503] Scaling gradients by 0.04108785465359688, model_norm_threshold=55587.69921875
2024-10-08 20:17:44,939 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.600e+11, grad_sumsq=4.749e+13, orig_rms_sq=1.179e-02
2024-10-08 20:17:46,454 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=10.63 vs. limit=5.2025
2024-10-08 20:17:47,394 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=810.0, ans=0.169625
2024-10-08 20:17:50,869 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=810.0, ans=0.46203125
2024-10-08 20:18:01,191 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=813.3333333333334, ans=0.5
2024-10-08 20:18:02,724 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=95.36 vs. limit=7.805
2024-10-08 20:18:04,493 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=816.6666666666666, ans=0.46171875
2024-10-08 20:18:05,677 INFO [train.py:1154] Epoch 1, batch 2450, loss[loss=1.259, simple_loss=0.6209, pruned_loss=0.7452, ctc_loss=1.018, over 4865.00 frames. ], tot_loss[loss=1.284, simple_loss=0.6477, pruned_loss=0.7549, ctc_loss=1.045, over 966635.17 frames. ], batch size: 22, lr: 4.39e-02,
2024-10-08 20:18:05,759 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=816.6666666666666, ans=0.8714166666666667
2024-10-08 20:18:06,447 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=9.15 vs. limit=7.80625
2024-10-08 20:18:09,103 WARNING [optim.py:503] Scaling gradients by 0.07606633752584457, model_norm_threshold=55587.69921875
2024-10-08 20:18:09,242 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.235e+11, grad_sumsq=1.831e+11, orig_rms_sq=1.221e+00
2024-10-08 20:18:13,299 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.71 vs. limit=5.408333333333333
2024-10-08 20:18:15,513 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.62 vs. limit=7.80625
2024-10-08 20:18:17,824 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=820.0, ans=8.115
2024-10-08 20:18:18,254 WARNING [optim.py:503] Scaling gradients by 0.04860810562968254, model_norm_threshold=55587.69921875
2024-10-08 20:18:18,393 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.972e+11, grad_sumsq=2.527e+13, orig_rms_sq=1.176e-02
2024-10-08 20:18:19,909 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=33.20 vs. limit=7.8075
2024-10-08 20:18:29,756 WARNING [optim.py:503] Scaling gradients by 0.07838365435600281, model_norm_threshold=55587.69921875
2024-10-08 20:18:29,894 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.562e+11, grad_sumsq=1.306e+13, orig_rms_sq=1.196e-02
2024-10-08 20:18:30,581 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.03 vs. limit=7.80875
2024-10-08 20:18:34,912 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=6.01 vs. limit=5.2058333333333335
2024-10-08 20:18:36,498 WARNING [optim.py:503] Scaling gradients by 0.03144995495676994, model_norm_threshold=55587.69921875
2024-10-08 20:18:36,635 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.201e+12, grad_sumsq=9.596e+11, orig_rms_sq=1.251e+00
2024-10-08 20:18:36,876 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=823.3333333333334, ans=0.46140625
2024-10-08 20:18:39,696 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.75 vs. limit=7.81
2024-10-08 20:18:40,291 WARNING [optim.py:503] Scaling gradients by 0.044546980410814285, model_norm_threshold=55587.69921875
2024-10-08 20:18:40,428 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.689e+11, grad_sumsq=4.637e+11, orig_rms_sq=1.227e+00
2024-10-08 20:18:42,134 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=116.58 vs. limit=7.81
2024-10-08 20:18:42,712 WARNING [optim.py:503] Scaling gradients by 0.037176940590143204, model_norm_threshold=55587.69921875
2024-10-08 20:18:42,852 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.761e+11, grad_sumsq=7.956e+11, orig_rms_sq=1.227e+00
2024-10-08 20:18:43,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=34.99 vs. limit=8.12
2024-10-08 20:18:48,578 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.prob, batch_count=826.6666666666666, ans=0.46125
2024-10-08 20:18:54,201 WARNING [optim.py:503] Scaling gradients by 0.04992133378982544, model_norm_threshold=55587.69921875
2024-10-08 20:18:54,339 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.347e+11, grad_sumsq=2.513e+11, orig_rms_sq=1.332e+00
2024-10-08 20:19:02,036 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.30 vs. limit=7.81125
2024-10-08 20:19:03,678 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.436e+03 2.775e+04 5.939e+04 2.437e+05 8.027e+06, threshold=1.188e+05, percent-clipped=51.0
2024-10-08 20:19:03,719 INFO [train.py:1154] Epoch 1, batch 2500, loss[loss=1.342, simple_loss=0.681, pruned_loss=0.7801, ctc_loss=1.107, over 4740.00 frames. ], tot_loss[loss=1.283, simple_loss=0.6468, pruned_loss=0.7536, ctc_loss=1.045, over 966280.93 frames. ], batch size: 26, lr: 4.38e-02,
2024-10-08 20:19:06,519 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=7.46 vs. limit=7.8125
2024-10-08 20:19:12,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.96 vs. limit=8.125
2024-10-08 20:19:14,408 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.47 vs. limit=3.1255
2024-10-08 20:19:18,798 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=836.6666666666666, ans=0.46078125000000003
2024-10-08 20:19:18,802 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=836.6666666666666, ans=0.16862500000000002
2024-10-08 20:19:20,282 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=26.23 vs. limit=8.1275
2024-10-08 20:19:31,661 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.76 vs. limit=5.21
2024-10-08 20:19:44,034 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=843.3333333333334, ans=0.46046875
2024-10-08 20:19:46,584 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=49.62 vs. limit=7.81625
2024-10-08 20:19:57,332 WARNING [optim.py:503] Scaling gradients by 0.04570619389414787, model_norm_threshold=118782.53125
2024-10-08 20:19:57,469 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.328e+12, grad_sumsq=1.742e+12, orig_rms_sq=1.336e+00
2024-10-08 20:20:00,134 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=850.0, ans=0.7585
2024-10-08 20:20:01,038 INFO [train.py:1154] Epoch 1, batch 2550, loss[loss=1.163, simple_loss=0.5882, pruned_loss=0.6843, ctc_loss=0.9207, over 4959.00 frames. ], tot_loss[loss=1.283, simple_loss=0.6466, pruned_loss=0.753, ctc_loss=1.044, over 966882.73 frames. ], batch size: 19, lr: 4.38e-02,
2024-10-08 20:20:04,488 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer2.prob, batch_count=850.0, ans=0.46015625
2024-10-08 20:20:07,850 WARNING [optim.py:503] Scaling gradients by 0.02422991581261158, model_norm_threshold=118782.53125
2024-10-08 20:20:07,991 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.814e+12, grad_sumsq=6.300e+14, orig_rms_sq=1.082e-02
2024-10-08 20:20:11,991 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=21.55 vs. limit=7.82
2024-10-08 20:20:15,993 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=853.3333333333334, ans=0.08080000000000001
2024-10-08 20:20:16,354 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.16 vs. limit=5.426666666666667
2024-10-08 20:20:24,600 WARNING [optim.py:503] Scaling gradients by 0.010238215327262878, model_norm_threshold=118782.53125
2024-10-08 20:20:24,737 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.632e+13, grad_sumsq=2.744e+13, orig_rms_sq=1.323e+00
2024-10-08 20:20:26,147 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=856.6666666666666, ans=5.535416666666666
2024-10-08 20:20:26,674 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=14.68 vs. limit=7.82125
2024-10-08 20:20:28,055 WARNING [optim.py:503] Scaling gradients by 0.012009721249341965, model_norm_threshold=118782.53125
2024-10-08 20:20:28,193 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.021e+13, grad_sumsq=7.227e+14, orig_rms_sq=4.179e-02
2024-10-08 20:20:31,824 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.skip_rate, batch_count=856.6666666666666, ans=0.5
2024-10-08 20:20:41,323 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.93 vs. limit=5.43
2024-10-08 20:20:56,010 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=863.3333333333334, ans=0.08057500000000001
2024-10-08 20:20:58,371 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.396e+03 3.715e+04 9.766e+04 2.955e+05 1.160e+07, threshold=1.953e+05, percent-clipped=45.0
2024-10-08 20:20:58,414 INFO [train.py:1154] Epoch 1, batch 2600, loss[loss=1.319, simple_loss=0.6597, pruned_loss=0.7795, ctc_loss=1.047, over 4859.00 frames. ], tot_loss[loss=1.282, simple_loss=0.6462, pruned_loss=0.7518, ctc_loss=1.044, over 966370.44 frames. ], batch size: 20, lr: 4.37e-02,
2024-10-08 20:21:06,845 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=27.18 vs. limit=8.15
2024-10-08 20:21:08,504 WARNING [optim.py:503] Scaling gradients by 0.06148666515946388, model_norm_threshold=195315.84375
2024-10-08 20:21:08,641 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.70, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.094e+12, grad_sumsq=6.573e+14, orig_rms_sq=1.079e-02
2024-10-08 20:21:12,513 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=27.76 vs. limit=7.82625
2024-10-08 20:21:12,809 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=34.36 vs. limit=8.1525
2024-10-08 20:21:13,878 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=32.82 vs. limit=8.1525
2024-10-08 20:21:14,637 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.prob, batch_count=870.0, ans=0.45921875
2024-10-08 20:21:15,896 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=17.62 vs. limit=4.348
2024-10-08 20:21:19,054 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=870.0, ans=0.2010625
2024-10-08 20:21:20,093 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=873.3333333333334, ans=0.200875
2024-10-08 20:21:21,015 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=4.05 vs. limit=4.174666666666667
2024-10-08 20:21:26,500 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=20.04 vs. limit=7.8275
2024-10-08 20:21:29,224 WARNING [optim.py:503] Scaling gradients by 0.09555232524871826, model_norm_threshold=195315.84375
2024-10-08 20:21:29,370 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.515e+12, grad_sumsq=1.087e+12, orig_rms_sq=1.393e+00
2024-10-08 20:21:32,238 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.52 vs. limit=8.1575
2024-10-08 20:21:37,144 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=876.6666666666666, ans=0.16712500000000002
2024-10-08 20:21:39,649 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=876.6666666666666, ans=0.5
2024-10-08 20:21:48,644 WARNING [optim.py:503] Scaling gradients by 0.023087386041879654, model_norm_threshold=195315.84375
2024-10-08 20:21:48,783 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.253e+13, grad_sumsq=8.891e+12, orig_rms_sq=1.409e+00
2024-10-08 20:21:51,442 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.45 vs. limit=5.22
2024-10-08 20:21:55,399 INFO [train.py:1154] Epoch 1, batch 2650, loss[loss=1.24, simple_loss=0.6318, pruned_loss=0.7127, ctc_loss=1.055, over 4827.00 frames. ], tot_loss[loss=1.286, simple_loss=0.6482, pruned_loss=0.7534, ctc_loss=1.047, over 966102.73 frames. ], batch size: 38, lr: 4.37e-02,
2024-10-08 20:21:57,168 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.65 vs. limit=7.83125
2024-10-08 20:21:58,281 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=58.99 vs. limit=7.83125
2024-10-08 20:21:58,345 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.59 vs. limit=8.1625
2024-10-08 20:22:06,721 WARNING [optim.py:503] Scaling gradients by 0.03265223279595375, model_norm_threshold=195315.84375
2024-10-08 20:22:06,860 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.070e+13, grad_sumsq=2.666e+14, orig_rms_sq=4.012e-02
2024-10-08 20:22:09,037 WARNING [optim.py:503] Scaling gradients by 0.04934229701757431, model_norm_threshold=195315.84375
2024-10-08 20:22:09,174 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.643e+12, grad_sumsq=4.140e+14, orig_rms_sq=1.122e-02
2024-10-08 20:22:12,893 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=5.80 vs. limit=4.354666666666667
2024-10-08 20:22:14,736 WARNING [optim.py:503] Scaling gradients by 0.03377368301153183, model_norm_threshold=195315.84375
2024-10-08 20:22:14,874 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.469e+13, grad_sumsq=1.315e+15, orig_rms_sq=1.117e-02
2024-10-08 20:22:24,192 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.skip_rate, batch_count=890.0, ans=0.5
2024-10-08 20:22:28,231 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=10.16 vs. limit=7.83375
2024-10-08 20:22:28,483 WARNING [optim.py:503] Scaling gradients by 0.03170826658606529, model_norm_threshold=195315.84375
2024-10-08 20:22:28,621 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.53, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.024e+13, grad_sumsq=1.865e+15, orig_rms_sq=1.085e-02
2024-10-08 20:22:35,233 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.23 vs. limit=8.17
2024-10-08 20:22:35,449 WARNING [optim.py:503] Scaling gradients by 0.03963731601834297, model_norm_threshold=195315.84375
2024-10-08 20:22:35,588 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.977e+12, grad_sumsq=6.770e+14, orig_rms_sq=1.031e-02
2024-10-08 20:22:37,788 WARNING [optim.py:503] Scaling gradients by 0.09887201339006424, model_norm_threshold=195315.84375
2024-10-08 20:22:37,928 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.132e+12, grad_sumsq=7.903e+11, orig_rms_sq=1.432e+00
2024-10-08 20:22:38,471 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.64 vs. limit=5.446666666666666
2024-10-08 20:22:42,576 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=896.6666666666666, ans=0.07982500000000001
2024-10-08 20:22:44,765 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=896.6666666666666, ans=0.3879166666666667
2024-10-08 20:22:46,307 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=56.90 vs. limit=7.83625
2024-10-08 20:22:46,866 WARNING [optim.py:503] Scaling gradients by 0.0681966245174408, model_norm_threshold=195315.84375
2024-10-08 20:22:47,003 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.690e+12, grad_sumsq=3.594e+14, orig_rms_sq=1.027e-02
2024-10-08 20:22:52,718 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.905e+03 6.596e+04 2.431e+05 7.247e+05 8.460e+06, threshold=4.862e+05, percent-clipped=54.0
2024-10-08 20:22:52,760 INFO [train.py:1154] Epoch 1, batch 2700, loss[loss=1.291, simple_loss=0.6407, pruned_loss=0.7563, ctc_loss=1.07, over 4854.00 frames. ], tot_loss[loss=1.283, simple_loss=0.6471, pruned_loss=0.7507, ctc_loss=1.047, over 966383.78 frames. ], batch size: 28, lr: 4.36e-02,
2024-10-08 20:22:53,941 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=900.0, ans=0.094375
2024-10-08 20:22:56,434 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.99 vs. limit=5.225
2024-10-08 20:23:06,913 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=172.98 vs. limit=7.83875
2024-10-08 20:23:10,867 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=903.3333333333334, ans=0.8683833333333334
2024-10-08 20:23:16,574 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=906.6666666666666, ans=0.2136
2024-10-08 20:23:18,918 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=906.6666666666666, ans=0.4575
2024-10-08 20:23:21,583 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=26.36 vs. limit=7.84
2024-10-08 20:23:26,706 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=910.0, ans=0.86815
2024-10-08 20:23:28,760 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.42 vs. limit=3.1365
2024-10-08 20:23:29,192 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=7.802e+01
2024-10-08 20:23:29,198 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=910.0, ans=0.45734375
2024-10-08 20:23:31,952 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.14 vs. limit=5.2275
2024-10-08 20:23:34,571 WARNING [optim.py:503] Scaling gradients by 0.0422295406460762, model_norm_threshold=486176.4375
2024-10-08 20:23:34,709 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.61, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.032e+13, grad_sumsq=8.440e+15, orig_rms_sq=9.516e-03
2024-10-08 20:23:38,374 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=913.3333333333334, ans=0.45718749999999997
2024-10-08 20:23:47,062 WARNING [optim.py:503] Scaling gradients by 0.016268393024802208, model_norm_threshold=486176.4375
2024-10-08 20:23:47,200 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.076e+14, grad_sumsq=3.219e+16, orig_rms_sq=9.557e-03
2024-10-08 20:23:48,754 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=16.14 vs. limit=4.365333333333333
2024-10-08 20:23:50,702 INFO [train.py:1154] Epoch 1, batch 2750, loss[loss=1.351, simple_loss=0.6822, pruned_loss=0.7975, ctc_loss=1.06, over 4799.00 frames. ], tot_loss[loss=1.291, simple_loss=0.6505, pruned_loss=0.7571, ctc_loss=1.048, over 967020.74 frames. ], batch size: 19, lr: 4.36e-02,
2024-10-08 20:23:55,926 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=4.80 vs. limit=5.229166666666667
2024-10-08 20:23:56,710 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten.whitening_limit, batch_count=916.6666666666666, ans=8.1875
2024-10-08 20:24:01,893 WARNING [optim.py:503] Scaling gradients by 0.0695045217871666, model_norm_threshold=486176.4375
2024-10-08 20:24:02,032 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.878e+12, grad_sumsq=6.229e+12, orig_rms_sq=1.425e+00
2024-10-08 20:24:02,450 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=43.99 vs. limit=7.845
2024-10-08 20:24:06,775 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=78.85 vs. limit=7.845
2024-10-08 20:24:07,093 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=16.54 vs. limit=7.845
2024-10-08 20:24:15,654 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=923.3333333333334, ans=0.165375
2024-10-08 20:24:21,307 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=33.69 vs. limit=5.461666666666667
2024-10-08 20:24:25,082 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=926.6666666666666, ans=8.195
2024-10-08 20:24:28,608 WARNING [optim.py:503] Scaling gradients by 0.062091097235679626, model_norm_threshold=486176.4375
2024-10-08 20:24:28,744 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.585e+13, grad_sumsq=2.706e+15, orig_rms_sq=9.554e-03
2024-10-08 20:24:31,067 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=926.6666666666666, ans=0.8675666666666667
2024-10-08 20:24:32,188 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=926.6666666666666, ans=0.07915
2024-10-08 20:24:33,861 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=18.58 vs. limit=7.8475
2024-10-08 20:24:40,567 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.68 vs. limit=5.465
2024-10-08 20:24:40,713 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=17.37 vs. limit=7.84875
2024-10-08 20:24:44,309 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn2.whiten.whitening_limit, batch_count=930.0, ans=8.1975
2024-10-08 20:24:47,013 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.499e+03 7.563e+04 1.971e+05 7.196e+05 2.988e+07, threshold=3.942e+05, percent-clipped=30.0
2024-10-08 20:24:47,056 INFO [train.py:1154] Epoch 1, batch 2800, loss[loss=1.296, simple_loss=0.658, pruned_loss=0.749, ctc_loss=1.09, over 4771.00 frames. ], tot_loss[loss=1.296, simple_loss=0.6526, pruned_loss=0.7606, ctc_loss=1.05, over 967098.21 frames. ], batch size: 53, lr: 4.36e-02,
2024-10-08 20:24:47,667 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=4.30 vs. limit=7.85
2024-10-08 20:24:50,578 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=933.3333333333334, ans=0.3833333333333333
2024-10-08 20:24:54,661 WARNING [optim.py:503] Scaling gradients by 0.0644623190164566, model_norm_threshold=394232.90625
2024-10-08 20:24:54,800 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.593e+12, grad_sumsq=5.045e+12, orig_rms_sq=1.505e+00
2024-10-08 20:24:57,906 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten.whitening_limit, batch_count=936.6666666666666, ans=7.85125
2024-10-08 20:25:09,822 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff3_skip_rate, batch_count=940.0, ans=0.07885
2024-10-08 20:25:09,831 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=940.0, ans=0.4559375
2024-10-08 20:25:10,868 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=940.0, ans=0.4559375
2024-10-08 20:25:12,000 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.min_positive, batch_count=940.0, ans=0.2406
2024-10-08 20:25:18,751 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.min_positive, batch_count=940.0, ans=0.0470625
2024-10-08 20:25:22,051 WARNING [optim.py:503] Scaling gradients by 0.01610572077333927, model_norm_threshold=394232.90625
2024-10-08 20:25:22,190 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.274e+14, grad_sumsq=2.399e+16, orig_rms_sq=9.475e-03
2024-10-08 20:25:25,508 WARNING [optim.py:503] Scaling gradients by 0.015077928081154823, model_norm_threshold=394232.90625
2024-10-08 20:25:25,656 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.811e+14, grad_sumsq=3.020e+16, orig_rms_sq=9.306e-03
2024-10-08 20:25:27,577 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=13.73 vs. limit=7.85375
2024-10-08 20:25:33,406 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=8.30 vs. limit=8.21
2024-10-08 20:25:37,894 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.89 vs. limit=8.21
2024-10-08 20:25:43,842 INFO [train.py:1154] Epoch 1, batch 2850, loss[loss=1.33, simple_loss=0.668, pruned_loss=0.7882, ctc_loss=1.041, over 4932.00 frames. ], tot_loss[loss=1.296, simple_loss=0.6521, pruned_loss=0.76, ctc_loss=1.051, over 966900.70 frames. ], batch size: 20, lr: 4.35e-02,
2024-10-08 20:25:44,982 WARNING [optim.py:503] Scaling gradients by 0.02649904228746891, model_norm_threshold=394232.90625
2024-10-08 20:25:45,121 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.779e+13, grad_sumsq=3.053e+13, orig_rms_sq=1.565e+00
2024-10-08 20:25:46,993 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten.whitening_limit, batch_count=950.0, ans=7.85625
2024-10-08 20:25:53,224 WARNING [optim.py:503] Scaling gradients by 0.01250503771007061, model_norm_threshold=394232.90625
2024-10-08 20:25:53,362 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.691e+14, grad_sumsq=1.096e+14, orig_rms_sq=1.544e+00
2024-10-08 20:26:00,460 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=953.3333333333334, ans=0.09404166666666668
2024-10-08 20:26:05,676 WARNING [optim.py:503] Scaling gradients by 0.06597088277339935, model_norm_threshold=394232.90625
2024-10-08 20:26:05,813 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.737e+13, grad_sumsq=1.789e+15, orig_rms_sq=9.711e-03
2024-10-08 20:26:06,374 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.55 vs. limit=5.239166666666667
2024-10-08 20:26:25,343 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.70 vs. limit=8.22
2024-10-08 20:26:31,609 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=963.3333333333334, ans=0.8662833333333334
2024-10-08 20:26:35,804 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.29 vs. limit=5.240833333333334
2024-10-08 20:26:36,751 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=5.73 vs. limit=5.4816666666666665
2024-10-08 20:26:40,798 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.520e+03 1.374e+05 4.289e+05 1.211e+06 3.153e+07, threshold=8.577e+05, percent-clipped=55.0
2024-10-08 20:26:40,838 INFO [train.py:1154] Epoch 1, batch 2900, loss[loss=1.214, simple_loss=0.6048, pruned_loss=0.7118, ctc_loss=0.9975, over 4746.00 frames. ], tot_loss[loss=1.294, simple_loss=0.651, pruned_loss=0.7587, ctc_loss=1.052, over 965979.15 frames. ], batch size: 20, lr: 4.35e-02,
2024-10-08 20:27:01,169 WARNING [optim.py:503] Scaling gradients by 0.0035396083258092403, model_norm_threshold=857718.8125
2024-10-08 20:27:01,307 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.73, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.275e+16, grad_sumsq=4.400e+18, orig_rms_sq=9.717e-03
2024-10-08 20:27:06,418 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.65 vs. limit=3.146
2024-10-08 20:27:06,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=973.3333333333334, ans=0.1635
2024-10-08 20:27:08,310 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=11.39 vs. limit=4.389333333333333
2024-10-08 20:27:09,594 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=24.74 vs. limit=7.865
2024-10-08 20:27:10,237 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer1.min_positive, batch_count=973.3333333333334, ans=0.04695833333333334
2024-10-08 20:27:10,796 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.94 vs. limit=7.865
2024-10-08 20:27:12,044 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=12.39 vs. limit=7.865
2024-10-08 20:27:22,693 WARNING [optim.py:503] Scaling gradients by 0.06340053677558899, model_norm_threshold=857718.8125
2024-10-08 20:27:22,832 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.139e+13, grad_sumsq=3.021e+13, orig_rms_sq=1.701e+00
2024-10-08 20:27:26,316 WARNING [optim.py:503] Scaling gradients by 0.0052859787829220295, model_norm_threshold=857718.8125
2024-10-08 20:27:26,454 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.067e+16, grad_sumsq=6.356e+15, orig_rms_sq=1.678e+00
2024-10-08 20:27:27,650 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer2.prob, batch_count=980.0, ans=0.4540625
2024-10-08 20:27:34,438 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=980.0, ans=0.8657
2024-10-08 20:27:35,883 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=24.71 vs. limit=8.235
2024-10-08 20:27:35,918 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.53 vs. limit=5.49
2024-10-08 20:27:37,779 INFO [train.py:1154] Epoch 1, batch 2950, loss[loss=1.243, simple_loss=0.6169, pruned_loss=0.7304, ctc_loss=1.019, over 4798.00 frames. ], tot_loss[loss=1.292, simple_loss=0.6499, pruned_loss=0.7572, ctc_loss=1.049, over 966555.58 frames. ], batch size: 19, lr: 4.34e-02,
2024-10-08 20:27:41,019 WARNING [optim.py:503] Scaling gradients by 0.09067757427692413, model_norm_threshold=857718.8125
2024-10-08 20:27:41,156 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.791e+13, grad_sumsq=3.993e+15, orig_rms_sq=9.496e-03
2024-10-08 20:27:46,552 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.69 vs. limit=7.86875
2024-10-08 20:27:46,706 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.43 vs. limit=8.2375
2024-10-08 20:27:48,906 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.19 vs. limit=8.24
2024-10-08 20:27:52,673 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=986.6666666666666, ans=0.04691666666666667
2024-10-08 20:27:58,072 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=986.6666666666666, ans=0.8654666666666667
2024-10-08 20:27:59,431 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=39.67 vs. limit=7.87125
2024-10-08 20:28:08,739 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=5.74 vs. limit=7.87125
2024-10-08 20:28:14,761 WARNING [optim.py:503] Scaling gradients by 0.036409251391887665, model_norm_threshold=857718.8125
2024-10-08 20:28:14,898 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.52, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.875e+14, grad_sumsq=2.683e+16, orig_rms_sq=1.071e-02
2024-10-08 20:28:17,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=993.3333333333334, ans=0.29006666666666664
2024-10-08 20:28:26,470 WARNING [optim.py:503] Scaling gradients by 0.030668877065181732, model_norm_threshold=857718.8125
2024-10-08 20:28:26,606 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.297e+14, grad_sumsq=3.087e+16, orig_rms_sq=1.068e-02
2024-10-08 20:28:28,453 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.40 vs. limit=5.2491666666666665
2024-10-08 20:28:29,121 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=996.6666666666666, ans=0.077575
2024-10-08 20:28:35,068 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.499e+04 2.184e+05 6.387e+05 1.663e+06 2.423e+08, threshold=1.277e+06, percent-clipped=44.0
2024-10-08 20:28:35,108 INFO [train.py:1154] Epoch 1, batch 3000, loss[loss=1.363, simple_loss=0.6799, pruned_loss=0.808, ctc_loss=1.075, over 4849.00 frames. ], tot_loss[loss=1.291, simple_loss=0.6493, pruned_loss=0.7566, ctc_loss=1.049, over 967211.69 frames. ], batch size: 21, lr: 4.34e-02,
2024-10-08 20:28:35,109 INFO [train.py:1177] Computing validation loss
2024-10-08 20:28:40,536 INFO [train.py:1186] Epoch 1, validation: loss=1.37, simple_loss=0.6895, pruned_loss=0.8068, ctc_loss=1.09, over 90464.00 frames.
2024-10-08 20:28:40,537 INFO [train.py:1187] Maximum memory allocated so far is 5914MB
2024-10-08 20:28:47,165 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=19.88 vs. limit=7.875
2024-10-08 20:29:04,022 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=17.89 vs. limit=7.8775
2024-10-08 20:29:05,475 WARNING [optim.py:503] Scaling gradients by 0.0033235386945307255, model_norm_threshold=1277361.25
2024-10-08 20:29:05,614 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.253e+16, grad_sumsq=4.954e+18, orig_rms_sq=1.060e-02
2024-10-08 20:29:07,822 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten.whitening_limit, batch_count=1006.6666666666666, ans=7.8775
2024-10-08 20:29:10,577 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.37 vs. limit=3.151
2024-10-08 20:29:12,692 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=29.80 vs. limit=7.8775
2024-10-08 20:29:18,603 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=29.85 vs. limit=7.87875
2024-10-08 20:29:27,391 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=1013.3333333333334, ans=0.162
2024-10-08 20:29:28,959 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.66 vs. limit=5.506666666666667
2024-10-08 20:29:29,488 WARNING [optim.py:503] Scaling gradients by 0.04190101847052574, model_norm_threshold=1277361.25
2024-10-08 20:29:29,626 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.031e+14, grad_sumsq=2.966e+16, orig_rms_sq=1.022e-02
2024-10-08 20:29:37,073 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=21.97 vs. limit=8.26
2024-10-08 20:29:38,840 INFO [train.py:1154] Epoch 1, batch 3050, loss[loss=1.312, simple_loss=0.648, pruned_loss=0.7775, ctc_loss=1.054, over 4750.00 frames. ], tot_loss[loss=1.292, simple_loss=0.6502, pruned_loss=0.7573, ctc_loss=1.05, over 966736.06 frames. ], batch size: 19, lr: 4.33e-02,
2024-10-08 20:29:41,288 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1016.6666666666666, ans=0.45234375
2024-10-08 20:29:43,757 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=226.96 vs. limit=7.88125
2024-10-08 20:29:44,434 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=1016.6666666666666, ans=5.635416666666667
2024-10-08 20:29:45,580 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=1016.6666666666666, ans=0.1928125
2024-10-08 20:29:55,862 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=54.46 vs. limit=7.8825
2024-10-08 20:29:59,333 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=57.67 vs. limit=7.8825
2024-10-08 20:30:02,392 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=1023.3333333333334, ans=5.639583333333333
2024-10-08 20:30:05,601 WARNING [optim.py:503] Scaling gradients by 0.06445491313934326, model_norm_threshold=1277361.25
2024-10-08 20:30:05,741 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.61, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.396e+14, grad_sumsq=2.399e+16, orig_rms_sq=9.986e-03
2024-10-08 20:30:07,587 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=39.22 vs. limit=7.88375
2024-10-08 20:30:10,444 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.prob, batch_count=1023.3333333333334, ans=0.45203125
2024-10-08 20:30:11,259 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=5.92 vs. limit=5.511666666666667
2024-10-08 20:30:17,074 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=1026.6666666666667, ans=0.451875
2024-10-08 20:30:19,453 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=1026.6666666666667, ans=0.1615
2024-10-08 20:30:24,834 WARNING [optim.py:503] Scaling gradients by 0.07369536906480789, model_norm_threshold=1277361.25
2024-10-08 20:30:24,973 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.318e+13, grad_sumsq=8.151e+15, orig_rms_sq=1.020e-02
2024-10-08 20:30:26,306 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer2.prob, batch_count=1030.0, ans=0.45171875
2024-10-08 20:30:35,040 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.124e+04 3.945e+05 1.100e+06 3.117e+06 3.843e+08, threshold=2.200e+06, percent-clipped=46.0
2024-10-08 20:30:35,082 INFO [train.py:1154] Epoch 1, batch 3100, loss[loss=1.202, simple_loss=0.6241, pruned_loss=0.695, ctc_loss=0.9741, over 4807.00 frames. ], tot_loss[loss=1.289, simple_loss=0.6489, pruned_loss=0.7553, ctc_loss=1.048, over 966426.09 frames. ], batch size: 38, lr: 4.33e-02,
2024-10-08 20:30:36,206 WARNING [optim.py:503] Scaling gradients by 0.05417774245142937, model_norm_threshold=2199758.75
2024-10-08 20:30:36,345 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.927e+14, grad_sumsq=3.851e+16, orig_rms_sq=1.020e-02
2024-10-08 20:30:38,142 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=13.45 vs. limit=7.8875
2024-10-08 20:30:43,548 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=24.47 vs. limit=7.8875
2024-10-08 20:30:44,836 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.92 vs. limit=7.8875
2024-10-08 20:30:49,412 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.99 vs. limit=5.5183333333333335
2024-10-08 20:30:56,511 WARNING [optim.py:503] Scaling gradients by 0.036979105323553085, model_norm_threshold=2199758.75
2024-10-08 20:30:56,649 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.061e+14, grad_sumsq=4.119e+14, orig_rms_sq=1.957e+00
2024-10-08 20:31:00,626 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=9.75 vs. limit=7.89
2024-10-08 20:31:02,957 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=28.47 vs. limit=8.28
2024-10-08 20:31:04,313 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=16.43 vs. limit=7.89
2024-10-08 20:31:09,593 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=25.45 vs. limit=7.89125
2024-10-08 20:31:31,448 WARNING [optim.py:503] Scaling gradients by 0.060764145106077194, model_norm_threshold=2199758.75
2024-10-08 20:31:31,590 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.296e+14, grad_sumsq=4.205e+16, orig_rms_sq=1.022e-02
2024-10-08 20:31:31,637 INFO [train.py:1154] Epoch 1, batch 3150, loss[loss=1.29, simple_loss=0.6597, pruned_loss=0.742, ctc_loss=1.088, over 4800.00 frames. ], tot_loss[loss=1.284, simple_loss=0.6465, pruned_loss=0.7517, ctc_loss=1.044, over 966760.36 frames. ], batch size: 40, lr: 4.32e-02,
2024-10-08 20:31:50,366 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=24.88 vs. limit=8.29
2024-10-08 20:31:52,162 WARNING [optim.py:503] Scaling gradients by 0.04242449626326561, model_norm_threshold=2199758.75
2024-10-08 20:31:52,300 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.073e+15, grad_sumsq=5.658e+14, orig_rms_sq=1.896e+00
2024-10-08 20:31:54,252 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=19.37 vs. limit=7.89625
2024-10-08 20:31:55,289 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=16.22 vs. limit=7.89625
2024-10-08 20:32:00,234 WARNING [optim.py:503] Scaling gradients by 0.014010068029165268, model_norm_threshold=2199758.75
2024-10-08 20:32:00,373 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.773e+15, grad_sumsq=3.043e+15, orig_rms_sq=1.897e+00
2024-10-08 20:32:01,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1056.6666666666667, ans=0.2894333333333333
2024-10-08 20:32:05,211 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=1060.0, ans=0.4503125
2024-10-08 20:32:05,616 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.24 vs. limit=5.265
2024-10-08 20:32:13,237 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=1060.0, ans=0.3675
2024-10-08 20:32:16,836 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=7.25 vs. limit=7.89875
2024-10-08 20:32:18,062 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.54 vs. limit=5.265833333333333
2024-10-08 20:32:18,728 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=1063.3333333333333, ans=0.1901875
2024-10-08 20:32:22,163 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=1063.3333333333333, ans=0.45015625
2024-10-08 20:32:22,251 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=1063.3333333333333, ans=0.8627833333333333
2024-10-08 20:32:26,532 WARNING [optim.py:503] Scaling gradients by 0.0353461354970932, model_norm_threshold=2199758.75
2024-10-08 20:32:26,671 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.464e+15, grad_sumsq=1.420e+17, orig_rms_sq=1.031e-02
2024-10-08 20:32:29,441 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.972e+03 5.141e+05 1.269e+06 4.957e+06 1.570e+08, threshold=2.538e+06, percent-clipped=43.0
2024-10-08 20:32:29,484 INFO [train.py:1154] Epoch 1, batch 3200, loss[loss=1.313, simple_loss=0.6508, pruned_loss=0.7829, ctc_loss=1.024, over 4743.00 frames. ], tot_loss[loss=1.286, simple_loss=0.647, pruned_loss=0.7539, ctc_loss=1.044, over 967211.05 frames. ], batch size: 20, lr: 4.32e-02,
2024-10-08 20:32:33,320 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.15 vs. limit=7.9
2024-10-08 20:32:34,875 WARNING [optim.py:503] Scaling gradients by 0.09076359868049622, model_norm_threshold=2537643.25
2024-10-08 20:32:35,013 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.450e+14, grad_sumsq=1.443e+16, orig_rms_sq=1.005e-02
2024-10-08 20:32:36,911 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=15.25 vs. limit=7.9
2024-10-08 20:32:40,659 WARNING [optim.py:503] Scaling gradients by 0.003712990088388324, model_norm_threshold=2537643.25
2024-10-08 20:32:40,798 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.749e+17, grad_sumsq=1.820e+19, orig_rms_sq=9.611e-03
2024-10-08 20:32:43,727 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=36.79 vs. limit=7.90125
2024-10-08 20:32:44,184 WARNING [optim.py:503] Scaling gradients by 0.009623702615499496, model_norm_threshold=2537643.25
2024-10-08 20:32:44,323 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.494e+16, grad_sumsq=2.676e+18, orig_rms_sq=9.320e-03
2024-10-08 20:32:51,749 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.72 vs. limit=5.2683333333333335
2024-10-08 20:32:56,560 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.17 vs. limit=5.536666666666667
2024-10-08 20:33:00,813 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=5.97 vs. limit=5.536666666666667
2024-10-08 20:33:04,172 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=14.83 vs. limit=7.9037500000000005
2024-10-08 20:33:06,411 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=32.18 vs. limit=8.307500000000001
2024-10-08 20:33:12,097 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=1076.6666666666667, ans=0.15962500000000002
2024-10-08 20:33:12,327 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=9.10 vs. limit=7.9037500000000005
2024-10-08 20:33:13,549 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=29.14 vs. limit=7.9037500000000005
2024-10-08 20:33:15,059 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=8.81 vs. limit=8.31
2024-10-08 20:33:19,086 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=127.95 vs. limit=7.905
2024-10-08 20:33:20,017 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=1080.0, ans=0.1595
2024-10-08 20:33:20,425 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=22.74 vs. limit=7.905
2024-10-08 20:33:26,763 INFO [train.py:1154] Epoch 1, batch 3250, loss[loss=1.283, simple_loss=0.6398, pruned_loss=0.7547, ctc_loss=1.043, over 4855.00 frames. ], tot_loss[loss=1.287, simple_loss=0.6472, pruned_loss=0.754, ctc_loss=1.045, over 967283.25 frames. ], batch size: 24, lr: 4.31e-02,
2024-10-08 20:33:28,991 WARNING [optim.py:503] Scaling gradients by 0.054711200296878815, model_norm_threshold=2537643.25
2024-10-08 20:33:29,128 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.75, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.623e+15, grad_sumsq=1.658e+17, orig_rms_sq=9.786e-03
2024-10-08 20:33:31,416 WARNING [optim.py:503] Scaling gradients by 0.094695083796978, model_norm_threshold=2537643.25
2024-10-08 20:33:31,553 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.669e+14, grad_sumsq=2.727e+16, orig_rms_sq=9.786e-03
2024-10-08 20:33:35,095 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1083.3333333333333, ans=0.2891666666666667
2024-10-08 20:33:35,302 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=23.22 vs. limit=7.90625
2024-10-08 20:33:35,629 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.42 vs. limit=3.1625
2024-10-08 20:33:50,829 WARNING [optim.py:503] Scaling gradients by 0.015002516098320484, model_norm_threshold=2537643.25
2024-10-08 20:33:50,966 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.837e+15, grad_sumsq=4.440e+15, orig_rms_sq=1.990e+00
2024-10-08 20:33:59,400 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=20.84 vs. limit=8.3175
2024-10-08 20:34:09,374 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.14 vs. limit=7.91
2024-10-08 20:34:09,882 WARNING [optim.py:503] Scaling gradients by 0.010859640315175056, model_norm_threshold=2537643.25
2024-10-08 20:34:10,021 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.134e+16, grad_sumsq=6.921e+15, orig_rms_sq=1.638e+00
2024-10-08 20:34:14,015 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=26.36 vs. limit=8.3225
2024-10-08 20:34:18,117 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=1096.6666666666667, ans=0.44859375
2024-10-08 20:34:21,219 WARNING [optim.py:503] Scaling gradients by 0.02665521577000618, model_norm_threshold=2537643.25
2024-10-08 20:34:21,357 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.146e+15, grad_sumsq=2.165e+17, orig_rms_sq=9.908e-03
2024-10-08 20:34:23,663 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.744e+04 9.201e+05 3.464e+06 1.004e+07 6.835e+08, threshold=6.929e+06, percent-clipped=57.0
2024-10-08 20:34:23,664 WARNING [optim.py:503] Scaling gradients by 0.020413313060998917, model_norm_threshold=6928899.0
2024-10-08 20:34:23,802 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.561e+16, grad_sumsq=1.394e+16, orig_rms_sq=1.838e+00
2024-10-08 20:34:23,847 INFO [train.py:1154] Epoch 1, batch 3300, loss[loss=1.333, simple_loss=0.6932, pruned_loss=0.7684, ctc_loss=1.09, over 4840.00 frames. ], tot_loss[loss=1.285, simple_loss=0.6467, pruned_loss=0.7532, ctc_loss=1.044, over 967719.24 frames. ], batch size: 43, lr: 4.31e-02,
2024-10-08 20:34:25,556 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.02 vs. limit=8.325
2024-10-08 20:34:27,047 WARNING [optim.py:503] Scaling gradients by 0.041552890092134476, model_norm_threshold=6928899.0
2024-10-08 20:34:27,185 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.726e+15, grad_sumsq=5.852e+17, orig_rms_sq=9.784e-03
2024-10-08 20:34:32,819 WARNING [optim.py:503] Scaling gradients by 0.017544982954859734, model_norm_threshold=6928899.0
2024-10-08 20:34:32,956 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.888e+16, grad_sumsq=1.527e+16, orig_rms_sq=1.892e+00
2024-10-08 20:34:37,698 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=37.11 vs. limit=8.3275
2024-10-08 20:34:46,281 WARNING [optim.py:503] Scaling gradients by 0.03298557549715042, model_norm_threshold=6928899.0
2024-10-08 20:34:46,421 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.264e+16, grad_sumsq=1.358e+18, orig_rms_sq=9.309e-03
2024-10-08 20:34:53,411 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1106.6666666666667, ans=0.2889333333333333
2024-10-08 20:34:57,113 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=12.37 vs. limit=7.91625
2024-10-08 20:34:59,572 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=4.15 vs. limit=4.444
2024-10-08 20:35:00,340 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=1110.0, ans=0.158375
2024-10-08 20:35:02,747 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten.whitening_limit, batch_count=1110.0, ans=7.91625
2024-10-08 20:35:03,133 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=21.25 vs. limit=8.3325
2024-10-08 20:35:06,358 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.01 vs. limit=5.555
2024-10-08 20:35:10,871 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=10.00 vs. limit=7.9175
2024-10-08 20:35:13,630 WARNING [optim.py:503] Scaling gradients by 0.07242488116025925, model_norm_threshold=6928899.0
2024-10-08 20:35:13,768 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.228e+15, grad_sumsq=3.609e+17, orig_rms_sq=8.944e-03
2024-10-08 20:35:18,234 WARNING [optim.py:503] Scaling gradients by 0.03555793687701225, model_norm_threshold=6928899.0
2024-10-08 20:35:18,372 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.570e+16, grad_sumsq=8.654e+15, orig_rms_sq=1.814e+00
2024-10-08 20:35:19,632 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=1116.6666666666667, ans=0.36041666666666666
2024-10-08 20:35:20,669 INFO [train.py:1154] Epoch 1, batch 3350, loss[loss=1.345, simple_loss=0.6786, pruned_loss=0.7832, ctc_loss=1.111, over 4802.00 frames. ], tot_loss[loss=1.289, simple_loss=0.649, pruned_loss=0.7549, ctc_loss=1.046, over 966981.25 frames. ], batch size: 40, lr: 4.30e-02,
2024-10-08 20:35:28,941 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=28.00 vs. limit=8.3375
2024-10-08 20:35:29,712 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=1116.6666666666667, ans=0.44765625
2024-10-08 20:35:30,197 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=34.34 vs. limit=7.91875
2024-10-08 20:35:33,193 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=1120.0, ans=0.7612
2024-10-08 20:35:33,362 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=28.97 vs. limit=8.34
2024-10-08 20:35:36,720 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=28.55 vs. limit=8.34
2024-10-08 20:35:44,286 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=1123.3333333333333, ans=0.44734375
2024-10-08 20:35:45,712 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=73.44 vs. limit=8.3425
2024-10-08 20:35:49,996 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=1123.3333333333333, ans=0.15787500000000002
2024-10-08 20:36:02,236 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1126.6666666666667, ans=0.28873333333333334
2024-10-08 20:36:09,165 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1130.0, ans=0.2887
2024-10-08 20:36:12,042 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=30.04 vs. limit=7.92375
2024-10-08 20:36:15,745 WARNING [optim.py:503] Scaling gradients by 0.04016019031405449, model_norm_threshold=6928899.0
2024-10-08 20:36:15,884 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.286e+16, grad_sumsq=1.240e+18, orig_rms_sq=1.038e-02
2024-10-08 20:36:16,548 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.55 vs. limit=8.3475
2024-10-08 20:36:17,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.skip_rate, batch_count=1133.3333333333333, ans=0.5
2024-10-08 20:36:18,464 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.321e+04 1.357e+06 3.465e+06 1.442e+07 3.949e+08, threshold=6.930e+06, percent-clipped=39.0
2024-10-08 20:36:18,504 INFO [train.py:1154] Epoch 1, batch 3400, loss[loss=1.257, simple_loss=0.6361, pruned_loss=0.7433, ctc_loss=0.9794, over 4959.00 frames. ], tot_loss[loss=1.287, simple_loss=0.6489, pruned_loss=0.7537, ctc_loss=1.046, over 966766.51 frames. ], batch size: 19, lr: 4.29e-02,
2024-10-08 20:36:19,067 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=8.96 vs. limit=7.925
2024-10-08 20:36:22,521 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=10.74 vs. limit=5.283333333333333
2024-10-08 20:36:36,341 WARNING [optim.py:503] Scaling gradients by 0.012035545893013477, model_norm_threshold=6930493.0
2024-10-08 20:36:36,481 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.702e+16, grad_sumsq=6.649e+18, orig_rms_sq=1.008e-02
2024-10-08 20:36:39,404 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.25 vs. limit=7.92625
2024-10-08 20:36:41,933 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=9.49 vs. limit=7.9275
2024-10-08 20:36:58,998 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1143.3333333333333, ans=0.28856666666666664
2024-10-08 20:36:59,425 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.11 vs. limit=5.285833333333334
2024-10-08 20:37:00,597 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=7.76 vs. limit=7.92875
2024-10-08 20:37:01,408 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=10.06 vs. limit=5.285833333333334
2024-10-08 20:37:04,264 WARNING [optim.py:503] Scaling gradients by 0.07362779229879379, model_norm_threshold=6930493.0
2024-10-08 20:37:04,402 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.821e+15, grad_sumsq=4.413e+16, orig_rms_sq=4.126e-02
2024-10-08 20:37:05,853 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=1146.6666666666667, ans=0.8598666666666667
2024-10-08 20:37:14,041 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.46 vs. limit=5.2875
2024-10-08 20:37:14,562 INFO [train.py:1154] Epoch 1, batch 3450, loss[loss=1.419, simple_loss=0.736, pruned_loss=0.8191, ctc_loss=1.16, over 4854.00 frames. ], tot_loss[loss=1.286, simple_loss=0.6482, pruned_loss=0.7523, ctc_loss=1.048, over 967096.41 frames. ], batch size: 43, lr: 4.29e-02,
2024-10-08 20:37:17,227 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=10.65 vs. limit=7.93125
2024-10-08 20:37:25,499 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.77 vs. limit=7.9325
2024-10-08 20:37:25,537 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.19 vs. limit=5.576666666666666
2024-10-08 20:37:28,843 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=34.68 vs. limit=5.576666666666666
2024-10-08 20:37:30,579 WARNING [optim.py:503] Scaling gradients by 0.07528235018253326, model_norm_threshold=6930493.0
2024-10-08 20:37:30,716 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.491e+15, grad_sumsq=2.641e+17, orig_rms_sq=9.433e-03
2024-10-08 20:37:33,905 WARNING [optim.py:503] Scaling gradients by 0.021361086517572403, model_norm_threshold=6930493.0
2024-10-08 20:37:34,042 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.727e+16, grad_sumsq=1.935e+17, orig_rms_sq=8.926e-02
2024-10-08 20:37:36,421 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=1156.6666666666667, ans=0.073975
2024-10-08 20:37:37,107 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=3.15 vs. limit=7.93375
2024-10-08 20:37:39,893 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=1156.6666666666667, ans=0.8595166666666667
2024-10-08 20:37:40,438 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.41 vs. limit=7.93375
2024-10-08 20:37:40,936 WARNING [optim.py:503] Scaling gradients by 0.05644027516245842, model_norm_threshold=6930493.0
2024-10-08 20:37:41,075 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.719e+15, grad_sumsq=2.903e+17, orig_rms_sq=9.366e-03
2024-10-08 20:37:42,181 WARNING [optim.py:503] Scaling gradients by 0.02048347145318985, model_norm_threshold=6930493.0
2024-10-08 20:37:42,318 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.213e+16, grad_sumsq=3.430e+18, orig_rms_sq=9.366e-03
2024-10-08 20:37:42,924 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.12 vs. limit=7.93375
2024-10-08 20:37:47,280 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=13.77 vs. limit=7.93375
2024-10-08 20:37:54,703 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=1160.0, ans=0.445625
2024-10-08 20:37:58,013 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=1160.0, ans=0.8594
2024-10-08 20:37:58,404 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=512, metric=7.65 vs. limit=7.935
2024-10-08 20:37:59,139 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1163.3333333333333, ans=0.28836666666666666
2024-10-08 20:38:07,930 WARNING [optim.py:503] Scaling gradients by 0.02625178173184395, model_norm_threshold=6930493.0
2024-10-08 20:38:08,069 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.801e+16, grad_sumsq=1.468e+16, orig_rms_sq=1.908e+00
2024-10-08 20:38:11,469 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.726e+04 1.489e+06 4.640e+06 1.084e+07 5.758e+08, threshold=9.279e+06, percent-clipped=34.0
2024-10-08 20:38:11,509 INFO [train.py:1154] Epoch 1, batch 3500, loss[loss=1.265, simple_loss=0.6401, pruned_loss=0.7437, ctc_loss=1.008, over 4883.00 frames. ], tot_loss[loss=1.283, simple_loss=0.647, pruned_loss=0.7508, ctc_loss=1.045, over 967403.75 frames. ], batch size: 19, lr: 4.28e-02,
2024-10-08 20:38:15,834 WARNING [optim.py:503] Scaling gradients by 0.0020623772870749235, model_norm_threshold=9279019.0
2024-10-08 20:38:15,971 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.006e+18, grad_sumsq=3.712e+18, orig_rms_sq=1.887e+00
2024-10-08 20:38:22,904 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=1170.0, ans=0.44515625000000003
2024-10-08 20:38:24,102 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=1170.0, ans=0.44515625000000003
2024-10-08 20:38:29,142 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.00 vs. limit=5.585
2024-10-08 20:38:31,262 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=17.49 vs. limit=7.93875
2024-10-08 20:38:31,294 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.55 vs. limit=7.93875
2024-10-08 20:38:38,565 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=1173.3333333333333, ans=0.445
2024-10-08 20:38:41,567 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=27.91 vs. limit=7.94
2024-10-08 20:38:46,983 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=31.85 vs. limit=8.3825
2024-10-08 20:38:47,836 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=1176.6666666666667, ans=0.44484375
2024-10-08 20:38:48,206 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=16.22 vs. limit=7.94125
2024-10-08 20:38:57,614 WARNING [optim.py:503] Scaling gradients by 0.02265719510614872, model_norm_threshold=9279019.0
2024-10-08 20:38:57,752 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.56, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.320e+16, grad_sumsq=4.571e+16, orig_rms_sq=2.039e+00
2024-10-08 20:39:02,530 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=1180.0, ans=0.15575
2024-10-08 20:39:03,529 WARNING [optim.py:503] Scaling gradients by 0.0619264654815197, model_norm_threshold=9279019.0
2024-10-08 20:39:03,669 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.311e+15, grad_sumsq=4.156e+15, orig_rms_sq=2.000e+00
2024-10-08 20:39:08,376 INFO [train.py:1154] Epoch 1, batch 3550, loss[loss=1.225, simple_loss=0.6215, pruned_loss=0.715, ctc_loss=0.9975, over 4792.00 frames. ], tot_loss[loss=1.282, simple_loss=0.6462, pruned_loss=0.7496, ctc_loss=1.044, over 967338.54 frames. ], batch size: 29, lr: 4.28e-02,
2024-10-08 20:39:08,758 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys.whitening_limit, batch_count=1183.3333333333333, ans=3.1775
2024-10-08 20:39:09,426 WARNING [optim.py:503] Scaling gradients by 0.007543659768998623, model_norm_threshold=9279019.0
2024-10-08 20:39:09,565 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.939e+17, grad_sumsq=1.472e+17, orig_rms_sq=1.996e+00
2024-10-08 20:39:10,662 WARNING [optim.py:503] Scaling gradients by 0.05628490820527077, model_norm_threshold=9279019.0
2024-10-08 20:39:10,800 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.058e+15, grad_sumsq=3.458e+15, orig_rms_sq=2.041e+00
2024-10-08 20:39:14,289 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=1183.3333333333333, ans=0.44453125
2024-10-08 20:39:16,342 WARNING [optim.py:503] Scaling gradients by 0.0297023206949234, model_norm_threshold=9279019.0
2024-10-08 20:39:16,481 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.082e+16, grad_sumsq=4.185e+18, orig_rms_sq=9.753e-03
2024-10-08 20:39:19,885 WARNING [optim.py:503] Scaling gradients by 0.0941595509648323, model_norm_threshold=9279019.0
2024-10-08 20:39:20,023 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.213e+15, grad_sumsq=2.327e+17, orig_rms_sq=9.510e-03
2024-10-08 20:39:29,386 WARNING [optim.py:503] Scaling gradients by 0.013786876574158669, model_norm_threshold=9279019.0
2024-10-08 20:39:29,525 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.61, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.753e+17, grad_sumsq=2.972e+19, orig_rms_sq=9.263e-03
2024-10-08 20:39:29,687 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=1186.6666666666667, ans=0.21780000000000002
2024-10-08 20:39:32,649 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=27.37 vs. limit=8.3925
2024-10-08 20:39:34,113 WARNING [optim.py:503] Scaling gradients by 0.008353441953659058, model_norm_threshold=9279019.0
2024-10-08 20:39:34,251 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.719e+17, grad_sumsq=1.047e+17, orig_rms_sq=2.596e+00
2024-10-08 20:39:36,785 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.const_attention_rate, batch_count=1190.0, ans=0.18306250000000002
2024-10-08 20:39:36,969 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten.whitening_limit, batch_count=1190.0, ans=7.94625
2024-10-08 20:39:40,586 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=32.19 vs. limit=7.94625
2024-10-08 20:39:42,388 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1193.3333333333333, ans=0.28806666666666664
2024-10-08 20:39:46,140 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=12.46 vs. limit=7.9475
2024-10-08 20:39:46,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=1193.3333333333333, ans=0.09254166666666667
2024-10-08 20:39:50,190 WARNING [optim.py:503] Scaling gradients by 0.054671771824359894, model_norm_threshold=9279019.0
2024-10-08 20:39:50,327 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.709e+15, grad_sumsq=6.997e+17, orig_rms_sq=9.588e-03
2024-10-08 20:39:52,344 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.58 vs. limit=5.298333333333333
2024-10-08 20:39:53,680 WARNING [optim.py:503] Scaling gradients by 0.022501274943351746, model_norm_threshold=9279019.0
2024-10-08 20:39:53,820 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.654e+16, grad_sumsq=3.863e+18, orig_rms_sq=9.461e-03
2024-10-08 20:39:54,903 WARNING [optim.py:503] Scaling gradients by 0.017820484936237335, model_norm_threshold=9279019.0
2024-10-08 20:39:55,043 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.713e+16, grad_sumsq=2.869e+16, orig_rms_sq=1.991e+00
2024-10-08 20:39:56,174 WARNING [optim.py:503] Scaling gradients by 0.04460602626204491, model_norm_threshold=9279019.0
2024-10-08 20:39:56,312 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.263e+16, grad_sumsq=6.343e+15, orig_rms_sq=1.991e+00
2024-10-08 20:39:56,825 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.31 vs. limit=3.1795
2024-10-08 20:40:01,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=1196.6666666666667, ans=0.155125
2024-10-08 20:40:04,362 WARNING [optim.py:503] Scaling gradients by 0.03775477409362793, model_norm_threshold=9279019.0
2024-10-08 20:40:04,500 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.152e+16, grad_sumsq=4.820e+17, orig_rms_sq=4.465e-02
2024-10-08 20:40:05,518 WARNING [optim.py:503] Scaling gradients by 0.07901595532894135, model_norm_threshold=9279019.0
2024-10-08 20:40:05,656 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.579e+15, grad_sumsq=1.230e+15, orig_rms_sq=2.910e+00
2024-10-08 20:40:07,095 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.083e+05 5.556e+06 1.449e+07 4.454e+07 4.499e+09, threshold=2.898e+07, percent-clipped=63.0
2024-10-08 20:40:07,138 INFO [train.py:1154] Epoch 1, batch 3600, loss[loss=1.233, simple_loss=0.6081, pruned_loss=0.7318, ctc_loss=0.9881, over 4946.00 frames. ], tot_loss[loss=1.286, simple_loss=0.6485, pruned_loss=0.7527, ctc_loss=1.047, over 967405.82 frames. ], batch size: 20, lr: 4.27e-02,
2024-10-08 20:40:08,239 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=1200.0, ans=0.44375
2024-10-08 20:40:09,970 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten.whitening_limit, batch_count=1200.0, ans=7.95
2024-10-08 20:40:11,993 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=8.93 vs. limit=5.3
2024-10-08 20:40:31,196 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=1206.6666666666667, ans=0.15475
2024-10-08 20:40:36,960 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.52 vs. limit=3.181
2024-10-08 20:40:40,660 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys.whitening_limit, batch_count=1210.0, ans=3.1814999999999998
2024-10-08 20:40:42,059 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1.whitening_limit, batch_count=1210.0, ans=5.3025
2024-10-08 20:40:49,217 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1210.0, ans=0.2879
2024-10-08 20:40:51,763 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.70 vs. limit=3.182
2024-10-08 20:40:52,102 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=25.09 vs. limit=8.41
2024-10-08 20:40:52,106 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.38 vs. limit=7.955
2024-10-08 20:40:53,768 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=1213.3333333333333, ans=0.09241666666666667
2024-10-08 20:40:57,167 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=1213.3333333333333, ans=0.1545
2024-10-08 20:40:57,178 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=1213.3333333333333, ans=0.2182
2024-10-08 20:41:04,610 INFO [train.py:1154] Epoch 1, batch 3650, loss[loss=1.182, simple_loss=0.5914, pruned_loss=0.6849, ctc_loss=1.008, over 4842.00 frames. ], tot_loss[loss=1.286, simple_loss=0.6481, pruned_loss=0.7524, ctc_loss=1.047, over 967903.82 frames. ], batch size: 31, lr: 4.27e-02,
2024-10-08 20:41:12,557 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=1216.6666666666667, ans=0.7621666666666667
2024-10-08 20:41:16,979 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=1220.0, ans=0.4428125
2024-10-08 20:41:22,423 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=1220.0, ans=0.34750000000000003
2024-10-08 20:41:23,907 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.80 vs. limit=7.9575
2024-10-08 20:41:24,611 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=1220.0, ans=0.7622
2024-10-08 20:41:29,921 WARNING [optim.py:503] Scaling gradients by 0.0036832757759839296, model_norm_threshold=28981882.0
2024-10-08 20:41:30,060 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.61, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.764e+19, grad_sumsq=4.027e+21, orig_rms_sq=9.346e-03
2024-10-08 20:41:31,330 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.skip_rate, batch_count=1223.3333333333333, ans=0.035
2024-10-08 20:41:33,055 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=47.35 vs. limit=7.95875
2024-10-08 20:41:33,729 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=3.221e+01
2024-10-08 20:41:34,841 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=1223.3333333333333, ans=0.21835000000000002
2024-10-08 20:41:39,282 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=1226.6666666666667, ans=0.4425
2024-10-08 20:41:42,947 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=34.65 vs. limit=8.42
2024-10-08 20:41:47,412 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=8.94 vs. limit=7.96
2024-10-08 20:41:50,284 WARNING [optim.py:503] Scaling gradients by 0.0068091354332864285, model_norm_threshold=28981882.0
2024-10-08 20:41:50,422 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.160e+18, grad_sumsq=8.195e+20, orig_rms_sq=9.957e-03
2024-10-08 20:41:55,808 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.89 vs. limit=5.3075
2024-10-08 20:41:58,382 WARNING [optim.py:503] Scaling gradients by 0.08175016194581985, model_norm_threshold=28981882.0
2024-10-08 20:41:58,519 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.317e+16, grad_sumsq=1.354e+16, orig_rms_sq=2.450e+00
2024-10-08 20:42:00,780 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.648e+05 3.609e+06 1.380e+07 4.803e+07 7.869e+09, threshold=2.759e+07, percent-clipped=31.0
2024-10-08 20:42:00,822 INFO [train.py:1154] Epoch 1, batch 3700, loss[loss=1.445, simple_loss=0.7329, pruned_loss=0.8496, ctc_loss=1.145, over 4827.00 frames. ], tot_loss[loss=1.288, simple_loss=0.6487, pruned_loss=0.7535, ctc_loss=1.049, over 967394.99 frames. ], batch size: 24, lr: 4.26e-02,
2024-10-08 20:42:03,212 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1233.3333333333333, ans=0.2876666666666667
2024-10-08 20:42:03,731 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.51 vs. limit=5.616666666666666
2024-10-08 20:42:03,767 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.24 vs. limit=5.616666666666666
2024-10-08 20:42:04,331 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=1233.3333333333333, ans=0.8568333333333333
2024-10-08 20:42:09,973 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=31.79 vs. limit=8.425
2024-10-08 20:42:11,234 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.27 vs. limit=5.618333333333333
2024-10-08 20:42:14,543 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.48 vs. limit=3.1855
2024-10-08 20:42:19,115 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=30.59 vs. limit=8.4275
2024-10-08 20:42:20,988 WARNING [optim.py:503] Scaling gradients by 0.006472774315625429, model_norm_threshold=27593882.0
2024-10-08 20:42:21,126 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.513e+18, grad_sumsq=5.795e+20, orig_rms_sq=9.514e-03
2024-10-08 20:42:23,398 WARNING [optim.py:503] Scaling gradients by 0.06955672800540924, model_norm_threshold=27593882.0
2024-10-08 20:42:23,538 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.71, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.120e+17, grad_sumsq=1.218e+19, orig_rms_sq=9.189e-03
2024-10-08 20:42:25,555 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.59 vs. limit=5.62
2024-10-08 20:42:30,058 WARNING [optim.py:503] Scaling gradients by 0.028803810477256775, model_norm_threshold=27593882.0
2024-10-08 20:42:30,196 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.058e+17, grad_sumsq=2.297e+19, orig_rms_sq=8.958e-03
2024-10-08 20:42:33,479 WARNING [optim.py:503] Scaling gradients by 0.004731994587928057, model_norm_threshold=27593882.0
2024-10-08 20:42:33,618 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.988e+18, grad_sumsq=1.125e+21, orig_rms_sq=8.878e-03
2024-10-08 20:42:38,921 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.35 vs. limit=5.310833333333333
2024-10-08 20:42:46,038 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.63 vs. limit=8.435
2024-10-08 20:42:48,873 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.prob, batch_count=1246.6666666666667, ans=0.44156249999999997
2024-10-08 20:42:50,988 WARNING [optim.py:503] Scaling gradients by 0.04151333495974541, model_norm_threshold=27593882.0
2024-10-08 20:42:51,127 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.458e+17, grad_sumsq=1.711e+19, orig_rms_sq=8.520e-03
2024-10-08 20:42:51,322 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=1246.6666666666667, ans=0.44156249999999997
2024-10-08 20:42:55,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.93 vs. limit=8.435
2024-10-08 20:42:57,145 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=1250.0, ans=0.8562500000000001
2024-10-08 20:42:58,039 WARNING [optim.py:503] Scaling gradients by 0.03909754380583763, model_norm_threshold=27593882.0
2024-10-08 20:42:58,178 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.070e+17, grad_sumsq=4.971e+16, orig_rms_sq=2.153e+00
2024-10-08 20:42:58,223 INFO [train.py:1154] Epoch 1, batch 3750, loss[loss=1.259, simple_loss=0.637, pruned_loss=0.7454, ctc_loss=0.975, over 4959.00 frames. ], tot_loss[loss=1.281, simple_loss=0.646, pruned_loss=0.7494, ctc_loss=1.045, over 967724.70 frames. ], batch size: 19, lr: 4.26e-02,
2024-10-08 20:43:00,017 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=29.54 vs. limit=8.4375
2024-10-08 20:43:02,869 WARNING [optim.py:503] Scaling gradients by 0.009699703194200993, model_norm_threshold=27593882.0
2024-10-08 20:43:03,007 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.534e+18, grad_sumsq=1.889e+20, orig_rms_sq=8.121e-03
2024-10-08 20:43:05,524 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1250.0, ans=0.2875
2024-10-08 20:43:06,471 WARNING [optim.py:503] Scaling gradients by 0.08541582524776459, model_norm_threshold=27593882.0
2024-10-08 20:43:06,609 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.502e+16, grad_sumsq=5.489e+17, orig_rms_sq=4.559e-02
2024-10-08 20:43:13,290 WARNING [optim.py:503] Scaling gradients by 0.05913769081234932, model_norm_threshold=27593882.0
2024-10-08 20:43:13,430 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.976e+16, grad_sumsq=1.930e+16, orig_rms_sq=2.577e+00
2024-10-08 20:43:17,991 WARNING [optim.py:503] Scaling gradients by 0.014963410794734955, model_norm_threshold=27593882.0
2024-10-08 20:43:18,132 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.383e+18, grad_sumsq=1.738e+20, orig_rms_sq=7.957e-03
2024-10-08 20:43:20,448 WARNING [optim.py:503] Scaling gradients by 0.0015892446972429752, model_norm_threshold=27593882.0
2024-10-08 20:43:20,586 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.249e+20, grad_sumsq=2.719e+21, orig_rms_sq=4.594e-02
2024-10-08 20:43:37,529 WARNING [optim.py:503] Scaling gradients by 0.01985316537320614, model_norm_threshold=27593882.0
2024-10-08 20:43:37,666 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.915e+17, grad_sumsq=7.571e+19, orig_rms_sq=7.813e-03
2024-10-08 20:43:39,495 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=31.38 vs. limit=7.9725
2024-10-08 20:43:40,790 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=45.82 vs. limit=7.9725
2024-10-08 20:43:41,192 WARNING [optim.py:503] Scaling gradients by 0.05714397877454758, model_norm_threshold=27593882.0
2024-10-08 20:43:41,331 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.782e+16, grad_sumsq=3.558e+16, orig_rms_sq=2.469e+00
2024-10-08 20:43:41,509 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=1260.0, ans=0.4409375
2024-10-08 20:43:51,790 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=18.31 vs. limit=7.97375
2024-10-08 20:43:55,817 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.68 vs. limit=8.45
2024-10-08 20:43:56,319 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.237e+05 4.892e+06 1.831e+07 8.449e+07 1.736e+10, threshold=3.661e+07, percent-clipped=40.0
2024-10-08 20:43:56,360 INFO [train.py:1154] Epoch 1, batch 3800, loss[loss=1.3, simple_loss=0.6662, pruned_loss=0.7585, ctc_loss=1.045, over 4760.00 frames. ], tot_loss[loss=1.286, simple_loss=0.6471, pruned_loss=0.7531, ctc_loss=1.046, over 967570.03 frames. ], batch size: 26, lr: 4.25e-02,
2024-10-08 20:44:06,336 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=25.39 vs. limit=8.45
2024-10-08 20:44:07,259 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=28.02 vs. limit=8.4525
2024-10-08 20:44:20,429 WARNING [optim.py:503] Scaling gradients by 0.04571237042546272, model_norm_threshold=36611940.0
2024-10-08 20:44:20,567 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.231e+17, grad_sumsq=5.058e+18, orig_rms_sq=4.411e-02
2024-10-08 20:44:24,766 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.29 vs. limit=5.636666666666667
2024-10-08 20:44:25,434 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.ff2_skip_rate, batch_count=1273.3333333333333, ans=0.07135
2024-10-08 20:44:26,656 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=19.38 vs. limit=7.9775
2024-10-08 20:44:30,779 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=1276.6666666666667, ans=0.44015625
2024-10-08 20:44:34,568 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.76 vs. limit=5.319166666666667
2024-10-08 20:44:35,375 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=1276.6666666666667, ans=0.21915
2024-10-08 20:44:42,206 WARNING [optim.py:503] Scaling gradients by 0.032551608979701996, model_norm_threshold=36611940.0
2024-10-08 20:44:42,355 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.719e+17, grad_sumsq=1.156e+17, orig_rms_sq=2.352e+00
2024-10-08 20:44:44,829 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1280.0, ans=0.2872
2024-10-08 20:44:47,334 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.17 vs. limit=8.46
2024-10-08 20:44:49,465 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=26.49 vs. limit=8.46
2024-10-08 20:44:50,546 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=14.95 vs. limit=7.98
2024-10-08 20:44:52,787 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.43 vs. limit=5.641666666666667
2024-10-08 20:44:53,337 INFO [train.py:1154] Epoch 1, batch 3850, loss[loss=1.369, simple_loss=0.6963, pruned_loss=0.796, ctc_loss=1.125, over 4830.00 frames. ], tot_loss[loss=1.291, simple_loss=0.6485, pruned_loss=0.7574, ctc_loss=1.047, over 967529.49 frames. ], batch size: 38, lr: 4.24e-02,
2024-10-08 20:44:55,032 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=4.65 vs. limit=4.513333333333334
2024-10-08 20:44:55,751 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=1283.3333333333333, ans=0.43984375
2024-10-08 20:44:57,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.72 vs. limit=8.4625
2024-10-08 20:44:58,284 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=17.51 vs. limit=7.98125
2024-10-08 20:45:09,141 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=21.65 vs. limit=7.9825
2024-10-08 20:45:10,397 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=27.33 vs. limit=8.465
2024-10-08 20:45:12,047 WARNING [optim.py:503] Scaling gradients by 0.09041085839271545, model_norm_threshold=36611940.0
2024-10-08 20:45:12,187 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.913e+16, grad_sumsq=2.163e+16, orig_rms_sq=2.271e+00
2024-10-08 20:45:12,440 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=1286.6666666666667, ans=0.15175
2024-10-08 20:45:14,599 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.min_abs, batch_count=1290.0, ans=0.21935000000000002
2024-10-08 20:45:24,244 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=9.22 vs. limit=8.4675
2024-10-08 20:45:30,177 WARNING [optim.py:503] Scaling gradients by 0.023811547085642815, model_norm_threshold=36611940.0
2024-10-08 20:45:30,315 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.141e+18, grad_sumsq=1.515e+20, orig_rms_sq=7.531e-03
2024-10-08 20:45:31,421 WARNING [optim.py:503] Scaling gradients by 0.02746710553765297, model_norm_threshold=36611940.0
2024-10-08 20:45:31,559 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.212e+17, grad_sumsq=6.774e+19, orig_rms_sq=7.694e-03
2024-10-08 20:45:34,277 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=10.09 vs. limit=7.985
2024-10-08 20:45:35,937 WARNING [optim.py:503] Scaling gradients by 0.09440585970878601, model_norm_threshold=36611940.0
2024-10-08 20:45:36,075 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.688e+16, grad_sumsq=9.687e+18, orig_rms_sq=7.937e-03
2024-10-08 20:45:44,618 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.37 vs. limit=5.648333333333333
2024-10-08 20:45:49,774 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.905e+05 1.005e+07 3.105e+07 9.336e+07 1.538e+09, threshold=6.210e+07, percent-clipped=43.0
2024-10-08 20:45:49,814 INFO [train.py:1154] Epoch 1, batch 3900, loss[loss=1.251, simple_loss=0.6229, pruned_loss=0.7304, ctc_loss=1.046, over 4743.00 frames. ], tot_loss[loss=1.29, simple_loss=0.6486, pruned_loss=0.7564, ctc_loss=1.048, over 967015.45 frames. ], batch size: 26, lr: 4.24e-02,
2024-10-08 20:45:51,055 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=1300.0, ans=0.4390625
2024-10-08 20:45:51,679 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=17.86 vs. limit=7.9875
2024-10-08 20:45:54,464 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1300.0, ans=0.287
2024-10-08 20:45:57,962 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=1300.0, ans=0.8545
2024-10-08 20:45:58,846 WARNING [optim.py:503] Scaling gradients by 0.0414559543132782, model_norm_threshold=62102156.0
2024-10-08 20:45:58,983 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.859e+17, grad_sumsq=7.640e+19, orig_rms_sq=8.978e-03
2024-10-08 20:46:01,414 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.skip_rate, batch_count=1303.3333333333333, ans=0.5
2024-10-08 20:46:06,268 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.07 vs. limit=5.325833333333334
2024-10-08 20:46:14,515 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.16 vs. limit=5.653333333333333
2024-10-08 20:46:30,440 WARNING [optim.py:503] Scaling gradients by 0.014209296554327011, model_norm_threshold=62102156.0
2024-10-08 20:46:30,578 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.061e+18, grad_sumsq=8.178e+20, orig_rms_sq=8.634e-03
2024-10-08 20:46:30,750 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=1310.0, ans=0.43859375
2024-10-08 20:46:34,838 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=18.11 vs. limit=7.99125
2024-10-08 20:46:47,505 INFO [train.py:1154] Epoch 1, batch 3950, loss[loss=1.196, simple_loss=0.6099, pruned_loss=0.6887, ctc_loss=1.013, over 4839.00 frames. ], tot_loss[loss=1.287, simple_loss=0.6475, pruned_loss=0.7547, ctc_loss=1.045, over 967247.74 frames. ], batch size: 36, lr: 4.23e-02,
2024-10-08 20:46:47,917 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=12.21 vs. limit=4.526666666666666
2024-10-08 20:46:48,600 WARNING [optim.py:503] Scaling gradients by 0.031237507238984108, model_norm_threshold=62102156.0
2024-10-08 20:46:48,739 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.333e+17, grad_sumsq=2.087e+19, orig_rms_sq=3.994e-02
2024-10-08 20:46:48,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.max_abs, batch_count=1316.6666666666667, ans=5.822916666666667
2024-10-08 20:46:53,571 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=50.95 vs. limit=7.99375
2024-10-08 20:46:55,318 WARNING [optim.py:503] Scaling gradients by 0.012477672658860683, model_norm_threshold=62102156.0
2024-10-08 20:46:55,456 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.564e+18, grad_sumsq=2.871e+18, orig_rms_sq=1.938e+00
2024-10-08 20:46:56,515 WARNING [optim.py:503] Scaling gradients by 0.05342299863696098, model_norm_threshold=62102156.0
2024-10-08 20:46:56,654 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.893e+17, grad_sumsq=7.270e+18, orig_rms_sq=3.980e-02
2024-10-08 20:47:11,036 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.ff2_skip_rate, batch_count=1323.3333333333333, ans=0.07022500000000001
2024-10-08 20:47:31,358 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=1326.6666666666667, ans=0.4378125
2024-10-08 20:47:35,922 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=1330.0, ans=0.43765624999999997
2024-10-08 20:47:38,246 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=30.19 vs. limit=8.4975
2024-10-08 20:47:40,518 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=39.51 vs. limit=7.99875
2024-10-08 20:47:40,633 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.38 vs. limit=5.3325
2024-10-08 20:47:42,405 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=3.708e+00
2024-10-08 20:47:42,874 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=3.96 vs. limit=4.532
2024-10-08 20:47:43,702 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_True_ctc_True_attdecoder_False_streaming_True/checkpoint-4000.pt
2024-10-08 20:47:45,974 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.129e+05 1.353e+07 3.766e+07 1.116e+08 4.977e+09, threshold=7.531e+07, percent-clipped=39.0
2024-10-08 20:47:46,018 INFO [train.py:1154] Epoch 1, batch 4000, loss[loss=1.206, simple_loss=0.6057, pruned_loss=0.7109, ctc_loss=0.9614, over 4815.00 frames. ], tot_loss[loss=1.287, simple_loss=0.648, pruned_loss=0.7537, ctc_loss=1.045, over 967198.32 frames. ], batch size: 19, lr: 4.23e-02,
2024-10-08 20:47:49,259 WARNING [optim.py:503] Scaling gradients by 0.011768346652388573, model_norm_threshold=75312944.0
2024-10-08 20:47:49,397 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.398e+19, grad_sumsq=1.657e+21, orig_rms_sq=8.434e-03
2024-10-08 20:47:50,697 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=1333.3333333333333, ans=0.2866666666666667
2024-10-08 20:47:54,826 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=17.85 vs. limit=8.0
2024-10-08 20:47:55,620 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=6.24 vs. limit=4.533333333333333
2024-10-08 20:48:03,486 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=1336.6666666666667, ans=0.43734375000000003
2024-10-08 20:48:07,602 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.50 vs. limit=5.668333333333333
2024-10-08 20:48:07,894 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=1340.0, ans=0.4371875
2024-10-08 20:48:12,508 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=1340.0, ans=0.14975
2024-10-08 20:48:14,640 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer2.prob, batch_count=1340.0, ans=0.4371875
2024-10-08 20:48:20,991 WARNING [optim.py:503] Scaling gradients by 0.03668135032057762, model_norm_threshold=75312944.0
2024-10-08 20:48:21,133 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.68, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.853e+18, grad_sumsq=3.383e+20, orig_rms_sq=8.432e-03
2024-10-08 20:48:22,245 WARNING [optim.py:503] Scaling gradients by 0.011238165199756622, model_norm_threshold=75312944.0
2024-10-08 20:48:22,386 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.872e+19, grad_sumsq=2.237e+21, orig_rms_sq=8.369e-03
2024-10-08 20:48:23,889 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=10.10 vs. limit=5.335833333333333
2024-10-08 20:48:28,031 WARNING [optim.py:503] Scaling gradients by 0.02197875641286373, model_norm_threshold=75312944.0
2024-10-08 20:48:28,171 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.565e+18, grad_sumsq=3.093e+20, orig_rms_sq=8.292e-03
2024-10-08 20:48:28,318 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=1343.3333333333333, ans=0.8529833333333333
2024-10-08 20:48:30,648 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=1346.6666666666667, ans=0.04579166666666667
2024-10-08 20:48:37,039 WARNING [optim.py:503] Scaling gradients by 0.04304877296090126, model_norm_threshold=75312944.0
2024-10-08 20:48:37,180 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.007e+18, grad_sumsq=4.632e+17, orig_rms_sq=2.174e+00
2024-10-08 20:48:38,243 WARNING [optim.py:503] Scaling gradients by 0.03941454738378525, model_norm_threshold=75312944.0
2024-10-08 20:48:38,382 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.893e+17, grad_sumsq=3.631e+17, orig_rms_sq=2.174e+00
2024-10-08 20:48:42,832 WARNING [optim.py:503] Scaling gradients by 0.031098434701561928, model_norm_threshold=75312944.0
2024-10-08 20:48:42,973 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.070e+18, grad_sumsq=4.836e+17, orig_rms_sq=2.212e+00
2024-10-08 20:48:43,019 INFO [train.py:1154] Epoch 1, batch 4050, loss[loss=1.461, simple_loss=0.7478, pruned_loss=0.8447, ctc_loss=1.211, over 4777.00 frames. ], tot_loss[loss=1.292, simple_loss=0.6512, pruned_loss=0.7566, ctc_loss=1.049, over 967549.96 frames. ], batch size: 53, lr: 4.22e-02,
2024-10-08 20:48:44,448 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=256, metric=8.07 vs. limit=8.00625
2024-10-08 20:48:46,255 WARNING [optim.py:503] Scaling gradients by 0.016923515126109123, model_norm_threshold=75312944.0
2024-10-08 20:48:46,394 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.799e+18, grad_sumsq=6.818e+20, orig_rms_sq=8.506e-03
2024-10-08 20:48:48,125 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=23.84 vs. limit=8.5125
2024-10-08 20:48:53,357 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=1353.3333333333333, ans=0.14925
2024-10-08 20:48:53,666 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=30.21 vs. limit=8.515
2024-10-08 20:48:58,926 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=1353.3333333333333, ans=0.4365625
2024-10-08 20:49:00,745 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.19 vs. limit=3.203
2024-10-08 20:49:01,594 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=15.47 vs. limit=8.0075
2024-10-08 20:49:01,904 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=16.53 vs. limit=8.515
2024-10-08 20:49:08,424 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.77 vs. limit=5.678333333333334
2024-10-08 20:49:09,595 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.07 vs. limit=8.00875
2024-10-08 20:49:09,939 WARNING [optim.py:503] Scaling gradients by 0.09323406964540482, model_norm_threshold=75312944.0
2024-10-08 20:49:10,080 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.704e+17, grad_sumsq=7.236e+16, orig_rms_sq=2.355e+00
2024-10-08 20:49:19,655 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.80 vs. limit=8.52
2024-10-08 20:49:20,892 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.52 vs. limit=8.52
2024-10-08 20:49:22,694 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=11.71 vs. limit=4.5440000000000005
2024-10-08 20:49:25,571 WARNING [optim.py:503] Scaling gradients by 0.046398039907217026, model_norm_threshold=75312944.0
2024-10-08 20:49:25,709 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.092e+18, grad_sumsq=4.698e+17, orig_rms_sq=2.325e+00
2024-10-08 20:49:26,360 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.06 vs. limit=5.34
2024-10-08 20:49:28,182 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=1363.3333333333333, ans=0.43609375
2024-10-08 20:49:31,208 WARNING [optim.py:503] Scaling gradients by 0.004947026260197163, model_norm_threshold=75312944.0
2024-10-08 20:49:31,348 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.452e+19, grad_sumsq=2.792e+19, orig_rms_sq=2.311e+00
2024-10-08 20:49:34,325 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.37 vs. limit=8.01125
2024-10-08 20:49:39,399 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.718e+06 2.303e+07 8.228e+07 3.391e+08 1.522e+10, threshold=1.646e+08, percent-clipped=52.0
2024-10-08 20:49:39,439 INFO [train.py:1154] Epoch 1, batch 4100, loss[loss=1.167, simple_loss=0.5827, pruned_loss=0.678, ctc_loss=0.9879, over 4861.00 frames. ], tot_loss[loss=1.295, simple_loss=0.6533, pruned_loss=0.7585, ctc_loss=1.051, over 966936.24 frames. ], batch size: 31, lr: 4.22e-02,
2024-10-08 20:49:42,830 WARNING [optim.py:503] Scaling gradients by 0.024418052285909653, model_norm_threshold=164566000.0
2024-10-08 20:49:42,970 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.267e+19, grad_sumsq=2.584e+21, orig_rms_sq=8.774e-03
2024-10-08 20:49:45,381 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1366.6666666666667, ans=0.28633333333333333
2024-10-08 20:49:59,946 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=1370.0, ans=0.069175
2024-10-08 20:50:03,395 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=1373.3333333333333, ans=5.858333333333333
2024-10-08 20:50:06,066 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.23 vs. limit=5.343333333333334
2024-10-08 20:50:18,780 WARNING [optim.py:503] Scaling gradients by 0.04904657602310181, model_norm_threshold=164566000.0
2024-10-08 20:50:18,920 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.825e+18, grad_sumsq=5.730e+20, orig_rms_sq=8.420e-03
2024-10-08 20:50:20,219 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=1376.6666666666667, ans=0.8518166666666667
2024-10-08 20:50:22,799 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=14.33 vs. limit=5.6883333333333335
2024-10-08 20:50:23,138 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=16.93 vs. limit=8.5325
2024-10-08 20:50:29,968 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.13 vs. limit=8.535
2024-10-08 20:50:31,408 WARNING [optim.py:503] Scaling gradients by 0.0034344056621193886, model_norm_threshold=164566000.0
2024-10-08 20:50:31,548 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.079e+20, grad_sumsq=8.410e+22, orig_rms_sq=8.417e-03
2024-10-08 20:50:36,025 INFO [train.py:1154] Epoch 1, batch 4150, loss[loss=1.313, simple_loss=0.641, pruned_loss=0.7779, ctc_loss=1.075, over 4749.00 frames. ], tot_loss[loss=1.293, simple_loss=0.6513, pruned_loss=0.7575, ctc_loss=1.047, over 967084.83 frames. ], batch size: 20, lr: 4.21e-02,
2024-10-08 20:50:40,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=1383.3333333333333, ans=0.43515625
2024-10-08 20:50:40,824 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=22.05 vs. limit=8.5375
2024-10-08 20:50:43,206 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=34.21 vs. limit=8.5375
2024-10-08 20:50:44,915 WARNING [optim.py:503] Scaling gradients by 0.00921182706952095, model_norm_threshold=164566000.0
2024-10-08 20:50:45,055 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.085e+20, grad_sumsq=4.697e+19, orig_rms_sq=2.311e+00
2024-10-08 20:50:48,938 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten.whitening_limit, batch_count=1386.6666666666667, ans=8.02
2024-10-08 20:50:50,244 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten.whitening_limit, batch_count=1386.6666666666667, ans=8.02
2024-10-08 20:50:54,671 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=29.34 vs. limit=8.54
2024-10-08 20:50:55,146 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.33 vs. limit=8.54
2024-10-08 20:50:55,147 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn2.whiten.whitening_limit, batch_count=1386.6666666666667, ans=8.54
2024-10-08 20:50:57,087 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=39.08 vs. limit=8.02
2024-10-08 20:50:57,178 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.82 vs. limit=8.54
2024-10-08 20:50:58,616 WARNING [optim.py:503] Scaling gradients by 0.009591083973646164, model_norm_threshold=164566000.0
2024-10-08 20:50:58,756 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.206e+20, grad_sumsq=1.493e+22, orig_rms_sq=8.074e-03
2024-10-08 20:51:01,016 WARNING [optim.py:503] Scaling gradients by 0.02329641580581665, model_norm_threshold=164566000.0
2024-10-08 20:51:01,156 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.300e+19, grad_sumsq=1.574e+21, orig_rms_sq=8.261e-03
2024-10-08 20:51:05,721 WARNING [optim.py:503] Scaling gradients by 0.008589471690356731, model_norm_threshold=164566000.0
2024-10-08 20:51:05,860 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.714e+19, grad_sumsq=3.267e+19, orig_rms_sq=2.361e+00
2024-10-08 20:51:06,956 WARNING [optim.py:503] Scaling gradients by 0.058593206107616425, model_norm_threshold=164566000.0
2024-10-08 20:51:07,096 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.894e+18, grad_sumsq=1.226e+18, orig_rms_sq=2.361e+00
2024-10-08 20:51:08,152 WARNING [optim.py:503] Scaling gradients by 0.009816639125347137, model_norm_threshold=164566000.0
2024-10-08 20:51:08,290 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.024e+19, grad_sumsq=1.060e+22, orig_rms_sq=8.513e-03
2024-10-08 20:51:09,934 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.35 vs. limit=5.3475
2024-10-08 20:51:11,563 WARNING [optim.py:503] Scaling gradients by 0.0545954629778862, model_norm_threshold=164566000.0
2024-10-08 20:51:11,703 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.077e+18, grad_sumsq=2.411e+20, orig_rms_sq=8.615e-03
2024-10-08 20:51:12,704 WARNING [optim.py:503] Scaling gradients by 0.07652121037244797, model_norm_threshold=164566000.0
2024-10-08 20:51:12,844 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.393e+17, grad_sumsq=3.998e+17, orig_rms_sq=2.349e+00
2024-10-08 20:51:16,692 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=13.79 vs. limit=4.557333333333333
2024-10-08 20:51:17,455 WARNING [optim.py:503] Scaling gradients by 0.07276229560375214, model_norm_threshold=164566000.0
2024-10-08 20:51:17,596 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.445e+18, grad_sumsq=6.160e+17, orig_rms_sq=2.346e+00
2024-10-08 20:51:22,092 WARNING [optim.py:503] Scaling gradients by 0.058261532336473465, model_norm_threshold=164566000.0
2024-10-08 20:51:22,242 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.082e+18, grad_sumsq=8.891e+17, orig_rms_sq=2.341e+00
2024-10-08 20:51:23,127 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.78 vs. limit=8.5475
2024-10-08 20:51:34,935 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.607e+06 2.019e+07 5.759e+07 4.769e+08 4.792e+10, threshold=1.152e+08, percent-clipped=33.0
2024-10-08 20:51:34,976 INFO [train.py:1154] Epoch 1, batch 4200, loss[loss=1.284, simple_loss=0.6588, pruned_loss=0.7497, ctc_loss=1.022, over 4858.00 frames. ], tot_loss[loss=1.293, simple_loss=0.6516, pruned_loss=0.7572, ctc_loss=1.049, over 967267.35 frames. ], batch size: 31, lr: 4.20e-02,
2024-10-08 20:51:35,985 WARNING [optim.py:503] Scaling gradients by 0.007437786553055048, model_norm_threshold=115176768.0
2024-10-08 20:51:36,125 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.52, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.236e+20, grad_sumsq=1.457e+22, orig_rms_sq=8.479e-03
2024-10-08 20:51:36,257 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.max_abs, batch_count=1400.0, ans=5.875
2024-10-08 20:51:36,858 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.84 vs. limit=4.5600000000000005
2024-10-08 20:51:41,072 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1400.0, ans=0.286
2024-10-08 20:51:42,944 WARNING [optim.py:503] Scaling gradients by 0.004059900995343924, model_norm_threshold=115176768.0
2024-10-08 20:51:43,083 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.020e+20, grad_sumsq=2.412e+22, orig_rms_sq=8.373e-03
2024-10-08 20:51:43,812 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.70 vs. limit=8.55
2024-10-08 20:51:46,471 WARNING [optim.py:503] Scaling gradients by 0.0660906508564949, model_norm_threshold=115176768.0
2024-10-08 20:51:46,610 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.549e+17, grad_sumsq=3.310e+17, orig_rms_sq=2.281e+00
2024-10-08 20:51:50,550 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=31.93 vs. limit=8.5525
2024-10-08 20:51:53,094 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.57 vs. limit=5.701666666666666
2024-10-08 20:51:53,389 WARNING [optim.py:503] Scaling gradients by 0.03929382935166359, model_norm_threshold=115176768.0
2024-10-08 20:51:53,529 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.774e+18, grad_sumsq=2.200e+20, orig_rms_sq=8.066e-03
2024-10-08 20:51:55,765 WARNING [optim.py:503] Scaling gradients by 0.08243119716644287, model_norm_threshold=115176768.0
2024-10-08 20:51:55,904 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.685e+17, grad_sumsq=5.807e+19, orig_rms_sq=8.066e-03
2024-10-08 20:52:04,670 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=38.62 vs. limit=8.0275
2024-10-08 20:52:05,809 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=25.74 vs. limit=8.0275
2024-10-08 20:52:08,064 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=34.68 vs. limit=8.0275
2024-10-08 20:52:10,852 WARNING [optim.py:503] Scaling gradients by 0.09303146600723267, model_norm_threshold=115176768.0
2024-10-08 20:52:10,992 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.693e+17, grad_sumsq=3.430e+19, orig_rms_sq=7.852e-03
2024-10-08 20:52:12,632 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=7.30 vs. limit=8.02875
2024-10-08 20:52:13,733 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.87 vs. limit=3.2115
2024-10-08 20:52:14,817 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.66 vs. limit=5.705
2024-10-08 20:52:18,107 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=1410.0, ans=0.85065
2024-10-08 20:52:19,029 WARNING [optim.py:503] Scaling gradients by 0.010719886980950832, model_norm_threshold=115176768.0
2024-10-08 20:52:19,168 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.612e+19, grad_sumsq=9.258e+20, orig_rms_sq=3.901e-02
2024-10-08 20:52:23,832 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=1413.3333333333333, ans=0.8505333333333334
2024-10-08 20:52:26,992 WARNING [optim.py:503] Scaling gradients by 0.07679542899131775, model_norm_threshold=115176768.0
2024-10-08 20:52:27,134 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.429e+17, grad_sumsq=6.799e+19, orig_rms_sq=7.986e-03
2024-10-08 20:52:27,528 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=13.70 vs. limit=5.706666666666667
2024-10-08 20:52:32,965 INFO [train.py:1154] Epoch 1, batch 4250, loss[loss=1.334, simple_loss=0.6585, pruned_loss=0.792, ctc_loss=1.065, over 4756.00 frames. ], tot_loss[loss=1.292, simple_loss=0.6508, pruned_loss=0.7569, ctc_loss=1.047, over 967210.84 frames. ], batch size: 19, lr: 4.20e-02,
2024-10-08 20:52:36,368 WARNING [optim.py:503] Scaling gradients by 0.0021356125362217426, model_norm_threshold=115176768.0
2024-10-08 20:52:36,506 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.979e+20, grad_sumsq=8.809e+22, orig_rms_sq=7.923e-03
2024-10-08 20:52:38,829 WARNING [optim.py:503] Scaling gradients by 0.012203077785670757, model_norm_threshold=115176768.0
2024-10-08 20:52:38,970 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.860e+19, grad_sumsq=2.348e+21, orig_rms_sq=7.923e-03
2024-10-08 20:52:39,151 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=1416.6666666666667, ans=0.43359375
2024-10-08 20:52:40,491 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=26.70 vs. limit=8.5625
2024-10-08 20:52:41,185 WARNING [optim.py:503] Scaling gradients by 0.07417824119329453, model_norm_threshold=115176768.0
2024-10-08 20:52:41,324 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.52, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.242e+18, grad_sumsq=1.569e+20, orig_rms_sq=7.918e-03
2024-10-08 20:52:46,462 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.40 vs. limit=5.71
2024-10-08 20:52:46,950 WARNING [optim.py:503] Scaling gradients by 0.026683049276471138, model_norm_threshold=115176768.0
2024-10-08 20:52:47,089 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.984e+18, grad_sumsq=1.723e+18, orig_rms_sq=2.312e+00
2024-10-08 20:52:50,147 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=38.79 vs. limit=8.0325
2024-10-08 20:52:53,469 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.73 vs. limit=8.0325
2024-10-08 20:52:56,108 WARNING [optim.py:503] Scaling gradients by 0.09900875389575958, model_norm_threshold=115176768.0
2024-10-08 20:52:56,248 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.042e+17, grad_sumsq=7.317e+19, orig_rms_sq=8.257e-03
2024-10-08 20:52:59,627 WARNING [optim.py:503] Scaling gradients by 0.08851518481969833, model_norm_threshold=115176768.0
2024-10-08 20:52:59,768 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.073e+17, grad_sumsq=2.080e+17, orig_rms_sq=2.439e+00
2024-10-08 20:53:03,862 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.98 vs. limit=8.567499999999999
2024-10-08 20:53:04,276 WARNING [optim.py:503] Scaling gradients by 0.034786153584718704, model_norm_threshold=115176768.0
2024-10-08 20:53:04,415 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.170e+18, grad_sumsq=1.296e+18, orig_rms_sq=2.446e+00
2024-10-08 20:53:09,047 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer2.min_positive, batch_count=1426.6666666666667, ans=0.09108333333333334
2024-10-08 20:53:11,890 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.83 vs. limit=8.57
2024-10-08 20:53:14,339 WARNING [optim.py:503] Scaling gradients by 0.07854199409484863, model_norm_threshold=115176768.0
2024-10-08 20:53:14,478 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.401e+17, grad_sumsq=5.415e+19, orig_rms_sq=8.127e-03
2024-10-08 20:53:15,690 WARNING [optim.py:503] Scaling gradients by 0.03412962332367897, model_norm_threshold=115176768.0
2024-10-08 20:53:15,829 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.560e+18, grad_sumsq=3.150e+20, orig_rms_sq=8.127e-03
2024-10-08 20:53:16,343 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten.whitening_limit, batch_count=1426.6666666666667, ans=8.57
2024-10-08 20:53:16,910 WARNING [optim.py:503] Scaling gradients by 0.0710558295249939, model_norm_threshold=115176768.0
2024-10-08 20:53:17,050 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.649e+17, grad_sumsq=7.572e+17, orig_rms_sq=7.461e-01
2024-10-08 20:53:20,170 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.89 vs. limit=5.3575
2024-10-08 20:53:25,198 WARNING [optim.py:503] Scaling gradients by 0.046761009842157364, model_norm_threshold=115176768.0
2024-10-08 20:53:25,338 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.240e+18, grad_sumsq=5.191e+17, orig_rms_sq=2.388e+00
2024-10-08 20:53:25,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1430.0, ans=0.2857
2024-10-08 20:53:31,001 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.085e+06 4.454e+07 1.237e+08 6.480e+08 5.393e+10, threshold=2.473e+08, percent-clipped=51.0
2024-10-08 20:53:31,041 INFO [train.py:1154] Epoch 1, batch 4300, loss[loss=1.171, simple_loss=0.5727, pruned_loss=0.6867, ctc_loss=0.9916, over 4849.00 frames. ], tot_loss[loss=1.291, simple_loss=0.6503, pruned_loss=0.7565, ctc_loss=1.049, over 967391.96 frames. ], batch size: 21, lr: 4.19e-02,
2024-10-08 20:53:34,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=1433.3333333333333, ans=0.2856666666666667
2024-10-08 20:53:42,980 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.40 vs. limit=8.5775
2024-10-08 20:53:55,108 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=30.27 vs. limit=8.58
2024-10-08 20:53:57,885 WARNING [optim.py:503] Scaling gradients by 0.05586540326476097, model_norm_threshold=247328320.0
2024-10-08 20:53:58,023 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.52, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.022e+19, grad_sumsq=1.275e+21, orig_rms_sq=8.014e-03
2024-10-08 20:53:58,232 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1440.0, ans=0.28559999999999997
2024-10-08 20:54:07,257 WARNING [optim.py:503] Scaling gradients by 0.045027051120996475, model_norm_threshold=247328320.0
2024-10-08 20:54:07,396 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.522e+19, grad_sumsq=1.904e+21, orig_rms_sq=7.997e-03
2024-10-08 20:54:11,897 WARNING [optim.py:503] Scaling gradients by 0.06726029515266418, model_norm_threshold=247328320.0
2024-10-08 20:54:12,037 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.038e+18, grad_sumsq=6.271e+20, orig_rms_sq=8.034e-03
2024-10-08 20:54:17,574 WARNING [optim.py:503] Scaling gradients by 0.013899545185267925, model_norm_threshold=247328320.0
2024-10-08 20:54:17,712 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.899e+19, grad_sumsq=2.428e+19, orig_rms_sq=2.430e+00
2024-10-08 20:54:20,643 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=30.43 vs. limit=8.0425
2024-10-08 20:54:25,590 WARNING [optim.py:503] Scaling gradients by 0.013308235444128513, model_norm_threshold=247328320.0
2024-10-08 20:54:25,730 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.966e+19, grad_sumsq=1.162e+22, orig_rms_sq=7.719e-03
2024-10-08 20:54:28,193 INFO [train.py:1154] Epoch 1, batch 4350, loss[loss=1.24, simple_loss=0.604, pruned_loss=0.7269, ctc_loss=1.057, over 4851.00 frames. ], tot_loss[loss=1.291, simple_loss=0.6504, pruned_loss=0.7559, ctc_loss=1.049, over 966230.05 frames. ], batch size: 21, lr: 4.19e-02,
2024-10-08 20:54:30,389 WARNING [optim.py:503] Scaling gradients by 0.08952750265598297, model_norm_threshold=247328320.0
2024-10-08 20:54:30,530 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.71, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.394e+18, grad_sumsq=7.024e+20, orig_rms_sq=7.679e-03
2024-10-08 20:54:31,605 WARNING [optim.py:503] Scaling gradients by 0.03788580372929573, model_norm_threshold=247328320.0
2024-10-08 20:54:31,744 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.043e+19, grad_sumsq=2.660e+21, orig_rms_sq=7.679e-03
2024-10-08 20:54:34,663 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.21 vs. limit=5.3625
2024-10-08 20:54:37,006 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.16 vs. limit=5.3625
2024-10-08 20:54:37,073 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.30 vs. limit=8.04375
2024-10-08 20:54:42,986 WARNING [optim.py:503] Scaling gradients by 0.006045145448297262, model_norm_threshold=247328320.0
2024-10-08 20:54:43,127 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.58, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.728e+20, grad_sumsq=1.272e+23, orig_rms_sq=7.647e-03
2024-10-08 20:54:47,473 WARNING [optim.py:503] Scaling gradients by 0.07247211039066315, model_norm_threshold=247328320.0
2024-10-08 20:54:47,612 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.855e+18, grad_sumsq=5.098e+20, orig_rms_sq=7.561e-03
2024-10-08 20:54:49,513 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.61 vs. limit=4.581333333333333
2024-10-08 20:54:51,445 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff2_skip_rate, batch_count=1456.6666666666667, ans=0.06722500000000001
2024-10-08 20:54:53,504 WARNING [optim.py:503] Scaling gradients by 0.06465066969394684, model_norm_threshold=247328320.0
2024-10-08 20:54:53,643 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.804e+18, grad_sumsq=7.520e+20, orig_rms_sq=7.718e-03
2024-10-08 20:54:58,563 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=1456.6666666666667, ans=0.7645666666666666
2024-10-08 20:55:05,040 WARNING [optim.py:503] Scaling gradients by 0.0009213205194100738, model_norm_threshold=247328320.0
2024-10-08 20:55:05,181 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.480e+22, grad_sumsq=5.729e+23, orig_rms_sq=4.328e-02
2024-10-08 20:55:07,522 WARNING [optim.py:503] Scaling gradients by 0.013082590885460377, model_norm_threshold=247328320.0
2024-10-08 20:55:07,662 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.080e+20, grad_sumsq=2.455e+21, orig_rms_sq=4.401e-02
2024-10-08 20:55:09,839 WARNING [optim.py:503] Scaling gradients by 0.09336613118648529, model_norm_threshold=247328320.0
2024-10-08 20:55:09,978 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.922e+18, grad_sumsq=4.368e+19, orig_rms_sq=4.401e-02
2024-10-08 20:55:12,600 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=27.17 vs. limit=5.73
2024-10-08 20:55:13,286 WARNING [optim.py:503] Scaling gradients by 0.03061548061668873, model_norm_threshold=247328320.0
2024-10-08 20:55:13,426 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.816e+19, grad_sumsq=7.767e+18, orig_rms_sq=2.338e+00
2024-10-08 20:55:15,010 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=10.98 vs. limit=8.04875
2024-10-08 20:55:15,787 WARNING [optim.py:503] Scaling gradients by 0.029994383454322815, model_norm_threshold=247328320.0
2024-10-08 20:55:15,927 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.822e+19, grad_sumsq=3.488e+21, orig_rms_sq=8.092e-03
2024-10-08 20:55:19,308 WARNING [optim.py:503] Scaling gradients by 0.014810544438660145, model_norm_threshold=247328320.0
2024-10-08 20:55:19,449 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.293e+20, grad_sumsq=1.593e+22, orig_rms_sq=8.116e-03
2024-10-08 20:55:20,067 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=4.90 vs. limit=8.04875
2024-10-08 20:55:22,798 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=9.21 vs. limit=8.5975
2024-10-08 20:55:25,860 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=1466.6666666666667, ans=0.43125
2024-10-08 20:55:26,781 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.101e+07 6.394e+07 2.383e+08 1.419e+09 2.684e+11, threshold=4.765e+08, percent-clipped=48.0
2024-10-08 20:55:26,822 INFO [train.py:1154] Epoch 1, batch 4400, loss[loss=1.311, simple_loss=0.6642, pruned_loss=0.7745, ctc_loss=1.019, over 4735.00 frames. ], tot_loss[loss=1.293, simple_loss=0.6513, pruned_loss=0.7569, ctc_loss=1.051, over 965771.58 frames. ], batch size: 26, lr: 4.18e-02,
2024-10-08 20:55:28,541 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=45.48 vs. limit=8.05
2024-10-08 20:55:29,937 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=1466.6666666666667, ans=8.6
2024-10-08 20:55:32,283 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.05 vs. limit=8.6
2024-10-08 20:55:37,899 WARNING [optim.py:503] Scaling gradients by 0.09999401122331619, model_norm_threshold=476531200.0
2024-10-08 20:55:38,038 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.341e+18, grad_sumsq=2.932e+18, orig_rms_sq=1.139e+00
2024-10-08 20:55:39,800 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=24.36 vs. limit=8.6025
2024-10-08 20:55:40,855 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=22.69 vs. limit=8.05125
2024-10-08 20:55:42,150 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=26.47 vs. limit=8.6025
2024-10-08 20:55:44,254 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.94 vs. limit=8.6025
2024-10-08 20:55:48,312 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.98 vs. limit=5.3675
2024-10-08 20:55:49,891 WARNING [optim.py:503] Scaling gradients by 0.0063299983739852905, model_norm_threshold=476531200.0
2024-10-08 20:55:50,029 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.020e+21, grad_sumsq=1.340e+23, orig_rms_sq=7.610e-03
2024-10-08 20:55:50,224 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=1473.3333333333333, ans=0.4309375
2024-10-08 20:55:55,571 WARNING [optim.py:503] Scaling gradients by 0.03957580402493477, model_norm_threshold=476531200.0
2024-10-08 20:55:55,710 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.575e+19, grad_sumsq=2.827e+19, orig_rms_sq=2.326e+00
2024-10-08 20:56:06,723 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=15.78 vs. limit=5.738333333333333
2024-10-08 20:56:06,790 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=39.18 vs. limit=8.6075
2024-10-08 20:56:10,677 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=8.669e+05
2024-10-08 20:56:18,701 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=1480.0, ans=0.8482000000000001
2024-10-08 20:56:19,140 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=46.64 vs. limit=8.055
2024-10-08 20:56:20,790 WARNING [optim.py:503] Scaling gradients by 0.06624766439199448, model_norm_threshold=476531200.0
2024-10-08 20:56:20,931 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.532e+19, grad_sumsq=4.089e+21, orig_rms_sq=6.193e-03
2024-10-08 20:56:22,515 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=15.20 vs. limit=5.74
2024-10-08 20:56:24,438 INFO [train.py:1154] Epoch 1, batch 4450, loss[loss=1.231, simple_loss=0.6095, pruned_loss=0.7228, ctc_loss=1.017, over 4883.00 frames. ], tot_loss[loss=1.289, simple_loss=0.6497, pruned_loss=0.7544, ctc_loss=1.05, over 966090.40 frames. ], batch size: 19, lr: 4.17e-02,
2024-10-08 20:56:25,600 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=1483.3333333333333, ans=0.1862791666666667
2024-10-08 20:56:27,995 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1483.3333333333333, ans=0.2851666666666667
2024-10-08 20:56:28,355 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.34 vs. limit=5.370833333333334
2024-10-08 20:56:34,078 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=39.17 vs. limit=8.05625
2024-10-08 20:56:40,337 WARNING [optim.py:503] Scaling gradients by 0.016536861658096313, model_norm_threshold=476531200.0
2024-10-08 20:56:40,477 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.329e+20, grad_sumsq=1.047e+20, orig_rms_sq=2.224e+00
2024-10-08 20:56:40,714 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.scale_min, batch_count=1486.6666666666667, ans=0.8479666666666666
2024-10-08 20:56:41,322 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.81 vs. limit=8.615
2024-10-08 20:56:43,871 WARNING [optim.py:503] Scaling gradients by 0.09322746098041534, model_norm_threshold=476531200.0
2024-10-08 20:56:44,010 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.711e+18, grad_sumsq=3.918e+18, orig_rms_sq=2.224e+00
2024-10-08 20:56:48,809 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1490.0, ans=0.28509999999999996
2024-10-08 20:56:52,106 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.prob, batch_count=1490.0, ans=0.43015625
2024-10-08 20:56:52,824 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.21 vs. limit=5.745
2024-10-08 20:56:59,483 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=18.23 vs. limit=8.06
2024-10-08 20:57:03,315 WARNING [optim.py:503] Scaling gradients by 0.017897866666316986, model_norm_threshold=476531200.0
2024-10-08 20:57:03,454 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.933e+20, grad_sumsq=1.294e+20, orig_rms_sq=2.266e+00
2024-10-08 20:57:04,410 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=7.07 vs. limit=4.298666666666667
2024-10-08 20:57:06,335 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.22 vs. limit=5.373333333333333
2024-10-08 20:57:20,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=24.29 vs. limit=8.0625
2024-10-08 20:57:21,327 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.767e+06 7.470e+07 1.949e+08 8.789e+08 7.528e+10, threshold=3.899e+08, percent-clipped=32.0
2024-10-08 20:57:21,370 INFO [train.py:1154] Epoch 1, batch 4500, loss[loss=1.218, simple_loss=0.6172, pruned_loss=0.7087, ctc_loss=1.002, over 4861.00 frames. ], tot_loss[loss=1.292, simple_loss=0.6516, pruned_loss=0.7555, ctc_loss=1.052, over 966208.89 frames. ], batch size: 28, lr: 4.17e-02,
2024-10-08 20:57:21,973 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.39 vs. limit=5.75
2024-10-08 20:57:23,996 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.89 vs. limit=5.75
2024-10-08 20:57:28,077 WARNING [optim.py:503] Scaling gradients by 0.09545790404081345, model_norm_threshold=389893408.0
2024-10-08 20:57:28,217 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.553e+18, grad_sumsq=7.062e+20, orig_rms_sq=7.863e-03
2024-10-08 20:57:28,377 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=1500.0, ans=0.5
2024-10-08 20:57:28,974 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.06 vs. limit=5.75
2024-10-08 20:57:31,667 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.65 vs. limit=8.625
2024-10-08 20:57:33,428 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=38.65 vs. limit=8.6275
2024-10-08 20:57:36,453 WARNING [optim.py:503] Scaling gradients by 0.04314887523651123, model_norm_threshold=389893408.0
2024-10-08 20:57:36,592 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.569e+19, grad_sumsq=4.606e+21, orig_rms_sq=7.749e-03
2024-10-08 20:57:47,133 WARNING [optim.py:503] Scaling gradients by 0.040094196796417236, model_norm_threshold=389893408.0
2024-10-08 20:57:47,273 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.636e+19, grad_sumsq=5.653e+21, orig_rms_sq=8.201e-03
2024-10-08 20:57:49,707 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1506.6666666666667, ans=0.429375
2024-10-08 20:57:53,968 WARNING [optim.py:503] Scaling gradients by 0.018032703548669815, model_norm_threshold=389893408.0
2024-10-08 20:57:54,106 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.751e+20, grad_sumsq=2.073e+22, orig_rms_sq=8.447e-03
2024-10-08 20:57:55,183 WARNING [optim.py:503] Scaling gradients by 0.08569920063018799, model_norm_threshold=389893408.0
2024-10-08 20:57:55,323 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.375e+18, grad_sumsq=2.290e+18, orig_rms_sq=2.347e+00
2024-10-08 20:57:56,420 WARNING [optim.py:503] Scaling gradients by 0.005560419522225857, model_norm_threshold=389893408.0
2024-10-08 20:57:56,560 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.56, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.729e+21, grad_sumsq=1.163e+21, orig_rms_sq=2.347e+00
2024-10-08 20:57:57,868 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=1510.0, ans=0.066025
2024-10-08 20:57:58,850 WARNING [optim.py:503] Scaling gradients by 0.009605258703231812, model_norm_threshold=389893408.0
2024-10-08 20:57:58,990 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.855e+20, grad_sumsq=2.090e+20, orig_rms_sq=2.323e+00
2024-10-08 20:58:00,325 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1510.0, ans=0.2849
2024-10-08 20:58:02,602 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=1510.0, ans=0.42921875
2024-10-08 20:58:19,315 INFO [train.py:1154] Epoch 1, batch 4550, loss[loss=1.273, simple_loss=0.6347, pruned_loss=0.7473, ctc_loss=1.043, over 4846.00 frames. ], tot_loss[loss=1.289, simple_loss=0.6506, pruned_loss=0.7539, ctc_loss=1.051, over 966014.55 frames. ], batch size: 20, lr: 4.16e-02,
2024-10-08 20:58:22,214 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.01 vs. limit=8.6375
2024-10-08 20:58:24,614 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module2.whiten, num_groups=1, num_channels=192, metric=3.41 vs. limit=8.06875
2024-10-08 20:58:25,260 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=23.95 vs. limit=8.6375
2024-10-08 20:58:28,374 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=1516.6666666666667, ans=0.143125
2024-10-08 20:58:38,106 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=27.62 vs. limit=8.64
2024-10-08 20:58:46,340 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.91 vs. limit=8.07125
2024-10-08 20:58:48,798 WARNING [optim.py:503] Scaling gradients by 0.008662576787173748, model_norm_threshold=389893408.0
2024-10-08 20:58:48,938 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.908e+20, grad_sumsq=2.492e+20, orig_rms_sq=2.371e+00
2024-10-08 20:58:54,269 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.64 vs. limit=5.381666666666667
2024-10-08 20:58:54,862 WARNING [optim.py:503] Scaling gradients by 0.0677066296339035, model_norm_threshold=389893408.0
2024-10-08 20:58:55,002 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.545e+19, grad_sumsq=6.614e+18, orig_rms_sq=2.336e+00
2024-10-08 20:58:56,644 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten.whitening_limit, batch_count=1526.6666666666667, ans=8.0725
2024-10-08 20:59:01,681 WARNING [optim.py:503] Scaling gradients by 0.0021933128591626883, model_norm_threshold=389893408.0
2024-10-08 20:59:01,819 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.873e+21, grad_sumsq=2.986e+21, orig_rms_sq=2.301e+00
2024-10-08 20:59:02,959 WARNING [optim.py:503] Scaling gradients by 0.06727004796266556, model_norm_threshold=389893408.0
2024-10-08 20:59:03,098 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.121e+19, grad_sumsq=4.884e+18, orig_rms_sq=2.295e+00
2024-10-08 20:59:03,309 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=1526.6666666666667, ans=0.4284375
2024-10-08 20:59:03,955 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=6.25 vs. limit=5.381666666666667
2024-10-08 20:59:05,268 WARNING [optim.py:503] Scaling gradients by 0.0016697980463504791, model_norm_threshold=389893408.0
2024-10-08 20:59:05,408 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.501e+22, grad_sumsq=3.816e+23, orig_rms_sq=3.932e-02
2024-10-08 20:59:06,458 WARNING [optim.py:503] Scaling gradients by 0.04963313043117523, model_norm_threshold=389893408.0
2024-10-08 20:59:06,600 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.022e+19, grad_sumsq=6.363e+18, orig_rms_sq=1.606e+00
2024-10-08 20:59:11,553 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=31.81 vs. limit=8.07375
2024-10-08 20:59:11,901 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.43 vs. limit=3.2295
2024-10-08 20:59:12,044 WARNING [optim.py:503] Scaling gradients by 0.015603960491716862, model_norm_threshold=389893408.0
2024-10-08 20:59:12,183 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.584e+20, grad_sumsq=6.591e+19, orig_rms_sq=2.403e+00
2024-10-08 20:59:14,185 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.41 vs. limit=8.6475
2024-10-08 20:59:16,922 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.162e+07 1.949e+08 6.596e+08 2.035e+09 2.335e+11, threshold=1.319e+09, percent-clipped=62.0
2024-10-08 20:59:16,963 INFO [train.py:1154] Epoch 1, batch 4600, loss[loss=1.389, simple_loss=0.7183, pruned_loss=0.8001, ctc_loss=1.148, over 4767.00 frames. ], tot_loss[loss=1.288, simple_loss=0.6502, pruned_loss=0.7531, ctc_loss=1.05, over 966350.88 frames. ], batch size: 45, lr: 4.15e-02,
2024-10-08 20:59:17,099 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass_mid.scale_min, batch_count=1533.3333333333333, ans=0.8463333333333334
2024-10-08 20:59:25,294 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=1533.3333333333333, ans=0.14250000000000002
2024-10-08 20:59:25,713 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.70 vs. limit=8.075
2024-10-08 20:59:25,816 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=14.80 vs. limit=8.075
2024-10-08 20:59:42,954 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.90 vs. limit=8.655
2024-10-08 20:59:47,873 WARNING [optim.py:503] Scaling gradients by 0.0998736172914505, model_norm_threshold=1319258496.0
2024-10-08 20:59:48,011 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.123e+19, grad_sumsq=1.347e+21, orig_rms_sq=3.803e-02
2024-10-08 20:59:56,277 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=1543.3333333333333, ans=0.42765625
2024-10-08 21:00:01,846 WARNING [optim.py:503] Scaling gradients by 0.019771920517086983, model_norm_threshold=1319258496.0
2024-10-08 21:00:01,984 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.363e+21, grad_sumsq=6.180e+20, orig_rms_sq=2.205e+00
2024-10-08 21:00:03,069 WARNING [optim.py:503] Scaling gradients by 0.003322049044072628, model_norm_threshold=1319258496.0
2024-10-08 21:00:03,209 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.401e+22, grad_sumsq=4.429e+24, orig_rms_sq=7.679e-03
2024-10-08 21:00:09,980 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=1546.6666666666667, ans=0.04516666666666667
2024-10-08 21:00:12,450 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=1546.6666666666667, ans=0.7654666666666666
2024-10-08 21:00:14,415 WARNING [optim.py:503] Scaling gradients by 0.037672046571969986, model_norm_threshold=1319258496.0
2024-10-08 21:00:14,562 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.820e+20, grad_sumsq=7.515e+22, orig_rms_sq=7.745e-03
2024-10-08 21:00:14,608 INFO [train.py:1154] Epoch 1, batch 4650, loss[loss=1.357, simple_loss=0.7022, pruned_loss=0.7867, ctc_loss=1.096, over 4824.00 frames. ], tot_loss[loss=1.29, simple_loss=0.6515, pruned_loss=0.7541, ctc_loss=1.052, over 965794.19 frames. ], batch size: 36, lr: 4.15e-02,
2024-10-08 21:00:15,250 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.66 vs. limit=8.08125
2024-10-08 21:00:16,312 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.68 vs. limit=3.2325
2024-10-08 21:00:29,771 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.03 vs. limit=8.665
2024-10-08 21:00:37,259 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=8.84 vs. limit=8.08375
2024-10-08 21:00:38,147 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=1556.6666666666667, ans=0.42703125
2024-10-08 21:00:47,003 WARNING [optim.py:503] Scaling gradients by 0.011048992164433002, model_norm_threshold=1319258496.0
2024-10-08 21:00:47,143 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.516e+21, grad_sumsq=1.477e+21, orig_rms_sq=2.380e+00
2024-10-08 21:00:52,621 WARNING [optim.py:503] Scaling gradients by 0.006699382793158293, model_norm_threshold=1319258496.0
2024-10-08 21:00:52,760 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.832e+22, grad_sumsq=2.351e+24, orig_rms_sq=7.795e-03
2024-10-08 21:00:54,135 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=1560.0, ans=0.7656
2024-10-08 21:00:55,139 WARNING [optim.py:503] Scaling gradients by 0.06607627123594284, model_norm_threshold=1319258496.0
2024-10-08 21:00:55,279 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.456e+20, grad_sumsq=1.905e+22, orig_rms_sq=7.642e-03
2024-10-08 21:00:58,591 WARNING [optim.py:503] Scaling gradients by 0.08267635107040405, model_norm_threshold=1319258496.0
2024-10-08 21:00:58,734 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.73, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.862e+20, grad_sumsq=2.476e+22, orig_rms_sq=7.520e-03
2024-10-08 21:01:11,468 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.prob, batch_count=1566.6666666666667, ans=0.4265625
2024-10-08 21:01:12,436 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.164e+06 8.105e+07 3.243e+08 1.857e+09 3.971e+11, threshold=6.486e+08, percent-clipped=29.0
2024-10-08 21:01:12,479 INFO [train.py:1154] Epoch 1, batch 4700, loss[loss=1.295, simple_loss=0.6495, pruned_loss=0.7681, ctc_loss=1.013, over 4940.00 frames. ], tot_loss[loss=1.291, simple_loss=0.6527, pruned_loss=0.7548, ctc_loss=1.051, over 965687.48 frames. ], batch size: 19, lr: 4.14e-02,
2024-10-08 21:01:13,852 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1566.6666666666667, ans=0.2843333333333333
2024-10-08 21:01:14,999 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=1566.6666666666667, ans=0.14125
2024-10-08 21:01:16,433 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=81.95 vs. limit=8.0875
2024-10-08 21:01:20,124 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.32 vs. limit=5.783333333333333
2024-10-08 21:01:24,961 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=5.04 vs. limit=5.0
2024-10-08 21:01:29,320 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=16.59 vs. limit=5.785
2024-10-08 21:01:31,563 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=41.95 vs. limit=8.08875
2024-10-08 21:01:33,230 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=1570.0, ans=0.141125
2024-10-08 21:01:34,346 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=1573.3333333333333, ans=0.8449333333333333
2024-10-08 21:01:38,277 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=25.77 vs. limit=8.68
2024-10-08 21:01:39,984 WARNING [optim.py:503] Scaling gradients by 0.0478762611746788, model_norm_threshold=648640704.0
2024-10-08 21:01:40,128 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.438e+19, grad_sumsq=8.487e+20, orig_rms_sq=4.051e-02
2024-10-08 21:01:44,689 WARNING [optim.py:503] Scaling gradients by 0.01844935119152069, model_norm_threshold=648640704.0
2024-10-08 21:01:44,829 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.263e+20, grad_sumsq=4.591e+22, orig_rms_sq=7.107e-03
2024-10-08 21:01:45,945 WARNING [optim.py:503] Scaling gradients by 0.06545987725257874, model_norm_threshold=648640704.0
2024-10-08 21:01:46,085 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.041e+19, grad_sumsq=5.535e+20, orig_rms_sq=3.687e-02
2024-10-08 21:01:52,523 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=21.94 vs. limit=8.09125
2024-10-08 21:01:58,699 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1580.0, ans=0.2842
2024-10-08 21:02:01,021 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=1580.0, ans=0.42593749999999997
2024-10-08 21:02:01,830 WARNING [optim.py:503] Scaling gradients by 0.019877728074789047, model_norm_threshold=648640704.0
2024-10-08 21:02:01,969 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.420e+20, grad_sumsq=4.883e+22, orig_rms_sq=7.004e-03
2024-10-08 21:02:02,663 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=31.98 vs. limit=8.0925
2024-10-08 21:02:03,054 WARNING [optim.py:503] Scaling gradients by 0.03398335352540016, model_norm_threshold=648640704.0
2024-10-08 21:02:03,193 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.925e+19, grad_sumsq=9.803e+21, orig_rms_sq=7.064e-03
2024-10-08 21:02:05,435 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=1580.0, ans=0.14075000000000001
2024-10-08 21:02:09,818 WARNING [optim.py:503] Scaling gradients by 0.05170225352048874, model_norm_threshold=648640704.0
2024-10-08 21:02:09,959 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.418e+19, grad_sumsq=9.014e+20, orig_rms_sq=3.792e-02
2024-10-08 21:02:10,005 INFO [train.py:1154] Epoch 1, batch 4750, loss[loss=1.361, simple_loss=0.6954, pruned_loss=0.7858, ctc_loss=1.135, over 4769.00 frames. ], tot_loss[loss=1.295, simple_loss=0.6546, pruned_loss=0.757, ctc_loss=1.051, over 965640.65 frames. ], batch size: 45, lr: 4.14e-02,
2024-10-08 21:02:14,729 WARNING [optim.py:503] Scaling gradients by 0.03524496406316757, model_norm_threshold=648640704.0
2024-10-08 21:02:14,870 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.516e+19, grad_sumsq=1.187e+22, orig_rms_sq=7.174e-03
2024-10-08 21:02:18,792 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=17.54 vs. limit=5.791666666666667
2024-10-08 21:02:23,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.16 vs. limit=5.3966666666666665
2024-10-08 21:02:28,917 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=30.67 vs. limit=8.69
2024-10-08 21:02:30,685 WARNING [optim.py:503] Scaling gradients by 0.06440874934196472, model_norm_threshold=648640704.0
2024-10-08 21:02:30,825 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.354e+19, grad_sumsq=3.158e+21, orig_rms_sq=7.455e-03
2024-10-08 21:02:31,025 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.max_positive, batch_count=1586.6666666666667, ans=0.7658666666666667
2024-10-08 21:02:31,885 WARNING [optim.py:503] Scaling gradients by 0.00027586601208895445, model_norm_threshold=648640704.0
2024-10-08 21:02:32,028 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.397e+24, grad_sumsq=3.673e+25, orig_rms_sq=3.802e-02
2024-10-08 21:02:34,244 WARNING [optim.py:503] Scaling gradients by 0.02309393882751465, model_norm_threshold=648640704.0
2024-10-08 21:02:34,383 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.999e+20, grad_sumsq=2.681e+22, orig_rms_sq=7.455e-03
2024-10-08 21:02:42,006 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten.whitening_limit, batch_count=1590.0, ans=8.692499999999999
2024-10-08 21:02:43,909 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=1593.3333333333333, ans=0.28406666666666663
2024-10-08 21:02:47,841 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=8.46 vs. limit=8.0975
2024-10-08 21:02:50,272 WARNING [optim.py:503] Scaling gradients by 0.016724489629268646, model_norm_threshold=648640704.0
2024-10-08 21:02:50,410 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.938e+20, grad_sumsq=4.007e+22, orig_rms_sq=7.331e-03
2024-10-08 21:02:53,396 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.02 vs. limit=5.398333333333333
2024-10-08 21:02:56,331 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=1596.6666666666667, ans=0.42515625
2024-10-08 21:02:58,826 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=9.32 vs. limit=5.798333333333334
2024-10-08 21:03:00,661 WARNING [optim.py:503] Scaling gradients by 0.06164461374282837, model_norm_threshold=648640704.0
2024-10-08 21:03:00,801 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.440e+19, grad_sumsq=6.604e+20, orig_rms_sq=3.695e-02
2024-10-08 21:03:03,798 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.08 vs. limit=5.399166666666667
2024-10-08 21:03:04,158 WARNING [optim.py:503] Scaling gradients by 0.08787867426872253, model_norm_threshold=648640704.0
2024-10-08 21:03:04,296 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.154e+19, grad_sumsq=2.769e+21, orig_rms_sq=7.781e-03
2024-10-08 21:03:04,831 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.47 vs. limit=3.2395
2024-10-08 21:03:07,468 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=7.00 vs. limit=4.64
2024-10-08 21:03:07,891 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.017e+07 1.961e+08 8.381e+08 2.211e+09 2.351e+12, threshold=1.676e+09, percent-clipped=54.0
2024-10-08 21:03:07,932 INFO [train.py:1154] Epoch 1, batch 4800, loss[loss=1.272, simple_loss=0.6223, pruned_loss=0.7477, ctc_loss=1.064, over 4873.00 frames. ], tot_loss[loss=1.295, simple_loss=0.6533, pruned_loss=0.7582, ctc_loss=1.049, over 965918.29 frames. ], batch size: 22, lr: 4.13e-02,
2024-10-08 21:03:18,247 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=1603.3333333333333, ans=0.06392500000000001
2024-10-08 21:03:18,256 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=1603.3333333333333, ans=0.06392500000000001
2024-10-08 21:03:20,217 WARNING [optim.py:503] Scaling gradients by 0.07459336519241333, model_norm_threshold=1676182656.0
2024-10-08 21:03:20,355 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.269e+20, grad_sumsq=1.631e+22, orig_rms_sq=7.781e-03
2024-10-08 21:03:28,489 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=1603.3333333333333, ans=0.139875
2024-10-08 21:03:31,812 WARNING [optim.py:503] Scaling gradients by 0.001127062481828034, model_norm_threshold=1676182656.0
2024-10-08 21:03:31,949 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.280e+23, grad_sumsq=1.703e+23, orig_rms_sq=2.513e+00
2024-10-08 21:03:33,208 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=1606.6666666666667, ans=0.06384999999999999
2024-10-08 21:03:34,474 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=1606.6666666666667, ans=0.06384999999999999
2024-10-08 21:03:37,169 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=43.12 vs. limit=8.1025
2024-10-08 21:03:38,000 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=1606.6666666666667, ans=6.004166666666666
2024-10-08 21:03:39,276 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=8.21 vs. limit=8.1025
2024-10-08 21:03:42,839 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=34.40 vs. limit=8.10375
2024-10-08 21:03:43,367 WARNING [optim.py:503] Scaling gradients by 0.010087127797305584, model_norm_threshold=1676182656.0
2024-10-08 21:03:43,506 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.637e+21, grad_sumsq=1.481e+23, orig_rms_sq=3.806e-02
2024-10-08 21:03:43,929 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys.whitening_limit, batch_count=1610.0, ans=3.2415
2024-10-08 21:03:44,932 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=1610.0, ans=6.00625
2024-10-08 21:03:49,329 WARNING [optim.py:503] Scaling gradients by 0.016293441876769066, model_norm_threshold=1676182656.0
2024-10-08 21:03:49,467 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.139e+21, grad_sumsq=4.541e+23, orig_rms_sq=6.912e-03
2024-10-08 21:03:50,788 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=1610.0, ans=0.42453125
2024-10-08 21:03:51,638 WARNING [optim.py:503] Scaling gradients by 0.048099350184202194, model_norm_threshold=1676182656.0
2024-10-08 21:03:51,776 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.278e+20, grad_sumsq=1.550e+20, orig_rms_sq=2.115e+00
2024-10-08 21:03:54,083 WARNING [optim.py:503] Scaling gradients by 0.038263678550720215, model_norm_threshold=1676182656.0
2024-10-08 21:03:54,220 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.542e+20, grad_sumsq=9.576e+22, orig_rms_sq=6.832e-03
2024-10-08 21:03:58,247 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=58.45 vs. limit=8.105
2024-10-08 21:04:05,655 INFO [train.py:1154] Epoch 1, batch 4850, loss[loss=1.245, simple_loss=0.6364, pruned_loss=0.7331, ctc_loss=0.9708, over 4848.00 frames. ], tot_loss[loss=1.292, simple_loss=0.6519, pruned_loss=0.7567, ctc_loss=1.047, over 966613.53 frames. ], batch size: 28, lr: 4.12e-02,
2024-10-08 21:04:05,986 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=35.33 vs. limit=8.7125
2024-10-08 21:04:12,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=1616.6666666666667, ans=0.1590625
2024-10-08 21:04:18,682 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.30 vs. limit=8.715
2024-10-08 21:04:19,851 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.95 vs. limit=4.648
2024-10-08 21:04:21,047 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.38 vs. limit=5.8100000000000005
2024-10-08 21:04:21,395 WARNING [optim.py:503] Scaling gradients by 0.0049617961049079895, model_norm_threshold=1676182656.0
2024-10-08 21:04:21,534 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.744e+22, grad_sumsq=8.166e+21, orig_rms_sq=2.135e+00
2024-10-08 21:04:21,739 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=1620.0, ans=0.158875
2024-10-08 21:04:22,181 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=13.98 vs. limit=8.1075
2024-10-08 21:04:34,623 WARNING [optim.py:503] Scaling gradients by 0.0015913377283141017, model_norm_threshold=1676182656.0
2024-10-08 21:04:34,762 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.167e+23, grad_sumsq=5.823e+24, orig_rms_sq=3.721e-02
2024-10-08 21:04:35,824 WARNING [optim.py:503] Scaling gradients by 0.09815625101327896, model_norm_threshold=1676182656.0
2024-10-08 21:04:35,961 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.245e+20, grad_sumsq=1.949e+22, orig_rms_sq=6.389e-03
2024-10-08 21:04:36,763 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.74 vs. limit=4.649333333333333
2024-10-08 21:04:40,086 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=34.79 vs. limit=8.11
2024-10-08 21:04:51,062 WARNING [optim.py:503] Scaling gradients by 0.09646347910165787, model_norm_threshold=1676182656.0
2024-10-08 21:04:51,201 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.160e+20, grad_sumsq=5.727e+19, orig_rms_sq=2.026e+00
2024-10-08 21:04:57,010 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff3_skip_rate, batch_count=1630.0, ans=0.06332499999999999
2024-10-08 21:04:57,911 WARNING [optim.py:503] Scaling gradients by 0.025641078129410744, model_norm_threshold=1676182656.0
2024-10-08 21:04:58,048 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.096e+21, grad_sumsq=3.159e+23, orig_rms_sq=6.635e-03
2024-10-08 21:04:59,161 WARNING [optim.py:503] Scaling gradients by 0.007393403444439173, model_norm_threshold=1676182656.0
2024-10-08 21:04:59,306 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.298e+22, grad_sumsq=1.070e+22, orig_rms_sq=2.147e+00
2024-10-08 21:05:00,385 WARNING [optim.py:503] Scaling gradients by 0.06491675227880478, model_norm_threshold=1676182656.0
2024-10-08 21:05:00,523 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.588e+20, grad_sumsq=3.725e+22, orig_rms_sq=6.946e-03
2024-10-08 21:05:04,057 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.994e+06 2.207e+08 6.924e+08 4.581e+09 1.487e+12, threshold=1.385e+09, percent-clipped=35.0
2024-10-08 21:05:04,097 INFO [train.py:1154] Epoch 1, batch 4900, loss[loss=1.212, simple_loss=0.5946, pruned_loss=0.7151, ctc_loss=0.9966, over 4854.00 frames. ], tot_loss[loss=1.293, simple_loss=0.6519, pruned_loss=0.7577, ctc_loss=1.047, over 967195.10 frames. ], batch size: 21, lr: 4.12e-02,
2024-10-08 21:05:04,192 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.prob, batch_count=1633.3333333333333, ans=0.4234375
2024-10-08 21:05:04,542 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=33.49 vs. limit=8.725
2024-10-08 21:05:05,545 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=25.81 vs. limit=8.725
2024-10-08 21:05:08,146 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.42 vs. limit=8.725
2024-10-08 21:05:09,876 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=1633.3333333333333, ans=0.13875
2024-10-08 21:05:12,921 WARNING [optim.py:503] Scaling gradients by 0.026243755593895912, model_norm_threshold=1384884864.0
2024-10-08 21:05:13,058 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.691e+20, grad_sumsq=1.992e+22, orig_rms_sq=3.359e-02
2024-10-08 21:05:15,093 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.06 vs. limit=8.7275
2024-10-08 21:05:15,710 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.06 vs. limit=3.2455
2024-10-08 21:05:16,344 WARNING [optim.py:503] Scaling gradients by 0.03231285139918327, model_norm_threshold=1384884864.0
2024-10-08 21:05:16,482 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.818e+20, grad_sumsq=3.365e+20, orig_rms_sq=2.026e+00
2024-10-08 21:05:22,057 WARNING [optim.py:503] Scaling gradients by 0.030431589111685753, model_norm_threshold=1384884864.0
2024-10-08 21:05:22,195 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.775e+20, grad_sumsq=2.250e+20, orig_rms_sq=2.122e+00
2024-10-08 21:05:27,794 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.37 vs. limit=8.73
2024-10-08 21:05:32,865 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=1640.0, ans=0.423125
2024-10-08 21:05:33,144 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=130.17 vs. limit=8.115
2024-10-08 21:05:33,437 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.99 vs. limit=8.115
2024-10-08 21:05:36,042 WARNING [optim.py:503] Scaling gradients by 0.07624945789575577, model_norm_threshold=1384884864.0
2024-10-08 21:05:36,180 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.098e+19, grad_sumsq=2.276e+19, orig_rms_sq=2.239e+00
2024-10-08 21:05:37,529 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=1643.3333333333333, ans=0.8424833333333334
2024-10-08 21:05:43,240 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=1643.3333333333333, ans=0.8424833333333334
2024-10-08 21:05:43,493 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=63.97 vs. limit=8.11625
2024-10-08 21:05:48,233 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.18 vs. limit=3.2465
2024-10-08 21:05:52,368 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.54 vs. limit=8.1175
2024-10-08 21:05:54,698 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=21.78 vs. limit=8.1175
2024-10-08 21:05:56,561 WARNING [optim.py:503] Scaling gradients by 0.034980859607458115, model_norm_threshold=1384884864.0
2024-10-08 21:05:56,698 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.886e+20, grad_sumsq=5.557e+22, orig_rms_sq=6.993e-03
2024-10-08 21:06:01,283 INFO [train.py:1154] Epoch 1, batch 4950, loss[loss=1.243, simple_loss=0.6335, pruned_loss=0.7038, ctc_loss=1.115, over 4775.00 frames. ], tot_loss[loss=1.292, simple_loss=0.6525, pruned_loss=0.7562, ctc_loss=1.049, over 966685.70 frames. ], batch size: 53, lr: 4.11e-02,
2024-10-08 21:06:02,024 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.66 vs. limit=5.4125
2024-10-08 21:06:05,997 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=1650.0, ans=0.42265625
2024-10-08 21:06:06,576 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.92 vs. limit=8.7375
2024-10-08 21:06:08,805 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.65 vs. limit=8.7375
2024-10-08 21:06:11,651 WARNING [optim.py:503] Scaling gradients by 0.09518753737211227, model_norm_threshold=1384884864.0
2024-10-08 21:06:11,789 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.088e+19, grad_sumsq=2.014e+19, orig_rms_sq=3.520e+00
2024-10-08 21:06:13,162 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=1653.3333333333333, ans=0.138
2024-10-08 21:06:13,505 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=29.19 vs. limit=8.74
2024-10-08 21:06:15,284 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=1653.3333333333333, ans=0.157
2024-10-08 21:06:15,487 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.57 vs. limit=3.248
2024-10-08 21:06:16,150 WARNING [optim.py:503] Scaling gradients by 0.07580506056547165, model_norm_threshold=1384884864.0
2024-10-08 21:06:16,289 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.615e+20, grad_sumsq=2.194e+22, orig_rms_sq=7.357e-03
2024-10-08 21:06:17,405 WARNING [optim.py:503] Scaling gradients by 0.008734377101063728, model_norm_threshold=1384884864.0
2024-10-08 21:06:17,542 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.371e+21, grad_sumsq=1.526e+23, orig_rms_sq=3.520e-02
2024-10-08 21:06:21,970 WARNING [optim.py:503] Scaling gradients by 0.027127694338560104, model_norm_threshold=1384884864.0
2024-10-08 21:06:22,124 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.791e+20, grad_sumsq=1.668e+22, orig_rms_sq=3.473e-02
2024-10-08 21:06:30,140 WARNING [optim.py:503] Scaling gradients by 0.005953405052423477, model_norm_threshold=1384884864.0
2024-10-08 21:06:30,276 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.811e+22, grad_sumsq=5.260e+23, orig_rms_sq=3.442e-02
2024-10-08 21:06:32,038 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.53 vs. limit=8.7425
2024-10-08 21:06:33,114 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.29 vs. limit=8.7425
2024-10-08 21:06:34,582 WARNING [optim.py:503] Scaling gradients by 0.052997566759586334, model_norm_threshold=1384884864.0
2024-10-08 21:06:34,719 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.281e+20, grad_sumsq=3.548e+19, orig_rms_sq=3.611e+00
2024-10-08 21:06:35,923 WARNING [optim.py:503] Scaling gradients by 0.033000875264406204, model_norm_threshold=1384884864.0
2024-10-08 21:06:36,061 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.354e+20, grad_sumsq=9.005e+21, orig_rms_sq=3.724e-02
2024-10-08 21:06:45,906 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.45 vs. limit=5.83
2024-10-08 21:06:49,818 WARNING [optim.py:503] Scaling gradients by 0.023160383105278015, model_norm_threshold=1384884864.0
2024-10-08 21:06:49,954 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.339e+20, grad_sumsq=1.428e+20, orig_rms_sq=3.739e+00
2024-10-08 21:06:53,068 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=30.32 vs. limit=8.12375
2024-10-08 21:06:54,129 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.18 vs. limit=3.2495
2024-10-08 21:06:56,306 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=10.10 vs. limit=5.4158333333333335
2024-10-08 21:06:56,375 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=18.03 vs. limit=8.12375
2024-10-08 21:06:59,639 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.523e+06 2.917e+08 1.006e+09 6.076e+09 2.326e+11, threshold=2.011e+09, percent-clipped=45.0
2024-10-08 21:06:59,682 INFO [train.py:1154] Epoch 1, batch 5000, loss[loss=1.352, simple_loss=0.6902, pruned_loss=0.7948, ctc_loss=1.062, over 4786.00 frames. ], tot_loss[loss=1.287, simple_loss=0.6496, pruned_loss=0.753, ctc_loss=1.046, over 967691.25 frames. ], batch size: 29, lr: 4.10e-02,
2024-10-08 21:07:01,536 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.69 vs. limit=5.416666666666667
2024-10-08 21:07:01,845 WARNING [optim.py:503] Scaling gradients by 0.011641016229987144, model_norm_threshold=2011472768.0
2024-10-08 21:07:01,984 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.527e+21, grad_sumsq=2.950e+21, orig_rms_sq=2.213e+00
2024-10-08 21:07:03,839 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=36.92 vs. limit=8.125
2024-10-08 21:07:05,740 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=20.80 vs. limit=8.75
2024-10-08 21:07:07,566 WARNING [optim.py:503] Scaling gradients by 0.028907299041748047, model_norm_threshold=2011472768.0
2024-10-08 21:07:07,705 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.404e+21, grad_sumsq=4.290e+22, orig_rms_sq=3.273e-02
2024-10-08 21:07:08,755 WARNING [optim.py:503] Scaling gradients by 0.037746720016002655, model_norm_threshold=2011472768.0
2024-10-08 21:07:08,893 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.315e+20, grad_sumsq=2.231e+22, orig_rms_sq=3.279e-02
2024-10-08 21:07:10,237 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.prob, batch_count=1670.0, ans=0.42171875000000003
2024-10-08 21:07:10,245 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=1670.0, ans=0.42171875000000003
2024-10-08 21:07:14,659 WARNING [optim.py:503] Scaling gradients by 0.00101780379191041, model_norm_threshold=2011472768.0
2024-10-08 21:07:14,797 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.292e+24, grad_sumsq=3.898e+25, orig_rms_sq=3.315e-02
2024-10-08 21:07:15,930 WARNING [optim.py:503] Scaling gradients by 0.05294502526521683, model_norm_threshold=2011472768.0
2024-10-08 21:07:16,069 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.192e+20, grad_sumsq=8.665e+22, orig_rms_sq=7.146e-03
2024-10-08 21:07:17,842 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.45 vs. limit=3.2505
2024-10-08 21:07:20,819 WARNING [optim.py:503] Scaling gradients by 0.0050625004805624485, model_norm_threshold=2011472768.0
2024-10-08 21:07:20,957 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.473e+22, grad_sumsq=2.179e+22, orig_rms_sq=2.052e+00
2024-10-08 21:07:23,037 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=5.79 vs. limit=4.669333333333333
2024-10-08 21:07:23,655 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=1673.3333333333333, ans=0.044770833333333336
2024-10-08 21:07:24,223 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=5.85 vs. limit=4.669333333333333
2024-10-08 21:07:25,797 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.balancer.min_positive, batch_count=1673.3333333333333, ans=0.23326666666666668
2024-10-08 21:07:30,194 WARNING [optim.py:503] Scaling gradients by 0.036990080028772354, model_norm_threshold=2011472768.0
2024-10-08 21:07:30,331 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.044e+21, grad_sumsq=5.084e+20, orig_rms_sq=2.054e+00
2024-10-08 21:07:30,926 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=57.90 vs. limit=8.1275
2024-10-08 21:07:32,401 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=11.99 vs. limit=8.1275
2024-10-08 21:07:38,262 WARNING [optim.py:503] Scaling gradients by 0.017893606796860695, model_norm_threshold=2011472768.0
2024-10-08 21:07:38,399 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.52, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.533e+21, grad_sumsq=9.223e+23, orig_rms_sq=7.084e-03
2024-10-08 21:07:38,551 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=1676.6666666666667, ans=0.8413166666666667
2024-10-08 21:07:43,592 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=28.34 vs. limit=8.12875
2024-10-08 21:07:45,394 WARNING [optim.py:503] Scaling gradients by 0.0027474630624055862, model_norm_threshold=2011472768.0
2024-10-08 21:07:45,532 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.426e+23, grad_sumsq=6.935e+22, orig_rms_sq=2.056e+00
2024-10-08 21:07:50,151 WARNING [optim.py:503] Scaling gradients by 0.016169030219316483, model_norm_threshold=2011472768.0
2024-10-08 21:07:50,290 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.580e+21, grad_sumsq=3.384e+23, orig_rms_sq=7.625e-03
2024-10-08 21:07:51,097 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.19 vs. limit=8.76
2024-10-08 21:07:52,784 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=1680.0, ans=0.8412000000000001
2024-10-08 21:07:54,844 WARNING [optim.py:503] Scaling gradients by 0.08963946998119354, model_norm_threshold=2011472768.0
2024-10-08 21:07:54,984 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.244e+20, grad_sumsq=1.600e+22, orig_rms_sq=7.773e-03
2024-10-08 21:07:56,795 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=43.29 vs. limit=8.13
2024-10-08 21:07:57,312 WARNING [optim.py:503] Scaling gradients by 0.049027733504772186, model_norm_threshold=2011472768.0
2024-10-08 21:07:57,450 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.315e+20, grad_sumsq=1.103e+20, orig_rms_sq=3.912e+00
2024-10-08 21:07:58,728 INFO [train.py:1154] Epoch 1, batch 5050, loss[loss=1.237, simple_loss=0.6126, pruned_loss=0.7313, ctc_loss=0.9977, over 4854.00 frames. ], tot_loss[loss=1.286, simple_loss=0.6485, pruned_loss=0.7534, ctc_loss=1.044, over 968668.79 frames. ], batch size: 19, lr: 4.10e-02,
2024-10-08 21:08:01,631 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.26 vs. limit=3.2525
2024-10-08 21:08:02,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.23 vs. limit=5.420833333333333
2024-10-08 21:08:05,466 WARNING [optim.py:503] Scaling gradients by 0.009156820364296436, model_norm_threshold=2011472768.0
2024-10-08 21:08:05,605 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.968e+21, grad_sumsq=5.761e+21, orig_rms_sq=1.730e+00
2024-10-08 21:08:09,571 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=14.96 vs. limit=8.1325
2024-10-08 21:08:20,505 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=27.95 vs. limit=8.765
2024-10-08 21:08:24,034 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=28.82 vs. limit=8.13375
2024-10-08 21:08:25,160 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=39.85 vs. limit=8.7675
2024-10-08 21:08:26,923 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=1690.0, ans=0.136625
2024-10-08 21:08:32,198 WARNING [optim.py:503] Scaling gradients by 0.09633225947618484, model_norm_threshold=2011472768.0
2024-10-08 21:08:32,336 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.112e+20, grad_sumsq=3.314e+21, orig_rms_sq=3.354e-02
2024-10-08 21:08:34,563 WARNING [optim.py:503] Scaling gradients by 0.02373463101685047, model_norm_threshold=2011472768.0
2024-10-08 21:08:34,701 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.432e+21, grad_sumsq=4.269e+22, orig_rms_sq=3.355e-02
2024-10-08 21:08:35,201 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=30.70 vs. limit=8.135
2024-10-08 21:08:35,906 WARNING [optim.py:503] Scaling gradients by 0.07053302228450775, model_norm_threshold=2011472768.0
2024-10-08 21:08:36,043 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.845e+20, grad_sumsq=2.449e+22, orig_rms_sq=7.535e-03
2024-10-08 21:08:36,824 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.98 vs. limit=5.846666666666667
2024-10-08 21:08:40,483 WARNING [optim.py:503] Scaling gradients by 0.07006581872701645, model_norm_threshold=2011472768.0
2024-10-08 21:08:40,621 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.307e+20, grad_sumsq=4.334e+22, orig_rms_sq=7.630e-03
2024-10-08 21:08:42,818 WARNING [optim.py:503] Scaling gradients by 0.05431978777050972, model_norm_threshold=2011472768.0
2024-10-08 21:08:42,967 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.965e+20, grad_sumsq=2.360e+20, orig_rms_sq=2.104e+00
2024-10-08 21:08:49,025 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=1696.6666666666667, ans=0.8406166666666667
2024-10-08 21:08:49,910 WARNING [optim.py:503] Scaling gradients by 0.0020015230402350426, model_norm_threshold=2011472768.0
2024-10-08 21:08:50,049 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.845e+23, grad_sumsq=1.323e+23, orig_rms_sq=2.149e+00
2024-10-08 21:08:51,113 WARNING [optim.py:503] Scaling gradients by 0.004089538939297199, model_norm_threshold=2011472768.0
2024-10-08 21:08:51,251 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.503e+22, grad_sumsq=2.560e+22, orig_rms_sq=2.149e+00
2024-10-08 21:08:52,656 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=1696.6666666666667, ans=0.42046875
2024-10-08 21:08:52,935 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=44.46 vs. limit=8.13625
2024-10-08 21:08:54,797 WARNING [optim.py:503] Scaling gradients by 0.06504873186349869, model_norm_threshold=2011472768.0
2024-10-08 21:08:54,935 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.531e+20, grad_sumsq=1.397e+22, orig_rms_sq=3.242e-02
2024-10-08 21:08:56,069 WARNING [optim.py:503] Scaling gradients by 0.049890175461769104, model_norm_threshold=2011472768.0
2024-10-08 21:08:56,205 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.179e+20, grad_sumsq=6.440e+22, orig_rms_sq=8.042e-03
2024-10-08 21:08:57,286 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.604e+08 1.485e+09 6.316e+09 1.682e+10 1.976e+12, threshold=1.263e+10, percent-clipped=69.0
2024-10-08 21:08:57,328 INFO [train.py:1154] Epoch 1, batch 5100, loss[loss=1.286, simple_loss=0.636, pruned_loss=0.755, ctc_loss=1.064, over 4816.00 frames. ], tot_loss[loss=1.291, simple_loss=0.6517, pruned_loss=0.755, ctc_loss=1.049, over 967968.84 frames. ], batch size: 19, lr: 4.09e-02,
2024-10-08 21:09:00,027 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.86 vs. limit=5.425
2024-10-08 21:09:03,651 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.97 vs. limit=5.425
2024-10-08 21:09:03,842 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=11.32 vs. limit=8.1375
2024-10-08 21:09:04,044 WARNING [optim.py:503] Scaling gradients by 0.08227182179689407, model_norm_threshold=12632006656.0
2024-10-08 21:09:04,182 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.567e+21, grad_sumsq=2.871e+21, orig_rms_sq=2.288e+00
2024-10-08 21:09:09,354 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=15.22 vs. limit=8.13875
2024-10-08 21:09:10,677 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.25 vs. limit=3.2555
2024-10-08 21:09:11,542 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=70.65 vs. limit=8.7775
2024-10-08 21:09:14,384 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=6.42 vs. limit=5.851666666666667
2024-10-08 21:09:15,139 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=40.29 vs. limit=8.13875
2024-10-08 21:09:16,440 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.00 vs. limit=5.851666666666667
2024-10-08 21:09:17,846 WARNING [optim.py:503] Scaling gradients by 0.006000927183777094, model_norm_threshold=12632006656.0
2024-10-08 21:09:17,984 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.193e+23, grad_sumsq=2.586e+25, orig_rms_sq=3.168e-02
2024-10-08 21:09:18,981 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.25 vs. limit=3.2555
2024-10-08 21:09:19,831 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=44.94 vs. limit=8.14
2024-10-08 21:09:21,473 WARNING [optim.py:503] Scaling gradients by 0.05242909863591194, model_norm_threshold=12632006656.0
2024-10-08 21:09:21,612 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.150e+22, grad_sumsq=5.141e+21, orig_rms_sq=2.238e+00
2024-10-08 21:09:22,178 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=5.98 vs. limit=8.14
2024-10-08 21:09:25,597 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=27.89 vs. limit=8.14
2024-10-08 21:09:31,155 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=18.00 vs. limit=8.14125
2024-10-08 21:09:32,126 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1710.0, ans=0.2829
2024-10-08 21:09:32,275 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.5.encoder.layers.1.self_attn_weights, loss-sum=6.047e-01
2024-10-08 21:09:37,491 WARNING [optim.py:503] Scaling gradients by 0.005551979877054691, model_norm_threshold=12632006656.0
2024-10-08 21:09:37,629 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.62, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.201e+24, grad_sumsq=4.761e+26, orig_rms_sq=6.723e-03
2024-10-08 21:09:41,039 WARNING [optim.py:503] Scaling gradients by 0.03630470484495163, model_norm_threshold=12632006656.0
2024-10-08 21:09:41,175 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.936e+22, grad_sumsq=5.762e+24, orig_rms_sq=6.831e-03
2024-10-08 21:09:53,162 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.31 vs. limit=5.428333333333334
2024-10-08 21:09:53,861 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer_ff2.min_abs, batch_count=1716.6666666666667, ans=0.04291666666666667
2024-10-08 21:09:54,924 INFO [train.py:1154] Epoch 1, batch 5150, loss[loss=1.309, simple_loss=0.677, pruned_loss=0.7531, ctc_loss=1.087, over 4814.00 frames. ], tot_loss[loss=1.288, simple_loss=0.651, pruned_loss=0.7528, ctc_loss=1.05, over 968096.19 frames. ], batch size: 36, lr: 4.09e-02,
2024-10-08 21:09:55,121 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=1716.6666666666667, ans=6.072916666666667
2024-10-08 21:09:55,970 WARNING [optim.py:503] Scaling gradients by 0.03955657780170441, model_norm_threshold=12632006656.0
2024-10-08 21:09:56,109 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.251e+22, grad_sumsq=3.373e+24, orig_rms_sq=6.673e-03
2024-10-08 21:09:58,191 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=6.45 vs. limit=4.6866666666666665
2024-10-08 21:09:59,532 WARNING [optim.py:503] Scaling gradients by 0.035691484808921814, model_norm_threshold=12632006656.0
2024-10-08 21:09:59,669 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.421e+22, grad_sumsq=9.940e+24, orig_rms_sq=6.459e-03
2024-10-08 21:10:01,429 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2.whitening_limit, batch_count=1716.6666666666667, ans=5.858333333333333
2024-10-08 21:10:03,096 WARNING [optim.py:503] Scaling gradients by 0.0013176397187635303, model_norm_threshold=12632006656.0
2024-10-08 21:10:03,234 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.376e+25, grad_sumsq=1.510e+25, orig_rms_sq=2.235e+00
2024-10-08 21:10:03,390 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=1716.6666666666667, ans=0.41953125
2024-10-08 21:10:08,705 WARNING [optim.py:503] Scaling gradients by 0.08456001430749893, model_norm_threshold=12632006656.0
2024-10-08 21:10:08,842 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.057e+21, grad_sumsq=3.613e+21, orig_rms_sq=2.230e+00
2024-10-08 21:10:09,007 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.0.self_attn_weights, loss-sum=1.684e+04
2024-10-08 21:10:09,510 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=16.87 vs. limit=8.145
2024-10-08 21:10:10,322 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=8.12 vs. limit=5.43
2024-10-08 21:10:15,175 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=17.20 vs. limit=8.145
2024-10-08 21:10:17,484 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=6.74 vs. limit=5.861666666666666
2024-10-08 21:10:23,142 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=5.21 vs. limit=4.689333333333334
2024-10-08 21:10:26,060 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=1723.3333333333333, ans=0.13537500000000002
2024-10-08 21:10:28,745 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=25.02 vs. limit=8.795
2024-10-08 21:10:34,281 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.38 vs. limit=4.690666666666667
2024-10-08 21:10:36,524 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.91 vs. limit=5.863333333333333
2024-10-08 21:10:40,072 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.63 vs. limit=5.4325
2024-10-08 21:10:42,910 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=1730.0, ans=0.41890625
2024-10-08 21:10:44,553 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=18.31 vs. limit=8.14875
2024-10-08 21:10:50,051 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=20.91 vs. limit=8.14875
2024-10-08 21:10:52,052 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.835e+07 5.630e+08 3.346e+09 1.968e+10 9.587e+12, threshold=6.692e+09, percent-clipped=31.0
2024-10-08 21:10:52,092 INFO [train.py:1154] Epoch 1, batch 5200, loss[loss=1.259, simple_loss=0.6442, pruned_loss=0.7254, ctc_loss=1.056, over 4793.00 frames. ], tot_loss[loss=1.286, simple_loss=0.6502, pruned_loss=0.751, ctc_loss=1.048, over 967730.18 frames. ], batch size: 29, lr: 4.08e-02,
2024-10-08 21:10:52,504 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=1733.3333333333333, ans=8.8
2024-10-08 21:10:58,723 WARNING [optim.py:503] Scaling gradients by 0.04061465710401535, model_norm_threshold=6692257280.0
2024-10-08 21:10:58,861 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.187e+22, grad_sumsq=5.327e+21, orig_rms_sq=2.228e+00
2024-10-08 21:10:59,906 WARNING [optim.py:503] Scaling gradients by 0.006962975487112999, model_norm_threshold=6692257280.0
2024-10-08 21:11:00,044 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.609e+23, grad_sumsq=4.880e+25, orig_rms_sq=7.396e-03
2024-10-08 21:11:01,785 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=384, metric=6.59 vs. limit=8.15
2024-10-08 21:11:03,981 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.72 vs. limit=5.434166666666667
2024-10-08 21:11:14,149 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=1740.0, ans=0.4184375
2024-10-08 21:11:16,202 WARNING [optim.py:503] Scaling gradients by 0.011463048867881298, model_norm_threshold=6692257280.0
2024-10-08 21:11:16,340 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.874e+22, grad_sumsq=1.843e+24, orig_rms_sq=3.187e-02
2024-10-08 21:11:26,073 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.79 vs. limit=3.2615
2024-10-08 21:11:26,786 WARNING [optim.py:503] Scaling gradients by 0.06261195987462997, model_norm_threshold=6692257280.0
2024-10-08 21:11:26,924 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.734e+21, grad_sumsq=5.210e+23, orig_rms_sq=7.167e-03
2024-10-08 21:11:27,985 WARNING [optim.py:503] Scaling gradients by 0.005370430648326874, model_norm_threshold=6692257280.0
2024-10-08 21:11:28,123 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.091e+23, grad_sumsq=5.524e+22, orig_rms_sq=5.595e+00
2024-10-08 21:11:32,654 WARNING [optim.py:503] Scaling gradients by 0.037733085453510284, model_norm_threshold=6692257280.0
2024-10-08 21:11:32,793 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.781e+21, grad_sumsq=2.181e+23, orig_rms_sq=3.109e-02
2024-10-08 21:11:33,020 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=1743.3333333333333, ans=0.1519375
2024-10-08 21:11:42,052 WARNING [optim.py:503] Scaling gradients by 0.006499531213194132, model_norm_threshold=6692257280.0
2024-10-08 21:11:42,190 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.681e+23, grad_sumsq=1.206e+23, orig_rms_sq=2.223e+00
2024-10-08 21:11:45,562 WARNING [optim.py:503] Scaling gradients by 0.004229182377457619, model_norm_threshold=6692257280.0
2024-10-08 21:11:45,700 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.855e+23, grad_sumsq=9.384e+25, orig_rms_sq=7.305e-03
2024-10-08 21:11:46,263 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=25.49 vs. limit=8.81
2024-10-08 21:11:47,901 WARNING [optim.py:503] Scaling gradients by 0.03011378087103367, model_norm_threshold=6692257280.0
2024-10-08 21:11:48,037 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.295e+22, grad_sumsq=1.759e+24, orig_rms_sq=7.362e-03
2024-10-08 21:11:50,326 WARNING [optim.py:503] Scaling gradients by 0.06147490814328194, model_norm_threshold=6692257280.0
2024-10-08 21:11:50,465 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.535e+21, grad_sumsq=1.113e+21, orig_rms_sq=2.278e+00
2024-10-08 21:11:50,511 INFO [train.py:1154] Epoch 1, batch 5250, loss[loss=1.383, simple_loss=0.696, pruned_loss=0.8193, ctc_loss=1.081, over 4857.00 frames. ], tot_loss[loss=1.281, simple_loss=0.6485, pruned_loss=0.7484, ctc_loss=1.044, over 967808.47 frames. ], batch size: 20, lr: 4.07e-02,
2024-10-08 21:11:51,137 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten.whitening_limit, batch_count=1750.0, ans=8.15625
2024-10-08 21:11:52,131 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=30.25 vs. limit=8.8125
2024-10-08 21:11:53,976 WARNING [optim.py:503] Scaling gradients by 0.03211947903037071, model_norm_threshold=6692257280.0
2024-10-08 21:11:54,115 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.338e+21, grad_sumsq=3.615e+21, orig_rms_sq=2.307e+00
2024-10-08 21:11:55,213 WARNING [optim.py:503] Scaling gradients by 0.002053663833066821, model_norm_threshold=6692257280.0
2024-10-08 21:11:55,351 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.057e+24, grad_sumsq=1.325e+24, orig_rms_sq=2.307e+00
2024-10-08 21:12:03,003 WARNING [optim.py:503] Scaling gradients by 0.019923662766814232, model_norm_threshold=6692257280.0
2024-10-08 21:12:03,141 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.164e+22, grad_sumsq=6.563e+23, orig_rms_sq=3.297e-02
2024-10-08 21:12:04,244 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.07 vs. limit=5.876666666666667
2024-10-08 21:12:04,414 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=51.03 vs. limit=8.815
2024-10-08 21:12:05,934 WARNING [optim.py:503] Scaling gradients by 0.027424616739153862, model_norm_threshold=6692257280.0
2024-10-08 21:12:06,073 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.586e+22, grad_sumsq=2.102e+24, orig_rms_sq=7.547e-03
2024-10-08 21:12:11,046 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=14.48 vs. limit=8.1575
2024-10-08 21:12:14,339 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1756.6666666666667, ans=0.2824333333333333
2024-10-08 21:12:17,629 WARNING [optim.py:503] Scaling gradients by 0.0004284324822947383, model_norm_threshold=6692257280.0
2024-10-08 21:12:17,765 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.563e+25, grad_sumsq=2.247e+27, orig_rms_sq=3.366e-02
2024-10-08 21:12:17,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=1756.6666666666667, ans=0.134125
2024-10-08 21:12:20,153 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.attention_skip_rate, batch_count=1756.6666666666667, ans=0.134125
2024-10-08 21:12:20,540 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=19.04 vs. limit=8.817499999999999
2024-10-08 21:12:21,832 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.47 vs. limit=8.817499999999999
2024-10-08 21:12:21,851 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=5.45 vs. limit=4.7026666666666666
2024-10-08 21:12:22,139 WARNING [optim.py:503] Scaling gradients by 0.023152127861976624, model_norm_threshold=6692257280.0
2024-10-08 21:12:22,277 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.770e+22, grad_sumsq=3.034e+21, orig_rms_sq=5.832e+00
2024-10-08 21:12:28,466 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=27.18 vs. limit=8.82
2024-10-08 21:12:29,867 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=25.42 vs. limit=8.16
2024-10-08 21:12:31,397 WARNING [optim.py:503] Scaling gradients by 0.024791337549686432, model_norm_threshold=6692257280.0
2024-10-08 21:12:31,536 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.340e+22, grad_sumsq=1.354e+22, orig_rms_sq=1.729e+00
2024-10-08 21:12:35,857 WARNING [optim.py:503] Scaling gradients by 0.03608141839504242, model_norm_threshold=6692257280.0
2024-10-08 21:12:35,995 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.264e+21, grad_sumsq=1.863e+23, orig_rms_sq=3.362e-02
2024-10-08 21:12:38,293 WARNING [optim.py:503] Scaling gradients by 0.06538292020559311, model_norm_threshold=6692257280.0
2024-10-08 21:12:38,431 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.847e+21, grad_sumsq=3.809e+23, orig_rms_sq=7.474e-03
2024-10-08 21:12:41,951 WARNING [optim.py:503] Scaling gradients by 0.008423845283687115, model_norm_threshold=6692257280.0
2024-10-08 21:12:42,087 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.607e+23, grad_sumsq=1.127e+23, orig_rms_sq=2.314e+00
2024-10-08 21:12:42,618 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.06 vs. limit=8.16125
2024-10-08 21:12:44,915 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=25.02 vs. limit=8.16125
2024-10-08 21:12:48,936 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.134e+08 1.613e+09 5.356e+09 2.931e+10 1.562e+13, threshold=1.071e+10, percent-clipped=44.0
2024-10-08 21:12:48,977 INFO [train.py:1154] Epoch 1, batch 5300, loss[loss=1.286, simple_loss=0.6695, pruned_loss=0.7423, ctc_loss=1.045, over 4825.00 frames. ], tot_loss[loss=1.284, simple_loss=0.6502, pruned_loss=0.7498, ctc_loss=1.045, over 967887.53 frames. ], batch size: 38, lr: 4.07e-02,
2024-10-08 21:12:49,970 WARNING [optim.py:503] Scaling gradients by 0.045814573764801025, model_norm_threshold=10711215104.0
2024-10-08 21:12:50,109 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.222e+22, grad_sumsq=3.688e+23, orig_rms_sq=3.314e-02
2024-10-08 21:12:52,272 WARNING [optim.py:503] Scaling gradients by 0.035574305802583694, model_norm_threshold=10711215104.0
2024-10-08 21:12:52,411 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.873e+22, grad_sumsq=3.607e+24, orig_rms_sq=7.965e-03
2024-10-08 21:12:55,306 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=26.56 vs. limit=8.1625
2024-10-08 21:12:55,332 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.27 vs. limit=3.265
2024-10-08 21:12:56,383 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.16 vs. limit=5.883333333333334
2024-10-08 21:12:56,572 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=5.95 vs. limit=5.883333333333334
2024-10-08 21:12:58,066 WARNING [optim.py:503] Scaling gradients by 0.06179196760058403, model_norm_threshold=10711215104.0
2024-10-08 21:12:58,205 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.047e+22, grad_sumsq=3.148e+23, orig_rms_sq=3.325e-02
2024-10-08 21:13:04,110 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=1770.0, ans=0.41703125
2024-10-08 21:13:06,064 WARNING [optim.py:503] Scaling gradients by 0.0010576489148661494, model_norm_threshold=10711215104.0
2024-10-08 21:13:06,203 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.950e+25, grad_sumsq=6.156e+27, orig_rms_sq=8.041e-03
2024-10-08 21:13:09,549 WARNING [optim.py:503] Scaling gradients by 0.06472644954919815, model_norm_threshold=10711215104.0
2024-10-08 21:13:09,685 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.048e+22, grad_sumsq=1.319e+24, orig_rms_sq=7.942e-03
2024-10-08 21:13:11,537 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.45 vs. limit=5.443333333333333
2024-10-08 21:13:12,594 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.22 vs. limit=3.266
2024-10-08 21:13:14,224 WARNING [optim.py:503] Scaling gradients by 0.0030354605987668037, model_norm_threshold=10711215104.0
2024-10-08 21:13:14,363 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.724e+24, grad_sumsq=8.147e+25, orig_rms_sq=3.344e-02
2024-10-08 21:13:19,891 WARNING [optim.py:503] Scaling gradients by 0.017456836998462677, model_norm_threshold=10711215104.0
2024-10-08 21:13:20,029 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.075e+23, grad_sumsq=1.372e+25, orig_rms_sq=7.836e-03
2024-10-08 21:13:24,426 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=6.46 vs. limit=4.355333333333333
2024-10-08 21:13:26,228 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.76 vs. limit=3.2665
2024-10-08 21:13:26,301 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.48 vs. limit=3.2665
2024-10-08 21:13:28,196 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1776.6666666666667, ans=0.28223333333333334
2024-10-08 21:13:28,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=1776.6666666666667, ans=0.8378166666666667
2024-10-08 21:13:28,608 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.49 vs. limit=5.888333333333334
2024-10-08 21:13:30,547 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1776.6666666666667, ans=0.41671875
2024-10-08 21:13:35,884 WARNING [optim.py:503] Scaling gradients by 0.03585609793663025, model_norm_threshold=10711215104.0
2024-10-08 21:13:36,024 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.080e+22, grad_sumsq=9.161e+21, orig_rms_sq=2.271e+00
2024-10-08 21:13:41,050 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=29.40 vs. limit=8.835
2024-10-08 21:13:41,299 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=6.06 vs. limit=4.712
2024-10-08 21:13:45,230 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=1783.3333333333333, ans=0.059875000000000005
2024-10-08 21:13:46,320 INFO [train.py:1154] Epoch 1, batch 5350, loss[loss=1.21, simple_loss=0.6031, pruned_loss=0.7189, ctc_loss=0.949, over 4978.00 frames. ], tot_loss[loss=1.285, simple_loss=0.6506, pruned_loss=0.7504, ctc_loss=1.048, over 967293.95 frames. ], batch size: 19, lr: 4.06e-02,
2024-10-08 21:13:47,051 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=82.09 vs. limit=8.16875
2024-10-08 21:13:51,427 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.01 vs. limit=8.16875
2024-10-08 21:13:55,601 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=16.46 vs. limit=8.16875
2024-10-08 21:13:59,114 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=6.13 vs. limit=4.714666666666667
2024-10-08 21:13:59,658 WARNING [optim.py:503] Scaling gradients by 0.003689049044623971, model_norm_threshold=10711215104.0
2024-10-08 21:13:59,797 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.627e+24, grad_sumsq=7.182e+23, orig_rms_sq=2.266e+00
2024-10-08 21:14:00,856 WARNING [optim.py:503] Scaling gradients by 0.009070705622434616, model_norm_threshold=10711215104.0
2024-10-08 21:14:00,994 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.338e+23, grad_sumsq=4.900e+22, orig_rms_sq=6.812e+00
2024-10-08 21:14:01,258 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=1786.6666666666667, ans=0.41625
2024-10-08 21:14:03,249 WARNING [optim.py:503] Scaling gradients by 0.05677977204322815, model_norm_threshold=10711215104.0
2024-10-08 21:14:03,385 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.881e+21, grad_sumsq=2.516e+21, orig_rms_sq=2.337e+00
2024-10-08 21:14:16,586 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=31.29 vs. limit=8.8425
2024-10-08 21:14:18,155 WARNING [optim.py:503] Scaling gradients by 0.0601872056722641, model_norm_threshold=10711215104.0
2024-10-08 21:14:18,290 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.201e+21, grad_sumsq=3.809e+21, orig_rms_sq=2.415e+00
2024-10-08 21:14:19,364 WARNING [optim.py:503] Scaling gradients by 0.002339813159778714, model_norm_threshold=10711215104.0
2024-10-08 21:14:19,502 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.790e+24, grad_sumsq=1.983e+24, orig_rms_sq=2.415e+00
2024-10-08 21:14:21,469 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=49.76 vs. limit=8.845
2024-10-08 21:14:21,872 WARNING [optim.py:503] Scaling gradients by 0.07612788677215576, model_norm_threshold=10711215104.0
2024-10-08 21:14:22,010 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.363e+21, grad_sumsq=6.240e+20, orig_rms_sq=6.992e+00
2024-10-08 21:14:29,397 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=40.78 vs. limit=8.1725
2024-10-08 21:14:30,947 WARNING [optim.py:503] Scaling gradients by 0.00925476849079132, model_norm_threshold=10711215104.0
2024-10-08 21:14:31,086 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.722e+23, grad_sumsq=2.140e+25, orig_rms_sq=3.141e-02
2024-10-08 21:14:32,741 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=14.67 vs. limit=8.17375
2024-10-08 21:14:37,769 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.25 vs. limit=8.17375
2024-10-08 21:14:38,210 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=1796.6666666666667, ans=0.41578125
2024-10-08 21:14:40,265 WARNING [optim.py:503] Scaling gradients by 0.07938126474618912, model_norm_threshold=10711215104.0
2024-10-08 21:14:40,404 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.935e+21, grad_sumsq=5.512e+20, orig_rms_sq=7.138e+00
2024-10-08 21:14:43,958 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.644e+07 2.053e+09 7.038e+09 3.136e+10 1.013e+13, threshold=1.408e+10, percent-clipped=43.0
2024-10-08 21:14:43,959 WARNING [optim.py:503] Scaling gradients by 0.09585071355104446, model_norm_threshold=14075955200.0
2024-10-08 21:14:44,099 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.276e+21, grad_sumsq=1.378e+23, orig_rms_sq=3.103e-02
2024-10-08 21:14:44,144 INFO [train.py:1154] Epoch 1, batch 5400, loss[loss=1.336, simple_loss=0.6941, pruned_loss=0.7662, ctc_loss=1.113, over 4773.00 frames. ], tot_loss[loss=1.288, simple_loss=0.653, pruned_loss=0.7516, ctc_loss=1.052, over 966532.54 frames. ], batch size: 49, lr: 4.05e-02,
2024-10-08 21:14:50,608 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=39.13 vs. limit=8.175
2024-10-08 21:14:52,917 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=5.91 vs. limit=5.9
2024-10-08 21:14:53,250 WARNING [optim.py:503] Scaling gradients by 0.019869878888130188, model_norm_threshold=14075955200.0
2024-10-08 21:14:53,390 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.375e+23, grad_sumsq=3.298e+25, orig_rms_sq=7.201e-03
2024-10-08 21:14:53,559 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=1800.0, ans=0.415625
2024-10-08 21:14:54,465 WARNING [optim.py:503] Scaling gradients by 0.021288840100169182, model_norm_threshold=14075955200.0
2024-10-08 21:14:54,603 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.110e+23, grad_sumsq=1.541e+25, orig_rms_sq=7.201e-03
2024-10-08 21:14:55,742 WARNING [optim.py:503] Scaling gradients by 0.037132177501916885, model_norm_threshold=14075955200.0
2024-10-08 21:14:55,882 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.310e+22, grad_sumsq=8.764e+24, orig_rms_sq=7.201e-03
2024-10-08 21:14:56,950 WARNING [optim.py:503] Scaling gradients by 0.008055749349296093, model_norm_threshold=14075955200.0
2024-10-08 21:14:57,087 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.112e+23, grad_sumsq=1.975e+23, orig_rms_sq=2.588e+00
2024-10-08 21:14:59,812 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.47 vs. limit=5.901666666666666
2024-10-08 21:15:00,502 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=1803.3333333333333, ans=0.059425000000000006
2024-10-08 21:15:02,528 WARNING [optim.py:503] Scaling gradients by 0.07045427709817886, model_norm_threshold=14075955200.0
2024-10-08 21:15:02,666 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.045e+22, grad_sumsq=2.816e+24, orig_rms_sq=7.262e-03
2024-10-08 21:15:04,808 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=5.23 vs. limit=5.0
2024-10-08 21:15:09,420 WARNING [optim.py:503] Scaling gradients by 0.01875327341258526, model_norm_threshold=14075955200.0
2024-10-08 21:15:09,557 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.318e+23, grad_sumsq=3.225e+25, orig_rms_sq=7.189e-03
2024-10-08 21:15:09,948 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=7.15 vs. limit=4.722666666666667
2024-10-08 21:15:12,409 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.64 vs. limit=5.903333333333333
2024-10-08 21:15:12,552 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.62 vs. limit=8.1775
2024-10-08 21:15:14,421 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=1806.6666666666667, ans=0.4153125
2024-10-08 21:15:14,996 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.92 vs. limit=8.1775
2024-10-08 21:15:23,001 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.20 vs. limit=5.4525
2024-10-08 21:15:26,555 WARNING [optim.py:503] Scaling gradients by 0.002308054594323039, model_norm_threshold=14075955200.0
2024-10-08 21:15:26,693 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.276e+24, grad_sumsq=1.929e+26, orig_rms_sq=3.254e-02
2024-10-08 21:15:31,497 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=8.41 vs. limit=4.725333333333333
2024-10-08 21:15:33,818 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.64 vs. limit=8.18
2024-10-08 21:15:35,195 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.64 vs. limit=8.18
2024-10-08 21:15:37,037 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1813.3333333333333, ans=0.28186666666666665
2024-10-08 21:15:37,161 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=17.34 vs. limit=8.18
2024-10-08 21:15:39,388 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=14.28 vs. limit=5.906666666666666
2024-10-08 21:15:41,281 INFO [train.py:1154] Epoch 1, batch 5450, loss[loss=1.19, simple_loss=0.5858, pruned_loss=0.7064, ctc_loss=0.9556, over 4940.00 frames. ], tot_loss[loss=1.292, simple_loss=0.6517, pruned_loss=0.756, ctc_loss=1.049, over 967208.69 frames. ], batch size: 19, lr: 4.05e-02,
2024-10-08 21:15:45,216 WARNING [optim.py:503] Scaling gradients by 0.02719992771744728, model_norm_threshold=14075955200.0
2024-10-08 21:15:45,355 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.242e+23, grad_sumsq=1.594e+25, orig_rms_sq=7.796e-03
2024-10-08 21:15:50,009 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.hidden_balancer.prob, batch_count=1816.6666666666667, ans=0.41484375
2024-10-08 21:15:54,359 WARNING [optim.py:503] Scaling gradients by 0.00984733086079359, model_norm_threshold=14075955200.0
2024-10-08 21:15:54,495 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.072e+23, grad_sumsq=5.193e+22, orig_rms_sq=7.842e+00
2024-10-08 21:15:58,393 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=17.17 vs. limit=8.1825
2024-10-08 21:15:58,544 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=16.63 vs. limit=8.1825
2024-10-08 21:15:59,818 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=4.79 vs. limit=4.728
2024-10-08 21:16:00,663 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.64 vs. limit=8.1825
2024-10-08 21:16:03,063 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.76 vs. limit=3.273
2024-10-08 21:16:06,221 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten.whitening_limit, batch_count=1823.3333333333333, ans=8.18375
2024-10-08 21:16:06,679 WARNING [optim.py:503] Scaling gradients by 0.003751401323825121, model_norm_threshold=14075955200.0
2024-10-08 21:16:06,818 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.578e+24, grad_sumsq=4.884e+26, orig_rms_sq=7.326e-03
2024-10-08 21:16:10,124 WARNING [optim.py:503] Scaling gradients by 0.005996205843985081, model_norm_threshold=14075955200.0
2024-10-08 21:16:10,262 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.125e+24, grad_sumsq=4.790e+23, orig_rms_sq=2.349e+00
2024-10-08 21:16:14,417 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=9.11 vs. limit=8.18375
2024-10-08 21:16:14,960 WARNING [optim.py:503] Scaling gradients by 0.0039834859780967236, model_norm_threshold=14075955200.0
2024-10-08 21:16:15,097 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.307e+24, grad_sumsq=2.808e+23, orig_rms_sq=8.214e+00
2024-10-08 21:16:18,494 WARNING [optim.py:503] Scaling gradients by 0.047417156398296356, model_norm_threshold=14075955200.0
2024-10-08 21:16:18,632 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.646e+22, grad_sumsq=4.971e+23, orig_rms_sq=3.311e-02
2024-10-08 21:16:19,666 WARNING [optim.py:503] Scaling gradients by 0.08168639987707138, model_norm_threshold=14075955200.0
2024-10-08 21:16:19,802 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.336e+21, grad_sumsq=3.189e+21, orig_rms_sq=2.301e+00
2024-10-08 21:16:25,506 WARNING [optim.py:503] Scaling gradients by 0.0181852076202631, model_norm_threshold=14075955200.0
2024-10-08 21:16:25,645 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.349e+22, grad_sumsq=4.144e+22, orig_rms_sq=2.256e+00
2024-10-08 21:16:26,710 WARNING [optim.py:503] Scaling gradients by 0.05866347253322601, model_norm_threshold=14075955200.0
2024-10-08 21:16:26,849 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.575e+22, grad_sumsq=2.097e+24, orig_rms_sq=7.511e-03
2024-10-08 21:16:27,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=1830.0, ans=0.2817
2024-10-08 21:16:28,001 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=1830.0, ans=0.1830725
2024-10-08 21:16:31,236 WARNING [optim.py:503] Scaling gradients by 0.033869754523038864, model_norm_threshold=14075955200.0
2024-10-08 21:16:31,374 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.447e+22, grad_sumsq=4.122e+21, orig_rms_sq=8.363e+00
2024-10-08 21:16:32,469 WARNING [optim.py:503] Scaling gradients by 0.027340468019247055, model_norm_threshold=14075955200.0
2024-10-08 21:16:32,608 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.371e+22, grad_sumsq=5.227e+21, orig_rms_sq=8.363e+00
2024-10-08 21:16:35,155 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=26.55 vs. limit=8.8725
2024-10-08 21:16:36,973 WARNING [optim.py:503] Scaling gradients by 0.001424766262061894, model_norm_threshold=14075955200.0
2024-10-08 21:16:37,110 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.051e+25, grad_sumsq=8.463e+24, orig_rms_sq=2.423e+00
2024-10-08 21:16:39,541 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.829e+08 4.142e+09 1.578e+10 1.150e+11 9.879e+12, threshold=3.155e+10, percent-clipped=53.0
2024-10-08 21:16:39,582 INFO [train.py:1154] Epoch 1, batch 5500, loss[loss=1.222, simple_loss=0.6457, pruned_loss=0.6919, ctc_loss=1.036, over 4811.00 frames. ], tot_loss[loss=1.293, simple_loss=0.6527, pruned_loss=0.7563, ctc_loss=1.051, over 967472.79 frames. ], batch size: 49, lr: 4.04e-02,
2024-10-08 21:16:43,078 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=1833.3333333333333, ans=0.4140625
2024-10-08 21:16:43,937 WARNING [optim.py:503] Scaling gradients by 0.011619404889643192, model_norm_threshold=31554822144.0
2024-10-08 21:16:44,074 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.834e+24, grad_sumsq=7.214e+23, orig_rms_sq=2.543e+00
2024-10-08 21:16:45,072 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.70 vs. limit=8.875
2024-10-08 21:16:47,566 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.prob, batch_count=1833.3333333333333, ans=0.4140625
2024-10-08 21:16:50,383 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.00 vs. limit=5.918333333333333
2024-10-08 21:17:00,266 WARNING [optim.py:503] Scaling gradients by 0.09670833498239517, model_norm_threshold=31554822144.0
2024-10-08 21:17:00,405 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.704e+22, grad_sumsq=6.251e+24, orig_rms_sq=7.525e-03
2024-10-08 21:17:05,063 WARNING [optim.py:503] Scaling gradients by 0.09134071320295334, model_norm_threshold=31554822144.0
2024-10-08 21:17:05,201 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.850e+22, grad_sumsq=1.228e+24, orig_rms_sq=3.135e-02
2024-10-08 21:17:07,775 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=1840.0, ans=0.08850000000000001
2024-10-08 21:17:08,623 WARNING [optim.py:503] Scaling gradients by 0.06398022174835205, model_norm_threshold=31554822144.0
2024-10-08 21:17:08,761 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.427e+22, grad_sumsq=1.783e+24, orig_rms_sq=3.044e-02
2024-10-08 21:17:09,323 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=63.43 vs. limit=8.19
2024-10-08 21:17:12,071 WARNING [optim.py:503] Scaling gradients by 0.004534023813903332, model_norm_threshold=31554822144.0
2024-10-08 21:17:12,209 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.481e+25, grad_sumsq=3.517e+27, orig_rms_sq=7.054e-03
2024-10-08 21:17:12,577 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=15.60 vs. limit=8.19
2024-10-08 21:17:17,044 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=1843.3333333333333, ans=0.41359375
2024-10-08 21:17:20,183 WARNING [optim.py:503] Scaling gradients by 0.024965321645140648, model_norm_threshold=31554822144.0
2024-10-08 21:17:20,322 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.353e+23, grad_sumsq=1.039e+26, orig_rms_sq=7.080e-03
2024-10-08 21:17:21,025 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.07 vs. limit=8.8825
2024-10-08 21:17:21,559 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=1843.3333333333333, ans=0.41359375
2024-10-08 21:17:22,970 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.51 vs. limit=5.921666666666667
2024-10-08 21:17:26,045 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff3_skip_rate, batch_count=1846.6666666666667, ans=0.05845
2024-10-08 21:17:27,775 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.70 vs. limit=3.277
2024-10-08 21:17:28,163 WARNING [optim.py:503] Scaling gradients by 0.010243707336485386, model_norm_threshold=31554822144.0
2024-10-08 21:17:28,301 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.742e+24, grad_sumsq=6.962e+23, orig_rms_sq=2.502e+00
2024-10-08 21:17:28,804 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.23 vs. limit=3.277
2024-10-08 21:17:30,940 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2.whitening_limit, batch_count=1846.6666666666667, ans=5.923333333333334
2024-10-08 21:17:35,218 WARNING [optim.py:503] Scaling gradients by 0.00023738191521260887, model_norm_threshold=31554822144.0
2024-10-08 21:17:35,360 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.284e+27, grad_sumsq=2.563e+27, orig_rms_sq=2.452e+00
2024-10-08 21:17:37,584 WARNING [optim.py:503] Scaling gradients by 0.016477251425385475, model_norm_threshold=31554822144.0
2024-10-08 21:17:37,720 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.285e+24, grad_sumsq=5.240e+23, orig_rms_sq=2.452e+00
2024-10-08 21:17:37,766 INFO [train.py:1154] Epoch 1, batch 5550, loss[loss=1.225, simple_loss=0.5999, pruned_loss=0.7191, ctc_loss=1.032, over 4799.00 frames. ], tot_loss[loss=1.292, simple_loss=0.6528, pruned_loss=0.7555, ctc_loss=1.05, over 967087.24 frames. ], batch size: 19, lr: 4.03e-02,
2024-10-08 21:17:41,185 WARNING [optim.py:503] Scaling gradients by 0.028736457228660583, model_norm_threshold=31554822144.0
2024-10-08 21:17:41,324 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.786e+23, grad_sumsq=4.002e+25, orig_rms_sq=6.961e-03
2024-10-08 21:17:45,732 WARNING [optim.py:503] Scaling gradients by 0.06445910036563873, model_norm_threshold=31554822144.0
2024-10-08 21:17:45,870 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.213e+22, grad_sumsq=1.046e+25, orig_rms_sq=6.897e-03
2024-10-08 21:17:48,311 WARNING [optim.py:503] Scaling gradients by 0.06660787016153336, model_norm_threshold=31554822144.0
2024-10-08 21:17:48,449 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.138e+22, grad_sumsq=1.650e+22, orig_rms_sq=2.508e+00
2024-10-08 21:17:49,617 WARNING [optim.py:503] Scaling gradients by 0.05254464969038963, model_norm_threshold=31554822144.0
2024-10-08 21:17:49,755 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.833e+22, grad_sumsq=2.970e+24, orig_rms_sq=2.974e-02
2024-10-08 21:17:52,434 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=41.42 vs. limit=8.195
2024-10-08 21:17:53,104 WARNING [optim.py:503] Scaling gradients by 0.0849846601486206, model_norm_threshold=31554822144.0
2024-10-08 21:17:53,242 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.060e+22, grad_sumsq=4.515e+24, orig_rms_sq=6.777e-03
2024-10-08 21:17:55,501 WARNING [optim.py:503] Scaling gradients by 0.05351216346025467, model_norm_threshold=31554822144.0
2024-10-08 21:17:55,640 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.450e+23, grad_sumsq=2.136e+25, orig_rms_sq=6.790e-03
2024-10-08 21:17:56,735 WARNING [optim.py:503] Scaling gradients by 0.020417416468262672, model_norm_threshold=31554822144.0
2024-10-08 21:17:56,872 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.465e+23, grad_sumsq=6.577e+25, orig_rms_sq=6.790e-03
2024-10-08 21:17:58,694 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=5.13 vs. limit=4.741333333333333
2024-10-08 21:17:59,113 WARNING [optim.py:503] Scaling gradients by 0.06517469137907028, model_norm_threshold=31554822144.0
2024-10-08 21:17:59,251 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.878e+22, grad_sumsq=5.531e+21, orig_rms_sq=8.820e+00
2024-10-08 21:18:01,092 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=12.05 vs. limit=8.19625
2024-10-08 21:18:02,851 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=1856.6666666666667, ans=0.2814333333333333
2024-10-08 21:18:05,931 WARNING [optim.py:503] Scaling gradients by 0.030972091481089592, model_norm_threshold=31554822144.0
2024-10-08 21:18:06,070 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.461e+23, grad_sumsq=5.059e+25, orig_rms_sq=6.841e-03
2024-10-08 21:18:08,976 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.59 vs. limit=5.464166666666666
2024-10-08 21:18:09,658 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=1856.6666666666667, ans=0.058225
2024-10-08 21:18:10,701 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=1856.6666666666667, ans=0.5
2024-10-08 21:18:15,146 WARNING [optim.py:503] Scaling gradients by 0.01903722994029522, model_norm_threshold=31554822144.0
2024-10-08 21:18:15,282 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.033e+24, grad_sumsq=1.520e+26, orig_rms_sq=6.794e-03
2024-10-08 21:18:16,661 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass.scale_min, batch_count=1860.0, ans=0.8349
2024-10-08 21:18:16,664 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=1860.0, ans=0.13024999999999998
2024-10-08 21:18:17,541 WARNING [optim.py:503] Scaling gradients by 0.049242813140153885, model_norm_threshold=31554822144.0
2024-10-08 21:18:17,680 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.301e+23, grad_sumsq=4.202e+24, orig_rms_sq=3.097e-02
2024-10-08 21:18:18,753 WARNING [optim.py:503] Scaling gradients by 0.038705408573150635, model_norm_threshold=31554822144.0
2024-10-08 21:18:18,891 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.648e+23, grad_sumsq=3.933e+25, orig_rms_sq=6.734e-03
2024-10-08 21:18:20,106 WARNING [optim.py:503] Scaling gradients by 0.014379673637449741, model_norm_threshold=31554822144.0
2024-10-08 21:18:20,245 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.149e+24, grad_sumsq=3.191e+26, orig_rms_sq=6.734e-03
2024-10-08 21:18:23,481 WARNING [optim.py:503] Scaling gradients by 0.011757397092878819, model_norm_threshold=31554822144.0
2024-10-08 21:18:23,619 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.849e+24, grad_sumsq=6.028e+25, orig_rms_sq=3.068e-02
2024-10-08 21:18:28,271 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=3.71 vs. limit=4.745333333333333
2024-10-08 21:18:28,872 WARNING [optim.py:503] Scaling gradients by 0.004970612470060587, model_norm_threshold=31554822144.0
2024-10-08 21:18:29,010 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.114e+24, grad_sumsq=1.408e+27, orig_rms_sq=6.472e-03
2024-10-08 21:18:32,322 WARNING [optim.py:503] Scaling gradients by 0.02869616262614727, model_norm_threshold=31554822144.0
2024-10-08 21:18:32,461 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.595e+23, grad_sumsq=1.214e+25, orig_rms_sq=2.961e-02
2024-10-08 21:18:33,510 WARNING [optim.py:503] Scaling gradients by 0.017076658084988594, model_norm_threshold=31554822144.0
2024-10-08 21:18:33,648 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.341e+23, grad_sumsq=2.817e+25, orig_rms_sq=2.961e-02
2024-10-08 21:18:35,060 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=1866.6666666666667, ans=0.4125
2024-10-08 21:18:35,959 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.846e+07 6.659e+09 4.602e+10 3.455e+11 1.329e+14, threshold=9.204e+10, percent-clipped=55.0
2024-10-08 21:18:36,000 INFO [train.py:1154] Epoch 1, batch 5600, loss[loss=1.294, simple_loss=0.6482, pruned_loss=0.7507, ctc_loss=1.098, over 4862.00 frames. ], tot_loss[loss=1.292, simple_loss=0.6533, pruned_loss=0.7555, ctc_loss=1.051, over 967173.94 frames. ], batch size: 28, lr: 4.03e-02,
2024-10-08 21:18:52,851 WARNING [optim.py:503] Scaling gradients by 0.0027409850154072046, model_norm_threshold=92035088384.0
2024-10-08 21:18:52,987 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.137e+26, grad_sumsq=2.327e+26, orig_rms_sq=2.208e+00
2024-10-08 21:18:54,432 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass.scale_min, batch_count=1870.0, ans=0.83455
2024-10-08 21:18:55,307 WARNING [optim.py:503] Scaling gradients by 0.05146671459078789, model_norm_threshold=92035088384.0
2024-10-08 21:18:55,446 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.354e+24, grad_sumsq=6.288e+23, orig_rms_sq=2.153e+00
2024-10-08 21:19:04,237 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=1873.3333333333333, ans=0.4121875
2024-10-08 21:19:06,393 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=1873.3333333333333, ans=0.4121875
2024-10-08 21:19:07,969 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=14.49 vs. limit=8.2025
2024-10-08 21:19:13,143 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=14.44 vs. limit=8.20375
2024-10-08 21:19:16,138 WARNING [optim.py:503] Scaling gradients by 0.037696029990911484, model_norm_threshold=92035088384.0
2024-10-08 21:19:16,277 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.944e+24, grad_sumsq=9.113e+23, orig_rms_sq=2.133e+00
2024-10-08 21:19:16,537 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=2.213e+02
2024-10-08 21:19:17,360 WARNING [optim.py:503] Scaling gradients by 0.0003931647806894034, model_norm_threshold=92035088384.0
2024-10-08 21:19:17,519 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.933e+28, grad_sumsq=4.577e+30, orig_rms_sq=6.409e-03
2024-10-08 21:19:18,655 WARNING [optim.py:503] Scaling gradients by 0.05544782802462578, model_norm_threshold=92035088384.0
2024-10-08 21:19:18,799 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.792e+23, grad_sumsq=2.360e+25, orig_rms_sq=2.879e-02
2024-10-08 21:19:22,257 WARNING [optim.py:503] Scaling gradients by 0.008865618146955967, model_norm_threshold=92035088384.0
2024-10-08 21:19:22,395 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.055e+25, grad_sumsq=1.473e+25, orig_rms_sq=2.074e+00
2024-10-08 21:19:24,132 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=33.72 vs. limit=8.205
2024-10-08 21:19:32,470 WARNING [optim.py:503] Scaling gradients by 0.026284242048859596, model_norm_threshold=92035088384.0
2024-10-08 21:19:32,610 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.451e+24, grad_sumsq=2.697e+23, orig_rms_sq=9.089e+00
2024-10-08 21:19:33,715 INFO [train.py:1154] Epoch 1, batch 5650, loss[loss=1.377, simple_loss=0.7144, pruned_loss=0.7944, ctc_loss=1.126, over 4739.00 frames. ], tot_loss[loss=1.289, simple_loss=0.6511, pruned_loss=0.7536, ctc_loss=1.047, over 967022.70 frames. ], batch size: 45, lr: 4.02e-02,
2024-10-08 21:19:35,332 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=41.22 vs. limit=8.9125
2024-10-08 21:19:36,858 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=10.90 vs. limit=5.470833333333333
2024-10-08 21:19:37,242 WARNING [optim.py:503] Scaling gradients by 0.07860694825649261, model_norm_threshold=92035088384.0
2024-10-08 21:19:37,380 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.046e+23, grad_sumsq=3.376e+22, orig_rms_sq=9.022e+00
2024-10-08 21:19:38,013 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=70.56 vs. limit=8.20625
2024-10-08 21:19:40,810 WARNING [optim.py:503] Scaling gradients by 0.025145191699266434, model_norm_threshold=92035088384.0
2024-10-08 21:19:40,948 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.757e+24, grad_sumsq=4.299e+26, orig_rms_sq=6.413e-03
2024-10-08 21:19:45,471 WARNING [optim.py:503] Scaling gradients by 0.06017185002565384, model_norm_threshold=92035088384.0
2024-10-08 21:19:45,610 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.512e+23, grad_sumsq=6.146e+22, orig_rms_sq=8.967e+00
2024-10-08 21:19:49,396 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=20.06 vs. limit=8.2075
2024-10-08 21:19:49,974 WARNING [optim.py:503] Scaling gradients by 0.08748391270637512, model_norm_threshold=92035088384.0
2024-10-08 21:19:50,113 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.390e+23, grad_sumsq=2.657e+22, orig_rms_sq=8.995e+00
2024-10-08 21:19:51,190 WARNING [optim.py:503] Scaling gradients by 0.01748587377369404, model_norm_threshold=92035088384.0
2024-10-08 21:19:51,328 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.481e+24, grad_sumsq=2.666e+26, orig_rms_sq=2.806e-02
2024-10-08 21:19:51,751 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=47.31 vs. limit=8.915
2024-10-08 21:19:54,573 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.81 vs. limit=5.471666666666667
2024-10-08 21:19:56,513 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.65 vs. limit=5.4725
2024-10-08 21:20:00,272 WARNING [optim.py:503] Scaling gradients by 0.025355705991387367, model_norm_threshold=92035088384.0
2024-10-08 21:20:00,411 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.486e+24, grad_sumsq=2.736e+23, orig_rms_sq=9.088e+00
2024-10-08 21:20:04,944 WARNING [optim.py:503] Scaling gradients by 0.003101916518062353, model_norm_threshold=92035088384.0
2024-10-08 21:20:05,083 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.450e+26, grad_sumsq=3.768e+28, orig_rms_sq=6.502e-03
2024-10-08 21:20:12,035 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=1893.3333333333333, ans=0.2633333333333333
2024-10-08 21:20:14,443 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=1893.3333333333333, ans=0.5
2024-10-08 21:20:18,806 WARNING [optim.py:503] Scaling gradients by 0.020515426993370056, model_norm_threshold=92035088384.0
2024-10-08 21:20:18,944 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.395e+24, grad_sumsq=2.490e+24, orig_rms_sq=2.167e+00
2024-10-08 21:20:19,593 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.15 vs. limit=5.474166666666667
2024-10-08 21:20:21,278 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=1896.6666666666667, ans=0.057325
2024-10-08 21:20:21,492 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=18.06 vs. limit=8.9225
2024-10-08 21:20:21,675 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=28.62 vs. limit=8.9225
2024-10-08 21:20:22,774 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=66.46 vs. limit=8.21125
2024-10-08 21:20:23,872 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=35.03 vs. limit=8.21125
2024-10-08 21:20:26,165 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.82 vs. limit=5.474166666666667
2024-10-08 21:20:26,447 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.58 vs. limit=5.474166666666667
2024-10-08 21:20:27,568 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.27 vs. limit=3.2845
2024-10-08 21:20:28,766 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=11.75 vs. limit=8.21125
2024-10-08 21:20:29,557 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=6.64 vs. limit=3.2845
2024-10-08 21:20:31,433 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.437e+08 8.703e+09 5.173e+10 2.822e+11 2.341e+14, threshold=1.035e+11, percent-clipped=41.0
2024-10-08 21:20:31,433 WARNING [optim.py:503] Scaling gradients by 0.07123202830553055, model_norm_threshold=103453999104.0
2024-10-08 21:20:31,571 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.616e+23, grad_sumsq=8.542e+25, orig_rms_sq=6.575e-03
2024-10-08 21:20:31,617 INFO [train.py:1154] Epoch 1, batch 5700, loss[loss=1.323, simple_loss=0.6544, pruned_loss=0.7727, ctc_loss=1.115, over 4888.00 frames. ], tot_loss[loss=1.285, simple_loss=0.6498, pruned_loss=0.751, ctc_loss=1.047, over 966420.61 frames. ], batch size: 22, lr: 4.02e-02,
2024-10-08 21:20:32,639 WARNING [optim.py:503] Scaling gradients by 0.07671833038330078, model_norm_threshold=103453999104.0
2024-10-08 21:20:32,778 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.238e+23, grad_sumsq=1.501e+23, orig_rms_sq=2.158e+00
2024-10-08 21:20:35,669 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=5.47 vs. limit=4.76
2024-10-08 21:20:37,212 WARNING [optim.py:503] Scaling gradients by 0.03694278746843338, model_norm_threshold=103453999104.0
2024-10-08 21:20:37,349 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.545e+24, grad_sumsq=2.374e+26, orig_rms_sq=6.509e-03
2024-10-08 21:20:37,486 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=1900.0, ans=0.143125
2024-10-08 21:20:40,894 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=1900.0, ans=0.8335
2024-10-08 21:20:40,926 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=1900.0, ans=0.28099999999999997
2024-10-08 21:20:44,688 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=32.67 vs. limit=8.21375
2024-10-08 21:20:45,828 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.21 vs. limit=8.9275
2024-10-08 21:21:03,052 WARNING [optim.py:503] Scaling gradients by 0.001231342670507729, model_norm_threshold=103453999104.0
2024-10-08 21:21:03,191 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.60, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.213e+27, grad_sumsq=6.309e+29, orig_rms_sq=6.678e-03
2024-10-08 21:21:03,413 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=1906.6666666666667, ans=0.8332666666666667
2024-10-08 21:21:04,897 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=23.84 vs. limit=8.21625
2024-10-08 21:21:07,590 WARNING [optim.py:503] Scaling gradients by 0.027011418715119362, model_norm_threshold=103453999104.0
2024-10-08 21:21:07,727 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.308e+24, grad_sumsq=1.443e+24, orig_rms_sq=2.293e+00
2024-10-08 21:21:09,117 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.min_positive, batch_count=1910.0, ans=0.0880625
2024-10-08 21:21:09,669 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.64 vs. limit=8.21625
2024-10-08 21:21:14,585 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=1910.0, ans=6.19375
2024-10-08 21:21:17,400 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.40 vs. limit=5.956666666666667
2024-10-08 21:21:20,313 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=1913.3333333333333, ans=0.41031249999999997
2024-10-08 21:21:24,767 WARNING [optim.py:503] Scaling gradients by 0.09805445373058319, model_norm_threshold=103453999104.0
2024-10-08 21:21:24,911 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.011e+23, grad_sumsq=3.339e+22, orig_rms_sq=9.017e+00
2024-10-08 21:21:25,509 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.54 vs. limit=3.287
2024-10-08 21:21:28,326 INFO [train.py:1154] Epoch 1, batch 5750, loss[loss=1.399, simple_loss=0.7129, pruned_loss=0.8114, ctc_loss=1.154, over 4851.00 frames. ], tot_loss[loss=1.288, simple_loss=0.6513, pruned_loss=0.7526, ctc_loss=1.048, over 966852.46 frames. ], batch size: 43, lr: 4.01e-02,
2024-10-08 21:21:30,543 WARNING [optim.py:503] Scaling gradients by 0.0575890988111496, model_norm_threshold=103453999104.0
2024-10-08 21:21:30,692 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.197e+23, grad_sumsq=2.495e+25, orig_rms_sq=2.884e-02
2024-10-08 21:21:32,878 WARNING [optim.py:503] Scaling gradients by 0.006619736086577177, model_norm_threshold=103453999104.0
2024-10-08 21:21:33,020 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.326e+25, grad_sumsq=8.560e+27, orig_rms_sq=7.390e-03
2024-10-08 21:21:38,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.scale_min, batch_count=1920.0, ans=0.8328
2024-10-08 21:21:40,822 WARNING [optim.py:503] Scaling gradients by 0.00020784737716894597, model_norm_threshold=103453999104.0
2024-10-08 21:21:40,963 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.115e+28, grad_sumsq=1.788e+30, orig_rms_sq=2.861e-02
2024-10-08 21:21:42,007 WARNING [optim.py:503] Scaling gradients by 0.011012107133865356, model_norm_threshold=103453999104.0
2024-10-08 21:21:42,147 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.164e+25, grad_sumsq=7.565e+26, orig_rms_sq=2.861e-02
2024-10-08 21:21:43,038 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.34 vs. limit=5.96
2024-10-08 21:21:43,234 WARNING [optim.py:503] Scaling gradients by 9.945889905793592e-05, model_norm_threshold=103453999104.0
2024-10-08 21:21:43,373 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.416e+29, grad_sumsq=4.611e+31, orig_rms_sq=7.408e-03
2024-10-08 21:21:47,772 WARNING [optim.py:503] Scaling gradients by 0.07674244791269302, model_norm_threshold=103453999104.0
2024-10-08 21:21:47,909 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.369e+23, grad_sumsq=4.558e+25, orig_rms_sq=7.391e-03
2024-10-08 21:21:49,008 WARNING [optim.py:503] Scaling gradients by 0.0874536782503128, model_norm_threshold=103453999104.0
2024-10-08 21:21:49,147 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.992e+23, grad_sumsq=1.033e+25, orig_rms_sq=2.896e-02
2024-10-08 21:21:49,814 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=7.60 vs. limit=5.48
2024-10-08 21:21:52,629 WARNING [optim.py:503] Scaling gradients by 0.0073730372823774815, model_norm_threshold=103453999104.0
2024-10-08 21:21:52,770 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.931e+25, grad_sumsq=1.339e+28, orig_rms_sq=7.414e-03
2024-10-08 21:21:54,965 WARNING [optim.py:503] Scaling gradients by 0.05933479592204094, model_norm_threshold=103453999104.0
2024-10-08 21:21:55,102 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.467e+24, grad_sumsq=5.093e+25, orig_rms_sq=2.879e-02
2024-10-08 21:21:59,165 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.46 vs. limit=8.22125
2024-10-08 21:22:00,708 WARNING [optim.py:503] Scaling gradients by 0.0041704075410962105, model_norm_threshold=103453999104.0
2024-10-08 21:22:00,845 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.487e+26, grad_sumsq=5.149e+27, orig_rms_sq=2.887e-02
2024-10-08 21:22:03,126 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=1926.6666666666667, ans=0.4096875
2024-10-08 21:22:09,868 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=1926.6666666666667, ans=0.4096875
2024-10-08 21:22:14,674 WARNING [optim.py:503] Scaling gradients by 0.09992717206478119, model_norm_threshold=103453999104.0
2024-10-08 21:22:14,810 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.802e+23, grad_sumsq=5.491e+25, orig_rms_sq=6.925e-03
2024-10-08 21:22:17,915 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.85 vs. limit=5.4825
2024-10-08 21:22:18,227 WARNING [optim.py:503] Scaling gradients by 0.020214566960930824, model_norm_threshold=103453999104.0
2024-10-08 21:22:18,364 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.146e+24, grad_sumsq=3.359e+24, orig_rms_sq=2.425e+00
2024-10-08 21:22:26,458 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.016e+08 1.747e+10 7.594e+10 4.252e+11 1.040e+15, threshold=1.519e+11, percent-clipped=49.0
2024-10-08 21:22:26,499 INFO [train.py:1154] Epoch 1, batch 5800, loss[loss=1.281, simple_loss=0.6626, pruned_loss=0.7331, ctc_loss=1.085, over 4846.00 frames. ], tot_loss[loss=1.282, simple_loss=0.6488, pruned_loss=0.7488, ctc_loss=1.046, over 966144.94 frames. ], batch size: 43, lr: 4.00e-02,
2024-10-08 21:22:26,938 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=26.53 vs. limit=8.95
2024-10-08 21:22:27,563 WARNING [optim.py:503] Scaling gradients by 0.00045115858665667474, model_norm_threshold=151886643200.0
2024-10-08 21:22:27,702 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.658e+28, grad_sumsq=9.258e+29, orig_rms_sq=2.871e-02
2024-10-08 21:22:34,733 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=1933.3333333333333, ans=0.14125
2024-10-08 21:22:35,827 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=1933.3333333333333, ans=0.1275
2024-10-08 21:22:36,065 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.21 vs. limit=5.483333333333333
2024-10-08 21:22:45,307 WARNING [optim.py:503] Scaling gradients by 0.0717490017414093, model_norm_threshold=151886643200.0
2024-10-08 21:22:45,444 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.130e+24, grad_sumsq=1.767e+26, orig_rms_sq=6.393e-03
2024-10-08 21:22:51,845 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.96 vs. limit=8.955
2024-10-08 21:22:53,642 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=17.32 vs. limit=8.955
2024-10-08 21:22:54,335 WARNING [optim.py:503] Scaling gradients by 0.05661456286907196, model_norm_threshold=151886643200.0
2024-10-08 21:22:54,471 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.748e+24, grad_sumsq=2.605e+26, orig_rms_sq=6.710e-03
2024-10-08 21:23:04,767 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.attention_skip_rate, batch_count=1943.3333333333333, ans=0.127125
2024-10-08 21:23:05,706 WARNING [optim.py:503] Scaling gradients by 0.038415953516960144, model_norm_threshold=151886643200.0
2024-10-08 21:23:05,845 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.549e+24, grad_sumsq=1.698e+26, orig_rms_sq=2.679e-02
2024-10-08 21:23:06,920 WARNING [optim.py:503] Scaling gradients by 0.03621424734592438, model_norm_threshold=151886643200.0
2024-10-08 21:23:07,059 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.006e+24, grad_sumsq=2.296e+24, orig_rms_sq=2.616e+00
2024-10-08 21:23:07,283 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=1943.3333333333333, ans=0.7694333333333333
2024-10-08 21:23:13,757 WARNING [optim.py:503] Scaling gradients by 0.03954235836863518, model_norm_threshold=151886643200.0
2024-10-08 21:23:13,895 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.801e+24, grad_sumsq=3.092e+23, orig_rms_sq=9.058e+00
2024-10-08 21:23:22,481 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.06 vs. limit=5.4875
2024-10-08 21:23:23,014 INFO [train.py:1154] Epoch 1, batch 5850, loss[loss=1.334, simple_loss=0.6991, pruned_loss=0.7716, ctc_loss=1.063, over 4733.00 frames. ], tot_loss[loss=1.283, simple_loss=0.6498, pruned_loss=0.7491, ctc_loss=1.046, over 966531.89 frames. ], batch size: 45, lr: 4.00e-02,
2024-10-08 21:23:23,446 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=13.82 vs. limit=5.975
2024-10-08 21:23:24,001 WARNING [optim.py:503] Scaling gradients by 0.07543207705020905, model_norm_threshold=151886643200.0
2024-10-08 21:23:24,140 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.379e+23, grad_sumsq=2.126e+25, orig_rms_sq=3.471e-02
2024-10-08 21:23:26,859 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten.whitening_limit, batch_count=1950.0, ans=8.23125
2024-10-08 21:23:28,162 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=33.95 vs. limit=8.23125
2024-10-08 21:23:28,733 WARNING [optim.py:503] Scaling gradients by 0.06729096919298172, model_norm_threshold=151886643200.0
2024-10-08 21:23:28,874 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.211e+24, grad_sumsq=1.695e+26, orig_rms_sq=7.143e-03
2024-10-08 21:23:33,223 WARNING [optim.py:503] Scaling gradients by 0.00032689564977772534, model_norm_threshold=151886643200.0
2024-10-08 21:23:33,361 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.468e+28, grad_sumsq=6.024e+27, orig_rms_sq=9.077e+00
2024-10-08 21:23:38,296 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=1953.3333333333333, ans=0.4084375
2024-10-08 21:23:38,700 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=25.14 vs. limit=8.965
2024-10-08 21:23:38,994 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.38 vs. limit=3.293
2024-10-08 21:23:43,695 WARNING [optim.py:503] Scaling gradients by 0.03631431236863136, model_norm_threshold=151886643200.0
2024-10-08 21:23:43,833 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.118e+24, grad_sumsq=1.871e+26, orig_rms_sq=2.736e-02
2024-10-08 21:23:46,220 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=1956.6666666666667, ans=0.40828125
2024-10-08 21:23:47,261 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=1956.6666666666667, ans=0.23043333333333332
2024-10-08 21:23:49,279 WARNING [optim.py:503] Scaling gradients by 0.0027465412858873606, model_norm_threshold=151886643200.0
2024-10-08 21:23:49,419 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.630e+26, grad_sumsq=2.805e+28, orig_rms_sq=2.720e-02
2024-10-08 21:23:55,043 WARNING [optim.py:503] Scaling gradients by 0.011217200197279453, model_norm_threshold=151886643200.0
2024-10-08 21:23:55,185 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.755e+25, grad_sumsq=2.520e+25, orig_rms_sq=2.681e+00
2024-10-08 21:23:57,070 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.32 vs. limit=5.98
2024-10-08 21:23:58,976 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.55 vs. limit=8.235
2024-10-08 21:24:00,715 WARNING [optim.py:503] Scaling gradients by 0.028112053871154785, model_norm_threshold=151886643200.0
2024-10-08 21:24:00,853 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.707e+24, grad_sumsq=3.301e+24, orig_rms_sq=2.637e+00
2024-10-08 21:24:01,902 WARNING [optim.py:503] Scaling gradients by 0.07309010624885559, model_norm_threshold=151886643200.0
2024-10-08 21:24:02,039 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.453e+24, grad_sumsq=5.511e+23, orig_rms_sq=2.636e+00
2024-10-08 21:24:02,822 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.52 vs. limit=8.97
2024-10-08 21:24:03,958 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.87 vs. limit=8.97
2024-10-08 21:24:05,556 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=1960.0, ans=0.1265
2024-10-08 21:24:09,765 WARNING [optim.py:503] Scaling gradients by 0.06559603661298752, model_norm_threshold=151886643200.0
2024-10-08 21:24:09,905 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.339e+24, grad_sumsq=8.878e+23, orig_rms_sq=2.634e+00
2024-10-08 21:24:12,768 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1.whitening_limit, batch_count=1963.3333333333333, ans=5.490833333333334
2024-10-08 21:24:17,407 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.88 vs. limit=5.9816666666666665
2024-10-08 21:24:20,343 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.519e+08 1.606e+10 4.178e+10 4.850e+11 4.646e+14, threshold=8.356e+10, percent-clipped=33.0
2024-10-08 21:24:20,383 INFO [train.py:1154] Epoch 1, batch 5900, loss[loss=1.301, simple_loss=0.666, pruned_loss=0.7573, ctc_loss=1.052, over 4808.00 frames. ], tot_loss[loss=1.284, simple_loss=0.6498, pruned_loss=0.7505, ctc_loss=1.044, over 966715.50 frames. ], batch size: 34, lr: 3.99e-02,
2024-10-08 21:24:20,880 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=25.44 vs. limit=8.975
2024-10-08 21:24:21,345 WARNING [optim.py:503] Scaling gradients by 0.071290984749794, model_norm_threshold=83564683264.0
2024-10-08 21:24:21,485 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.878e+23, grad_sumsq=1.113e+23, orig_rms_sq=2.586e+00
2024-10-08 21:24:24,189 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=24.30 vs. limit=8.975
2024-10-08 21:24:25,188 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=9.75 vs. limit=8.2375
2024-10-08 21:24:26,886 WARNING [optim.py:503] Scaling gradients by 0.01996646821498871, model_norm_threshold=83564683264.0
2024-10-08 21:24:27,025 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.357e+24, grad_sumsq=1.351e+27, orig_rms_sq=6.187e-03
2024-10-08 21:24:28,390 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=1966.6666666666667, ans=0.139375
2024-10-08 21:24:29,245 WARNING [optim.py:503] Scaling gradients by 0.07459060847759247, model_norm_threshold=83564683264.0
2024-10-08 21:24:29,383 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.349e+23, grad_sumsq=6.982e+25, orig_rms_sq=6.228e-03
2024-10-08 21:24:31,612 WARNING [optim.py:503] Scaling gradients by 0.01697998121380806, model_norm_threshold=83564683264.0
2024-10-08 21:24:31,751 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.976e+24, grad_sumsq=1.441e+27, orig_rms_sq=6.228e-03
2024-10-08 21:24:36,506 WARNING [optim.py:503] Scaling gradients by 0.03403133153915405, model_norm_threshold=83564683264.0
2024-10-08 21:24:36,644 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.648e+24, grad_sumsq=2.636e+26, orig_rms_sq=6.255e-03
2024-10-08 21:24:43,052 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.66 vs. limit=5.493333333333333
2024-10-08 21:24:49,944 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten.whitening_limit, batch_count=1973.3333333333333, ans=8.24
2024-10-08 21:24:52,428 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.28 vs. limit=8.98
2024-10-08 21:24:55,273 WARNING [optim.py:503] Scaling gradients by 0.049034539610147476, model_norm_threshold=83564683264.0
2024-10-08 21:24:55,409 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.465e+24, grad_sumsq=2.206e+26, orig_rms_sq=6.643e-03
2024-10-08 21:24:57,166 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.10 vs. limit=8.24125
2024-10-08 21:25:01,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.24 vs. limit=5.494166666666667
2024-10-08 21:25:03,303 WARNING [optim.py:503] Scaling gradients by 0.002791580045595765, model_norm_threshold=83564683264.0
2024-10-08 21:25:03,440 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.691e+26, grad_sumsq=5.988e+25, orig_rms_sq=2.824e+00
2024-10-08 21:25:04,561 WARNING [optim.py:503] Scaling gradients by 0.04738805070519447, model_norm_threshold=83564683264.0
2024-10-08 21:25:04,699 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.386e+23, grad_sumsq=7.846e+25, orig_rms_sq=6.865e-03
2024-10-08 21:25:06,918 WARNING [optim.py:503] Scaling gradients by 0.016585692763328552, model_norm_threshold=83564683264.0
2024-10-08 21:25:07,055 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.421e+24, grad_sumsq=1.627e+26, orig_rms_sq=2.718e-02
2024-10-08 21:25:08,273 WARNING [optim.py:503] Scaling gradients by 0.03884347900748253, model_norm_threshold=83564683264.0
2024-10-08 21:25:08,895 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.365e+24, grad_sumsq=5.020e+25, orig_rms_sq=2.718e-02
2024-10-08 21:25:09,646 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=22.34 vs. limit=8.2425
2024-10-08 21:25:12,130 WARNING [optim.py:503] Scaling gradients by 0.07927737385034561, model_norm_threshold=83564683264.0
2024-10-08 21:25:12,268 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.819e+23, grad_sumsq=5.387e+25, orig_rms_sq=7.089e-03
2024-10-08 21:25:12,921 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=43.35 vs. limit=8.2425
2024-10-08 21:25:14,392 WARNING [optim.py:503] Scaling gradients by 0.0008580650901421905, model_norm_threshold=83564683264.0
2024-10-08 21:25:14,531 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.58, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.483e+27, grad_sumsq=7.736e+29, orig_rms_sq=7.089e-03
2024-10-08 21:25:15,565 WARNING [optim.py:503] Scaling gradients by 0.025177691131830215, model_norm_threshold=83564683264.0
2024-10-08 21:25:15,703 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.532e+24, grad_sumsq=4.983e+26, orig_rms_sq=7.089e-03
2024-10-08 21:25:19,209 INFO [train.py:1154] Epoch 1, batch 5950, loss[loss=1.218, simple_loss=0.6216, pruned_loss=0.7064, ctc_loss=1.004, over 4823.00 frames. ], tot_loss[loss=1.284, simple_loss=0.6491, pruned_loss=0.7507, ctc_loss=1.043, over 966052.83 frames. ], batch size: 34, lr: 3.98e-02,
2024-10-08 21:25:21,442 WARNING [optim.py:503] Scaling gradients by 0.07319875061511993, model_norm_threshold=83564683264.0
2024-10-08 21:25:21,582 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.994e+23, grad_sumsq=4.423e+22, orig_rms_sq=9.030e+00
2024-10-08 21:25:23,881 WARNING [optim.py:503] Scaling gradients by 0.06175863370299339, model_norm_threshold=83564683264.0
2024-10-08 21:25:24,019 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.137e+23, grad_sumsq=5.814e+25, orig_rms_sq=7.116e-03
2024-10-08 21:25:29,993 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.11 vs. limit=5.496666666666667
2024-10-08 21:25:34,347 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=6.676e+04
2024-10-08 21:25:34,727 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.97 vs. limit=5.993333333333333
2024-10-08 21:25:37,619 WARNING [optim.py:503] Scaling gradients by 0.05025327205657959, model_norm_threshold=83564683264.0
2024-10-08 21:25:37,756 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.959e+23, grad_sumsq=2.534e+25, orig_rms_sq=2.746e-02
2024-10-08 21:25:40,041 WARNING [optim.py:503] Scaling gradients by 0.011107615195214748, model_norm_threshold=83564683264.0
2024-10-08 21:25:40,180 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.670e+24, grad_sumsq=1.070e+24, orig_rms_sq=9.040e+00
2024-10-08 21:25:42,430 WARNING [optim.py:503] Scaling gradients by 0.09240736812353134, model_norm_threshold=83564683264.0
2024-10-08 21:25:42,569 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.452e+23, grad_sumsq=2.075e+25, orig_rms_sq=6.997e-03
2024-10-08 21:25:45,125 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=1990.0, ans=0.055225
2024-10-08 21:25:45,747 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.whiten.whitening_limit, batch_count=1990.0, ans=4.796
2024-10-08 21:25:47,906 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=35.34 vs. limit=8.24625
2024-10-08 21:25:51,676 WARNING [optim.py:503] Scaling gradients by 0.012460725381970406, model_norm_threshold=83564683264.0
2024-10-08 21:25:51,814 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.472e+24, grad_sumsq=1.054e+24, orig_rms_sq=8.988e+00
2024-10-08 21:25:55,186 WARNING [optim.py:503] Scaling gradients by 0.03863450512290001, model_norm_threshold=83564683264.0
2024-10-08 21:25:55,325 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.262e+23, grad_sumsq=2.637e+23, orig_rms_sq=2.754e+00
2024-10-08 21:25:56,416 WARNING [optim.py:503] Scaling gradients by 0.0005085412412881851, model_norm_threshold=83564683264.0
2024-10-08 21:25:56,555 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.035e+27, grad_sumsq=1.821e+29, orig_rms_sq=2.764e-02
2024-10-08 21:26:00,540 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.03 vs. limit=8.2475
2024-10-08 21:26:03,413 WARNING [optim.py:503] Scaling gradients by 0.053984250873327255, model_norm_threshold=83564683264.0
2024-10-08 21:26:03,549 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.685e+23, grad_sumsq=7.506e+22, orig_rms_sq=8.906e+00
2024-10-08 21:26:17,433 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.292e+08 1.644e+10 7.554e+10 5.193e+11 1.643e+14, threshold=1.511e+11, percent-clipped=48.0
2024-10-08 21:26:17,474 INFO [train.py:1154] Epoch 1, batch 6000, loss[loss=1.329, simple_loss=0.6882, pruned_loss=0.7583, ctc_loss=1.134, over 4795.00 frames. ], tot_loss[loss=1.285, simple_loss=0.65, pruned_loss=0.7512, ctc_loss=1.045, over 966652.71 frames. ], batch size: 49, lr: 3.98e-02,
2024-10-08 21:26:17,474 INFO [train.py:1177] Computing validation loss
2024-10-08 21:26:22,884 INFO [train.py:1186] Epoch 1, validation: loss=1.349, simple_loss=0.696, pruned_loss=0.7845, ctc_loss=1.084, over 90464.00 frames.
2024-10-08 21:26:22,885 INFO [train.py:1187] Maximum memory allocated so far is 5914MB
2024-10-08 21:26:24,539 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=25.61 vs. limit=9.0
2024-10-08 21:26:27,497 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=2000.0, ans=0.8300000000000001
2024-10-08 21:26:30,520 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1.whitening_limit, batch_count=2000.0, ans=5.5
2024-10-08 21:26:35,318 WARNING [optim.py:503] Scaling gradients by 0.019822414964437485, model_norm_threshold=151073193984.0
2024-10-08 21:26:35,455 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.347e+25, grad_sumsq=3.357e+27, orig_rms_sq=6.991e-03
2024-10-08 21:26:38,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=21.80 vs. limit=9.0025
2024-10-08 21:26:39,651 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten.whitening_limit, batch_count=2003.3333333333333, ans=8.25125
2024-10-08 21:26:40,758 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=13.90 vs. limit=8.25125
2024-10-08 21:26:41,340 WARNING [optim.py:503] Scaling gradients by 0.008414935320615768, model_norm_threshold=151073193984.0
2024-10-08 21:26:41,478 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.072e+25, grad_sumsq=2.932e+25, orig_rms_sq=2.071e+00
2024-10-08 21:26:42,172 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.00 vs. limit=5.5008333333333335
2024-10-08 21:26:47,709 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.53 vs. limit=5.501666666666667
2024-10-08 21:26:48,307 WARNING [optim.py:503] Scaling gradients by 0.056738439947366714, model_norm_threshold=151073193984.0
2024-10-08 21:26:48,446 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.310e+24, grad_sumsq=5.053e+23, orig_rms_sq=2.592e+00
2024-10-08 21:26:52,381 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=67.81 vs. limit=8.2525
2024-10-08 21:26:53,726 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.24 vs. limit=6.003333333333334
2024-10-08 21:26:56,538 WARNING [optim.py:503] Scaling gradients by 0.036235030740499496, model_norm_threshold=151073193984.0
2024-10-08 21:26:56,675 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.555e+24, grad_sumsq=5.508e+26, orig_rms_sq=6.454e-03
2024-10-08 21:26:57,240 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=29.27 vs. limit=9.0075
2024-10-08 21:26:57,979 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=4.410e+02
2024-10-08 21:26:58,836 WARNING [optim.py:503] Scaling gradients by 0.0024000408593565226, model_norm_threshold=151073193984.0
2024-10-08 21:26:58,974 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.791e+26, grad_sumsq=2.510e+28, orig_rms_sq=3.503e-02
2024-10-08 21:27:02,220 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.14 vs. limit=3.3015
2024-10-08 21:27:02,497 WARNING [optim.py:503] Scaling gradients by 0.0668572410941124, model_norm_threshold=151073193984.0
2024-10-08 21:27:02,638 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.135e+24, grad_sumsq=3.388e+26, orig_rms_sq=6.301e-03
2024-10-08 21:27:03,528 INFO [scaling.py:1024] Whitening: name=encoder_embed.convnext.out_whiten, num_groups=1, num_channels=128, metric=5.22 vs. limit=5.0
2024-10-08 21:27:05,981 WARNING [optim.py:503] Scaling gradients by 0.07347795367240906, model_norm_threshold=151073193984.0
2024-10-08 21:27:06,119 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.696e+24, grad_sumsq=2.744e+26, orig_rms_sq=6.181e-03
2024-10-08 21:27:09,064 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.03 vs. limit=5.503333333333333
2024-10-08 21:27:10,560 WARNING [optim.py:503] Scaling gradients by 0.003367480356246233, model_norm_threshold=151073193984.0
2024-10-08 21:27:10,699 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.501e+26, grad_sumsq=1.035e+28, orig_rms_sq=3.384e-02
2024-10-08 21:27:11,969 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.min_positive, batch_count=2013.3333333333333, ans=0.043708333333333335
2024-10-08 21:27:12,912 WARNING [optim.py:503] Scaling gradients by 0.054980337619781494, model_norm_threshold=151073193984.0
2024-10-08 21:27:13,049 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.605e+24, grad_sumsq=2.612e+26, orig_rms_sq=6.146e-03
2024-10-08 21:27:20,793 INFO [train.py:1154] Epoch 1, batch 6050, loss[loss=1.353, simple_loss=0.6818, pruned_loss=0.7974, ctc_loss=1.075, over 4813.00 frames. ], tot_loss[loss=1.287, simple_loss=0.6504, pruned_loss=0.7526, ctc_loss=1.044, over 966584.47 frames. ], batch size: 19, lr: 3.97e-02,
2024-10-08 21:27:29,341 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.39 vs. limit=8.25625
2024-10-08 21:27:29,927 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=2016.6666666666667, ans=0.054625
2024-10-08 21:27:32,330 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=2020.0, ans=0.4053125
2024-10-08 21:27:33,568 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=2020.0, ans=0.12425
2024-10-08 21:27:36,748 WARNING [optim.py:503] Scaling gradients by 0.011796055361628532, model_norm_threshold=151073193984.0
2024-10-08 21:27:36,887 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.073e+25, grad_sumsq=9.247e+26, orig_rms_sq=3.323e-02
2024-10-08 21:27:37,924 WARNING [optim.py:503] Scaling gradients by 0.00869145616889, model_norm_threshold=151073193984.0
2024-10-08 21:27:38,082 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.377e+25, grad_sumsq=2.042e+25, orig_rms_sq=2.633e+00
2024-10-08 21:27:44,478 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=13.92 vs. limit=8.25875
2024-10-08 21:27:44,841 WARNING [optim.py:503] Scaling gradients by 0.007570657879114151, model_norm_threshold=151073193984.0
2024-10-08 21:27:44,979 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.332e+25, grad_sumsq=3.028e+27, orig_rms_sq=2.751e-02
2024-10-08 21:27:49,522 WARNING [optim.py:503] Scaling gradients by 0.0034457265865057707, model_norm_threshold=151073193984.0
2024-10-08 21:27:49,662 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.896e+26, grad_sumsq=8.195e+28, orig_rms_sq=5.974e-03
2024-10-08 21:27:51,390 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=45.58 vs. limit=8.25875
2024-10-08 21:27:53,892 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=22.79 vs. limit=8.25875
2024-10-08 21:27:57,466 WARNING [optim.py:503] Scaling gradients by 0.0002020949759753421, model_norm_threshold=151073193984.0
2024-10-08 21:27:57,605 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.383e+29, grad_sumsq=5.074e+28, orig_rms_sq=2.726e+00
2024-10-08 21:27:57,835 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=2026.6666666666667, ans=0.405
2024-10-08 21:27:59,796 WARNING [optim.py:503] Scaling gradients by 0.024742435663938522, model_norm_threshold=151073193984.0
2024-10-08 21:27:59,935 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.783e+24, grad_sumsq=1.112e+24, orig_rms_sq=8.801e+00
2024-10-08 21:28:02,098 WARNING [optim.py:503] Scaling gradients by 0.051200781017541885, model_norm_threshold=151073193984.0
2024-10-08 21:28:02,726 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.972e+24, grad_sumsq=2.240e+23, orig_rms_sq=8.801e+00
2024-10-08 21:28:04,528 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.72 vs. limit=6.013333333333334
2024-10-08 21:28:08,856 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=2030.0, ans=0.5
2024-10-08 21:28:12,019 WARNING [optim.py:503] Scaling gradients by 0.00017403917445335537, model_norm_threshold=151073193984.0
2024-10-08 21:28:12,158 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.927e+29, grad_sumsq=7.060e+28, orig_rms_sq=2.729e+00
2024-10-08 21:28:13,293 WARNING [optim.py:503] Scaling gradients by 0.0316404327750206, model_norm_threshold=151073193984.0
2024-10-08 21:28:13,431 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.525e+24, grad_sumsq=2.025e+24, orig_rms_sq=2.729e+00
2024-10-08 21:28:19,021 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 8.997e+08 3.290e+10 1.377e+11 5.574e+11 8.680e+14, threshold=2.754e+11, percent-clipped=49.0
2024-10-08 21:28:19,062 INFO [train.py:1154] Epoch 1, batch 6100, loss[loss=1.335, simple_loss=0.6872, pruned_loss=0.7741, ctc_loss=1.084, over 4787.00 frames. ], tot_loss[loss=1.287, simple_loss=0.65, pruned_loss=0.7526, ctc_loss=1.045, over 966083.82 frames. ], batch size: 34, lr: 3.96e-02,
2024-10-08 21:28:25,804 WARNING [optim.py:503] Scaling gradients by 0.03494611382484436, model_norm_threshold=275400196096.0
2024-10-08 21:28:25,943 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.070e+25, grad_sumsq=3.759e+26, orig_rms_sq=2.847e-02
2024-10-08 21:28:32,850 WARNING [optim.py:503] Scaling gradients by 0.00656879972666502, model_norm_threshold=275400196096.0
2024-10-08 21:28:32,988 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.467e+26, grad_sumsq=1.594e+28, orig_rms_sq=2.802e-02
2024-10-08 21:28:40,797 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.19 vs. limit=8.26375
2024-10-08 21:28:44,494 WARNING [optim.py:503] Scaling gradients by 0.03451938554644585, model_norm_threshold=275400196096.0
2024-10-08 21:28:44,631 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.160e+25, grad_sumsq=8.361e+24, orig_rms_sq=2.584e+00
2024-10-08 21:28:46,807 WARNING [optim.py:503] Scaling gradients by 0.0009904967155307531, model_norm_threshold=275400196096.0
2024-10-08 21:28:46,945 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.780e+28, grad_sumsq=7.101e+27, orig_rms_sq=2.507e+00
2024-10-08 21:28:49,062 WARNING [optim.py:503] Scaling gradients by 0.01701021008193493, model_norm_threshold=275400196096.0
2024-10-08 21:28:49,199 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.301e+26, grad_sumsq=2.279e+28, orig_rms_sq=5.710e-03
2024-10-08 21:28:51,324 WARNING [optim.py:503] Scaling gradients by 0.0018596776062622666, model_norm_threshold=275400196096.0
2024-10-08 21:28:51,462 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.349e+27, grad_sumsq=1.306e+30, orig_rms_sq=5.629e-03
2024-10-08 21:28:52,776 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2043.3333333333333, ans=0.27956666666666663
2024-10-08 21:28:53,122 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=30.06 vs. limit=9.0325
2024-10-08 21:28:54,826 WARNING [optim.py:503] Scaling gradients by 0.01481618732213974, model_norm_threshold=275400196096.0
2024-10-08 21:28:54,963 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.258e+26, grad_sumsq=2.234e+28, orig_rms_sq=5.629e-03
2024-10-08 21:28:55,664 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.79 vs. limit=9.0325
2024-10-08 21:28:58,989 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=14.72 vs. limit=8.26625
2024-10-08 21:29:00,503 WARNING [optim.py:503] Scaling gradients by 0.012083923444151878, model_norm_threshold=275400196096.0
2024-10-08 21:29:00,642 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.385e+26, grad_sumsq=4.693e+27, orig_rms_sq=2.950e-02
2024-10-08 21:29:03,414 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.64 vs. limit=6.0216666666666665
2024-10-08 21:29:06,394 WARNING [optim.py:503] Scaling gradients by 0.08148429542779922, model_norm_threshold=275400196096.0
2024-10-08 21:29:06,531 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.72, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.202e+24, grad_sumsq=1.479e+27, orig_rms_sq=5.546e-03
2024-10-08 21:29:10,191 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer2.prob, batch_count=2046.6666666666667, ans=0.4040625
2024-10-08 21:29:15,219 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys.whitening_limit, batch_count=2046.6666666666667, ans=3.307
2024-10-08 21:29:16,181 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.56 vs. limit=6.025
2024-10-08 21:29:16,779 WARNING [optim.py:503] Scaling gradients by 0.037622179836034775, model_norm_threshold=275400196096.0
2024-10-08 21:29:16,919 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.59, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.155e+25, grad_sumsq=5.593e+27, orig_rms_sq=5.641e-03
2024-10-08 21:29:16,965 INFO [train.py:1154] Epoch 1, batch 6150, loss[loss=1.313, simple_loss=0.6736, pruned_loss=0.7509, ctc_loss=1.127, over 4829.00 frames. ], tot_loss[loss=1.286, simple_loss=0.6503, pruned_loss=0.7515, ctc_loss=1.045, over 966182.97 frames. ], batch size: 43, lr: 3.96e-02,
2024-10-08 21:29:19,176 WARNING [optim.py:503] Scaling gradients by 0.08307268470525742, model_norm_threshold=275400196096.0
2024-10-08 21:29:19,316 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.991e+24, grad_sumsq=8.018e+23, orig_rms_sq=2.483e+00
2024-10-08 21:29:22,605 WARNING [optim.py:503] Scaling gradients by 0.028268875554203987, model_norm_threshold=275400196096.0
2024-10-08 21:29:22,743 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.809e+25, grad_sumsq=7.285e+24, orig_rms_sq=2.483e+00
2024-10-08 21:29:23,566 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.52 vs. limit=6.025
2024-10-08 21:29:30,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=2053.3333333333335, ans=0.8281333333333334
2024-10-08 21:29:33,250 WARNING [optim.py:503] Scaling gradients by 0.04142779856920242, model_norm_threshold=275400196096.0
2024-10-08 21:29:33,390 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.050e+25, grad_sumsq=5.516e+24, orig_rms_sq=1.904e+00
2024-10-08 21:29:34,178 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.77 vs. limit=9.040000000000001
2024-10-08 21:29:35,737 WARNING [optim.py:503] Scaling gradients by 0.02788677252829075, model_norm_threshold=275400196096.0
2024-10-08 21:29:35,877 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.211e+25, grad_sumsq=1.052e+27, orig_rms_sq=3.051e-02
2024-10-08 21:29:39,825 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=39.36 vs. limit=9.0425
2024-10-08 21:29:40,344 WARNING [optim.py:503] Scaling gradients by 0.04870562627911568, model_norm_threshold=275400196096.0
2024-10-08 21:29:40,483 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.060e+24, grad_sumsq=6.692e+23, orig_rms_sq=9.056e+00
2024-10-08 21:29:41,021 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.74 vs. limit=6.028333333333333
2024-10-08 21:29:41,597 WARNING [optim.py:503] Scaling gradients by 0.005602212157100439, model_norm_threshold=275400196096.0
2024-10-08 21:29:41,736 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.238e+26, grad_sumsq=2.077e+28, orig_rms_sq=3.004e-02
2024-10-08 21:29:42,937 WARNING [optim.py:503] Scaling gradients by 0.06051096320152283, model_norm_threshold=275400196096.0
2024-10-08 21:29:43,077 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.362e+24, grad_sumsq=2.164e+26, orig_rms_sq=2.940e-02
2024-10-08 21:29:45,937 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=5.11 vs. limit=8.27125
2024-10-08 21:29:48,086 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=2056.6666666666665, ans=0.23085
2024-10-08 21:29:49,013 WARNING [optim.py:503] Scaling gradients by 0.0029343224596232176, model_norm_threshold=275400196096.0
2024-10-08 21:29:49,152 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.307e+27, grad_sumsq=8.663e+26, orig_rms_sq=2.663e+00
2024-10-08 21:29:50,181 WARNING [optim.py:503] Scaling gradients by 0.01858752965927124, model_norm_threshold=275400196096.0
2024-10-08 21:29:50,318 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.416e+25, grad_sumsq=1.658e+25, orig_rms_sq=2.663e+00
2024-10-08 21:30:01,728 WARNING [optim.py:503] Scaling gradients by 0.09184981137514114, model_norm_threshold=275400196096.0
2024-10-08 21:30:01,867 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.476e+24, grad_sumsq=4.104e+26, orig_rms_sq=6.033e-03
2024-10-08 21:30:07,006 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=15.60 vs. limit=5.515833333333333
2024-10-08 21:30:07,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=53.41 vs. limit=8.27375
2024-10-08 21:30:09,594 WARNING [optim.py:503] Scaling gradients by 0.002311387564986944, model_norm_threshold=275400196096.0
2024-10-08 21:30:09,734 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.381e+27, grad_sumsq=7.309e+29, orig_rms_sq=5.994e-03
2024-10-08 21:30:15,672 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.268e+09 7.434e+10 3.560e+11 1.993e+12 2.780e+14, threshold=7.120e+11, percent-clipped=54.0
2024-10-08 21:30:15,712 INFO [train.py:1154] Epoch 1, batch 6200, loss[loss=1.463, simple_loss=0.7468, pruned_loss=0.8569, ctc_loss=1.163, over 4786.00 frames. ], tot_loss[loss=1.286, simple_loss=0.6509, pruned_loss=0.7514, ctc_loss=1.047, over 966399.01 frames. ], batch size: 29, lr: 3.95e-02,
2024-10-08 21:30:34,470 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=7.07 vs. limit=4.828
2024-10-08 21:30:34,838 WARNING [optim.py:503] Scaling gradients by 0.09095463156700134, model_norm_threshold=711986577408.0
2024-10-08 21:30:34,975 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.122e+25, grad_sumsq=7.418e+24, orig_rms_sq=2.861e+00
2024-10-08 21:30:36,088 WARNING [optim.py:503] Scaling gradients by 0.0034347742330282927, model_norm_threshold=711986577408.0
2024-10-08 21:30:36,227 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.065e+28, grad_sumsq=7.220e+27, orig_rms_sq=2.861e+00
2024-10-08 21:30:38,545 WARNING [optim.py:503] Scaling gradients by 0.013722194358706474, model_norm_threshold=711986577408.0
2024-10-08 21:30:38,682 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.61, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.653e+27, grad_sumsq=5.790e+26, orig_rms_sq=2.856e+00
2024-10-08 21:30:42,098 WARNING [optim.py:503] Scaling gradients by 0.012418021447956562, model_norm_threshold=711986577408.0
2024-10-08 21:30:42,237 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.501e+27, grad_sumsq=5.258e+26, orig_rms_sq=2.856e+00
2024-10-08 21:30:44,153 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.68 vs. limit=9.055
2024-10-08 21:30:44,473 WARNING [optim.py:503] Scaling gradients by 0.04280495643615723, model_norm_threshold=711986577408.0
2024-10-08 21:30:44,612 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.183e+26, grad_sumsq=4.095e+25, orig_rms_sq=2.890e+00
2024-10-08 21:30:48,389 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=2073.3333333333335, ans=0.12225
2024-10-08 21:30:49,455 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=2076.6666666666665, ans=0.053275
2024-10-08 21:30:49,486 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=2076.6666666666665, ans=0.40265625
2024-10-08 21:30:50,483 WARNING [optim.py:503] Scaling gradients by 0.0011153798550367355, model_norm_threshold=711986577408.0
2024-10-08 21:30:50,620 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.352e+29, grad_sumsq=2.321e+31, orig_rms_sq=5.825e-03
2024-10-08 21:30:57,596 WARNING [optim.py:503] Scaling gradients by 0.024797750636935234, model_norm_threshold=711986577408.0
2024-10-08 21:30:57,736 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.416e+26, grad_sumsq=3.994e+28, orig_rms_sq=6.047e-03
2024-10-08 21:31:06,994 WARNING [optim.py:503] Scaling gradients by 0.03852299973368645, model_norm_threshold=711986577408.0
2024-10-08 21:31:07,134 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.58, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.969e+26, grad_sumsq=3.335e+28, orig_rms_sq=5.905e-03
2024-10-08 21:31:07,841 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.41 vs. limit=8.28
2024-10-08 21:31:07,971 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=10.34 vs. limit=9.06
2024-10-08 21:31:08,276 WARNING [optim.py:503] Scaling gradients by 0.07807089388370514, model_norm_threshold=711986577408.0
2024-10-08 21:31:08,413 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.509e+25, grad_sumsq=7.876e+24, orig_rms_sq=3.186e+00
2024-10-08 21:31:10,567 WARNING [optim.py:503] Scaling gradients by 0.024001505225896835, model_norm_threshold=711986577408.0
2024-10-08 21:31:10,705 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.257e+26, grad_sumsq=7.734e+27, orig_rms_sq=2.919e-02
2024-10-08 21:31:14,047 WARNING [optim.py:503] Scaling gradients by 0.04682858660817146, model_norm_threshold=711986577408.0
2024-10-08 21:31:14,190 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.400e+25, grad_sumsq=2.209e+27, orig_rms_sq=2.897e-02
2024-10-08 21:31:14,236 INFO [train.py:1154] Epoch 1, batch 6250, loss[loss=1.283, simple_loss=0.6444, pruned_loss=0.7464, ctc_loss=1.073, over 4731.00 frames. ], tot_loss[loss=1.283, simple_loss=0.6498, pruned_loss=0.7491, ctc_loss=1.044, over 966780.96 frames. ], batch size: 26, lr: 3.94e-02,
2024-10-08 21:31:14,354 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2083.3333333333335, ans=0.2791666666666667
2024-10-08 21:31:16,674 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=2083.3333333333335, ans=0.40234375
2024-10-08 21:31:22,555 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=45.01 vs. limit=8.28125
2024-10-08 21:31:23,039 WARNING [optim.py:503] Scaling gradients by 0.06724949926137924, model_norm_threshold=711986577408.0
2024-10-08 21:31:23,181 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.113e+25, grad_sumsq=5.572e+27, orig_rms_sq=5.587e-03
2024-10-08 21:31:24,492 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.prob, batch_count=2086.6666666666665, ans=0.40218750000000003
2024-10-08 21:31:26,564 WARNING [optim.py:503] Scaling gradients by 0.09654094278812408, model_norm_threshold=711986577408.0
2024-10-08 21:31:26,705 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.049e+25, grad_sumsq=3.679e+27, orig_rms_sq=5.568e-03
2024-10-08 21:31:27,813 WARNING [optim.py:503] Scaling gradients by 0.05168074369430542, model_norm_threshold=711986577408.0
2024-10-08 21:31:27,959 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.887e+25, grad_sumsq=6.981e+27, orig_rms_sq=5.568e-03
2024-10-08 21:31:31,272 WARNING [optim.py:503] Scaling gradients by 0.05942719429731369, model_norm_threshold=711986577408.0
2024-10-08 21:31:31,413 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.400e+25, grad_sumsq=4.297e+27, orig_rms_sq=5.585e-03
2024-10-08 21:31:32,037 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=41.43 vs. limit=8.2825
2024-10-08 21:31:32,621 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.min_abs, batch_count=2086.6666666666665, ans=0.2313
2024-10-08 21:31:33,868 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=2086.6666666666665, ans=0.2313
2024-10-08 21:31:37,014 WARNING [optim.py:503] Scaling gradients by 0.012580465525388718, model_norm_threshold=711986577408.0
2024-10-08 21:31:37,154 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.196e+27, grad_sumsq=1.382e+26, orig_rms_sq=8.656e+00
2024-10-08 21:31:41,687 WARNING [optim.py:503] Scaling gradients by 0.014001894742250443, model_norm_threshold=711986577408.0
2024-10-08 21:31:41,825 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.054e+27, grad_sumsq=1.887e+29, orig_rms_sq=5.588e-03
2024-10-08 21:31:46,645 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=2090.0, ans=0.5
2024-10-08 21:31:55,322 WARNING [optim.py:503] Scaling gradients by 0.06432604789733887, model_norm_threshold=711986577408.0
2024-10-08 21:31:55,461 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.568e+25, grad_sumsq=1.019e+28, orig_rms_sq=5.462e-03
2024-10-08 21:31:56,524 WARNING [optim.py:503] Scaling gradients by 0.019950632005929947, model_norm_threshold=711986577408.0
2024-10-08 21:31:56,662 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.568e+26, grad_sumsq=8.362e+28, orig_rms_sq=5.462e-03
2024-10-08 21:32:01,327 WARNING [optim.py:503] Scaling gradients by 0.0863608792424202, model_norm_threshold=711986577408.0
2024-10-08 21:32:01,468 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.53, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.606e+25, grad_sumsq=6.579e+27, orig_rms_sq=5.481e-03
2024-10-08 21:32:02,719 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.prob, batch_count=2096.6666666666665, ans=0.40171875
2024-10-08 21:32:02,754 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=2096.6666666666665, ans=0.12137500000000001
2024-10-08 21:32:03,112 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=16.38 vs. limit=8.286249999999999
2024-10-08 21:32:03,725 WARNING [optim.py:503] Scaling gradients by 0.02116602100431919, model_norm_threshold=711986577408.0
2024-10-08 21:32:03,863 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.350e+26, grad_sumsq=8.194e+27, orig_rms_sq=2.868e-02
2024-10-08 21:32:05,410 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=19.43 vs. limit=6.048333333333333
2024-10-08 21:32:06,817 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.33 vs. limit=6.048333333333333
2024-10-08 21:32:08,269 WARNING [optim.py:503] Scaling gradients by 0.00911065936088562, model_norm_threshold=711986577408.0
2024-10-08 21:32:08,407 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.585e+27, grad_sumsq=4.620e+29, orig_rms_sq=5.595e-03
2024-10-08 21:32:10,785 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2100.0, ans=0.27899999999999997
2024-10-08 21:32:11,834 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.028e+09 1.414e+11 5.092e+11 4.068e+12 6.383e+14, threshold=1.018e+12, percent-clipped=44.0
2024-10-08 21:32:11,874 INFO [train.py:1154] Epoch 1, batch 6300, loss[loss=1.34, simple_loss=0.6613, pruned_loss=0.7964, ctc_loss=1.063, over 4978.00 frames. ], tot_loss[loss=1.281, simple_loss=0.6487, pruned_loss=0.7478, ctc_loss=1.043, over 966447.34 frames. ], batch size: 19, lr: 3.94e-02,
2024-10-08 21:32:12,295 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=66.63 vs. limit=8.2875
2024-10-08 21:32:13,612 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.80 vs. limit=9.075
2024-10-08 21:32:14,229 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=2100.0, ans=0.08687500000000001
2024-10-08 21:32:15,093 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=8.88 vs. limit=4.42
2024-10-08 21:32:15,247 WARNING [optim.py:503] Scaling gradients by 0.01178473886102438, model_norm_threshold=1018328383488.0
2024-10-08 21:32:15,386 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.541e+27, grad_sumsq=6.271e+29, orig_rms_sq=5.647e-03
2024-10-08 21:32:16,458 WARNING [optim.py:503] Scaling gradients by 0.028870081529021263, model_norm_threshold=1018328383488.0
2024-10-08 21:32:16,597 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.61, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.557e+26, grad_sumsq=1.326e+29, orig_rms_sq=5.697e-03
2024-10-08 21:32:17,857 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=2100.0, ans=0.4015625
2024-10-08 21:32:19,622 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten.whitening_limit, batch_count=2100.0, ans=8.2875
2024-10-08 21:32:22,393 WARNING [optim.py:503] Scaling gradients by 0.06591567397117615, model_norm_threshold=1018328383488.0
2024-10-08 21:32:22,531 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.036e+26, grad_sumsq=1.794e+28, orig_rms_sq=5.775e-03
2024-10-08 21:32:22,936 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.00 vs. limit=5.525833333333333
2024-10-08 21:32:23,575 WARNING [optim.py:503] Scaling gradients by 0.013342628255486488, model_norm_threshold=1018328383488.0
2024-10-08 21:32:23,713 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.200e+27, grad_sumsq=2.078e+29, orig_rms_sq=5.775e-03
2024-10-08 21:32:27,345 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=2103.3333333333335, ans=0.121125
2024-10-08 21:32:29,023 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.45 vs. limit=6.051666666666667
2024-10-08 21:32:30,506 WARNING [optim.py:503] Scaling gradients by 0.004362873733043671, model_norm_threshold=1018328383488.0
2024-10-08 21:32:30,643 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.436e+28, grad_sumsq=8.498e+27, orig_rms_sq=1.690e+00
2024-10-08 21:32:33,961 WARNING [optim.py:503] Scaling gradients by 0.04262414202094078, model_norm_threshold=1018328383488.0
2024-10-08 21:32:34,099 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.416e+26, grad_sumsq=4.100e+28, orig_rms_sq=5.894e-03
2024-10-08 21:32:34,733 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=34.33 vs. limit=8.29
2024-10-08 21:32:43,275 WARNING [optim.py:503] Scaling gradients by 0.06949884444475174, model_norm_threshold=1018328383488.0
2024-10-08 21:32:43,415 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.277e+25, grad_sumsq=1.097e+28, orig_rms_sq=5.720e-03
2024-10-08 21:32:44,527 WARNING [optim.py:503] Scaling gradients by 0.003510403912514448, model_norm_threshold=1018328383488.0
2024-10-08 21:32:44,666 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.154e+28, grad_sumsq=3.830e+30, orig_rms_sq=5.625e-03
2024-10-08 21:32:49,230 WARNING [optim.py:503] Scaling gradients by 0.0029377778992056847, model_norm_threshold=1018328383488.0
2024-10-08 21:32:49,369 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.238e+28, grad_sumsq=7.559e+30, orig_rms_sq=5.606e-03
2024-10-08 21:32:49,762 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=15.44 vs. limit=6.055
2024-10-08 21:32:52,085 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=44.79 vs. limit=8.29125
2024-10-08 21:32:52,593 WARNING [optim.py:503] Scaling gradients by 0.0009435797692276537, model_norm_threshold=1018328383488.0
2024-10-08 21:32:52,732 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.523e+29, grad_sumsq=8.731e+28, orig_rms_sq=2.889e+00
2024-10-08 21:32:58,166 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.65 vs. limit=5.528333333333333
2024-10-08 21:32:58,638 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=2113.3333333333335, ans=0.08679166666666667
2024-10-08 21:32:59,111 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=35.83 vs. limit=9.085
2024-10-08 21:33:01,881 WARNING [optim.py:503] Scaling gradients by 0.07557441294193268, model_norm_threshold=1018328383488.0
2024-10-08 21:33:02,020 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.085e+25, grad_sumsq=1.102e+27, orig_rms_sq=2.799e-02
2024-10-08 21:33:05,051 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=30.10 vs. limit=8.2925
2024-10-08 21:33:06,262 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=6.76 vs. limit=4.8453333333333335
2024-10-08 21:33:10,212 WARNING [optim.py:503] Scaling gradients by 0.04398999363183975, model_norm_threshold=1018328383488.0
2024-10-08 21:33:10,351 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.333e+26, grad_sumsq=4.330e+28, orig_rms_sq=5.388e-03
2024-10-08 21:33:10,397 INFO [train.py:1154] Epoch 1, batch 6350, loss[loss=1.383, simple_loss=0.7057, pruned_loss=0.7934, ctc_loss=1.182, over 4817.00 frames. ], tot_loss[loss=1.281, simple_loss=0.6478, pruned_loss=0.7486, ctc_loss=1.043, over 966135.03 frames. ], batch size: 36, lr: 3.93e-02,
2024-10-08 21:33:12,658 WARNING [optim.py:503] Scaling gradients by 0.006190385669469833, model_norm_threshold=1018328383488.0
2024-10-08 21:33:12,797 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.57, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.535e+28, grad_sumsq=2.853e+30, orig_rms_sq=5.379e-03
2024-10-08 21:33:13,905 WARNING [optim.py:503] Scaling gradients by 0.05952294543385506, model_norm_threshold=1018328383488.0
2024-10-08 21:33:14,045 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.806e+25, grad_sumsq=2.101e+25, orig_rms_sq=2.763e+00
2024-10-08 21:33:15,079 WARNING [optim.py:503] Scaling gradients by 0.022784486413002014, model_norm_threshold=1018328383488.0
2024-10-08 21:33:15,217 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.812e+26, grad_sumsq=5.299e+25, orig_rms_sq=9.081e+00
2024-10-08 21:33:16,271 WARNING [optim.py:503] Scaling gradients by 0.003175397403538227, model_norm_threshold=1018328383488.0
2024-10-08 21:33:16,408 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.038e+28, grad_sumsq=9.366e+30, orig_rms_sq=5.379e-03
2024-10-08 21:33:18,859 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=2116.6666666666665, ans=0.04338541666666667
2024-10-08 21:33:20,806 WARNING [optim.py:503] Scaling gradients by 0.004256766755133867, model_norm_threshold=1018328383488.0
2024-10-08 21:33:20,945 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.308e+28, grad_sumsq=4.263e+30, orig_rms_sq=5.413e-03
2024-10-08 21:33:25,511 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=27.17 vs. limit=8.295
2024-10-08 21:33:26,272 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=2120.0, ans=0.1205
2024-10-08 21:33:27,153 WARNING [optim.py:503] Scaling gradients by 0.0017909468151628971, model_norm_threshold=1018328383488.0
2024-10-08 21:33:27,293 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.326e+28, grad_sumsq=3.370e+28, orig_rms_sq=2.767e+00
2024-10-08 21:33:27,714 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=20.84 vs. limit=9.09
2024-10-08 21:33:29,045 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=27.14 vs. limit=8.295
2024-10-08 21:33:31,939 WARNING [optim.py:503] Scaling gradients by 0.0920279324054718, model_norm_threshold=1018328383488.0
2024-10-08 21:33:32,078 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.729e+25, grad_sumsq=1.023e+28, orig_rms_sq=5.600e-03
2024-10-08 21:33:32,933 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.66 vs. limit=8.295
2024-10-08 21:33:33,732 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=17.50 vs. limit=8.29625
2024-10-08 21:33:34,948 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=19.27 vs. limit=8.29625
2024-10-08 21:33:35,796 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.bypass.skip_rate, batch_count=2123.3333333333335, ans=0.07
2024-10-08 21:33:36,146 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.98 vs. limit=5.530833333333334
2024-10-08 21:33:38,787 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=16.11 vs. limit=9.092500000000001
2024-10-08 21:33:42,881 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=2123.3333333333335, ans=0.27876666666666666
2024-10-08 21:33:46,201 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=2126.6666666666665, ans=0.8255666666666667
2024-10-08 21:33:54,445 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=3.550e+00
2024-10-08 21:33:56,181 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.73 vs. limit=9.094999999999999
2024-10-08 21:33:59,386 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.90 vs. limit=6.0649999999999995
2024-10-08 21:33:59,484 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2.whitening_limit, batch_count=2130.0, ans=6.0649999999999995
2024-10-08 21:34:00,040 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.prob, batch_count=2130.0, ans=0.40015625
2024-10-08 21:34:00,966 WARNING [optim.py:503] Scaling gradients by 0.009222231805324554, model_norm_threshold=1018328383488.0
2024-10-08 21:34:01,103 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.008e+27, grad_sumsq=8.084e+29, orig_rms_sq=6.195e-03
2024-10-08 21:34:09,301 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.579e+09 1.977e+11 8.837e+11 6.557e+12 1.079e+15, threshold=1.767e+12, percent-clipped=46.0
2024-10-08 21:34:09,341 INFO [train.py:1154] Epoch 1, batch 6400, loss[loss=1.313, simple_loss=0.6531, pruned_loss=0.7735, ctc_loss=1.063, over 4882.00 frames. ], tot_loss[loss=1.28, simple_loss=0.6469, pruned_loss=0.7488, ctc_loss=1.041, over 965893.85 frames. ], batch size: 23, lr: 3.92e-02,
2024-10-08 21:34:09,526 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.attention_skip_rate, batch_count=2133.3333333333335, ans=0.12
2024-10-08 21:34:10,441 WARNING [optim.py:503] Scaling gradients by 0.037246569991111755, model_norm_threshold=1767377469440.0
2024-10-08 21:34:10,579 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.223e+26, grad_sumsq=2.203e+26, orig_rms_sq=2.824e+00
2024-10-08 21:34:10,970 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=22.02 vs. limit=8.3
2024-10-08 21:34:11,913 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2133.3333333333335, ans=0.4
2024-10-08 21:34:12,897 WARNING [optim.py:503] Scaling gradients by 0.07760169357061386, model_norm_threshold=1767377469440.0
2024-10-08 21:34:13,036 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.331e+26, grad_sumsq=3.885e+28, orig_rms_sq=6.001e-03
2024-10-08 21:34:15,251 WARNING [optim.py:503] Scaling gradients by 0.007914935238659382, model_norm_threshold=1767377469440.0
2024-10-08 21:34:15,389 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.582e+27, grad_sumsq=9.539e+26, orig_rms_sq=8.996e+00
2024-10-08 21:34:24,714 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=2136.6666666666665, ans=0.11987500000000001
2024-10-08 21:34:30,199 WARNING [optim.py:503] Scaling gradients by 0.0018064521718770266, model_norm_threshold=1767377469440.0
2024-10-08 21:34:30,336 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.103e+29, grad_sumsq=2.319e+28, orig_rms_sq=9.066e+00
2024-10-08 21:34:46,230 WARNING [optim.py:503] Scaling gradients by 0.0002359928475925699, model_norm_threshold=1767377469440.0
2024-10-08 21:34:46,370 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.019e+31, grad_sumsq=5.393e+33, orig_rms_sq=5.599e-03
2024-10-08 21:34:58,204 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.94 vs. limit=8.305
2024-10-08 21:35:00,400 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=26.02 vs. limit=8.305
2024-10-08 21:35:01,936 WARNING [optim.py:503] Scaling gradients by 0.00022791692754253745, model_norm_threshold=1767377469440.0
2024-10-08 21:35:02,079 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.306e+31, grad_sumsq=1.464e+30, orig_rms_sq=8.923e+00
2024-10-08 21:35:03,165 WARNING [optim.py:503] Scaling gradients by 0.008063143119215965, model_norm_threshold=1767377469440.0
2024-10-08 21:35:03,306 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.016e+28, grad_sumsq=3.533e+30, orig_rms_sq=5.707e-03
2024-10-08 21:35:03,983 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=31.97 vs. limit=9.11
2024-10-08 21:35:04,049 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.34 vs. limit=9.11
2024-10-08 21:35:05,572 WARNING [optim.py:503] Scaling gradients by 0.0690092146396637, model_norm_threshold=1767377469440.0
2024-10-08 21:35:05,711 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.305e+26, grad_sumsq=4.435e+27, orig_rms_sq=2.941e-02
2024-10-08 21:35:06,842 WARNING [optim.py:503] Scaling gradients by 0.07661188393831253, model_norm_threshold=1767377469440.0
2024-10-08 21:35:06,982 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.812e+26, grad_sumsq=3.160e+28, orig_rms_sq=5.733e-03
2024-10-08 21:35:07,028 INFO [train.py:1154] Epoch 1, batch 6450, loss[loss=1.282, simple_loss=0.6443, pruned_loss=0.7479, ctc_loss=1.062, over 4740.00 frames. ], tot_loss[loss=1.281, simple_loss=0.6469, pruned_loss=0.7493, ctc_loss=1.044, over 965238.39 frames. ], batch size: 26, lr: 3.92e-02,
2024-10-08 21:35:07,129 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=2150.0, ans=0.1290625
2024-10-08 21:35:13,683 WARNING [optim.py:503] Scaling gradients by 0.010212917812168598, model_norm_threshold=1767377469440.0
2024-10-08 21:35:13,821 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.162e+27, grad_sumsq=1.748e+29, orig_rms_sq=2.953e-02
2024-10-08 21:35:17,237 WARNING [optim.py:503] Scaling gradients by 0.0011187114287167788, model_norm_threshold=1767377469440.0
2024-10-08 21:35:17,379 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.910e+29, grad_sumsq=1.062e+32, orig_rms_sq=5.563e-03
2024-10-08 21:35:20,673 WARNING [optim.py:503] Scaling gradients by 0.025674201548099518, model_norm_threshold=1767377469440.0
2024-10-08 21:35:20,812 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.024e+27, grad_sumsq=3.457e+28, orig_rms_sq=2.962e-02
2024-10-08 21:35:22,516 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=64.33 vs. limit=8.307500000000001
2024-10-08 21:35:23,041 WARNING [optim.py:503] Scaling gradients by 0.00205455394461751, model_norm_threshold=1767377469440.0
2024-10-08 21:35:23,180 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.641e+29, grad_sumsq=8.930e+30, orig_rms_sq=2.958e-02
2024-10-08 21:35:24,244 WARNING [optim.py:503] Scaling gradients by 0.003536328673362732, model_norm_threshold=1767377469440.0
2024-10-08 21:35:24,382 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.657e+28, grad_sumsq=5.328e+27, orig_rms_sq=8.741e+00
2024-10-08 21:35:24,527 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2153.3333333333335, ans=0.3990625
2024-10-08 21:35:40,392 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=2160.0, ans=0.119
2024-10-08 21:35:40,800 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.42 vs. limit=8.31
2024-10-08 21:35:41,862 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=19.00 vs. limit=8.31
2024-10-08 21:35:42,428 WARNING [optim.py:503] Scaling gradients by 0.045124221593141556, model_norm_threshold=1767377469440.0
2024-10-08 21:35:42,567 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.002e+26, grad_sumsq=1.383e+28, orig_rms_sq=2.894e-02
2024-10-08 21:35:43,683 WARNING [optim.py:503] Scaling gradients by 0.00648095877841115, model_norm_threshold=1767377469440.0
2024-10-08 21:35:43,822 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.985e+28, grad_sumsq=4.938e+30, orig_rms_sq=6.046e-03
2024-10-08 21:35:44,665 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.whiten.whitening_limit, batch_count=2160.0, ans=8.31
2024-10-08 21:35:52,360 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.42 vs. limit=3.3245
2024-10-08 21:35:56,431 WARNING [optim.py:503] Scaling gradients by 0.00010072500299429521, model_norm_threshold=1767377469440.0
2024-10-08 21:35:56,569 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.044e+32, grad_sumsq=3.608e+33, orig_rms_sq=2.892e-02
2024-10-08 21:35:59,324 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=11.90 vs. limit=6.081666666666667
2024-10-08 21:36:01,116 WARNING [optim.py:503] Scaling gradients by 0.021742893382906914, model_norm_threshold=1767377469440.0
2024-10-08 21:36:01,254 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.793e+27, grad_sumsq=6.161e+28, orig_rms_sq=2.909e-02
2024-10-08 21:36:02,410 WARNING [optim.py:503] Scaling gradients by 0.07496543973684311, model_norm_threshold=1767377469440.0
2024-10-08 21:36:02,549 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.712e+26, grad_sumsq=5.884e+27, orig_rms_sq=2.909e-02
2024-10-08 21:36:03,605 WARNING [optim.py:503] Scaling gradients by 0.059264495968818665, model_norm_threshold=1767377469440.0
2024-10-08 21:36:03,743 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.308e+26, grad_sumsq=7.933e+27, orig_rms_sq=2.909e-02
2024-10-08 21:36:04,921 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.605e+09 3.838e+11 1.918e+12 9.076e+12 1.755e+16, threshold=3.836e+12, percent-clipped=52.0
2024-10-08 21:36:04,921 WARNING [optim.py:503] Scaling gradients by 0.09025397896766663, model_norm_threshold=3835695464448.0
2024-10-08 21:36:05,057 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.770e+26, grad_sumsq=2.662e+28, orig_rms_sq=2.918e-02
2024-10-08 21:36:05,102 INFO [train.py:1154] Epoch 1, batch 6500, loss[loss=1.282, simple_loss=0.6445, pruned_loss=0.7558, ctc_loss=1.017, over 4735.00 frames. ], tot_loss[loss=1.281, simple_loss=0.6456, pruned_loss=0.7494, ctc_loss=1.042, over 964973.97 frames. ], batch size: 26, lr: 3.91e-02,
2024-10-08 21:36:09,564 WARNING [optim.py:503] Scaling gradients by 0.06361369788646698, model_norm_threshold=3835695464448.0
2024-10-08 21:36:09,702 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.189e+27, grad_sumsq=2.031e+29, orig_rms_sq=5.855e-03
2024-10-08 21:36:10,166 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=63.59 vs. limit=9.125
2024-10-08 21:36:10,811 WARNING [optim.py:503] Scaling gradients by 0.011906028725206852, model_norm_threshold=3835695464448.0
2024-10-08 21:36:10,951 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.166e+28, grad_sumsq=3.699e+30, orig_rms_sq=5.855e-03
2024-10-08 21:36:11,085 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=2166.6666666666665, ans=0.11875000000000001
2024-10-08 21:36:11,737 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=17.20 vs. limit=9.125
2024-10-08 21:36:16,922 WARNING [optim.py:503] Scaling gradients by 0.04567347466945648, model_norm_threshold=3835695464448.0
2024-10-08 21:36:17,061 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.267e+27, grad_sumsq=4.406e+28, orig_rms_sq=2.875e-02
2024-10-08 21:36:19,487 WARNING [optim.py:503] Scaling gradients by 0.01398730929940939, model_norm_threshold=3835695464448.0
2024-10-08 21:36:19,625 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.148e+28, grad_sumsq=6.913e+27, orig_rms_sq=3.107e+00
2024-10-08 21:36:25,375 WARNING [optim.py:503] Scaling gradients by 0.07665789127349854, model_norm_threshold=3835695464448.0
2024-10-08 21:36:25,514 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.657e+26, grad_sumsq=1.629e+28, orig_rms_sq=2.859e-02
2024-10-08 21:36:28,443 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=1.93 vs. limit=8.315
2024-10-08 21:36:29,841 WARNING [optim.py:503] Scaling gradients by 0.08406001329421997, model_norm_threshold=3835695464448.0
2024-10-08 21:36:29,992 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.266e+26, grad_sumsq=7.182e+28, orig_rms_sq=5.939e-03
2024-10-08 21:36:31,540 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.44 vs. limit=8.315
2024-10-08 21:36:34,521 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.prob, batch_count=2173.3333333333335, ans=0.398125
2024-10-08 21:36:35,353 WARNING [optim.py:503] Scaling gradients by 0.08656997978687286, model_norm_threshold=3835695464448.0
2024-10-08 21:36:35,493 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.467e+26, grad_sumsq=2.258e+28, orig_rms_sq=2.863e-02
2024-10-08 21:36:47,334 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=22.55 vs. limit=8.31625
2024-10-08 21:36:47,387 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=6.36 vs. limit=4.870666666666667
2024-10-08 21:36:50,303 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_skip_rate, batch_count=2180.0, ans=0.11825
2024-10-08 21:36:51,279 WARNING [optim.py:503] Scaling gradients by 0.03611603379249573, model_norm_threshold=3835695464448.0
2024-10-08 21:36:51,419 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.363e+27, grad_sumsq=6.415e+29, orig_rms_sq=5.243e-03
2024-10-08 21:36:52,664 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.scale_min, batch_count=2180.0, ans=0.8237
2024-10-08 21:36:55,522 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.07 vs. limit=5.545
2024-10-08 21:36:59,851 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=2180.0, ans=0.8237
2024-10-08 21:37:03,160 INFO [train.py:1154] Epoch 1, batch 6550, loss[loss=1.242, simple_loss=0.6238, pruned_loss=0.736, ctc_loss=0.9681, over 4978.00 frames. ], tot_loss[loss=1.281, simple_loss=0.6445, pruned_loss=0.7507, ctc_loss=1.039, over 964827.84 frames. ], batch size: 19, lr: 3.91e-02,
2024-10-08 21:37:10,147 WARNING [optim.py:503] Scaling gradients by 0.009158243425190449, model_norm_threshold=3835695464448.0
2024-10-08 21:37:10,286 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.703e+28, grad_sumsq=1.387e+30, orig_rms_sq=2.671e-02
2024-10-08 21:37:13,682 WARNING [optim.py:503] Scaling gradients by 0.08996637910604477, model_norm_threshold=3835695464448.0
2024-10-08 21:37:13,823 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.873e+26, grad_sumsq=1.758e+26, orig_rms_sq=2.772e+00
2024-10-08 21:37:15,153 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer2.prob, batch_count=2186.6666666666665, ans=0.3975
2024-10-08 21:37:18,266 WARNING [optim.py:503] Scaling gradients by 0.04580184444785118, model_norm_threshold=3835695464448.0
2024-10-08 21:37:18,404 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.804e+27, grad_sumsq=7.413e+29, orig_rms_sq=5.131e-03
2024-10-08 21:37:19,794 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=2186.6666666666665, ans=0.127
2024-10-08 21:37:26,403 WARNING [optim.py:503] Scaling gradients by 0.052568819373846054, model_norm_threshold=3835695464448.0
2024-10-08 21:37:26,542 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.033e+27, grad_sumsq=3.745e+26, orig_rms_sq=2.758e+00
2024-10-08 21:37:28,785 WARNING [optim.py:503] Scaling gradients by 0.07328135520219803, model_norm_threshold=3835695464448.0
2024-10-08 21:37:28,922 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.52, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.411e+27, grad_sumsq=2.593e+29, orig_rms_sq=5.442e-03
2024-10-08 21:37:32,252 WARNING [optim.py:503] Scaling gradients by 0.04241405799984932, model_norm_threshold=3835695464448.0
2024-10-08 21:37:32,389 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.826e+27, grad_sumsq=6.982e+28, orig_rms_sq=2.616e-02
2024-10-08 21:37:34,643 WARNING [optim.py:503] Scaling gradients by 0.09441033005714417, model_norm_threshold=3835695464448.0
2024-10-08 21:37:34,781 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.281e+26, grad_sumsq=1.250e+28, orig_rms_sq=2.624e-02
2024-10-08 21:37:36,055 WARNING [optim.py:503] Scaling gradients by 0.021131465211510658, model_norm_threshold=3835695464448.0
2024-10-08 21:37:36,193 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.248e+28, grad_sumsq=4.561e+27, orig_rms_sq=2.736e+00
2024-10-08 21:37:37,285 WARNING [optim.py:503] Scaling gradients by 0.011086154729127884, model_norm_threshold=3835695464448.0
2024-10-08 21:37:37,424 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.995e+28, grad_sumsq=1.095e+28, orig_rms_sq=2.736e+00
2024-10-08 21:37:41,573 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=26.74 vs. limit=9.145
2024-10-08 21:37:42,133 WARNING [optim.py:503] Scaling gradients by 0.03178311511874199, model_norm_threshold=3835695464448.0
2024-10-08 21:37:42,271 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.788e+27, grad_sumsq=1.435e+29, orig_rms_sq=2.640e-02
2024-10-08 21:37:43,354 WARNING [optim.py:503] Scaling gradients by 0.07878810167312622, model_norm_threshold=3835695464448.0
2024-10-08 21:37:43,493 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.571e+26, grad_sumsq=2.081e+28, orig_rms_sq=2.678e-02
2024-10-08 21:37:48,203 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=2193.3333333333335, ans=0.08629166666666667
2024-10-08 21:37:52,097 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=83.76 vs. limit=8.32375
2024-10-08 21:37:55,304 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.19 vs. limit=8.32375
2024-10-08 21:37:56,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1.whitening_limit, batch_count=2196.6666666666665, ans=5.549166666666666
2024-10-08 21:37:57,127 WARNING [optim.py:503] Scaling gradients by 0.02467130310833454, model_norm_threshold=3835695464448.0
2024-10-08 21:37:57,265 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.572e+27, grad_sumsq=8.941e+29, orig_rms_sq=6.232e-03
2024-10-08 21:38:01,955 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.029e+10 4.543e+11 1.762e+12 1.043e+13 4.188e+14, threshold=3.523e+12, percent-clipped=36.0
2024-10-08 21:38:01,955 WARNING [optim.py:503] Scaling gradients by 0.019688356667757034, model_norm_threshold=3523440279552.0
2024-10-08 21:38:02,094 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.468e+28, grad_sumsq=2.360e+30, orig_rms_sq=6.222e-03
2024-10-08 21:38:02,140 INFO [train.py:1154] Epoch 1, batch 6600, loss[loss=1.334, simple_loss=0.6636, pruned_loss=0.7753, ctc_loss=1.135, over 4835.00 frames. ], tot_loss[loss=1.28, simple_loss=0.644, pruned_loss=0.7501, ctc_loss=1.04, over 965321.62 frames. ], batch size: 23, lr: 3.90e-02,
2024-10-08 21:38:05,417 WARNING [optim.py:503] Scaling gradients by 0.08071392774581909, model_norm_threshold=3523440279552.0
2024-10-08 21:38:05,558 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.479e+26, grad_sumsq=5.591e+28, orig_rms_sq=6.222e-03
2024-10-08 21:38:07,411 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.01 vs. limit=8.325
2024-10-08 21:38:07,925 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.out_balancer.prob, batch_count=2200.0, ans=0.396875
2024-10-08 21:38:10,820 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=60.98 vs. limit=8.325
2024-10-08 21:38:11,981 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=28.58 vs. limit=9.15
2024-10-08 21:38:17,782 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=23.89 vs. limit=6.101666666666667
2024-10-08 21:38:20,218 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=14.32 vs. limit=9.1525
2024-10-08 21:38:21,819 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.ff3_skip_rate, batch_count=2203.3333333333335, ans=0.050425
2024-10-08 21:38:24,161 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=2206.6666666666665, ans=0.08620833333333335
2024-10-08 21:38:26,255 WARNING [optim.py:503] Scaling gradients by 0.06533161550760269, model_norm_threshold=3523440279552.0
2024-10-08 21:38:26,394 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.199e+26, grad_sumsq=1.317e+29, orig_rms_sq=6.227e-03
2024-10-08 21:38:28,788 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=2206.6666666666665, ans=0.22416666666666668
2024-10-08 21:38:31,451 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=10.70 vs. limit=5.551666666666667
2024-10-08 21:38:38,997 WARNING [optim.py:503] Scaling gradients by 0.015603845939040184, model_norm_threshold=3523440279552.0
2024-10-08 21:38:39,136 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.984e+27, grad_sumsq=3.197e+27, orig_rms_sq=2.810e+00
2024-10-08 21:38:39,579 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=71.30 vs. limit=8.32875
2024-10-08 21:38:39,921 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=9.19 vs. limit=5.5525
2024-10-08 21:38:40,198 WARNING [optim.py:503] Scaling gradients by 0.058241333812475204, model_norm_threshold=3523440279552.0
2024-10-08 21:38:40,336 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.772e+26, grad_sumsq=8.696e+25, orig_rms_sq=8.937e+00
2024-10-08 21:38:42,747 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.min_positive, batch_count=2210.0, ans=0.04309375
2024-10-08 21:38:44,970 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=2210.0, ans=0.39640625
2024-10-08 21:38:51,968 WARNING [optim.py:503] Scaling gradients by 0.018228882923722267, model_norm_threshold=3523440279552.0
2024-10-08 21:38:52,107 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.522e+27, grad_sumsq=2.921e+29, orig_rms_sq=2.917e-02
2024-10-08 21:38:54,489 WARNING [optim.py:503] Scaling gradients by 0.06232539564371109, model_norm_threshold=3523440279552.0
2024-10-08 21:38:54,630 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.116e+26, grad_sumsq=1.226e+29, orig_rms_sq=5.805e-03
2024-10-08 21:38:56,063 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=2213.3333333333335, ans=0.39625
2024-10-08 21:39:00,537 WARNING [optim.py:503] Scaling gradients by 0.07124586403369904, model_norm_threshold=3523440279552.0
2024-10-08 21:39:00,677 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.010e+26, grad_sumsq=1.197e+29, orig_rms_sq=5.855e-03
2024-10-08 21:39:00,726 INFO [train.py:1154] Epoch 1, batch 6650, loss[loss=1.353, simple_loss=0.6655, pruned_loss=0.7989, ctc_loss=1.105, over 4748.00 frames. ], tot_loss[loss=1.28, simple_loss=0.6431, pruned_loss=0.7502, ctc_loss=1.039, over 967033.23 frames. ], batch size: 20, lr: 3.89e-02,
2024-10-08 21:39:09,914 WARNING [optim.py:503] Scaling gradients by 0.006557605229318142, model_norm_threshold=3523440279552.0
2024-10-08 21:39:10,057 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.128e+28, grad_sumsq=2.867e+30, orig_rms_sq=2.835e-02
2024-10-08 21:39:11,129 WARNING [optim.py:503] Scaling gradients by 0.0028875290881842375, model_norm_threshold=3523440279552.0
2024-10-08 21:39:11,267 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.763e+29, grad_sumsq=1.327e+31, orig_rms_sq=2.835e-02
2024-10-08 21:39:15,209 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.14 vs. limit=6.11
2024-10-08 21:39:16,827 WARNING [optim.py:503] Scaling gradients by 0.08495045453310013, model_norm_threshold=3523440279552.0
2024-10-08 21:39:16,967 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.458e+26, grad_sumsq=2.047e+26, orig_rms_sq=2.667e+00
2024-10-08 21:39:25,113 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=2223.3333333333335, ans=0.04997499999999999
2024-10-08 21:39:26,036 WARNING [optim.py:503] Scaling gradients by 0.012767031788825989, model_norm_threshold=3523440279552.0
2024-10-08 21:39:26,176 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.468e+28, grad_sumsq=5.565e+27, orig_rms_sq=2.638e+00
2024-10-08 21:39:26,562 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=9.19 vs. limit=8.33375
2024-10-08 21:39:28,335 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.34 vs. limit=6.111666666666666
2024-10-08 21:39:32,227 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=2223.3333333333335, ans=6.389583333333333
2024-10-08 21:39:34,716 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=2226.6666666666665, ans=0.395625
2024-10-08 21:39:35,199 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=27.17 vs. limit=9.17
2024-10-08 21:39:36,325 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.31 vs. limit=6.113333333333333
2024-10-08 21:39:38,895 WARNING [optim.py:503] Scaling gradients by 0.03983037546277046, model_norm_threshold=3523440279552.0
2024-10-08 21:39:39,032 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.231e+27, grad_sumsq=5.765e+29, orig_rms_sq=5.605e-03
2024-10-08 21:39:40,817 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=45.05 vs. limit=8.335
2024-10-08 21:39:41,730 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2.whitening_limit, batch_count=2226.6666666666665, ans=6.113333333333333
2024-10-08 21:39:44,348 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=17.43 vs. limit=8.335
2024-10-08 21:39:49,050 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.43 vs. limit=5.5575
2024-10-08 21:39:49,425 WARNING [optim.py:503] Scaling gradients by 0.05681423470377922, model_norm_threshold=3523440279552.0
2024-10-08 21:39:49,566 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.260e+26, grad_sumsq=2.716e+28, orig_rms_sq=2.674e-02
2024-10-08 21:39:50,022 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=69.74 vs. limit=8.33625
2024-10-08 21:39:58,767 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 7.293e+09 3.443e+11 1.814e+12 1.849e+13 1.220e+15, threshold=3.627e+12, percent-clipped=44.0
2024-10-08 21:39:58,807 INFO [train.py:1154] Epoch 1, batch 6700, loss[loss=1.326, simple_loss=0.6561, pruned_loss=0.7807, ctc_loss=1.087, over 4938.00 frames. ], tot_loss[loss=1.278, simple_loss=0.641, pruned_loss=0.7502, ctc_loss=1.035, over 969213.20 frames. ], batch size: 20, lr: 3.89e-02,
2024-10-08 21:39:59,960 WARNING [optim.py:503] Scaling gradients by 0.001811550697311759, model_norm_threshold=3627492835328.0
2024-10-08 21:40:00,098 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.075e+29, grad_sumsq=4.949e+29, orig_rms_sq=1.632e+00
2024-10-08 21:40:01,192 WARNING [optim.py:503] Scaling gradients by 0.017681626603007317, model_norm_threshold=3627492835328.0
2024-10-08 21:40:01,330 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.355e+27, grad_sumsq=3.895e+27, orig_rms_sq=1.632e+00
2024-10-08 21:40:02,389 WARNING [optim.py:503] Scaling gradients by 0.08376190811395645, model_norm_threshold=3627492835328.0
2024-10-08 21:40:02,529 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.240e+26, grad_sumsq=1.206e+28, orig_rms_sq=2.688e-02
2024-10-08 21:40:08,325 WARNING [optim.py:503] Scaling gradients by 0.04619433730840683, model_norm_threshold=3627492835328.0
2024-10-08 21:40:08,464 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.585e+27, grad_sumsq=2.893e+29, orig_rms_sq=5.479e-03
2024-10-08 21:40:08,695 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=2233.3333333333335, ans=0.3953125
2024-10-08 21:40:11,199 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=8.01 vs. limit=5.559166666666666
2024-10-08 21:40:11,922 WARNING [optim.py:503] Scaling gradients by 0.017135795205831528, model_norm_threshold=3627492835328.0
2024-10-08 21:40:12,062 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.090e+28, grad_sumsq=3.975e+29, orig_rms_sq=2.743e-02
2024-10-08 21:40:13,276 WARNING [optim.py:503] Scaling gradients by 0.02806960418820381, model_norm_threshold=3627492835328.0
2024-10-08 21:40:13,416 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.003e+27, grad_sumsq=1.451e+30, orig_rms_sq=5.515e-03
2024-10-08 21:40:15,114 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=5.28 vs. limit=4.894666666666667
2024-10-08 21:40:20,532 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=2236.6666666666665, ans=0.116125
2024-10-08 21:40:26,909 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.63 vs. limit=8.34
2024-10-08 21:40:28,823 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=13.81 vs. limit=8.34
2024-10-08 21:40:28,948 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=132.37 vs. limit=8.34
2024-10-08 21:40:29,201 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=7.97 vs. limit=8.34
2024-10-08 21:40:30,702 WARNING [optim.py:503] Scaling gradients by 0.02960275486111641, model_norm_threshold=3627492835328.0
2024-10-08 21:40:30,840 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.117e+27, grad_sumsq=1.824e+29, orig_rms_sq=2.805e-02
2024-10-08 21:40:37,086 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=2243.3333333333335, ans=0.39484375
2024-10-08 21:40:39,158 WARNING [optim.py:503] Scaling gradients by 0.02603181265294552, model_norm_threshold=3627492835328.0
2024-10-08 21:40:39,297 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.904e+27, grad_sumsq=1.782e+29, orig_rms_sq=2.752e-02
2024-10-08 21:40:39,946 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.23 vs. limit=6.121666666666667
2024-10-08 21:40:43,832 WARNING [optim.py:503] Scaling gradients by 0.0060502407141029835, model_norm_threshold=3627492835328.0
2024-10-08 21:40:43,970 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.722e+28, grad_sumsq=2.466e+30, orig_rms_sq=2.726e-02
2024-10-08 21:40:48,813 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=14.60 vs. limit=8.3425
2024-10-08 21:40:49,492 WARNING [optim.py:503] Scaling gradients by 0.01363235805183649, model_norm_threshold=3627492835328.0
2024-10-08 21:40:49,630 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.332e+28, grad_sumsq=4.935e+29, orig_rms_sq=2.700e-02
2024-10-08 21:40:51,476 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.32 vs. limit=9.185
2024-10-08 21:40:54,236 WARNING [optim.py:503] Scaling gradients by 0.00734684569761157, model_norm_threshold=3627492835328.0
2024-10-08 21:40:54,374 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.307e+28, grad_sumsq=1.970e+30, orig_rms_sq=2.694e-02
2024-10-08 21:40:55,281 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.40 vs. limit=9.185
2024-10-08 21:40:57,904 INFO [train.py:1154] Epoch 1, batch 6750, loss[loss=1.144, simple_loss=0.5777, pruned_loss=0.6709, ctc_loss=0.9223, over 4911.00 frames. ], tot_loss[loss=1.266, simple_loss=0.6346, pruned_loss=0.7435, ctc_loss=1.024, over 972265.95 frames. ], batch size: 19, lr: 3.88e-02,
2024-10-08 21:40:59,264 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=2250.0, ans=0.11562499999999999
2024-10-08 21:41:03,776 WARNING [optim.py:503] Scaling gradients by 0.002329911570996046, model_norm_threshold=3627492835328.0
2024-10-08 21:41:03,916 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.883e+29, grad_sumsq=1.428e+31, orig_rms_sq=2.719e-02
2024-10-08 21:41:06,222 WARNING [optim.py:503] Scaling gradients by 0.09710917621850967, model_norm_threshold=3627492835328.0
2024-10-08 21:41:06,361 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.651e+26, grad_sumsq=2.951e+25, orig_rms_sq=8.984e+00
2024-10-08 21:41:15,729 WARNING [optim.py:503] Scaling gradients by 0.04274049401283264, model_norm_threshold=3627492835328.0
2024-10-08 21:41:15,869 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.115e+27, grad_sumsq=5.524e+29, orig_rms_sq=5.639e-03
2024-10-08 21:41:16,977 WARNING [optim.py:503] Scaling gradients by 0.002536873100325465, model_norm_threshold=3627492835328.0
2024-10-08 21:41:17,115 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.659e+29, grad_sumsq=8.261e+31, orig_rms_sq=5.639e-03
2024-10-08 21:41:20,585 WARNING [optim.py:503] Scaling gradients by 0.0004661896382458508, model_norm_threshold=3627492835328.0
2024-10-08 21:41:20,725 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.763e+31, grad_sumsq=6.336e+32, orig_rms_sq=2.783e-02
2024-10-08 21:41:23,156 WARNING [optim.py:503] Scaling gradients by 0.04512719437479973, model_norm_threshold=3627492835328.0
2024-10-08 21:41:23,304 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.064e+27, grad_sumsq=3.699e+29, orig_rms_sq=5.579e-03
2024-10-08 21:41:26,287 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.70 vs. limit=8.34625
2024-10-08 21:41:30,640 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.26 vs. limit=8.34625
2024-10-08 21:41:34,224 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=26.23 vs. limit=9.195
2024-10-08 21:41:34,780 WARNING [optim.py:503] Scaling gradients by 0.003190676448866725, model_norm_threshold=3627492835328.0
2024-10-08 21:41:34,919 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.369e+29, grad_sumsq=1.163e+32, orig_rms_sq=5.477e-03
2024-10-08 21:41:35,578 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=39.31 vs. limit=8.3475
2024-10-08 21:41:38,444 WARNING [optim.py:503] Scaling gradients by 0.0010742050362750888, model_norm_threshold=3627492835328.0
2024-10-08 21:41:38,584 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.456e+30, grad_sumsq=1.532e+30, orig_rms_sq=1.604e+00
2024-10-08 21:41:43,227 WARNING [optim.py:503] Scaling gradients by 0.0407567173242569, model_norm_threshold=3627492835328.0
2024-10-08 21:41:43,365 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.795e+27, grad_sumsq=6.280e+28, orig_rms_sq=2.859e-02
2024-10-08 21:41:43,834 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=44.60 vs. limit=9.195
2024-10-08 21:41:49,246 WARNING [optim.py:503] Scaling gradients by 0.09322509169578552, model_norm_threshold=3627492835328.0
2024-10-08 21:41:49,385 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.564e+26, grad_sumsq=1.592e+28, orig_rms_sq=2.867e-02
2024-10-08 21:41:51,658 WARNING [optim.py:503] Scaling gradients by 0.0006565004587173462, model_norm_threshold=3627492835328.0
2024-10-08 21:41:51,800 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.984e+30, grad_sumsq=2.086e+32, orig_rms_sq=2.869e-02
2024-10-08 21:41:57,161 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=2266.6666666666665, ans=0.5
2024-10-08 21:41:57,711 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=17.26 vs. limit=9.2
2024-10-08 21:41:58,050 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.597e+10 8.993e+11 5.835e+12 3.356e+13 7.781e+15, threshold=1.167e+13, percent-clipped=52.0
2024-10-08 21:41:58,090 INFO [train.py:1154] Epoch 1, batch 6800, loss[loss=1.235, simple_loss=0.6193, pruned_loss=0.7291, ctc_loss=0.9802, over 4912.00 frames. ], tot_loss[loss=1.265, simple_loss=0.6351, pruned_loss=0.7428, ctc_loss=1.021, over 974591.32 frames. ], batch size: 19, lr: 3.87e-02,
2024-10-08 21:41:59,142 WARNING [optim.py:503] Scaling gradients by 0.030243203043937683, model_norm_threshold=11669613838336.0
2024-10-08 21:41:59,281 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.514e+28, grad_sumsq=1.189e+28, orig_rms_sq=2.955e+00
2024-10-08 21:42:00,386 WARNING [optim.py:503] Scaling gradients by 0.035464316606521606, model_norm_threshold=11669613838336.0
2024-10-08 21:42:00,526 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.123e+28, grad_sumsq=1.126e+30, orig_rms_sq=2.773e-02
2024-10-08 21:42:02,914 WARNING [optim.py:503] Scaling gradients by 0.014039527624845505, model_norm_threshold=11669613838336.0
2024-10-08 21:42:03,055 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.797e+29, grad_sumsq=3.255e+31, orig_rms_sq=5.522e-03
2024-10-08 21:42:04,118 WARNING [optim.py:503] Scaling gradients by 0.017962798476219177, model_norm_threshold=11669613838336.0
2024-10-08 21:42:04,259 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.737e+28, grad_sumsq=2.299e+28, orig_rms_sq=2.930e+00
2024-10-08 21:42:09,974 WARNING [optim.py:503] Scaling gradients by 0.05831408500671387, model_norm_threshold=11669613838336.0
2024-10-08 21:42:10,113 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.976e+27, grad_sumsq=1.447e+30, orig_rms_sq=5.511e-03
2024-10-08 21:42:15,121 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=2270.0, ans=0.12231249999999999
2024-10-08 21:42:15,997 WARNING [optim.py:503] Scaling gradients by 0.010470378212630749, model_norm_threshold=11669613838336.0
2024-10-08 21:42:16,136 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.768e+29, grad_sumsq=6.743e+31, orig_rms_sq=5.589e-03
2024-10-08 21:42:19,012 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=42.31 vs. limit=8.35125
2024-10-08 21:42:25,194 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.57 vs. limit=9.205
2024-10-08 21:42:26,280 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.65 vs. limit=6.136666666666667
2024-10-08 21:42:26,668 WARNING [optim.py:503] Scaling gradients by 0.07234735041856766, model_norm_threshold=11669613838336.0
2024-10-08 21:42:26,806 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.755e+27, grad_sumsq=1.067e+30, orig_rms_sq=6.329e-03
2024-10-08 21:42:29,748 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.39 vs. limit=8.3525
2024-10-08 21:42:34,445 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.02 vs. limit=6.138333333333334
2024-10-08 21:42:36,306 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=2276.6666666666665, ans=0.114625
2024-10-08 21:42:43,526 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=8.26 vs. limit=4.910666666666667
2024-10-08 21:42:44,087 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=2280.0, ans=0.12175
2024-10-08 21:42:44,693 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=13.50 vs. limit=8.355
2024-10-08 21:42:54,180 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=9.61 vs. limit=9.21
2024-10-08 21:42:56,888 INFO [train.py:1154] Epoch 1, batch 6850, loss[loss=1.244, simple_loss=0.6169, pruned_loss=0.728, ctc_loss=1.036, over 4978.00 frames. ], tot_loss[loss=1.258, simple_loss=0.6316, pruned_loss=0.7396, ctc_loss=1.014, over 978927.87 frames. ], batch size: 19, lr: 3.87e-02,
2024-10-08 21:42:57,271 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=63.22 vs. limit=8.35625
2024-10-08 21:42:58,161 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_True_ctc_True_attdecoder_False_streaming_True/epoch-1.pt
2024-10-08 21:43:13,534 INFO [train.py:1154] Epoch 2, batch 0, loss[loss=1.185, simple_loss=0.5937, pruned_loss=0.6952, ctc_loss=0.9659, over 4855.00 frames. ], tot_loss[loss=1.185, simple_loss=0.5937, pruned_loss=0.6952, ctc_loss=0.9659, over 4855.00 frames. ], batch size: 19, lr: 3.79e-02,
2024-10-08 21:43:13,535 INFO [train.py:1177] Computing validation loss
2024-10-08 21:43:18,396 INFO [train.py:1186] Epoch 2, validation: loss=1.353, simple_loss=0.6937, pruned_loss=0.7888, ctc_loss=1.089, over 90464.00 frames.
2024-10-08 21:43:18,397 INFO [train.py:1187] Maximum memory allocated so far is 5927MB
2024-10-08 21:43:21,788 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=29.79 vs. limit=8.3565
2024-10-08 21:43:32,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2287.3333333333335, ans=0.27712666666666663
2024-10-08 21:43:34,231 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=24.83 vs. limit=9.2155
2024-10-08 21:43:38,620 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=2287.3333333333335, ans=0.8199433333333334
2024-10-08 21:43:39,498 WARNING [optim.py:503] Scaling gradients by 0.060953956097364426, model_norm_threshold=11669613838336.0
2024-10-08 21:43:39,638 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.276e+28, grad_sumsq=1.926e+30, orig_rms_sq=6.622e-03
2024-10-08 21:43:44,588 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=2290.6666666666665, ans=0.392625
2024-10-08 21:43:47,194 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=29.46 vs. limit=9.218
2024-10-08 21:43:48,490 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.26 vs. limit=3.3436
2024-10-08 21:43:49,067 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=2290.6666666666665, ans=0.8198266666666667
2024-10-08 21:43:55,047 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.56 vs. limit=6.147
2024-10-08 21:43:56,760 WARNING [optim.py:503] Scaling gradients by 0.02363119088113308, model_norm_threshold=11669613838336.0
2024-10-08 21:43:56,899 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.657e+28, grad_sumsq=7.437e+30, orig_rms_sq=6.262e-03
2024-10-08 21:43:59,698 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=33.84 vs. limit=9.2205
2024-10-08 21:44:03,779 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=2297.3333333333335, ans=0.3923125
2024-10-08 21:44:07,867 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.61 vs. limit=5.574333333333334
2024-10-08 21:44:11,090 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.36 vs. limit=9.223
2024-10-08 21:44:11,945 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten.whitening_limit, batch_count=2297.3333333333335, ans=8.3615
2024-10-08 21:44:12,810 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.601e+09 5.505e+11 2.385e+12 1.615e+13 1.115e+15, threshold=4.769e+12, percent-clipped=31.0
2024-10-08 21:44:12,925 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=2297.3333333333335, ans=0.04830999999999999
2024-10-08 21:44:13,015 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer2.prob, batch_count=2297.3333333333335, ans=0.3923125
2024-10-08 21:44:14,748 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=12.58 vs. limit=8.36275
2024-10-08 21:44:15,225 INFO [train.py:1154] Epoch 2, batch 50, loss[loss=1.193, simple_loss=0.5941, pruned_loss=0.7037, ctc_loss=0.9592, over 4909.00 frames. ], tot_loss[loss=1.274, simple_loss=0.6484, pruned_loss=0.7394, ctc_loss=1.051, over 217758.01 frames. ], batch size: 19, lr: 3.79e-02,
2024-10-08 21:44:18,271 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.84 vs. limit=9.2255
2024-10-08 21:44:20,831 WARNING [optim.py:503] Scaling gradients by 0.0021582897752523422, model_norm_threshold=4769307426816.0
2024-10-08 21:44:20,965 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.53, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.576e+30, grad_sumsq=4.597e+32, orig_rms_sq=5.603e-03
2024-10-08 21:44:22,705 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1.whitening_limit, batch_count=2300.6666666666665, ans=5.575166666666666
2024-10-08 21:44:25,756 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=2304.0, ans=0.392
2024-10-08 21:44:26,612 WARNING [optim.py:503] Scaling gradients by 0.04681352898478508, model_norm_threshold=4769307426816.0
2024-10-08 21:44:26,751 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.604e+27, grad_sumsq=2.919e+29, orig_rms_sq=5.497e-03
2024-10-08 21:44:28,150 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=2304.0, ans=0.1136
2024-10-08 21:44:32,447 WARNING [optim.py:503] Scaling gradients by 0.010591196827590466, model_norm_threshold=4769307426816.0
2024-10-08 21:44:32,587 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.148e+28, grad_sumsq=9.798e+27, orig_rms_sq=3.213e+00
2024-10-08 21:44:37,471 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten2.whitening_limit, batch_count=2307.3333333333335, ans=6.153666666666667
2024-10-08 21:44:41,055 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.37 vs. limit=6.153666666666667
2024-10-08 21:44:41,564 WARNING [optim.py:503] Scaling gradients by 0.019680000841617584, model_norm_threshold=4769307426816.0
2024-10-08 21:44:41,704 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.823e+28, grad_sumsq=5.593e+27, orig_rms_sq=3.260e+00
2024-10-08 21:44:43,050 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=2307.3333333333335, ans=0.39184375
2024-10-08 21:44:55,151 WARNING [optim.py:503] Scaling gradients by 0.0012570539256557822, model_norm_threshold=4769307426816.0
2024-10-08 21:44:55,290 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.864e+30, grad_sumsq=1.046e+33, orig_rms_sq=5.607e-03
2024-10-08 21:45:02,357 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.0.self_attn_weights, loss-sum=3.256e+02
2024-10-08 21:45:05,616 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=2314.0, ans=0.39153125
2024-10-08 21:45:07,028 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.37 vs. limit=5.5785
2024-10-08 21:45:08,995 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=2314.0, ans=0.04276875
2024-10-08 21:45:12,404 INFO [train.py:1154] Epoch 2, batch 100, loss[loss=1.267, simple_loss=0.6389, pruned_loss=0.7454, ctc_loss=1.011, over 4755.00 frames. ], tot_loss[loss=1.285, simple_loss=0.654, pruned_loss=0.7461, ctc_loss=1.06, over 383327.18 frames. ], batch size: 19, lr: 3.78e-02,
2024-10-08 21:45:13,435 WARNING [optim.py:503] Scaling gradients by 0.025333022698760033, model_norm_threshold=4769307426816.0
2024-10-08 21:45:13,574 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.898e+27, grad_sumsq=1.556e+30, orig_rms_sq=5.720e-03
2024-10-08 21:45:14,621 WARNING [optim.py:503] Scaling gradients by 0.0927937850356102, model_norm_threshold=4769307426816.0
2024-10-08 21:45:14,761 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.954e+26, grad_sumsq=2.686e+26, orig_rms_sq=1.844e+00
2024-10-08 21:45:17,755 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=15.06 vs. limit=8.369
2024-10-08 21:45:23,678 WARNING [optim.py:503] Scaling gradients by 0.09295246005058289, model_norm_threshold=4769307426816.0
2024-10-08 21:45:23,819 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.913e+26, grad_sumsq=1.493e+26, orig_rms_sq=3.291e+00
2024-10-08 21:45:27,316 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=2320.6666666666665, ans=0.39121875
2024-10-08 21:45:35,914 WARNING [optim.py:503] Scaling gradients by 0.02489824779331684, model_norm_threshold=4769307426816.0
2024-10-08 21:45:36,052 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.438e+27, grad_sumsq=2.864e+27, orig_rms_sq=3.295e+00
2024-10-08 21:45:38,185 WARNING [optim.py:503] Scaling gradients by 0.02974914014339447, model_norm_threshold=4769307426816.0
2024-10-08 21:45:38,323 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.227e+28, grad_sumsq=2.172e+30, orig_rms_sq=5.647e-03
2024-10-08 21:45:44,237 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer2.prob, batch_count=2324.0, ans=0.3910625
2024-10-08 21:45:45,124 WARNING [optim.py:503] Scaling gradients by 0.04998083785176277, model_norm_threshold=4769307426816.0
2024-10-08 21:45:45,263 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.081e+27, grad_sumsq=8.053e+28, orig_rms_sq=2.584e-02
2024-10-08 21:45:46,846 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=35.57 vs. limit=9.2455
2024-10-08 21:45:48,702 WARNING [optim.py:503] Scaling gradients by 0.007099118549376726, model_norm_threshold=4769307426816.0
2024-10-08 21:45:48,840 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.907e+28, grad_sumsq=1.093e+28, orig_rms_sq=9.061e+00
2024-10-08 21:45:49,076 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=2327.3333333333335, ans=0.5
2024-10-08 21:45:59,346 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.90 vs. limit=6.165333333333333
2024-10-08 21:46:02,247 WARNING [optim.py:503] Scaling gradients by 0.08937428146600723, model_norm_threshold=4769307426816.0
2024-10-08 21:46:02,386 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.641e+26, grad_sumsq=2.116e+28, orig_rms_sq=2.665e-02
2024-10-08 21:46:07,302 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.202e+09 1.758e+11 1.408e+12 1.068e+13 3.794e+15, threshold=2.816e+12, percent-clipped=34.0
2024-10-08 21:46:09,677 INFO [train.py:1154] Epoch 2, batch 150, loss[loss=1.289, simple_loss=0.6441, pruned_loss=0.7602, ctc_loss=1.033, over 4908.00 frames. ], tot_loss[loss=1.277, simple_loss=0.6478, pruned_loss=0.7423, ctc_loss=1.052, over 513287.79 frames. ], batch size: 19, lr: 3.77e-02,
2024-10-08 21:46:11,075 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=2334.0, ans=0.07
2024-10-08 21:46:13,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.85 vs. limit=8.37525
2024-10-08 21:46:19,739 WARNING [optim.py:503] Scaling gradients by 0.01303129643201828, model_norm_threshold=2816225050624.0
2024-10-08 21:46:19,881 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.245e+28, grad_sumsq=3.796e+27, orig_rms_sq=3.279e+00
2024-10-08 21:46:21,415 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=6.62 vs. limit=4.934933333333333
2024-10-08 21:46:22,290 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=2337.3333333333335, ans=0.3904375
2024-10-08 21:46:24,773 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=29.32 vs. limit=9.253
2024-10-08 21:46:25,498 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2337.3333333333335, ans=0.3904375
2024-10-08 21:46:25,880 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.73 vs. limit=8.3765
2024-10-08 21:46:26,994 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=36.57 vs. limit=8.3765
2024-10-08 21:46:27,532 WARNING [optim.py:503] Scaling gradients by 0.003993450663983822, model_norm_threshold=2816225050624.0
2024-10-08 21:46:27,670 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.115e+29, grad_sumsq=3.788e+31, orig_rms_sq=5.582e-03
2024-10-08 21:46:34,559 WARNING [optim.py:503] Scaling gradients by 0.06639695167541504, model_norm_threshold=2816225050624.0
2024-10-08 21:46:34,700 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.103e+26, grad_sumsq=7.215e+28, orig_rms_sq=5.687e-03
2024-10-08 21:46:36,944 WARNING [optim.py:503] Scaling gradients by 0.07169852405786514, model_norm_threshold=2816225050624.0
2024-10-08 21:46:37,082 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.527e+26, grad_sumsq=6.201e+28, orig_rms_sq=5.687e-03
2024-10-08 21:46:38,717 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=7.71 vs. limit=4.936266666666667
2024-10-08 21:46:44,933 WARNING [optim.py:503] Scaling gradients by 0.09268151968717575, model_norm_threshold=2816225050624.0
2024-10-08 21:46:45,072 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.675e+26, grad_sumsq=8.218e+28, orig_rms_sq=5.688e-03
2024-10-08 21:46:46,114 WARNING [optim.py:503] Scaling gradients by 0.0637921467423439, model_norm_threshold=2816225050624.0
2024-10-08 21:46:46,253 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.251e+26, grad_sumsq=1.493e+28, orig_rms_sq=2.848e-02
2024-10-08 21:46:46,968 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.85 vs. limit=5.586
2024-10-08 21:46:51,841 WARNING [optim.py:503] Scaling gradients by 0.013441385701298714, model_norm_threshold=2816225050624.0
2024-10-08 21:46:51,981 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.348e+27, grad_sumsq=3.616e+29, orig_rms_sq=2.585e-02
2024-10-08 21:46:53,902 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=11.70 vs. limit=5.586
2024-10-08 21:46:56,498 WARNING [optim.py:503] Scaling gradients by 0.055748336017131805, model_norm_threshold=2816225050624.0
2024-10-08 21:46:56,637 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.324e+26, grad_sumsq=2.821e+28, orig_rms_sq=2.596e-02
2024-10-08 21:47:02,301 WARNING [optim.py:503] Scaling gradients by 0.0560106560587883, model_norm_threshold=2816225050624.0
2024-10-08 21:47:02,441 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.889e+26, grad_sumsq=6.674e+28, orig_rms_sq=5.827e-03
2024-10-08 21:47:04,565 WARNING [optim.py:503] Scaling gradients by 0.011164859868586063, model_norm_threshold=2816225050624.0
2024-10-08 21:47:04,707 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.181e+28, grad_sumsq=2.026e+30, orig_rms_sq=5.827e-03
2024-10-08 21:47:06,060 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=2350.6666666666665, ans=0.3898125
2024-10-08 21:47:07,059 INFO [train.py:1154] Epoch 2, batch 200, loss[loss=1.372, simple_loss=0.7028, pruned_loss=0.7864, ctc_loss=1.173, over 4761.00 frames. ], tot_loss[loss=1.273, simple_loss=0.6447, pruned_loss=0.7408, ctc_loss=1.047, over 613727.61 frames. ], batch size: 45, lr: 3.77e-02,
2024-10-08 21:47:09,146 WARNING [optim.py:503] Scaling gradients by 0.010627171956002712, model_norm_threshold=2816225050624.0
2024-10-08 21:47:09,286 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.563e+28, grad_sumsq=4.578e+27, orig_rms_sq=3.414e+00
2024-10-08 21:47:10,345 WARNING [optim.py:503] Scaling gradients by 0.09636271744966507, model_norm_threshold=2816225050624.0
2024-10-08 21:47:10,483 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.936e+26, grad_sumsq=1.500e+28, orig_rms_sq=2.624e-02
2024-10-08 21:47:12,941 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=2350.6666666666665, ans=0.8177266666666667
2024-10-08 21:47:16,821 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=25.36 vs. limit=9.263
2024-10-08 21:47:16,871 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.25 vs. limit=9.263
2024-10-08 21:47:20,928 WARNING [optim.py:503] Scaling gradients by 0.008320200257003307, model_norm_threshold=2816225050624.0
2024-10-08 21:47:21,069 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.523e+28, grad_sumsq=9.766e+29, orig_rms_sq=2.584e-02
2024-10-08 21:47:22,897 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.56 vs. limit=6.177
2024-10-08 21:47:23,784 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=23.67 vs. limit=9.2655
2024-10-08 21:47:24,539 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=2354.0, ans=0.38965625
2024-10-08 21:47:29,053 WARNING [optim.py:503] Scaling gradients by 0.05010872334241867, model_norm_threshold=2816225050624.0
2024-10-08 21:47:29,193 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.430e+26, grad_sumsq=2.466e+28, orig_rms_sq=2.607e-02
2024-10-08 21:47:35,508 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.70 vs. limit=6.1786666666666665
2024-10-08 21:47:36,630 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=24.29 vs. limit=9.268
2024-10-08 21:47:39,308 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.35 vs. limit=3.3536
2024-10-08 21:47:40,120 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=512, metric=5.18 vs. limit=8.384
2024-10-08 21:47:41,687 WARNING [optim.py:503] Scaling gradients by 0.0025580800138413906, model_norm_threshold=2816225050624.0
2024-10-08 21:47:41,828 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.083e+29, grad_sumsq=1.165e+31, orig_rms_sq=2.646e-02
2024-10-08 21:47:48,467 WARNING [optim.py:503] Scaling gradients by 0.04526533558964729, model_norm_threshold=2816225050624.0
2024-10-08 21:47:48,605 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.782e+26, grad_sumsq=2.571e+26, orig_rms_sq=3.416e+00
2024-10-08 21:47:51,159 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=115.56 vs. limit=8.38525
2024-10-08 21:47:51,744 WARNING [optim.py:503] Scaling gradients by 0.05188601464033127, model_norm_threshold=2816225050624.0
2024-10-08 21:47:51,886 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.502e+27, grad_sumsq=2.616e+29, orig_rms_sq=5.744e-03
2024-10-08 21:47:56,341 WARNING [optim.py:503] Scaling gradients by 0.05541983246803284, model_norm_threshold=2816225050624.0
2024-10-08 21:47:56,482 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.053e+26, grad_sumsq=1.053e+29, orig_rms_sq=5.750e-03
2024-10-08 21:47:57,735 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=2364.0, ans=0.04681
2024-10-08 21:47:58,200 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=23.22 vs. limit=9.273
2024-10-08 21:48:02,084 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.236e+10 2.571e+11 2.690e+12 1.709e+13 1.101e+15, threshold=5.381e+12, percent-clipped=47.0
2024-10-08 21:48:03,884 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten.whitening_limit, batch_count=2367.3333333333335, ans=8.38775
2024-10-08 21:48:04,314 INFO [train.py:1154] Epoch 2, batch 250, loss[loss=1.205, simple_loss=0.615, pruned_loss=0.692, ctc_loss=1.03, over 4832.00 frames. ], tot_loss[loss=1.27, simple_loss=0.6431, pruned_loss=0.739, ctc_loss=1.046, over 692427.42 frames. ], batch size: 38, lr: 3.76e-02,
2024-10-08 21:48:06,337 WARNING [optim.py:503] Scaling gradients by 0.04896944388747215, model_norm_threshold=5380618321920.0
2024-10-08 21:48:06,476 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.340e+27, grad_sumsq=6.801e+26, orig_rms_sq=3.440e+00
2024-10-08 21:48:09,254 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=10.65 vs. limit=5.591833333333334
2024-10-08 21:48:09,757 WARNING [optim.py:503] Scaling gradients by 0.0012670032447203994, model_norm_threshold=5380618321920.0
2024-10-08 21:48:09,896 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.257e+30, grad_sumsq=4.748e+29, orig_rms_sq=8.966e+00
2024-10-08 21:48:15,608 WARNING [optim.py:503] Scaling gradients by 0.018492810428142548, model_norm_threshold=5380618321920.0
2024-10-08 21:48:15,746 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.718e+28, grad_sumsq=5.120e+27, orig_rms_sq=3.355e+00
2024-10-08 21:48:16,306 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=4.53 vs. limit=4.948266666666667
2024-10-08 21:48:18,559 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.08 vs. limit=9.278
2024-10-08 21:48:22,471 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.07 vs. limit=9.278
2024-10-08 21:48:26,410 WARNING [optim.py:503] Scaling gradients by 0.0980127826333046, model_norm_threshold=5380618321920.0
2024-10-08 21:48:26,550 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.542e+27, grad_sumsq=2.652e+29, orig_rms_sq=5.815e-03
2024-10-08 21:48:27,242 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=28.07 vs. limit=8.389
2024-10-08 21:48:27,967 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=2374.0, ans=0.38871875
2024-10-08 21:48:28,891 WARNING [optim.py:503] Scaling gradients by 0.005396941676735878, model_norm_threshold=5380618321920.0
2024-10-08 21:48:29,032 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.257e+29, grad_sumsq=3.881e+31, orig_rms_sq=5.815e-03
2024-10-08 21:48:32,163 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.43 vs. limit=8.39025
2024-10-08 21:48:44,257 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=8.55 vs. limit=4.950933333333333
2024-10-08 21:48:46,283 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.min_positive, batch_count=2377.3333333333335, ans=0.042570833333333336
2024-10-08 21:48:49,729 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=2377.3333333333335, ans=0.11084999999999999
2024-10-08 21:48:50,035 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=20.51 vs. limit=8.3915
2024-10-08 21:48:50,641 WARNING [optim.py:503] Scaling gradients by 0.0600726380944252, model_norm_threshold=5380618321920.0
2024-10-08 21:48:50,781 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.489e+27, grad_sumsq=4.414e+26, orig_rms_sq=3.373e+00
2024-10-08 21:48:51,378 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=31.55 vs. limit=8.39275
2024-10-08 21:48:55,321 WARNING [optim.py:503] Scaling gradients by 0.017772672697901726, model_norm_threshold=5380618321920.0
2024-10-08 21:48:55,463 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.898e+28, grad_sumsq=3.286e+30, orig_rms_sq=5.776e-03
2024-10-08 21:48:57,857 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=2380.6666666666665, ans=0.11608750000000001
2024-10-08 21:48:59,568 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.66 vs. limit=5.595166666666667
2024-10-08 21:49:03,412 INFO [train.py:1154] Epoch 2, batch 300, loss[loss=1.26, simple_loss=0.6267, pruned_loss=0.7262, ctc_loss=1.102, over 4786.00 frames. ], tot_loss[loss=1.268, simple_loss=0.6424, pruned_loss=0.7381, ctc_loss=1.046, over 752845.68 frames. ], batch size: 32, lr: 3.75e-02,
2024-10-08 21:49:05,961 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=13.45 vs. limit=8.394
2024-10-08 21:49:06,638 WARNING [optim.py:503] Scaling gradients by 0.016224805265665054, model_norm_threshold=5380618321920.0
2024-10-08 21:49:06,777 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.504e+28, grad_sumsq=1.047e+28, orig_rms_sq=3.345e+00
2024-10-08 21:49:07,905 WARNING [optim.py:503] Scaling gradients by 0.00171062839217484, model_norm_threshold=5380618321920.0
2024-10-08 21:49:08,044 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.976e+30, grad_sumsq=5.390e+32, orig_rms_sq=5.521e-03
2024-10-08 21:49:09,964 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.68 vs. limit=8.394
2024-10-08 21:49:11,816 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer1.prob, batch_count=2384.0, ans=0.38825
2024-10-08 21:49:12,760 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=2384.0, ans=0.1159
2024-10-08 21:49:18,469 WARNING [optim.py:503] Scaling gradients by 0.003227215027436614, model_norm_threshold=5380618321920.0
2024-10-08 21:49:18,611 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.963e+29, grad_sumsq=1.603e+32, orig_rms_sq=5.590e-03
2024-10-08 21:49:21,107 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=2387.3333333333335, ans=0.11047499999999999
2024-10-08 21:49:22,449 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.45 vs. limit=5.5968333333333335
2024-10-08 21:49:29,147 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.61 vs. limit=8.3965
2024-10-08 21:49:29,283 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=22.62 vs. limit=8.3965
2024-10-08 21:49:30,585 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=17.20 vs. limit=8.3965
2024-10-08 21:49:41,781 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=11.04 vs. limit=5.5985
2024-10-08 21:49:45,916 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2394.0, ans=0.27605999999999997
2024-10-08 21:49:50,717 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.41 vs. limit=3.3596
2024-10-08 21:49:51,587 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.08 vs. limit=8.399000000000001
2024-10-08 21:49:56,233 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=25.00 vs. limit=8.399000000000001
2024-10-08 21:49:56,362 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=5.70 vs. limit=4.958933333333333
2024-10-08 21:49:56,450 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.35 vs. limit=8.399000000000001
2024-10-08 21:49:58,170 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.305e+09 4.064e+11 1.753e+12 7.740e+12 4.247e+15, threshold=3.507e+12, percent-clipped=29.0
2024-10-08 21:49:58,398 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=2397.3333333333335, ans=6.498333333333333
2024-10-08 21:50:00,359 INFO [train.py:1154] Epoch 2, batch 350, loss[loss=1.303, simple_loss=0.6447, pruned_loss=0.7726, ctc_loss=1.04, over 4883.00 frames. ], tot_loss[loss=1.266, simple_loss=0.6412, pruned_loss=0.7366, ctc_loss=1.045, over 800170.97 frames. ], batch size: 19, lr: 3.75e-02,
2024-10-08 21:50:04,552 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=28.13 vs. limit=8.40025
2024-10-08 21:50:06,058 WARNING [optim.py:503] Scaling gradients by 0.02581368200480938, model_norm_threshold=3506807242752.0
2024-10-08 21:50:06,197 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.404e+27, grad_sumsq=1.943e+27, orig_rms_sq=3.295e+00
2024-10-08 21:50:06,426 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=2400.6666666666665, ans=0.38746875000000003
2024-10-08 21:50:07,813 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.83 vs. limit=5.6001666666666665
2024-10-08 21:50:16,363 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2404.0, ans=0.27596
2024-10-08 21:50:16,878 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=21.22 vs. limit=9.303
2024-10-08 21:50:18,599 WARNING [optim.py:503] Scaling gradients by 0.047174520790576935, model_norm_threshold=3506807242752.0
2024-10-08 21:50:18,735 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.719e+27, grad_sumsq=5.181e+26, orig_rms_sq=3.318e+00
2024-10-08 21:50:20,974 WARNING [optim.py:503] Scaling gradients by 0.0011917270021513104, model_norm_threshold=3506807242752.0
2024-10-08 21:50:21,113 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.385e+30, grad_sumsq=8.686e+31, orig_rms_sq=2.746e-02
2024-10-08 21:50:21,279 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff2_skip_rate, batch_count=2404.0, ans=0.04591
2024-10-08 21:50:24,555 WARNING [optim.py:503] Scaling gradients by 0.0007465966627933085, model_norm_threshold=3506807242752.0
2024-10-08 21:50:24,693 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.993e+30, grad_sumsq=2.414e+30, orig_rms_sq=3.312e+00
2024-10-08 21:50:26,956 WARNING [optim.py:503] Scaling gradients by 3.6228961107553914e-05, model_norm_threshold=3506807242752.0
2024-10-08 21:50:27,096 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.689e+33, grad_sumsq=8.068e+32, orig_rms_sq=3.332e+00
2024-10-08 21:50:32,425 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.05 vs. limit=6.203666666666667
2024-10-08 21:50:37,495 WARNING [optim.py:503] Scaling gradients by 0.01238657720386982, model_norm_threshold=3506807242752.0
2024-10-08 21:50:37,633 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.206e+28, grad_sumsq=3.665e+30, orig_rms_sq=6.018e-03
2024-10-08 21:50:39,656 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=5.09 vs. limit=4.964266666666667
2024-10-08 21:50:40,009 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=2410.6666666666665, ans=0.19866666666666666
2024-10-08 21:50:42,123 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=2410.6666666666665, ans=0.8156266666666667
2024-10-08 21:50:51,673 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.51 vs. limit=9.3105
2024-10-08 21:50:52,209 WARNING [optim.py:503] Scaling gradients by 0.06007230281829834, model_norm_threshold=3506807242752.0
2024-10-08 21:50:52,350 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.309e+27, grad_sumsq=2.141e+29, orig_rms_sq=6.115e-03
2024-10-08 21:50:55,589 WARNING [optim.py:503] Scaling gradients by 0.0033429653849452734, model_norm_threshold=3506807242752.0
2024-10-08 21:50:55,727 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.406e+29, grad_sumsq=3.896e+31, orig_rms_sq=6.177e-03
2024-10-08 21:50:55,965 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2414.0, ans=0.38684375
2024-10-08 21:50:58,111 INFO [train.py:1154] Epoch 2, batch 400, loss[loss=1.305, simple_loss=0.6555, pruned_loss=0.7658, ctc_loss=1.058, over 4879.00 frames. ], tot_loss[loss=1.268, simple_loss=0.6424, pruned_loss=0.7377, ctc_loss=1.044, over 836930.10 frames. ], batch size: 22, lr: 3.74e-02,
2024-10-08 21:50:58,251 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=2417.3333333333335, ans=0.3866875
2024-10-08 21:51:00,192 WARNING [optim.py:503] Scaling gradients by 0.001077024731785059, model_norm_threshold=3506807242752.0
2024-10-08 21:51:00,334 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.734e+30, grad_sumsq=1.040e+32, orig_rms_sq=2.629e-02
2024-10-08 21:51:05,420 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=11.62 vs. limit=8.4065
2024-10-08 21:51:05,870 WARNING [optim.py:503] Scaling gradients by 0.030188579112291336, model_norm_threshold=3506807242752.0
2024-10-08 21:51:06,012 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.058e+27, grad_sumsq=7.960e+29, orig_rms_sq=6.354e-03
2024-10-08 21:51:07,725 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.50 vs. limit=5.604333333333333
2024-10-08 21:51:08,230 WARNING [optim.py:503] Scaling gradients by 0.016330620273947716, model_norm_threshold=3506807242752.0
2024-10-08 21:51:08,370 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.520e+28, grad_sumsq=1.708e+27, orig_rms_sq=8.900e+00
2024-10-08 21:51:09,457 WARNING [optim.py:503] Scaling gradients by 0.007647782564163208, model_norm_threshold=3506807242752.0
2024-10-08 21:51:09,598 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.251e+28, grad_sumsq=1.574e+30, orig_rms_sq=2.701e-02
2024-10-08 21:51:09,734 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=2420.6666666666665, ans=0.08487083333333334
2024-10-08 21:51:12,745 WARNING [optim.py:503] Scaling gradients by 0.04839640110731125, model_norm_threshold=3506807242752.0
2024-10-08 21:51:12,883 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.879e+26, grad_sumsq=3.642e+28, orig_rms_sq=2.712e-02
2024-10-08 21:51:17,378 WARNING [optim.py:503] Scaling gradients by 0.016618166118860245, model_norm_threshold=3506807242752.0
2024-10-08 21:51:17,516 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.287e+28, grad_sumsq=4.778e+29, orig_rms_sq=2.694e-02
2024-10-08 21:51:19,607 WARNING [optim.py:503] Scaling gradients by 0.002146046841517091, model_norm_threshold=3506807242752.0
2024-10-08 21:51:19,754 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.294e+29, grad_sumsq=1.594e+31, orig_rms_sq=2.694e-02
2024-10-08 21:51:21,060 WARNING [optim.py:503] Scaling gradients by 0.0698506161570549, model_norm_threshold=3506807242752.0
2024-10-08 21:51:21,200 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.423e+26, grad_sumsq=2.024e+28, orig_rms_sq=2.679e-02
2024-10-08 21:51:25,859 WARNING [optim.py:503] Scaling gradients by 0.026105014607310295, model_norm_threshold=3506807242752.0
2024-10-08 21:51:25,998 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.498e+27, grad_sumsq=7.428e+29, orig_rms_sq=6.056e-03
2024-10-08 21:51:26,164 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=2424.0, ans=0.386375
2024-10-08 21:51:26,192 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2424.0, ans=0.386375
2024-10-08 21:51:28,190 WARNING [optim.py:503] Scaling gradients by 0.042799144983291626, model_norm_threshold=3506807242752.0
2024-10-08 21:51:28,330 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.128e+27, grad_sumsq=3.514e+29, orig_rms_sq=6.056e-03
2024-10-08 21:51:29,111 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.03 vs. limit=9.318
2024-10-08 21:51:33,406 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=2427.3333333333335, ans=9.3205
2024-10-08 21:51:33,794 WARNING [optim.py:503] Scaling gradients by 0.00043940061004832387, model_norm_threshold=3506807242752.0
2024-10-08 21:51:33,934 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.718e+31, grad_sumsq=2.870e+33, orig_rms_sq=5.987e-03
2024-10-08 21:51:36,175 WARNING [optim.py:503] Scaling gradients by 0.007931179367005825, model_norm_threshold=3506807242752.0
2024-10-08 21:51:36,315 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.017e+28, grad_sumsq=1.474e+30, orig_rms_sq=2.725e-02
2024-10-08 21:51:37,390 WARNING [optim.py:503] Scaling gradients by 0.011863862164318562, model_norm_threshold=3506807242752.0
2024-10-08 21:51:37,531 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.673e+28, grad_sumsq=6.140e+29, orig_rms_sq=2.725e-02
2024-10-08 21:51:38,080 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=33.92 vs. limit=9.3205
2024-10-08 21:51:49,751 WARNING [optim.py:503] Scaling gradients by 0.03427698835730553, model_norm_threshold=3506807242752.0
2024-10-08 21:51:49,891 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.808e+27, grad_sumsq=4.596e+29, orig_rms_sq=6.110e-03
2024-10-08 21:51:51,012 WARNING [optim.py:503] Scaling gradients by 0.009822248481214046, model_norm_threshold=3506807242752.0
2024-10-08 21:51:51,152 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.040e+28, grad_sumsq=6.611e+30, orig_rms_sq=6.110e-03
2024-10-08 21:51:52,225 WARNING [optim.py:503] Scaling gradients by 0.08798830211162567, model_norm_threshold=3506807242752.0
2024-10-08 21:51:52,366 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.583e+26, grad_sumsq=1.112e+26, orig_rms_sq=3.221e+00
2024-10-08 21:51:53,461 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.076e+10 5.562e+11 2.623e+12 3.009e+13 9.680e+16, threshold=5.247e+12, percent-clipped=46.0
2024-10-08 21:51:53,642 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=2430.6666666666665, ans=0.10885
2024-10-08 21:51:54,693 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=2434.0, ans=0.19574999999999998
2024-10-08 21:51:55,183 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=29.38 vs. limit=9.3255
2024-10-08 21:51:55,677 INFO [train.py:1154] Epoch 2, batch 450, loss[loss=1.342, simple_loss=0.6802, pruned_loss=0.7895, ctc_loss=1.064, over 4871.00 frames. ], tot_loss[loss=1.271, simple_loss=0.6445, pruned_loss=0.7396, ctc_loss=1.045, over 865484.37 frames. ], batch size: 23, lr: 3.73e-02,
2024-10-08 21:52:01,309 WARNING [optim.py:503] Scaling gradients by 0.06937770545482635, model_norm_threshold=5246527471616.0
2024-10-08 21:52:01,449 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.715e+27, grad_sumsq=6.259e+26, orig_rms_sq=2.740e+00
2024-10-08 21:52:03,471 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=13.78 vs. limit=8.412749999999999
2024-10-08 21:52:05,341 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.14 vs. limit=4.9736
2024-10-08 21:52:14,089 WARNING [optim.py:503] Scaling gradients by 0.012130788527429104, model_norm_threshold=5246527471616.0
2024-10-08 21:52:14,225 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.268e+28, grad_sumsq=1.210e+30, orig_rms_sq=2.700e-02
2024-10-08 21:52:15,328 WARNING [optim.py:503] Scaling gradients by 0.08829645067453384, model_norm_threshold=5246527471616.0
2024-10-08 21:52:15,465 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.766e+26, grad_sumsq=1.346e+29, orig_rms_sq=6.512e-03
2024-10-08 21:52:31,384 WARNING [optim.py:503] Scaling gradients by 0.0980953723192215, model_norm_threshold=5246527471616.0
2024-10-08 21:52:31,525 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.956e+26, grad_sumsq=3.040e+26, orig_rms_sq=3.275e+00
2024-10-08 21:52:37,796 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.92 vs. limit=9.333
2024-10-08 21:52:39,464 WARNING [optim.py:503] Scaling gradients by 0.025276729837059975, model_norm_threshold=5246527471616.0
2024-10-08 21:52:39,603 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.333e+28, grad_sumsq=5.010e+29, orig_rms_sq=2.660e-02
2024-10-08 21:52:40,770 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2447.3333333333335, ans=0.27552666666666664
2024-10-08 21:52:41,106 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.64 vs. limit=6.2236666666666665
2024-10-08 21:52:42,907 WARNING [optim.py:503] Scaling gradients by 0.0036235940642654896, model_norm_threshold=5246527471616.0
2024-10-08 21:52:43,047 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.431e+29, grad_sumsq=1.383e+32, orig_rms_sq=6.818e-03
2024-10-08 21:52:45,442 WARNING [optim.py:503] Scaling gradients by 0.016410063952207565, model_norm_threshold=5246527471616.0
2024-10-08 21:52:45,581 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.583e+28, grad_sumsq=9.624e+29, orig_rms_sq=2.684e-02
2024-10-08 21:52:46,315 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.36 vs. limit=6.2236666666666665
2024-10-08 21:52:46,679 WARNING [optim.py:503] Scaling gradients by 0.02601415105164051, model_norm_threshold=5246527471616.0
2024-10-08 21:52:46,820 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.423e+27, grad_sumsq=3.138e+29, orig_rms_sq=2.684e-02
2024-10-08 21:52:48,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=2447.3333333333335, ans=0.38528125
2024-10-08 21:52:48,064 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=2447.3333333333335, ans=0.38528125
2024-10-08 21:52:52,849 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=2450.6666666666665, ans=0.044860000000000004
2024-10-08 21:52:53,715 INFO [train.py:1154] Epoch 2, batch 500, loss[loss=1.311, simple_loss=0.6765, pruned_loss=0.7594, ctc_loss=1.067, over 4837.00 frames. ], tot_loss[loss=1.266, simple_loss=0.6419, pruned_loss=0.7373, ctc_loss=1.041, over 888098.43 frames. ], batch size: 34, lr: 3.73e-02,
2024-10-08 21:52:57,045 WARNING [optim.py:503] Scaling gradients by 0.05149708688259125, model_norm_threshold=5246527471616.0
2024-10-08 21:52:57,184 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.364e+27, grad_sumsq=4.743e+29, orig_rms_sq=7.092e-03
2024-10-08 21:53:00,152 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=11.63 vs. limit=4.980266666666667
2024-10-08 21:53:00,931 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=2450.6666666666665, ans=0.8142266666666667
2024-10-08 21:53:03,250 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=5.808e+01
2024-10-08 21:53:12,564 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=2454.0, ans=0.38496874999999997
2024-10-08 21:53:17,873 WARNING [optim.py:503] Scaling gradients by 0.07464831322431564, model_norm_threshold=5246527471616.0
2024-10-08 21:53:18,013 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.538e+26, grad_sumsq=1.420e+29, orig_rms_sq=6.715e-03
2024-10-08 21:53:19,057 WARNING [optim.py:503] Scaling gradients by 0.0059997220523655415, model_norm_threshold=5246527471616.0
2024-10-08 21:53:19,198 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.857e+29, grad_sumsq=5.729e+28, orig_rms_sq=3.241e+00
2024-10-08 21:53:19,701 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.99 vs. limit=8.4215
2024-10-08 21:53:24,242 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.88 vs. limit=5.614333333333334
2024-10-08 21:53:25,522 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.41 vs. limit=8.4215
2024-10-08 21:53:29,196 WARNING [optim.py:503] Scaling gradients by 0.0018545478815212846, model_norm_threshold=5246527471616.0
2024-10-08 21:53:29,337 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.247e+30, grad_sumsq=8.654e+31, orig_rms_sq=2.596e-02
2024-10-08 21:53:32,581 WARNING [optim.py:503] Scaling gradients by 0.021700182929635048, model_norm_threshold=5246527471616.0
2024-10-08 21:53:32,721 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.325e+28, grad_sumsq=5.123e+29, orig_rms_sq=2.587e-02
2024-10-08 21:53:32,862 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=2460.6666666666665, ans=0.107725
2024-10-08 21:53:34,030 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=2460.6666666666665, ans=6.537916666666667
2024-10-08 21:53:40,441 WARNING [optim.py:503] Scaling gradients by 0.002183331409469247, model_norm_threshold=5246527471616.0
2024-10-08 21:53:40,580 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.471e+30, grad_sumsq=5.627e+31, orig_rms_sq=2.614e-02
2024-10-08 21:53:42,851 WARNING [optim.py:503] Scaling gradients by 0.0003103560011368245, model_norm_threshold=5246527471616.0
2024-10-08 21:53:42,993 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.194e+31, grad_sumsq=2.753e+33, orig_rms_sq=2.614e-02
2024-10-08 21:53:44,089 WARNING [optim.py:503] Scaling gradients by 0.0554165281355381, model_norm_threshold=5246527471616.0
2024-10-08 21:53:44,227 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.597e+27, grad_sumsq=3.860e+29, orig_rms_sq=6.728e-03
2024-10-08 21:53:44,417 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer1.prob, batch_count=2464.0, ans=0.3845
2024-10-08 21:53:45,284 WARNING [optim.py:503] Scaling gradients by 0.05013187229633331, model_norm_threshold=5246527471616.0
2024-10-08 21:53:45,424 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.759e+27, grad_sumsq=1.049e+29, orig_rms_sq=2.630e-02
2024-10-08 21:53:46,570 WARNING [optim.py:503] Scaling gradients by 0.00793344248086214, model_norm_threshold=5246527471616.0
2024-10-08 21:53:46,710 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.121e+29, grad_sumsq=4.264e+30, orig_rms_sq=2.630e-02
2024-10-08 21:53:49,180 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.352e+10 5.968e+11 2.570e+12 2.077e+13 1.690e+16, threshold=5.140e+12, percent-clipped=42.0
2024-10-08 21:53:49,326 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2464.0, ans=0.27536
2024-10-08 21:53:49,553 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.13 vs. limit=8.424
2024-10-08 21:53:50,445 WARNING [optim.py:503] Scaling gradients by 0.08221770077943802, model_norm_threshold=5140045627392.0
2024-10-08 21:53:50,586 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.858e+26, grad_sumsq=2.907e+26, orig_rms_sq=3.392e+00
2024-10-08 21:53:51,679 INFO [train.py:1154] Epoch 2, batch 550, loss[loss=1.321, simple_loss=0.6848, pruned_loss=0.7633, ctc_loss=1.077, over 4790.00 frames. ], tot_loss[loss=1.27, simple_loss=0.6441, pruned_loss=0.7391, ctc_loss=1.043, over 905465.58 frames. ], batch size: 40, lr: 3.72e-02,
2024-10-08 21:53:51,790 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=2467.3333333333335, ans=0.23701
2024-10-08 21:53:53,308 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=27.69 vs. limit=8.42525
2024-10-08 21:53:55,289 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=2467.3333333333335, ans=0.38434375
2024-10-08 21:53:56,289 WARNING [optim.py:503] Scaling gradients by 0.038886383175849915, model_norm_threshold=5140045627392.0
2024-10-08 21:53:56,429 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.544e+27, grad_sumsq=1.164e+30, orig_rms_sq=6.480e-03
2024-10-08 21:53:57,486 WARNING [optim.py:503] Scaling gradients by 0.0014983335277065635, model_norm_threshold=5140045627392.0
2024-10-08 21:53:57,623 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.660e+30, grad_sumsq=8.062e+29, orig_rms_sq=3.299e+00
2024-10-08 21:53:58,194 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=36.61 vs. limit=8.42525
2024-10-08 21:54:00,059 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=2467.3333333333335, ans=0.8136433333333334
2024-10-08 21:54:00,940 WARNING [optim.py:503] Scaling gradients by 0.009944099001586437, model_norm_threshold=5140045627392.0
2024-10-08 21:54:01,080 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.707e+28, grad_sumsq=2.983e+30, orig_rms_sq=2.584e-02
2024-10-08 21:54:05,272 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.64 vs. limit=9.353
2024-10-08 21:54:05,819 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward2.hidden_balancer.prob, batch_count=2470.6666666666665, ans=0.3841875
2024-10-08 21:54:07,096 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.max_positive, batch_count=2470.6666666666665, ans=0.7747066666666667
2024-10-08 21:54:17,915 WARNING [optim.py:503] Scaling gradients by 0.004494118969887495, model_norm_threshold=5140045627392.0
2024-10-08 21:54:18,056 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.257e+29, grad_sumsq=7.572e+31, orig_rms_sq=5.621e-03
2024-10-08 21:54:19,136 WARNING [optim.py:503] Scaling gradients by 0.008871372789144516, model_norm_threshold=5140045627392.0
2024-10-08 21:54:19,273 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.536e+28, grad_sumsq=2.898e+30, orig_rms_sq=2.600e-02
2024-10-08 21:54:19,796 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.39 vs. limit=5.6185
2024-10-08 21:54:29,060 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.80 vs. limit=9.358
2024-10-08 21:54:29,662 WARNING [optim.py:503] Scaling gradients by 0.012391607277095318, model_norm_threshold=5140045627392.0
2024-10-08 21:54:29,799 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.955e+28, grad_sumsq=7.098e+30, orig_rms_sq=5.573e-03
2024-10-08 21:54:33,867 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=11.23 vs. limit=5.6193333333333335
2024-10-08 21:54:35,541 WARNING [optim.py:503] Scaling gradients by 0.02774864062666893, model_norm_threshold=5140045627392.0
2024-10-08 21:54:35,678 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.13, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.535e+27, grad_sumsq=8.135e+29, orig_rms_sq=5.574e-03
2024-10-08 21:54:38,418 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.32 vs. limit=8.430250000000001
2024-10-08 21:54:38,753 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.70 vs. limit=9.3605
2024-10-08 21:54:41,270 WARNING [optim.py:503] Scaling gradients by 0.06944937258958817, model_norm_threshold=5140045627392.0
2024-10-08 21:54:41,411 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.155e+27, grad_sumsq=4.472e+28, orig_rms_sq=2.583e-02
2024-10-08 21:54:42,474 WARNING [optim.py:503] Scaling gradients by 0.00037719024112448096, model_norm_threshold=5140045627392.0
2024-10-08 21:54:42,613 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.454e+31, grad_sumsq=1.337e+33, orig_rms_sq=2.583e-02
2024-10-08 21:54:46,623 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=20.88 vs. limit=8.430250000000001
2024-10-08 21:54:47,955 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.55 vs. limit=6.240333333333333
2024-10-08 21:54:48,301 WARNING [optim.py:503] Scaling gradients by 0.005035668145865202, model_norm_threshold=5140045627392.0
2024-10-08 21:54:48,439 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.148e+29, grad_sumsq=8.845e+31, orig_rms_sq=5.820e-03
2024-10-08 21:54:49,632 INFO [train.py:1154] Epoch 2, batch 600, loss[loss=1.291, simple_loss=0.6735, pruned_loss=0.7384, ctc_loss=1.078, over 4810.00 frames. ], tot_loss[loss=1.272, simple_loss=0.6452, pruned_loss=0.7406, ctc_loss=1.045, over 919259.30 frames. ], batch size: 38, lr: 3.72e-02,
2024-10-08 21:54:50,707 WARNING [optim.py:503] Scaling gradients by 0.0044419351033866405, model_norm_threshold=5140045627392.0
2024-10-08 21:54:50,846 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.251e+29, grad_sumsq=9.997e+28, orig_rms_sq=3.252e+00
2024-10-08 21:54:58,273 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.26 vs. limit=8.4315
2024-10-08 21:54:58,909 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=2484.0, ans=0.11027500000000001
2024-10-08 21:55:01,999 WARNING [optim.py:503] Scaling gradients by 0.006574002560228109, model_norm_threshold=5140045627392.0
2024-10-08 21:55:02,152 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.540e+29, grad_sumsq=5.864e+30, orig_rms_sq=2.626e-02
2024-10-08 21:55:07,456 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.80 vs. limit=5.621833333333333
2024-10-08 21:55:11,532 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.ff2_skip_rate, batch_count=2490.6666666666665, ans=0.04396
2024-10-08 21:55:11,539 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=2490.6666666666665, ans=0.1066
2024-10-08 21:55:13,213 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.83 vs. limit=9.368
2024-10-08 21:55:14,747 WARNING [optim.py:503] Scaling gradients by 0.0420663096010685, model_norm_threshold=5140045627392.0
2024-10-08 21:55:14,887 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.908e+27, grad_sumsq=4.863e+29, orig_rms_sq=5.978e-03
2024-10-08 21:55:15,963 WARNING [optim.py:503] Scaling gradients by 4.357259604148567e-05, model_norm_threshold=5140045627392.0
2024-10-08 21:55:16,104 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.024e+33, grad_sumsq=3.332e+32, orig_rms_sq=9.074e+00
2024-10-08 21:55:20,910 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=2490.6666666666665, ans=0.8128266666666667
2024-10-08 21:55:23,052 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=2494.0, ans=0.81271
2024-10-08 21:55:23,799 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.96 vs. limit=5.6235
2024-10-08 21:55:27,096 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=33.54 vs. limit=8.43525
2024-10-08 21:55:27,595 WARNING [optim.py:503] Scaling gradients by 0.018520204350352287, model_norm_threshold=5140045627392.0
2024-10-08 21:55:27,734 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.466e+28, grad_sumsq=5.591e+30, orig_rms_sq=6.199e-03
2024-10-08 21:55:29,865 WARNING [optim.py:503] Scaling gradients by 0.020423762500286102, model_norm_threshold=5140045627392.0
2024-10-08 21:55:30,002 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.632e+28, grad_sumsq=6.250e+29, orig_rms_sq=2.612e-02
2024-10-08 21:55:31,640 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=4.82 vs. limit=8.43525
2024-10-08 21:55:33,362 WARNING [optim.py:503] Scaling gradients by 0.018454939126968384, model_norm_threshold=5140045627392.0
2024-10-08 21:55:33,503 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.338e+28, grad_sumsq=3.727e+30, orig_rms_sq=6.273e-03
2024-10-08 21:55:34,102 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=45.02 vs. limit=8.43525
2024-10-08 21:55:35,689 WARNING [optim.py:503] Scaling gradients by 0.004877923522144556, model_norm_threshold=5140045627392.0
2024-10-08 21:55:35,829 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.177e+29, grad_sumsq=6.772e+28, orig_rms_sq=3.214e+00
2024-10-08 21:55:36,924 WARNING [optim.py:503] Scaling gradients by 0.05041635408997536, model_norm_threshold=5140045627392.0
2024-10-08 21:55:37,064 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.956e+27, grad_sumsq=9.197e+26, orig_rms_sq=3.214e+00
2024-10-08 21:55:37,210 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2497.3333333333335, ans=0.27502666666666664
2024-10-08 21:55:39,464 WARNING [optim.py:503] Scaling gradients by 0.059503063559532166, model_norm_threshold=5140045627392.0
2024-10-08 21:55:39,603 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.374e+27, grad_sumsq=7.384e+26, orig_rms_sq=3.214e+00
2024-10-08 21:55:44,039 WARNING [optim.py:503] Scaling gradients by 0.09503962844610214, model_norm_threshold=5140045627392.0
2024-10-08 21:55:44,177 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.940e+26, grad_sumsq=2.605e+28, orig_rms_sq=2.665e-02
2024-10-08 21:55:45,299 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.430e+10 5.318e+11 3.042e+12 3.748e+13 1.180e+17, threshold=6.085e+12, percent-clipped=40.0
2024-10-08 21:55:45,299 WARNING [optim.py:503] Scaling gradients by 0.0426911935210228, model_norm_threshold=6084658462720.0
2024-10-08 21:55:45,441 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.644e+27, grad_sumsq=1.745e+29, orig_rms_sq=2.661e-02
2024-10-08 21:55:47,748 INFO [train.py:1154] Epoch 2, batch 650, loss[loss=1.214, simple_loss=0.5999, pruned_loss=0.7102, ctc_loss=1.021, over 4843.00 frames. ], tot_loss[loss=1.271, simple_loss=0.6434, pruned_loss=0.7405, ctc_loss=1.044, over 930188.13 frames. ], batch size: 21, lr: 3.71e-02,
2024-10-08 21:55:50,903 WARNING [optim.py:503] Scaling gradients by 0.008153525181114674, model_norm_threshold=6084658462720.0
2024-10-08 21:55:51,042 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.289e+29, grad_sumsq=4.852e+30, orig_rms_sq=2.656e-02
2024-10-08 21:55:52,156 WARNING [optim.py:503] Scaling gradients by 0.008515954948961735, model_norm_threshold=6084658462720.0
2024-10-08 21:55:52,295 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.539e+29, grad_sumsq=5.795e+30, orig_rms_sq=2.656e-02
2024-10-08 21:55:53,319 WARNING [optim.py:503] Scaling gradients by 0.08658052235841751, model_norm_threshold=6084658462720.0
2024-10-08 21:55:53,459 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.526e+26, grad_sumsq=3.210e+28, orig_rms_sq=2.656e-02
2024-10-08 21:55:59,619 WARNING [optim.py:503] Scaling gradients by 0.0818093791604042, model_norm_threshold=6084658462720.0
2024-10-08 21:55:59,756 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.808e+26, grad_sumsq=3.104e+26, orig_rms_sq=3.159e+00
2024-10-08 21:56:01,971 WARNING [optim.py:503] Scaling gradients by 0.022831035777926445, model_norm_threshold=6084658462720.0
2024-10-08 21:56:02,111 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.525e+28, grad_sumsq=5.645e+29, orig_rms_sq=2.701e-02
2024-10-08 21:56:07,840 WARNING [optim.py:503] Scaling gradients by 0.0005134443053975701, model_norm_threshold=6084658462720.0
2024-10-08 21:56:07,980 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.737e+31, grad_sumsq=9.991e+32, orig_rms_sq=2.740e-02
2024-10-08 21:56:09,117 WARNING [optim.py:503] Scaling gradients by 0.005687137134373188, model_norm_threshold=6084658462720.0
2024-10-08 21:56:09,258 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.651e+29, grad_sumsq=8.455e+28, orig_rms_sq=3.135e+00
2024-10-08 21:56:10,441 WARNING [optim.py:503] Scaling gradients by 0.01943301036953926, model_norm_threshold=6084658462720.0
2024-10-08 21:56:10,582 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.557e+28, grad_sumsq=1.134e+28, orig_rms_sq=3.137e+00
2024-10-08 21:56:12,819 WARNING [optim.py:503] Scaling gradients by 0.015480389818549156, model_norm_threshold=6084658462720.0
2024-10-08 21:56:12,958 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.802e+28, grad_sumsq=1.374e+30, orig_rms_sq=2.767e-02
2024-10-08 21:56:15,827 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.18 vs. limit=9.3805
2024-10-08 21:56:17,008 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=7.26 vs. limit=5.002933333333333
2024-10-08 21:56:18,587 WARNING [optim.py:503] Scaling gradients by 0.07945325970649719, model_norm_threshold=6084658462720.0
2024-10-08 21:56:18,725 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.52, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.026e+27, grad_sumsq=4.572e+29, orig_rms_sq=6.617e-03
2024-10-08 21:56:19,774 WARNING [optim.py:503] Scaling gradients by 0.001289946842007339, model_norm_threshold=6084658462720.0
2024-10-08 21:56:19,913 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.216e+30, grad_sumsq=7.788e+32, orig_rms_sq=6.697e-03
2024-10-08 21:56:20,996 WARNING [optim.py:503] Scaling gradients by 0.0019060421036556363, model_norm_threshold=6084658462720.0
2024-10-08 21:56:21,134 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.537e+30, grad_sumsq=8.117e+29, orig_rms_sq=3.126e+00
2024-10-08 21:56:21,884 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.23 vs. limit=8.44025
2024-10-08 21:56:23,066 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=7.39 vs. limit=5.002933333333333
2024-10-08 21:56:31,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.74 vs. limit=9.383
2024-10-08 21:56:34,868 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=2514.0, ans=0.38215625
2024-10-08 21:56:35,405 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=6.56 vs. limit=5.0056
2024-10-08 21:56:35,662 WARNING [optim.py:503] Scaling gradients by 0.009513240307569504, model_norm_threshold=6084658462720.0
2024-10-08 21:56:35,802 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.350e+28, grad_sumsq=3.410e+30, orig_rms_sq=2.742e-02
2024-10-08 21:56:37,710 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.62 vs. limit=3.3771
2024-10-08 21:56:39,619 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.48 vs. limit=5.6285
2024-10-08 21:56:43,359 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=13.67 vs. limit=9.3855
2024-10-08 21:56:43,613 WARNING [optim.py:503] Scaling gradients by 0.01779020018875599, model_norm_threshold=6084658462720.0
2024-10-08 21:56:43,753 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.62, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.288e+28, grad_sumsq=2.634e+30, orig_rms_sq=2.767e-02
2024-10-08 21:56:47,130 INFO [train.py:1154] Epoch 2, batch 700, loss[loss=1.365, simple_loss=0.6775, pruned_loss=0.8091, ctc_loss=1.085, over 4740.00 frames. ], tot_loss[loss=1.274, simple_loss=0.6449, pruned_loss=0.7428, ctc_loss=1.045, over 937971.52 frames. ], batch size: 19, lr: 3.70e-02,
2024-10-08 21:56:47,210 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.balancer.prob, batch_count=2517.3333333333335, ans=0.382
2024-10-08 21:56:48,261 WARNING [optim.py:503] Scaling gradients by 0.04898909851908684, model_norm_threshold=6084658462720.0
2024-10-08 21:56:48,399 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.001e+27, grad_sumsq=2.201e+29, orig_rms_sq=2.727e-02
2024-10-08 21:56:56,375 WARNING [optim.py:503] Scaling gradients by 0.054949432611465454, model_norm_threshold=6084658462720.0
2024-10-08 21:56:56,514 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.454e+27, grad_sumsq=8.135e+26, orig_rms_sq=3.017e+00
2024-10-08 21:56:56,721 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer1.prob, batch_count=2517.3333333333335, ans=0.382
2024-10-08 21:56:57,808 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.ff3_skip_rate, batch_count=2520.6666666666665, ans=0.043285000000000004
2024-10-08 21:56:58,730 WARNING [optim.py:503] Scaling gradients by 0.009591127745807171, model_norm_threshold=6084658462720.0
2024-10-08 21:56:58,870 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.742e+29, grad_sumsq=5.775e+28, orig_rms_sq=3.017e+00
2024-10-08 21:56:59,930 WARNING [optim.py:503] Scaling gradients by 0.04509957507252693, model_norm_threshold=6084658462720.0
2024-10-08 21:57:00,067 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.907e+27, grad_sumsq=4.368e+26, orig_rms_sq=8.944e+00
2024-10-08 21:57:01,138 WARNING [optim.py:503] Scaling gradients by 0.07184750586748123, model_norm_threshold=6084658462720.0
2024-10-08 21:57:01,276 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.708e+27, grad_sumsq=6.730e+28, orig_rms_sq=2.538e-02
2024-10-08 21:57:06,897 WARNING [optim.py:503] Scaling gradients by 0.013042616657912731, model_norm_threshold=6084658462720.0
2024-10-08 21:57:07,036 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.001e+28, grad_sumsq=1.599e+28, orig_rms_sq=3.128e+00
2024-10-08 21:57:15,951 WARNING [optim.py:503] Scaling gradients by 0.016609542071819305, model_norm_threshold=6084658462720.0
2024-10-08 21:57:16,092 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.459e+28, grad_sumsq=1.367e+30, orig_rms_sq=2.530e-02
2024-10-08 21:57:16,774 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=27.47 vs. limit=8.4465
2024-10-08 21:57:17,226 WARNING [optim.py:503] Scaling gradients by 0.008755148388445377, model_norm_threshold=6084658462720.0
2024-10-08 21:57:17,366 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.049e+29, grad_sumsq=3.036e+31, orig_rms_sq=6.750e-03
2024-10-08 21:57:17,761 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=24.13 vs. limit=9.393
2024-10-08 21:57:22,667 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=12.97 vs. limit=6.2636666666666665
2024-10-08 21:57:24,318 WARNING [optim.py:503] Scaling gradients by 0.02407490648329258, model_norm_threshold=6084658462720.0
2024-10-08 21:57:24,456 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.231e+28, grad_sumsq=1.364e+27, orig_rms_sq=9.026e+00
2024-10-08 21:57:29,582 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.21 vs. limit=8.44775
2024-10-08 21:57:35,046 WARNING [optim.py:503] Scaling gradients by 0.013050269335508347, model_norm_threshold=6084658462720.0
2024-10-08 21:57:35,186 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.422e+28, grad_sumsq=1.109e+31, orig_rms_sq=6.692e-03
2024-10-08 21:57:38,512 WARNING [optim.py:503] Scaling gradients by 0.07109785079956055, model_norm_threshold=6084658462720.0
2024-10-08 21:57:38,654 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.455e+27, grad_sumsq=4.626e+26, orig_rms_sq=3.144e+00
2024-10-08 21:57:43,041 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.28 vs. limit=5.632666666666666
2024-10-08 21:57:43,367 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.766e+10 1.250e+12 5.452e+12 7.028e+13 1.185e+16, threshold=1.090e+13, percent-clipped=48.0
2024-10-08 21:57:45,717 INFO [train.py:1154] Epoch 2, batch 750, loss[loss=1.233, simple_loss=0.6192, pruned_loss=0.7243, ctc_loss=0.9969, over 4894.00 frames. ], tot_loss[loss=1.272, simple_loss=0.6431, pruned_loss=0.7416, ctc_loss=1.044, over 944733.25 frames. ], batch size: 22, lr: 3.70e-02,
2024-10-08 21:57:47,865 WARNING [optim.py:503] Scaling gradients by 0.00020001857774332166, model_norm_threshold=10904799281152.0
2024-10-08 21:57:48,005 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.778e+32, grad_sumsq=3.043e+34, orig_rms_sq=2.556e-02
2024-10-08 21:57:49,074 WARNING [optim.py:503] Scaling gradients by 0.07406099885702133, model_norm_threshold=10904799281152.0
2024-10-08 21:57:49,214 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.980e+27, grad_sumsq=5.638e+26, orig_rms_sq=8.832e+00
2024-10-08 21:58:01,763 WARNING [optim.py:503] Scaling gradients by 0.047061737626791, model_norm_threshold=10904799281152.0
2024-10-08 21:58:01,901 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.234e+28, grad_sumsq=3.711e+27, orig_rms_sq=3.325e+00
2024-10-08 21:58:06,250 WARNING [optim.py:503] Scaling gradients by 0.03273649886250496, model_norm_threshold=10904799281152.0
2024-10-08 21:58:06,391 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.578e+28, grad_sumsq=1.023e+30, orig_rms_sq=2.520e-02
2024-10-08 21:58:07,564 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=2540.6666666666665, ans=0.1070875
2024-10-08 21:58:08,405 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=9.90 vs. limit=8.45275
2024-10-08 21:58:08,789 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=2540.6666666666665, ans=0.104725
2024-10-08 21:58:10,127 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.92 vs. limit=3.3811
2024-10-08 21:58:10,176 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=15.69 vs. limit=8.45275
2024-10-08 21:58:16,703 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer1.prob, batch_count=2540.6666666666665, ans=0.38090625
2024-10-08 21:58:18,015 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=18.90 vs. limit=9.4055
2024-10-08 21:58:20,657 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.79 vs. limit=9.408
2024-10-08 21:58:23,787 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=10.88 vs. limit=5.636
2024-10-08 21:58:24,406 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=2544.0, ans=0.04276
2024-10-08 21:58:27,040 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=4.50 vs. limit=5.0176
2024-10-08 21:58:27,601 WARNING [optim.py:503] Scaling gradients by 0.04973640292882919, model_norm_threshold=10904799281152.0
2024-10-08 21:58:27,740 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.015e+28, grad_sumsq=1.129e+27, orig_rms_sq=8.991e+00
2024-10-08 21:58:32,483 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=2547.3333333333335, ans=0.10447499999999998
2024-10-08 21:58:32,788 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.65 vs. limit=3.3821
2024-10-08 21:58:33,356 WARNING [optim.py:503] Scaling gradients by 0.012119006365537643, model_norm_threshold=10904799281152.0
2024-10-08 21:58:33,502 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.321e+29, grad_sumsq=9.288e+30, orig_rms_sq=2.499e-02
2024-10-08 21:58:36,826 WARNING [optim.py:503] Scaling gradients by 0.08493787795305252, model_norm_threshold=10904799281152.0
2024-10-08 21:58:36,965 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.740e+27, grad_sumsq=6.182e+29, orig_rms_sq=6.050e-03
2024-10-08 21:58:38,105 WARNING [optim.py:503] Scaling gradients by 0.09327582269906998, model_norm_threshold=10904799281152.0
2024-10-08 21:58:38,244 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.772e+27, grad_sumsq=4.641e+29, orig_rms_sq=5.972e-03
2024-10-08 21:58:40,762 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=2547.3333333333335, ans=0.38059375
2024-10-08 21:58:42,403 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=10.18 vs. limit=5.637666666666666
2024-10-08 21:58:42,959 INFO [train.py:1154] Epoch 2, batch 800, loss[loss=1.158, simple_loss=0.5749, pruned_loss=0.6777, ctc_loss=0.9616, over 4852.00 frames. ], tot_loss[loss=1.27, simple_loss=0.6423, pruned_loss=0.7405, ctc_loss=1.043, over 949630.50 frames. ], batch size: 19, lr: 3.69e-02,
2024-10-08 21:58:45,706 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=5.64 vs. limit=5.637666666666666
2024-10-08 21:58:48,465 WARNING [optim.py:503] Scaling gradients by 0.003986341413110495, model_norm_threshold=10904799281152.0
2024-10-08 21:58:48,605 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.798e+30, grad_sumsq=5.137e+29, orig_rms_sq=3.500e+00
2024-10-08 21:58:55,534 WARNING [optim.py:503] Scaling gradients by 0.032023780047893524, model_norm_threshold=10904799281152.0
2024-10-08 21:58:55,673 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.071e+28, grad_sumsq=3.393e+27, orig_rms_sq=9.051e+00
2024-10-08 21:59:02,750 WARNING [optim.py:503] Scaling gradients by 0.0637931078672409, model_norm_threshold=10904799281152.0
2024-10-08 21:59:02,890 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.330e+27, grad_sumsq=1.746e+27, orig_rms_sq=3.626e+00
2024-10-08 21:59:03,120 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=2554.0, ans=0.10422499999999998
2024-10-08 21:59:03,608 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=16.34 vs. limit=8.45775
2024-10-08 21:59:18,688 WARNING [optim.py:503] Scaling gradients by 0.09560061991214752, model_norm_threshold=10904799281152.0
2024-10-08 21:59:18,827 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.984e+27, grad_sumsq=3.296e+26, orig_rms_sq=9.054e+00
2024-10-08 21:59:22,092 WARNING [optim.py:503] Scaling gradients by 0.08455681800842285, model_norm_threshold=10904799281152.0
2024-10-08 21:59:22,233 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.096e+27, grad_sumsq=1.208e+29, orig_rms_sq=2.563e-02
2024-10-08 21:59:23,322 WARNING [optim.py:503] Scaling gradients by 0.09547177702188492, model_norm_threshold=10904799281152.0
2024-10-08 21:59:23,460 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.828e+27, grad_sumsq=6.604e+29, orig_rms_sq=5.796e-03
2024-10-08 21:59:24,603 WARNING [optim.py:503] Scaling gradients by 0.0027557939756661654, model_norm_threshold=10904799281152.0
2024-10-08 21:59:24,740 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.296e+30, grad_sumsq=2.066e+32, orig_rms_sq=2.563e-02
2024-10-08 21:59:33,576 WARNING [optim.py:503] Scaling gradients by 0.004141886718571186, model_norm_threshold=10904799281152.0
2024-10-08 21:59:33,715 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.845e+30, grad_sumsq=5.193e+29, orig_rms_sq=3.552e+00
2024-10-08 21:59:37,031 WARNING [optim.py:503] Scaling gradients by 0.08096470683813095, model_norm_threshold=10904799281152.0
2024-10-08 21:59:37,171 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.990e+27, grad_sumsq=1.686e+27, orig_rms_sq=3.552e+00
2024-10-08 21:59:38,335 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.626e+09 4.099e+11 2.229e+12 4.886e+13 5.452e+16, threshold=4.458e+12, percent-clipped=37.0
2024-10-08 21:59:40,600 INFO [train.py:1154] Epoch 2, batch 850, loss[loss=1.299, simple_loss=0.6537, pruned_loss=0.75, ctc_loss=1.109, over 4804.00 frames. ], tot_loss[loss=1.267, simple_loss=0.6406, pruned_loss=0.7388, ctc_loss=1.041, over 953989.62 frames. ], batch size: 29, lr: 3.69e-02,
2024-10-08 21:59:43,876 WARNING [optim.py:503] Scaling gradients by 0.020932428538799286, model_norm_threshold=4458023485440.0
2024-10-08 21:59:44,014 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.889e+28, grad_sumsq=5.464e+27, orig_rms_sq=3.458e+00
2024-10-08 21:59:44,156 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=2567.3333333333335, ans=6.604583333333333
2024-10-08 21:59:46,043 WARNING [optim.py:503] Scaling gradients by 0.0022775372490286827, model_norm_threshold=4458023485440.0
2024-10-08 21:59:46,183 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.159e+30, grad_sumsq=3.352e+29, orig_rms_sq=3.458e+00
2024-10-08 21:59:47,640 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer1.min_positive, batch_count=2567.3333333333335, ans=0.04197708333333333
2024-10-08 21:59:48,988 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=15.92 vs. limit=8.46275
2024-10-08 21:59:49,401 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.94 vs. limit=9.4255
2024-10-08 21:59:49,600 WARNING [optim.py:503] Scaling gradients by 8.474642527289689e-05, model_norm_threshold=4458023485440.0
2024-10-08 21:59:49,738 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.338e+33, grad_sumsq=3.896e+32, orig_rms_sq=3.435e+00
2024-10-08 21:59:50,784 WARNING [optim.py:503] Scaling gradients by 0.007414339575916529, model_norm_threshold=4458023485440.0
2024-10-08 21:59:50,924 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.163e+29, grad_sumsq=3.386e+28, orig_rms_sq=3.435e+00
2024-10-08 21:59:53,186 WARNING [optim.py:503] Scaling gradients by 0.02886560745537281, model_norm_threshold=4458023485440.0
2024-10-08 21:59:53,324 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.59, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.398e+28, grad_sumsq=4.083e+27, orig_rms_sq=3.424e+00
2024-10-08 21:59:54,359 WARNING [optim.py:503] Scaling gradients by 0.02217564731836319, model_norm_threshold=4458023485440.0
2024-10-08 21:59:54,497 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.453e+28, grad_sumsq=4.244e+27, orig_rms_sq=3.424e+00
2024-10-08 21:59:55,176 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=2570.6666666666665, ans=9.428
2024-10-08 21:59:55,557 WARNING [optim.py:503] Scaling gradients by 0.0014571209903806448, model_norm_threshold=4458023485440.0
2024-10-08 21:59:55,698 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.401e+30, grad_sumsq=1.285e+30, orig_rms_sq=3.424e+00
2024-10-08 21:59:57,356 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten.whitening_limit, batch_count=2570.6666666666665, ans=9.428
2024-10-08 21:59:58,506 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=18.15 vs. limit=8.464
2024-10-08 21:59:58,517 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=12.33 vs. limit=8.464
2024-10-08 21:59:59,149 WARNING [optim.py:503] Scaling gradients by 0.04314954951405525, model_norm_threshold=4458023485440.0
2024-10-08 21:59:59,288 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.55, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.844e+27, grad_sumsq=1.708e+27, orig_rms_sq=3.421e+00
2024-10-08 22:00:00,312 WARNING [optim.py:503] Scaling gradients by 0.03002535179257393, model_norm_threshold=4458023485440.0
2024-10-08 22:00:00,463 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.579e+27, grad_sumsq=2.508e+27, orig_rms_sq=3.421e+00
2024-10-08 22:00:01,539 WARNING [optim.py:503] Scaling gradients by 0.04768789932131767, model_norm_threshold=4458023485440.0
2024-10-08 22:00:01,678 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.914e+27, grad_sumsq=1.138e+27, orig_rms_sq=3.440e+00
2024-10-08 22:00:02,711 WARNING [optim.py:503] Scaling gradients by 0.042903587222099304, model_norm_threshold=4458023485440.0
2024-10-08 22:00:02,850 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.049e+27, grad_sumsq=1.177e+27, orig_rms_sq=3.440e+00
2024-10-08 22:00:06,103 WARNING [optim.py:503] Scaling gradients by 0.009967582300305367, model_norm_threshold=4458023485440.0
2024-10-08 22:00:06,241 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.493e+28, grad_sumsq=1.840e+28, orig_rms_sq=3.530e+00
2024-10-08 22:00:07,305 WARNING [optim.py:503] Scaling gradients by 0.06553438305854797, model_norm_threshold=4458023485440.0
2024-10-08 22:00:07,442 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.495e+27, grad_sumsq=2.746e+29, orig_rms_sq=5.444e-03
2024-10-08 22:00:07,824 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.whiten.whitening_limit, batch_count=2574.0, ans=5.0296
2024-10-08 22:00:11,131 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.ff3_skip_rate, batch_count=2574.0, ans=0.042085
2024-10-08 22:00:11,941 WARNING [optim.py:503] Scaling gradients by 0.0007898587500676513, model_norm_threshold=4458023485440.0
2024-10-08 22:00:12,080 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.57, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.803e+31, grad_sumsq=3.168e+33, orig_rms_sq=5.691e-03
2024-10-08 22:00:13,126 WARNING [optim.py:503] Scaling gradients by 0.048748087137937546, model_norm_threshold=4458023485440.0
2024-10-08 22:00:13,265 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.376e+27, grad_sumsq=5.931e+29, orig_rms_sq=5.691e-03
2024-10-08 22:00:14,983 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=54.31 vs. limit=8.4665
2024-10-08 22:00:16,178 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=23.71 vs. limit=9.433
2024-10-08 22:00:17,734 WARNING [optim.py:503] Scaling gradients by 0.04078689217567444, model_norm_threshold=4458023485440.0
2024-10-08 22:00:17,875 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.56, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.745e+27, grad_sumsq=1.148e+30, orig_rms_sq=5.876e-03
2024-10-08 22:00:19,992 WARNING [optim.py:503] Scaling gradients by 0.07348823547363281, model_norm_threshold=4458023485440.0
2024-10-08 22:00:20,131 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.157e+26, grad_sumsq=3.004e+28, orig_rms_sq=2.383e-02
2024-10-08 22:00:20,654 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=38.05 vs. limit=9.433
2024-10-08 22:00:21,174 WARNING [optim.py:503] Scaling gradients by 0.0012357892701402307, model_norm_threshold=4458023485440.0
2024-10-08 22:00:21,316 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.62, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.062e+30, grad_sumsq=1.346e+33, orig_rms_sq=5.991e-03
2024-10-08 22:00:23,494 WARNING [optim.py:503] Scaling gradients by 0.0034943101927638054, model_norm_threshold=4458023485440.0
2024-10-08 22:00:23,634 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.053e+29, grad_sumsq=1.010e+32, orig_rms_sq=5.991e-03
2024-10-08 22:00:24,715 WARNING [optim.py:503] Scaling gradients by 0.060927584767341614, model_norm_threshold=4458023485440.0
2024-10-08 22:00:24,856 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.241e+27, grad_sumsq=5.199e+28, orig_rms_sq=2.387e-02
2024-10-08 22:00:25,593 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.63 vs. limit=5.644333333333334
2024-10-08 22:00:26,693 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=8.75 vs. limit=8.46775
2024-10-08 22:00:27,016 WARNING [optim.py:503] Scaling gradients by 0.004633176140487194, model_norm_threshold=4458023485440.0
2024-10-08 22:00:27,156 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.223e+29, grad_sumsq=5.943e+28, orig_rms_sq=3.740e+00
2024-10-08 22:00:28,899 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=25.27 vs. limit=9.4355
2024-10-08 22:00:29,877 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.27 vs. limit=9.4355
2024-10-08 22:00:31,540 WARNING [optim.py:503] Scaling gradients by 0.03506404161453247, model_norm_threshold=4458023485440.0
2024-10-08 22:00:31,680 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.764e+27, grad_sumsq=1.288e+30, orig_rms_sq=6.029e-03
2024-10-08 22:00:36,923 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=10.36 vs. limit=5.645166666666666
2024-10-08 22:00:37,378 WARNING [optim.py:503] Scaling gradients by 0.017436150461435318, model_norm_threshold=4458023485440.0
2024-10-08 22:00:37,520 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.339e+28, grad_sumsq=5.530e+30, orig_rms_sq=6.037e-03
2024-10-08 22:00:38,716 INFO [train.py:1154] Epoch 2, batch 900, loss[loss=1.349, simple_loss=0.6806, pruned_loss=0.7967, ctc_loss=1.059, over 4855.00 frames. ], tot_loss[loss=1.269, simple_loss=0.642, pruned_loss=0.7394, ctc_loss=1.044, over 956955.00 frames. ], batch size: 19, lr: 3.68e-02,
2024-10-08 22:00:43,564 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.79 vs. limit=6.292
2024-10-08 22:00:46,176 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.29 vs. limit=8.469
2024-10-08 22:00:46,541 WARNING [optim.py:503] Scaling gradients by 0.05769771337509155, model_norm_threshold=4458023485440.0
2024-10-08 22:00:46,681 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.908e+27, grad_sumsq=3.149e+29, orig_rms_sq=6.059e-03
2024-10-08 22:00:47,948 WARNING [optim.py:503] Scaling gradients by 0.03548920527100563, model_norm_threshold=4458023485440.0
2024-10-08 22:00:48,087 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.911e+27, grad_sumsq=8.067e+29, orig_rms_sq=6.087e-03
2024-10-08 22:00:50,310 WARNING [optim.py:503] Scaling gradients by 0.0009212564909830689, model_norm_threshold=4458023485440.0
2024-10-08 22:00:50,449 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.755e+30, grad_sumsq=1.973e+32, orig_rms_sq=2.410e-02
2024-10-08 22:00:51,671 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.balancer.min_positive, batch_count=2587.3333333333335, ans=0.22412666666666667
2024-10-08 22:00:53,722 WARNING [optim.py:503] Scaling gradients by 0.00022232523770071566, model_norm_threshold=4458023485440.0
2024-10-08 22:00:53,860 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.55, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.226e+32, grad_sumsq=3.624e+34, orig_rms_sq=6.142e-03
2024-10-08 22:00:55,435 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=15.04 vs. limit=8.47025
2024-10-08 22:00:56,110 WARNING [optim.py:503] Scaling gradients by 0.034521400928497314, model_norm_threshold=4458023485440.0
2024-10-08 22:00:56,249 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.491e+27, grad_sumsq=1.220e+30, orig_rms_sq=6.142e-03
2024-10-08 22:00:57,358 WARNING [optim.py:503] Scaling gradients by 0.03370010852813721, model_norm_threshold=4458023485440.0
2024-10-08 22:00:57,497 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.494e+27, grad_sumsq=2.232e+29, orig_rms_sq=2.461e-02
2024-10-08 22:01:01,072 WARNING [optim.py:503] Scaling gradients by 0.05487451329827309, model_norm_threshold=4458023485440.0
2024-10-08 22:01:01,211 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.59, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.892e+27, grad_sumsq=6.269e+29, orig_rms_sq=6.209e-03
2024-10-08 22:01:02,303 WARNING [optim.py:503] Scaling gradients by 0.0662749856710434, model_norm_threshold=4458023485440.0
2024-10-08 22:01:02,442 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.067e+27, grad_sumsq=1.728e+29, orig_rms_sq=6.175e-03
2024-10-08 22:01:02,630 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=2590.6666666666665, ans=0.1761666666666667
2024-10-08 22:01:05,080 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=7.33 vs. limit=5.036266666666666
2024-10-08 22:01:06,211 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.87 vs. limit=6.295333333333334
2024-10-08 22:01:08,889 WARNING [optim.py:503] Scaling gradients by 0.023446310311555862, model_norm_threshold=4458023485440.0
2024-10-08 22:01:09,028 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.62, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.246e+28, grad_sumsq=3.646e+30, orig_rms_sq=6.159e-03
2024-10-08 22:01:10,134 WARNING [optim.py:503] Scaling gradients by 0.08888893574476242, model_norm_threshold=4458023485440.0
2024-10-08 22:01:10,273 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.524e+26, grad_sumsq=2.593e+28, orig_rms_sq=2.516e-02
2024-10-08 22:01:13,901 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=2594.0, ans=0.80921
2024-10-08 22:01:16,093 WARNING [optim.py:503] Scaling gradients by 0.01104679424315691, model_norm_threshold=4458023485440.0
2024-10-08 22:01:16,234 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.672e+28, grad_sumsq=7.766e+30, orig_rms_sq=6.017e-03
2024-10-08 22:01:18,720 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.prob, batch_count=2594.0, ans=0.37840625
2024-10-08 22:01:19,550 WARNING [optim.py:503] Scaling gradients by 0.005475841462612152, model_norm_threshold=4458023485440.0
2024-10-08 22:01:19,690 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.203e+29, grad_sumsq=2.000e+31, orig_rms_sq=6.017e-03
2024-10-08 22:01:22,464 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.53 vs. limit=8.47275
2024-10-08 22:01:24,162 WARNING [optim.py:503] Scaling gradients by 0.08238738775253296, model_norm_threshold=4458023485440.0
2024-10-08 22:01:24,302 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.325e+26, grad_sumsq=3.302e+28, orig_rms_sq=2.521e-02
2024-10-08 22:01:25,379 WARNING [optim.py:503] Scaling gradients by 0.0003824344603344798, model_norm_threshold=4458023485440.0
2024-10-08 22:01:25,520 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.305e+31, grad_sumsq=9.206e+33, orig_rms_sq=5.763e-03
2024-10-08 22:01:26,798 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=2597.3333333333335, ans=0.37825
2024-10-08 22:01:29,655 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=17.01 vs. limit=8.474
2024-10-08 22:01:33,352 WARNING [optim.py:503] Scaling gradients by 0.03486147150397301, model_norm_threshold=4458023485440.0
2024-10-08 22:01:33,491 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.503e+27, grad_sumsq=3.883e+26, orig_rms_sq=9.021e+00
2024-10-08 22:01:34,792 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.137e+10 2.203e+12 1.575e+13 1.271e+14 5.260e+16, threshold=3.150e+13, percent-clipped=62.0
2024-10-08 22:01:34,792 WARNING [optim.py:503] Scaling gradients by 0.010596220381557941, model_norm_threshold=31496213626880.0
2024-10-08 22:01:34,938 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.300e+30, grad_sumsq=5.709e+32, orig_rms_sq=5.779e-03
2024-10-08 22:01:36,004 WARNING [optim.py:503] Scaling gradients by 0.056223753839731216, model_norm_threshold=31496213626880.0
2024-10-08 22:01:36,150 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.456e+28, grad_sumsq=3.748e+30, orig_rms_sq=2.523e-02
2024-10-08 22:01:36,387 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2600.6666666666665, ans=0.2739933333333333
2024-10-08 22:01:37,420 INFO [train.py:1154] Epoch 2, batch 950, loss[loss=1.203, simple_loss=0.6175, pruned_loss=0.7061, ctc_loss=0.9429, over 4817.00 frames. ], tot_loss[loss=1.27, simple_loss=0.6434, pruned_loss=0.7396, ctc_loss=1.045, over 958786.93 frames. ], batch size: 19, lr: 3.67e-02,
2024-10-08 22:01:37,857 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.17 vs. limit=6.300333333333333
2024-10-08 22:01:44,408 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=2600.6666666666665, ans=0.37809375
2024-10-08 22:01:48,407 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=6.71 vs. limit=5.0416
2024-10-08 22:01:54,427 WARNING [optim.py:503] Scaling gradients by 0.03787095099687576, model_norm_threshold=31496213626880.0
2024-10-08 22:01:54,567 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.345e+29, grad_sumsq=4.000e+31, orig_rms_sq=5.862e-03
2024-10-08 22:01:57,028 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=2604.0, ans=0.04141
2024-10-08 22:02:09,134 WARNING [optim.py:503] Scaling gradients by 0.04696611315011978, model_norm_threshold=31496213626880.0
2024-10-08 22:02:09,274 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.327e+29, grad_sumsq=5.055e+30, orig_rms_sq=2.626e-02
2024-10-08 22:02:16,134 WARNING [optim.py:503] Scaling gradients by 0.09634186327457428, model_norm_threshold=31496213626880.0
2024-10-08 22:02:16,272 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.179e+28, grad_sumsq=9.414e+27, orig_rms_sq=3.377e+00
2024-10-08 22:02:21,829 WARNING [optim.py:503] Scaling gradients by 0.00015570114192087203, model_norm_threshold=31496213626880.0
2024-10-08 22:02:21,968 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.103e+34, grad_sumsq=4.197e+35, orig_rms_sq=2.629e-02
2024-10-08 22:02:25,495 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2614.0, ans=0.27386
2024-10-08 22:02:27,541 WARNING [optim.py:503] Scaling gradients by 0.00018370195175521076, model_norm_threshold=31496213626880.0
2024-10-08 22:02:27,677 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.526e+33, grad_sumsq=2.489e+33, orig_rms_sq=3.425e+00
2024-10-08 22:02:28,747 WARNING [optim.py:503] Scaling gradients by 0.0029254413675516844, model_norm_threshold=31496213626880.0
2024-10-08 22:02:28,885 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.312e+31, grad_sumsq=1.277e+33, orig_rms_sq=2.594e-02
2024-10-08 22:02:31,129 WARNING [optim.py:503] Scaling gradients by 0.03425728529691696, model_norm_threshold=31496213626880.0
2024-10-08 22:02:31,267 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.383e+29, grad_sumsq=9.253e+30, orig_rms_sq=2.575e-02
2024-10-08 22:02:34,552 WARNING [optim.py:503] Scaling gradients by 0.002341874875128269, model_norm_threshold=31496213626880.0
2024-10-08 22:02:34,690 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.296e+31, grad_sumsq=2.474e+33, orig_rms_sq=2.545e-02
2024-10-08 22:02:34,737 INFO [train.py:1154] Epoch 2, batch 1000, loss[loss=1.234, simple_loss=0.622, pruned_loss=0.7242, ctc_loss=0.9964, over 4946.00 frames. ], tot_loss[loss=1.271, simple_loss=0.6452, pruned_loss=0.7396, ctc_loss=1.046, over 960713.58 frames. ], batch size: 20, lr: 3.67e-02,
2024-10-08 22:02:35,381 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=22.02 vs. limit=8.4815
2024-10-08 22:02:37,592 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=20.97 vs. limit=8.4815
2024-10-08 22:02:38,276 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=2617.3333333333335, ans=0.10277499999999998
2024-10-08 22:02:40,302 WARNING [optim.py:503] Scaling gradients by 0.048505350947380066, model_norm_threshold=31496213626880.0
2024-10-08 22:02:40,443 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.245e+29, grad_sumsq=4.953e+30, orig_rms_sq=2.514e-02
2024-10-08 22:02:40,980 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=18.69 vs. limit=8.4815
2024-10-08 22:02:42,643 WARNING [optim.py:503] Scaling gradients by 0.0963507667183876, model_norm_threshold=31496213626880.0
2024-10-08 22:02:42,782 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.904e+28, grad_sumsq=5.004e+30, orig_rms_sq=5.804e-03
2024-10-08 22:02:44,940 WARNING [optim.py:503] Scaling gradients by 0.06040763109922409, model_norm_threshold=31496213626880.0
2024-10-08 22:02:45,078 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.994e+28, grad_sumsq=2.791e+30, orig_rms_sq=2.505e-02
2024-10-08 22:02:45,318 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=2620.6666666666665, ans=0.37715624999999997
2024-10-08 22:02:52,683 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=26.84 vs. limit=9.4655
2024-10-08 22:02:53,555 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.23 vs. limit=5.655166666666666
2024-10-08 22:02:54,344 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=8.687e+01
2024-10-08 22:02:55,170 WARNING [optim.py:503] Scaling gradients by 0.025625282898545265, model_norm_threshold=31496213626880.0
2024-10-08 22:02:55,311 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.118e+29, grad_sumsq=9.117e+28, orig_rms_sq=3.420e+00
2024-10-08 22:02:57,580 WARNING [optim.py:503] Scaling gradients by 0.03345371410250664, model_norm_threshold=31496213626880.0
2024-10-08 22:02:57,718 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.005e+29, grad_sumsq=8.002e+30, orig_rms_sq=2.505e-02
2024-10-08 22:03:06,827 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.37 vs. limit=9.468
2024-10-08 22:03:07,057 WARNING [optim.py:503] Scaling gradients by 0.002091814763844013, model_norm_threshold=31496213626880.0
2024-10-08 22:03:07,195 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.870e+31, grad_sumsq=2.912e+31, orig_rms_sq=3.389e+00
2024-10-08 22:03:07,439 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer2.prob, batch_count=2624.0, ans=0.377
2024-10-08 22:03:07,628 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=14.58 vs. limit=8.484
2024-10-08 22:03:08,819 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=3.97 vs. limit=5.0496
2024-10-08 22:03:09,810 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=7.78 vs. limit=5.050933333333333
2024-10-08 22:03:11,897 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2627.3333333333335, ans=0.2737266666666667
2024-10-08 22:03:15,904 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=5.44 vs. limit=5.050933333333333
2024-10-08 22:03:16,865 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=13.55 vs. limit=8.48525
2024-10-08 22:03:21,851 WARNING [optim.py:503] Scaling gradients by 0.01916044019162655, model_norm_threshold=31496213626880.0
2024-10-08 22:03:21,988 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.731e+29, grad_sumsq=2.697e+31, orig_rms_sq=2.496e-02
2024-10-08 22:03:22,432 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=42.47 vs. limit=8.4865
2024-10-08 22:03:25,436 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=2630.6666666666665, ans=0.3766875
2024-10-08 22:03:27,557 WARNING [optim.py:503] Scaling gradients by 0.031474146991968155, model_norm_threshold=31496213626880.0
2024-10-08 22:03:27,698 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.143e+29, grad_sumsq=8.783e+28, orig_rms_sq=3.578e+00
2024-10-08 22:03:28,571 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.36 vs. limit=6.315333333333333
2024-10-08 22:03:30,923 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.068e+10 1.719e+12 1.018e+13 1.412e+14 2.023e+17, threshold=2.037e+13, percent-clipped=38.0
2024-10-08 22:03:33,121 WARNING [optim.py:503] Scaling gradients by 0.05849417299032211, model_norm_threshold=20365004767232.0
2024-10-08 22:03:33,260 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.131e+28, grad_sumsq=8.722e+27, orig_rms_sq=3.590e+00
2024-10-08 22:03:33,306 INFO [train.py:1154] Epoch 2, batch 1050, loss[loss=1.306, simple_loss=0.6658, pruned_loss=0.7653, ctc_loss=1.041, over 4823.00 frames. ], tot_loss[loss=1.27, simple_loss=0.6441, pruned_loss=0.7389, ctc_loss=1.044, over 962809.92 frames. ], batch size: 25, lr: 3.66e-02,
2024-10-08 22:03:33,387 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_skip_rate, batch_count=2634.0, ans=0.101225
2024-10-08 22:03:36,200 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=21.97 vs. limit=8.48775
2024-10-08 22:03:40,035 WARNING [optim.py:503] Scaling gradients by 0.025958647951483727, model_norm_threshold=20365004767232.0
2024-10-08 22:03:40,174 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.577e+29, grad_sumsq=1.026e+31, orig_rms_sq=2.512e-02
2024-10-08 22:03:45,577 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.52 vs. limit=9.478
2024-10-08 22:03:51,055 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=15.32 vs. limit=6.318666666666667
2024-10-08 22:03:56,037 WARNING [optim.py:503] Scaling gradients by 0.03271406888961792, model_norm_threshold=20365004767232.0
2024-10-08 22:03:56,175 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.129e+29, grad_sumsq=1.853e+31, orig_rms_sq=6.094e-03
2024-10-08 22:03:56,322 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2640.6666666666665, ans=0.37621875
2024-10-08 22:04:00,492 WARNING [optim.py:503] Scaling gradients by 0.0011296962620690465, model_norm_threshold=20365004767232.0
2024-10-08 22:04:00,632 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.920e+31, grad_sumsq=3.550e+33, orig_rms_sq=2.513e-02
2024-10-08 22:04:02,373 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten.whitening_limit, batch_count=2640.6666666666665, ans=8.49025
2024-10-08 22:04:05,067 WARNING [optim.py:503] Scaling gradients by 0.06910952180624008, model_norm_threshold=20365004767232.0
2024-10-08 22:04:05,205 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.992e+28, grad_sumsq=7.921e+29, orig_rms_sq=2.515e-02
2024-10-08 22:04:05,433 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=2640.6666666666665, ans=0.08349583333333334
2024-10-08 22:04:06,478 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=2644.0, ans=0.16949999999999998
2024-10-08 22:04:10,828 WARNING [optim.py:503] Scaling gradients by 0.002350468887016177, model_norm_threshold=20365004767232.0
2024-10-08 22:04:10,969 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.720e+31, grad_sumsq=6.790e+32, orig_rms_sq=2.533e-02
2024-10-08 22:04:13,181 WARNING [optim.py:503] Scaling gradients by 0.019333282485604286, model_norm_threshold=20365004767232.0
2024-10-08 22:04:13,329 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.790e+29, grad_sumsq=7.991e+31, orig_rms_sq=5.994e-03
2024-10-08 22:04:14,405 WARNING [optim.py:503] Scaling gradients by 0.017405981197953224, model_norm_threshold=20365004767232.0
2024-10-08 22:04:14,548 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.637e+29, grad_sumsq=6.067e+31, orig_rms_sq=5.994e-03
2024-10-08 22:04:23,995 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.27 vs. limit=5.661833333333333
2024-10-08 22:04:30,271 WARNING [optim.py:503] Scaling gradients by 0.04906674101948738, model_norm_threshold=20365004767232.0
2024-10-08 22:04:30,412 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.947e+28, grad_sumsq=1.895e+30, orig_rms_sq=2.611e-02
2024-10-08 22:04:30,458 INFO [train.py:1154] Epoch 2, batch 1100, loss[loss=1.18, simple_loss=0.5903, pruned_loss=0.6918, ctc_loss=0.9671, over 4862.00 frames. ], tot_loss[loss=1.268, simple_loss=0.6428, pruned_loss=0.7384, ctc_loss=1.042, over 964135.11 frames. ], batch size: 20, lr: 3.65e-02,
2024-10-08 22:04:30,948 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=29.62 vs. limit=9.488
2024-10-08 22:04:31,472 WARNING [optim.py:503] Scaling gradients by 0.0011813122546300292, model_norm_threshold=20365004767232.0
2024-10-08 22:04:31,611 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.700e+31, grad_sumsq=2.950e+33, orig_rms_sq=2.611e-02
2024-10-08 22:04:31,777 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer2.prob, batch_count=2650.6666666666665, ans=0.37575000000000003
2024-10-08 22:04:32,323 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=14.57 vs. limit=6.325333333333333
2024-10-08 22:04:34,814 WARNING [optim.py:503] Scaling gradients by 0.0014108410105109215, model_norm_threshold=20365004767232.0
2024-10-08 22:04:34,952 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.128e+32, grad_sumsq=1.832e+34, orig_rms_sq=6.156e-03
2024-10-08 22:04:37,110 WARNING [optim.py:503] Scaling gradients by 0.027402831241488457, model_norm_threshold=20365004767232.0
2024-10-08 22:04:37,250 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.334e+29, grad_sumsq=2.167e+31, orig_rms_sq=6.156e-03
2024-10-08 22:04:37,466 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=2650.6666666666665, ans=0.10060000000000001
2024-10-08 22:04:38,010 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=15.64 vs. limit=5.6626666666666665
2024-10-08 22:04:39,443 WARNING [optim.py:503] Scaling gradients by 0.008675521239638329, model_norm_threshold=20365004767232.0
2024-10-08 22:04:39,583 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.252e+30, grad_sumsq=2.040e+32, orig_rms_sq=6.138e-03
2024-10-08 22:04:40,656 WARNING [optim.py:503] Scaling gradients by 0.012728402391076088, model_norm_threshold=20365004767232.0
2024-10-08 22:04:40,796 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.021e+29, grad_sumsq=1.144e+32, orig_rms_sq=6.138e-03
2024-10-08 22:04:41,845 WARNING [optim.py:503] Scaling gradients by 0.039330724626779556, model_norm_threshold=20365004767232.0
2024-10-08 22:04:41,984 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.334e+28, grad_sumsq=8.690e+30, orig_rms_sq=6.138e-03
2024-10-08 22:04:46,948 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=7.69 vs. limit=5.0616
2024-10-08 22:04:47,682 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.const_attention_rate, batch_count=2654.0, ans=0.10071250000000001
2024-10-08 22:04:49,699 WARNING [optim.py:503] Scaling gradients by 0.03524913638830185, model_norm_threshold=20365004767232.0
2024-10-08 22:04:49,839 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.202e+29, grad_sumsq=1.945e+31, orig_rms_sq=6.179e-03
2024-10-08 22:04:50,896 WARNING [optim.py:503] Scaling gradients by 0.05235864222049713, model_norm_threshold=20365004767232.0
2024-10-08 22:04:51,035 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.782e+28, grad_sumsq=1.058e+30, orig_rms_sq=2.630e-02
2024-10-08 22:04:52,074 WARNING [optim.py:503] Scaling gradients by 0.010948047041893005, model_norm_threshold=20365004767232.0
2024-10-08 22:04:52,210 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.74, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.562e+30, grad_sumsq=4.146e+32, orig_rms_sq=6.179e-03
2024-10-08 22:04:57,268 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.86 vs. limit=3.3986
2024-10-08 22:05:04,945 WARNING [optim.py:503] Scaling gradients by 0.03833690285682678, model_norm_threshold=20365004767232.0
2024-10-08 22:05:05,084 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.512e+28, grad_sumsq=1.372e+31, orig_rms_sq=6.204e-03
2024-10-08 22:05:07,552 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=2660.6666666666665, ans=0.04168541666666667
2024-10-08 22:05:08,843 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=12.06 vs. limit=8.49775
2024-10-08 22:05:09,502 WARNING [optim.py:503] Scaling gradients by 0.07637174427509308, model_norm_threshold=20365004767232.0
2024-10-08 22:05:09,642 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.205e+28, grad_sumsq=8.709e+29, orig_rms_sq=2.531e-02
2024-10-08 22:05:10,059 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.75 vs. limit=3.3991
2024-10-08 22:05:12,114 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=2660.6666666666665, ans=0.5
2024-10-08 22:05:13,639 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=7.69 vs. limit=6.330333333333333
2024-10-08 22:05:15,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.whiten.whitening_limit, batch_count=2664.0, ans=5.0656
2024-10-08 22:05:22,036 WARNING [optim.py:503] Scaling gradients by 0.002524601761251688, model_norm_threshold=20365004767232.0
2024-10-08 22:05:22,175 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.630e+31, grad_sumsq=6.306e+32, orig_rms_sq=2.586e-02
2024-10-08 22:05:24,729 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_True_ctc_True_attdecoder_False_streaming_True/checkpoint-8000.pt
2024-10-08 22:05:26,829 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.631e+10 3.377e+12 1.825e+13 1.160e+14 1.803e+16, threshold=3.650e+13, percent-clipped=48.0
2024-10-08 22:05:27,893 WARNING [optim.py:503] Scaling gradients by 0.025553904473781586, model_norm_threshold=36500880228352.0
2024-10-08 22:05:28,031 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.157e+29, grad_sumsq=1.987e+31, orig_rms_sq=2.596e-02
2024-10-08 22:05:28,626 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=5.19 vs. limit=3.4001
2024-10-08 22:05:29,102 WARNING [optim.py:503] Scaling gradients by 0.011512314900755882, model_norm_threshold=36500880228352.0
2024-10-08 22:05:29,241 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.61, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.160e+30, grad_sumsq=9.940e+32, orig_rms_sq=6.197e-03
2024-10-08 22:05:29,287 INFO [train.py:1154] Epoch 2, batch 1150, loss[loss=1.191, simple_loss=0.5872, pruned_loss=0.7016, ctc_loss=0.9804, over 4865.00 frames. ], tot_loss[loss=1.269, simple_loss=0.6435, pruned_loss=0.7391, ctc_loss=1.043, over 964398.55 frames. ], batch size: 20, lr: 3.65e-02,
2024-10-08 22:05:32,131 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=50.48 vs. limit=9.5005
2024-10-08 22:05:34,273 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=13.62 vs. limit=8.50025
2024-10-08 22:05:40,462 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2.whitening_limit, batch_count=2670.6666666666665, ans=6.335333333333333
2024-10-08 22:05:41,200 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.25 vs. limit=9.503
2024-10-08 22:05:47,744 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=2670.6666666666665, ans=0.03991
2024-10-08 22:05:52,707 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=15.37 vs. limit=8.50275
2024-10-08 22:05:54,501 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=29.73 vs. limit=9.5055
2024-10-08 22:06:05,195 WARNING [optim.py:503] Scaling gradients by 0.0004915607278235257, model_norm_threshold=36500880228352.0
2024-10-08 22:06:05,336 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.733e+33, grad_sumsq=2.921e+35, orig_rms_sq=5.933e-03
2024-10-08 22:06:08,466 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=8.11 vs. limit=5.669333333333333
2024-10-08 22:06:09,175 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.00 vs. limit=8.504
2024-10-08 22:06:11,310 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=2677.3333333333335, ans=0.09939999999999999
2024-10-08 22:06:12,152 WARNING [optim.py:503] Scaling gradients by 0.03216017410159111, model_norm_threshold=36500880228352.0
2024-10-08 22:06:12,291 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.767e+29, grad_sumsq=1.082e+31, orig_rms_sq=2.557e-02
2024-10-08 22:06:15,628 WARNING [optim.py:503] Scaling gradients by 0.0014006092678755522, model_norm_threshold=36500880228352.0
2024-10-08 22:06:15,769 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.769e+32, grad_sumsq=6.919e+33, orig_rms_sq=2.557e-02
2024-10-08 22:06:16,193 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.89 vs. limit=9.5105
2024-10-08 22:06:19,345 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=2680.6666666666665, ans=0.37434375
2024-10-08 22:06:20,248 WARNING [optim.py:503] Scaling gradients by 0.014950050972402096, model_norm_threshold=36500880228352.0
2024-10-08 22:06:20,388 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.869e+30, grad_sumsq=3.158e+32, orig_rms_sq=5.916e-03
2024-10-08 22:06:20,617 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_skip_rate, batch_count=2680.6666666666665, ans=0.09947500000000001
2024-10-08 22:06:21,461 WARNING [optim.py:503] Scaling gradients by 0.06538047641515732, model_norm_threshold=36500880228352.0
2024-10-08 22:06:21,600 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.109e+29, grad_sumsq=1.837e+31, orig_rms_sq=6.038e-03
2024-10-08 22:06:22,191 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.48 vs. limit=6.340333333333334
2024-10-08 22:06:23,863 WARNING [optim.py:503] Scaling gradients by 0.02438163571059704, model_norm_threshold=36500880228352.0
2024-10-08 22:06:24,003 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.865e+29, grad_sumsq=1.515e+31, orig_rms_sq=2.552e-02
2024-10-08 22:06:26,184 WARNING [optim.py:503] Scaling gradients by 0.04478984698653221, model_norm_threshold=36500880228352.0
2024-10-08 22:06:26,323 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.789e+29, grad_sumsq=4.695e+28, orig_rms_sq=3.811e+00
2024-10-08 22:06:26,369 INFO [train.py:1154] Epoch 2, batch 1200, loss[loss=1.279, simple_loss=0.6361, pruned_loss=0.7443, ctc_loss=1.084, over 4808.00 frames. ], tot_loss[loss=1.269, simple_loss=0.643, pruned_loss=0.7384, ctc_loss=1.044, over 964262.33 frames. ], batch size: 25, lr: 3.64e-02,
2024-10-08 22:06:26,523 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=2684.0, ans=0.3741875
2024-10-08 22:06:27,433 WARNING [optim.py:503] Scaling gradients by 0.014507216401398182, model_norm_threshold=36500880228352.0
2024-10-08 22:06:27,571 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.635e+30, grad_sumsq=2.670e+32, orig_rms_sq=6.124e-03
2024-10-08 22:06:28,641 WARNING [optim.py:503] Scaling gradients by 0.03131810948252678, model_norm_threshold=36500880228352.0
2024-10-08 22:06:28,779 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.122e+29, grad_sumsq=1.344e+29, orig_rms_sq=3.811e+00
2024-10-08 22:06:30,490 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=28.49 vs. limit=9.513
2024-10-08 22:06:30,927 WARNING [optim.py:503] Scaling gradients by 0.008185491897165775, model_norm_threshold=36500880228352.0
2024-10-08 22:06:31,066 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.979e+30, grad_sumsq=9.609e+32, orig_rms_sq=6.222e-03
2024-10-08 22:06:35,747 WARNING [optim.py:503] Scaling gradients by 0.08576644957065582, model_norm_threshold=36500880228352.0
2024-10-08 22:06:35,885 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.812e+28, grad_sumsq=6.202e+30, orig_rms_sq=6.146e-03
2024-10-08 22:06:37,930 WARNING [optim.py:503] Scaling gradients by 0.07122267037630081, model_norm_threshold=36500880228352.0
2024-10-08 22:06:38,070 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.889e+28, grad_sumsq=9.582e+30, orig_rms_sq=6.146e-03
2024-10-08 22:06:38,819 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.34 vs. limit=9.5155
2024-10-08 22:06:39,381 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=2687.3333333333335, ans=0.37403125000000004
2024-10-08 22:06:41,210 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=10.25 vs. limit=5.671833333333334
2024-10-08 22:06:48,696 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=10.06 vs. limit=6.3453333333333335
2024-10-08 22:06:49,355 WARNING [optim.py:503] Scaling gradients by 0.04627799615263939, model_norm_threshold=36500880228352.0
2024-10-08 22:06:49,495 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.674e+29, grad_sumsq=6.417e+30, orig_rms_sq=2.609e-02
2024-10-08 22:06:50,544 WARNING [optim.py:503] Scaling gradients by 0.0500674732029438, model_norm_threshold=36500880228352.0
2024-10-08 22:06:50,684 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.131e+28, grad_sumsq=1.516e+31, orig_rms_sq=6.024e-03
2024-10-08 22:06:51,722 WARNING [optim.py:503] Scaling gradients by 0.006592136342078447, model_norm_threshold=36500880228352.0
2024-10-08 22:06:51,860 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.945e+30, grad_sumsq=3.429e+32, orig_rms_sq=2.609e-02
2024-10-08 22:06:52,763 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.55 vs. limit=9.518
2024-10-08 22:06:53,320 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module2.balancer1.prob, batch_count=2690.6666666666665, ans=0.373875
2024-10-08 22:06:54,153 WARNING [optim.py:503] Scaling gradients by 0.05259181559085846, model_norm_threshold=36500880228352.0
2024-10-08 22:06:54,292 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.547e+29, grad_sumsq=4.391e+28, orig_rms_sq=3.523e+00
2024-10-08 22:06:54,499 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=2690.6666666666665, ans=0.03946
2024-10-08 22:06:57,606 WARNING [optim.py:503] Scaling gradients by 0.07953594624996185, model_norm_threshold=36500880228352.0
2024-10-08 22:06:57,745 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.701e+28, grad_sumsq=2.186e+28, orig_rms_sq=3.523e+00
2024-10-08 22:07:00,570 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=5.32 vs. limit=5.0776
2024-10-08 22:07:05,929 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=21.48 vs. limit=8.51025
2024-10-08 22:07:06,470 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=11.06 vs. limit=8.51025
2024-10-08 22:07:06,517 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=43.98 vs. limit=8.51025
2024-10-08 22:07:10,033 WARNING [optim.py:503] Scaling gradients by 0.015463666059076786, model_norm_threshold=36500880228352.0
2024-10-08 22:07:10,174 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.994e+30, grad_sumsq=5.899e+29, orig_rms_sq=3.380e+00
2024-10-08 22:07:12,343 WARNING [optim.py:503] Scaling gradients by 0.022686515003442764, model_norm_threshold=36500880228352.0
2024-10-08 22:07:12,482 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.041e+30, grad_sumsq=3.092e+29, orig_rms_sq=3.367e+00
2024-10-08 22:07:12,623 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer2.prob, batch_count=2697.3333333333335, ans=0.3735625
2024-10-08 22:07:20,375 WARNING [optim.py:503] Scaling gradients by 0.09979350864887238, model_norm_threshold=36500880228352.0
2024-10-08 22:07:20,514 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.57, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.611e+28, grad_sumsq=1.373e+31, orig_rms_sq=5.545e-03
2024-10-08 22:07:21,076 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.03 vs. limit=5.674333333333333
2024-10-08 22:07:21,688 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.158e+11 1.288e+12 6.427e+12 2.562e+14 7.426e+16, threshold=1.285e+13, percent-clipped=35.0
2024-10-08 22:07:24,040 INFO [train.py:1154] Epoch 2, batch 1250, loss[loss=1.256, simple_loss=0.6404, pruned_loss=0.7228, ctc_loss=1.065, over 4745.00 frames. ], tot_loss[loss=1.271, simple_loss=0.6437, pruned_loss=0.7402, ctc_loss=1.045, over 964185.52 frames. ], batch size: 32, lr: 3.64e-02,
2024-10-08 22:07:30,094 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=3.61 vs. limit=8.51275
2024-10-08 22:07:31,882 WARNING [optim.py:503] Scaling gradients by 0.057735852897167206, model_norm_threshold=12854247817216.0
2024-10-08 22:07:32,023 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.163e+28, grad_sumsq=3.805e+30, orig_rms_sq=5.684e-03
2024-10-08 22:07:36,476 WARNING [optim.py:503] Scaling gradients by 0.08228480815887451, model_norm_threshold=12854247817216.0
2024-10-08 22:07:36,615 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.030e+27, grad_sumsq=1.464e+27, orig_rms_sq=3.437e+00
2024-10-08 22:07:39,230 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=2704.0, ans=0.37324999999999997
2024-10-08 22:07:39,298 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=2704.0, ans=0.37324999999999997
2024-10-08 22:07:42,323 WARNING [optim.py:503] Scaling gradients by 0.002986092818900943, model_norm_threshold=12854247817216.0
2024-10-08 22:07:42,463 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.184e+30, grad_sumsq=1.649e+32, orig_rms_sq=2.538e-02
2024-10-08 22:07:43,255 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.93 vs. limit=5.676
2024-10-08 22:07:44,300 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=35.49 vs. limit=9.528
2024-10-08 22:07:46,991 WARNING [optim.py:503] Scaling gradients by 0.050134845077991486, model_norm_threshold=12854247817216.0
2024-10-08 22:07:47,129 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.500e+28, grad_sumsq=4.304e+27, orig_rms_sq=3.485e+00
2024-10-08 22:07:50,506 WARNING [optim.py:503] Scaling gradients by 0.07691917568445206, model_norm_threshold=12854247817216.0
2024-10-08 22:07:50,645 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.581e+27, grad_sumsq=1.872e+27, orig_rms_sq=3.516e+00
2024-10-08 22:07:51,267 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=27.46 vs. limit=9.5305
2024-10-08 22:07:51,705 WARNING [optim.py:503] Scaling gradients by 0.025440238416194916, model_norm_threshold=12854247817216.0
2024-10-08 22:07:51,845 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.412e+28, grad_sumsq=1.715e+30, orig_rms_sq=2.572e-02
2024-10-08 22:07:54,002 WARNING [optim.py:503] Scaling gradients by 7.603086123708636e-05, model_norm_threshold=12854247817216.0
2024-10-08 22:07:54,140 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.510e+33, grad_sumsq=2.902e+35, orig_rms_sq=2.587e-02
2024-10-08 22:07:55,452 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=2707.3333333333335, ans=0.37309375
2024-10-08 22:07:55,581 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=11.73 vs. limit=8.51525
2024-10-08 22:07:56,260 WARNING [optim.py:503] Scaling gradients by 0.0012623162474483252, model_norm_threshold=12854247817216.0
2024-10-08 22:07:56,400 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.636e+31, grad_sumsq=1.019e+33, orig_rms_sq=2.587e-02
2024-10-08 22:07:58,010 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=14.18 vs. limit=8.5165
2024-10-08 22:07:58,700 WARNING [optim.py:503] Scaling gradients by 0.00820485781878233, model_norm_threshold=12854247817216.0
2024-10-08 22:07:58,839 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.895e+29, grad_sumsq=2.268e+31, orig_rms_sq=2.599e-02
2024-10-08 22:08:02,098 WARNING [optim.py:503] Scaling gradients by 0.04087559133768082, model_norm_threshold=12854247817216.0
2024-10-08 22:08:02,237 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.947e+28, grad_sumsq=1.134e+30, orig_rms_sq=2.599e-02
2024-10-08 22:08:03,323 WARNING [optim.py:503] Scaling gradients by 0.06854841113090515, model_norm_threshold=12854247817216.0
2024-10-08 22:08:03,462 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.089e+28, grad_sumsq=4.174e+29, orig_rms_sq=2.610e-02
2024-10-08 22:08:05,834 WARNING [optim.py:503] Scaling gradients by 0.06635474413633347, model_norm_threshold=12854247817216.0
2024-10-08 22:08:05,975 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.243e+28, grad_sumsq=2.061e+30, orig_rms_sq=6.033e-03
2024-10-08 22:08:07,062 WARNING [optim.py:503] Scaling gradients by 0.005647622048854828, model_norm_threshold=12854247817216.0
2024-10-08 22:08:07,202 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.086e+30, grad_sumsq=1.800e+32, orig_rms_sq=6.033e-03
2024-10-08 22:08:08,000 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.38 vs. limit=9.533
2024-10-08 22:08:10,189 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.25 vs. limit=9.535499999999999
2024-10-08 22:08:12,864 WARNING [optim.py:503] Scaling gradients by 0.0794011801481247, model_norm_threshold=12854247817216.0
2024-10-08 22:08:13,005 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.508e+27, grad_sumsq=2.485e+29, orig_rms_sq=2.619e-02
2024-10-08 22:08:14,233 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.prob, batch_count=2714.0, ans=0.37278125
2024-10-08 22:08:15,426 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=2714.0, ans=0.09822499999999999
2024-10-08 22:08:15,653 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.45 vs. limit=5.6785
2024-10-08 22:08:18,097 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=32.56 vs. limit=9.535499999999999
2024-10-08 22:08:18,750 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.hidden_balancer.prob, batch_count=2714.0, ans=0.37278125
2024-10-08 22:08:22,043 INFO [train.py:1154] Epoch 2, batch 1300, loss[loss=1.277, simple_loss=0.6669, pruned_loss=0.7326, ctc_loss=1.052, over 4826.00 frames. ], tot_loss[loss=1.27, simple_loss=0.643, pruned_loss=0.7399, ctc_loss=1.045, over 965636.55 frames. ], batch size: 43, lr: 3.63e-02,
2024-10-08 22:08:22,764 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.95 vs. limit=9.538
2024-10-08 22:08:23,070 WARNING [optim.py:503] Scaling gradients by 0.010113399475812912, model_norm_threshold=12854247817216.0
2024-10-08 22:08:23,210 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.503e+29, grad_sumsq=5.789e+31, orig_rms_sq=6.051e-03
2024-10-08 22:08:30,004 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=2717.3333333333335, ans=0.09809999999999999
2024-10-08 22:08:31,580 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=15.88 vs. limit=8.519
2024-10-08 22:08:32,368 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=2720.6666666666665, ans=0.1599166666666667
2024-10-08 22:08:33,263 WARNING [optim.py:503] Scaling gradients by 0.06813141703605652, model_norm_threshold=12854247817216.0
2024-10-08 22:08:33,403 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.827e+27, grad_sumsq=2.161e+27, orig_rms_sq=3.622e+00
2024-10-08 22:08:37,790 WARNING [optim.py:503] Scaling gradients by 0.08505585044622421, model_norm_threshold=12854247817216.0
2024-10-08 22:08:37,931 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.261e+27, grad_sumsq=1.999e+29, orig_rms_sq=2.631e-02
2024-10-08 22:08:38,630 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.76 vs. limit=8.52025
2024-10-08 22:08:41,323 WARNING [optim.py:503] Scaling gradients by 0.0012515540001913905, model_norm_threshold=12854247817216.0
2024-10-08 22:08:41,463 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.831e+31, grad_sumsq=1.070e+33, orig_rms_sq=2.645e-02
2024-10-08 22:08:42,715 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=2720.6666666666665, ans=0.24081000000000002
2024-10-08 22:08:47,240 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2724.0, ans=0.3723125
2024-10-08 22:08:47,898 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.70 vs. limit=8.5215
2024-10-08 22:08:48,716 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=10.68 vs. limit=5.0896
2024-10-08 22:08:49,050 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=19.86 vs. limit=8.5215
2024-10-08 22:08:55,212 WARNING [optim.py:503] Scaling gradients by 0.08958472311496735, model_norm_threshold=12854247817216.0
2024-10-08 22:08:55,350 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.846e+27, grad_sumsq=1.813e+29, orig_rms_sq=2.673e-02
2024-10-08 22:08:55,895 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.00 vs. limit=8.52275
2024-10-08 22:08:56,432 WARNING [optim.py:503] Scaling gradients by 0.015025184489786625, model_norm_threshold=12854247817216.0
2024-10-08 22:08:56,570 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.833e+29, grad_sumsq=1.060e+31, orig_rms_sq=2.673e-02
2024-10-08 22:08:57,640 WARNING [optim.py:503] Scaling gradients by 0.02833331562578678, model_norm_threshold=12854247817216.0
2024-10-08 22:08:57,782 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.362e+28, grad_sumsq=2.006e+30, orig_rms_sq=2.673e-02
2024-10-08 22:08:59,446 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.35 vs. limit=9.5455
2024-10-08 22:09:02,003 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.18 vs. limit=5.6818333333333335
2024-10-08 22:09:02,443 WARNING [optim.py:503] Scaling gradients by 0.006084333639591932, model_norm_threshold=12854247817216.0
2024-10-08 22:09:02,587 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.218e+30, grad_sumsq=1.999e+32, orig_rms_sq=6.095e-03
2024-10-08 22:09:03,924 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=2727.3333333333335, ans=0.09658749999999997
2024-10-08 22:09:08,416 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2730.6666666666665, ans=0.27269333333333334
2024-10-08 22:09:11,721 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=2730.6666666666665, ans=0.09640000000000001
2024-10-08 22:09:14,073 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=2730.6666666666665, ans=0.372
2024-10-08 22:09:17,339 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.194e+10 1.536e+12 7.434e+12 7.633e+13 1.691e+17, threshold=1.487e+13, percent-clipped=40.0
2024-10-08 22:09:18,335 WARNING [optim.py:503] Scaling gradients by 0.024589160457253456, model_norm_threshold=14868219428864.0
2024-10-08 22:09:18,474 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.318e+28, grad_sumsq=1.699e+28, orig_rms_sq=3.718e+00
2024-10-08 22:09:19,538 WARNING [optim.py:503] Scaling gradients by 0.022285358980298042, model_norm_threshold=14868219428864.0
2024-10-08 22:09:19,677 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.935e+28, grad_sumsq=2.403e+28, orig_rms_sq=3.718e+00
2024-10-08 22:09:19,723 INFO [train.py:1154] Epoch 2, batch 1350, loss[loss=1.329, simple_loss=0.6667, pruned_loss=0.779, ctc_loss=1.081, over 4833.00 frames. ], tot_loss[loss=1.27, simple_loss=0.6434, pruned_loss=0.7401, ctc_loss=1.043, over 966306.21 frames. ], batch size: 21, lr: 3.62e-02,
2024-10-08 22:09:21,530 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=8.86 vs. limit=5.6835
2024-10-08 22:09:22,303 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=2734.0, ans=0.37184375000000003
2024-10-08 22:09:25,653 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=2734.0, ans=0.8043100000000001
2024-10-08 22:09:28,788 WARNING [optim.py:503] Scaling gradients by 0.014899329282343388, model_norm_threshold=14868219428864.0
2024-10-08 22:09:28,928 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.194e+29, grad_sumsq=9.327e+30, orig_rms_sq=2.352e-02
2024-10-08 22:09:29,424 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.94 vs. limit=9.5505
2024-10-08 22:09:32,517 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2737.3333333333335, ans=0.27262666666666663
2024-10-08 22:09:33,372 WARNING [optim.py:503] Scaling gradients by 0.05224987864494324, model_norm_threshold=14868219428864.0
2024-10-08 22:09:33,509 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.672e+28, grad_sumsq=1.011e+30, orig_rms_sq=2.642e-02
2024-10-08 22:09:35,663 WARNING [optim.py:503] Scaling gradients by 0.035212162882089615, model_norm_threshold=14868219428864.0
2024-10-08 22:09:35,800 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.713e+28, grad_sumsq=1.557e+28, orig_rms_sq=3.668e+00
2024-10-08 22:09:36,805 WARNING [optim.py:503] Scaling gradients by 0.0309665035456419, model_norm_threshold=14868219428864.0
2024-10-08 22:09:36,943 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.442e+28, grad_sumsq=2.817e+30, orig_rms_sq=2.642e-02
2024-10-08 22:09:38,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=2737.3333333333335, ans=0.08289166666666667
2024-10-08 22:09:48,047 WARNING [optim.py:503] Scaling gradients by 0.021829478442668915, model_norm_threshold=14868219428864.0
2024-10-08 22:09:48,188 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.135e+29, grad_sumsq=4.306e+30, orig_rms_sq=2.636e-02
2024-10-08 22:09:49,273 WARNING [optim.py:503] Scaling gradients by 0.0027762367390096188, model_norm_threshold=14868219428864.0
2024-10-08 22:09:49,412 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.477e+30, grad_sumsq=2.457e+32, orig_rms_sq=2.636e-02
2024-10-08 22:09:50,830 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=2740.6666666666665, ans=0.37153125
2024-10-08 22:09:53,091 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.balancer1.prob, batch_count=2744.0, ans=0.371375
2024-10-08 22:09:55,685 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=25.93 vs. limit=9.558
2024-10-08 22:09:57,347 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass_mid.scale_min, batch_count=2744.0, ans=0.80396
2024-10-08 22:09:57,796 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=60.91 vs. limit=9.558
2024-10-08 22:10:01,957 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=2744.0, ans=0.038259999999999995
2024-10-08 22:10:03,110 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.min_positive, batch_count=2744.0, ans=0.08285000000000001
2024-10-08 22:10:09,437 WARNING [optim.py:503] Scaling gradients by 0.025672616437077522, model_norm_threshold=14868219428864.0
2024-10-08 22:10:09,577 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.028e+29, grad_sumsq=3.898e+30, orig_rms_sq=2.637e-02
2024-10-08 22:10:15,143 WARNING [optim.py:503] Scaling gradients by 0.0210091732442379, model_norm_threshold=14868219428864.0
2024-10-08 22:10:15,283 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.061e+29, grad_sumsq=1.853e+31, orig_rms_sq=5.724e-03
2024-10-08 22:10:16,511 INFO [train.py:1154] Epoch 2, batch 1400, loss[loss=1.231, simple_loss=0.614, pruned_loss=0.724, ctc_loss=0.9995, over 4940.00 frames. ], tot_loss[loss=1.269, simple_loss=0.6427, pruned_loss=0.739, ctc_loss=1.042, over 966654.78 frames. ], batch size: 19, lr: 3.62e-02,
2024-10-08 22:10:17,521 WARNING [optim.py:503] Scaling gradients by 0.015536749735474586, model_norm_threshold=14868219428864.0
2024-10-08 22:10:17,661 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.588e+29, grad_sumsq=7.072e+28, orig_rms_sq=3.660e+00
2024-10-08 22:10:19,545 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=2750.6666666666665, ans=0.37106249999999996
2024-10-08 22:10:24,378 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=23.20 vs. limit=9.563
2024-10-08 22:10:24,580 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.04 vs. limit=8.5315
2024-10-08 22:10:27,050 WARNING [optim.py:503] Scaling gradients by 0.08331086486577988, model_norm_threshold=14868219428864.0
2024-10-08 22:10:27,190 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.761e+27, grad_sumsq=1.275e+30, orig_rms_sq=6.088e-03
2024-10-08 22:10:29,723 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=2754.0, ans=0.0827875
2024-10-08 22:10:31,862 WARNING [optim.py:503] Scaling gradients by 0.0033635064028203487, model_norm_threshold=14868219428864.0
2024-10-08 22:10:32,000 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.076e+30, grad_sumsq=1.056e+30, orig_rms_sq=3.859e+00
2024-10-08 22:10:33,725 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.41 vs. limit=9.5655
2024-10-08 22:10:35,321 WARNING [optim.py:503] Scaling gradients by 0.06532835215330124, model_norm_threshold=14868219428864.0
2024-10-08 22:10:35,460 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.351e+28, grad_sumsq=2.164e+30, orig_rms_sq=6.245e-03
2024-10-08 22:10:42,003 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.83 vs. limit=6.378666666666667
2024-10-08 22:10:42,339 WARNING [optim.py:503] Scaling gradients by 0.02373400330543518, model_norm_threshold=14868219428864.0
2024-10-08 22:10:42,477 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.791e+28, grad_sumsq=2.255e+28, orig_rms_sq=3.899e+00
2024-10-08 22:10:44,682 WARNING [optim.py:503] Scaling gradients by 0.006932868156582117, model_norm_threshold=14868219428864.0
2024-10-08 22:10:44,820 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.625e+29, grad_sumsq=2.212e+29, orig_rms_sq=3.899e+00
2024-10-08 22:10:45,545 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=13.46 vs. limit=8.534
2024-10-08 22:10:46,446 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=70.82 vs. limit=8.534
2024-10-08 22:10:46,984 WARNING [optim.py:503] Scaling gradients by 0.033143848180770874, model_norm_threshold=14868219428864.0
2024-10-08 22:10:47,122 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.066e+28, grad_sumsq=1.494e+30, orig_rms_sq=2.721e-02
2024-10-08 22:10:53,680 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.10 vs. limit=8.53525
2024-10-08 22:11:00,879 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=2760.6666666666665, ans=0.37059375
2024-10-08 22:11:07,587 WARNING [optim.py:503] Scaling gradients by 0.054517712444067, model_norm_threshold=14868219428864.0
2024-10-08 22:11:07,726 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.750e+28, grad_sumsq=3.079e+30, orig_rms_sq=5.682e-03
2024-10-08 22:11:08,802 WARNING [optim.py:503] Scaling gradients by 0.0026912738103419542, model_norm_threshold=14868219428864.0
2024-10-08 22:11:08,945 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.947e+30, grad_sumsq=2.265e+32, orig_rms_sq=2.626e-02
2024-10-08 22:11:12,742 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.77 vs. limit=8.5365
2024-10-08 22:11:13,440 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.399e+10 1.733e+12 1.336e+13 7.541e+13 5.525e+15, threshold=2.671e+13, percent-clipped=46.0
2024-10-08 22:11:14,498 WARNING [optim.py:503] Scaling gradients by 0.010804598219692707, model_norm_threshold=26711297097728.0
2024-10-08 22:11:14,637 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.825e+30, grad_sumsq=3.125e+32, orig_rms_sq=5.840e-03
2024-10-08 22:11:15,731 INFO [train.py:1154] Epoch 2, batch 1450, loss[loss=1.274, simple_loss=0.6581, pruned_loss=0.7358, ctc_loss=1.044, over 4795.00 frames. ], tot_loss[loss=1.266, simple_loss=0.6421, pruned_loss=0.7369, ctc_loss=1.042, over 966538.71 frames. ], batch size: 34, lr: 3.61e-02,
2024-10-08 22:11:17,749 WARNING [optim.py:503] Scaling gradients by 0.012059549801051617, model_norm_threshold=26711297097728.0
2024-10-08 22:11:17,890 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.263e+30, grad_sumsq=4.711e+31, orig_rms_sq=2.681e-02
2024-10-08 22:11:18,940 WARNING [optim.py:503] Scaling gradients by 0.014157270081341267, model_norm_threshold=26711297097728.0
2024-10-08 22:11:19,079 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.042e+30, grad_sumsq=1.752e+32, orig_rms_sq=5.948e-03
2024-10-08 22:11:22,910 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.85 vs. limit=5.691833333333333
2024-10-08 22:11:26,977 WARNING [optim.py:503] Scaling gradients by 0.01505964808166027, model_norm_threshold=26711297097728.0
2024-10-08 22:11:27,114 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.914e+29, grad_sumsq=2.880e+31, orig_rms_sq=2.748e-02
2024-10-08 22:11:29,206 WARNING [optim.py:503] Scaling gradients by 0.053856004029512405, model_norm_threshold=26711297097728.0
2024-10-08 22:11:29,345 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.494e+28, grad_sumsq=2.000e+30, orig_rms_sq=2.748e-02
2024-10-08 22:11:30,422 WARNING [optim.py:503] Scaling gradients by 0.02974669635295868, model_norm_threshold=26711297097728.0
2024-10-08 22:11:30,559 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.331e+29, grad_sumsq=3.574e+28, orig_rms_sq=3.724e+00
2024-10-08 22:11:31,729 WARNING [optim.py:503] Scaling gradients by 0.06239183619618416, model_norm_threshold=26711297097728.0
2024-10-08 22:11:31,871 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.350e+28, grad_sumsq=1.028e+31, orig_rms_sq=6.176e-03
2024-10-08 22:11:32,903 WARNING [optim.py:503] Scaling gradients by 0.013696220703423023, model_norm_threshold=26711297097728.0
2024-10-08 22:11:33,042 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.076e+30, grad_sumsq=1.742e+32, orig_rms_sq=6.176e-03
2024-10-08 22:11:38,562 WARNING [optim.py:503] Scaling gradients by 0.008923380635678768, model_norm_threshold=26711297097728.0
2024-10-08 22:11:38,703 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.892e+30, grad_sumsq=6.766e+31, orig_rms_sq=2.797e-02
2024-10-08 22:11:43,174 WARNING [optim.py:503] Scaling gradients by 0.0006031959783285856, model_norm_threshold=26711297097728.0
2024-10-08 22:11:43,314 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.322e+32, grad_sumsq=1.338e+35, orig_rms_sq=6.219e-03
2024-10-08 22:11:43,712 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.65 vs. limit=5.6935
2024-10-08 22:11:46,120 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.75 vs. limit=8.54025
2024-10-08 22:11:48,874 WARNING [optim.py:503] Scaling gradients by 0.004713431000709534, model_norm_threshold=26711297097728.0
2024-10-08 22:11:49,015 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.590e+30, grad_sumsq=2.392e+32, orig_rms_sq=2.756e-02
2024-10-08 22:11:51,214 WARNING [optim.py:503] Scaling gradients by 0.05018124729394913, model_norm_threshold=26711297097728.0
2024-10-08 22:11:51,354 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.583e+28, grad_sumsq=1.584e+31, orig_rms_sq=6.049e-03
2024-10-08 22:11:56,557 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=21.60 vs. limit=9.583
2024-10-08 22:12:01,596 WARNING [optim.py:503] Scaling gradients by 0.017145901918411255, model_norm_threshold=26711297097728.0
2024-10-08 22:12:01,736 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.266e+29, grad_sumsq=2.030e+31, orig_rms_sq=2.594e-02
2024-10-08 22:12:01,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2780.6666666666665, ans=0.27219333333333334
2024-10-08 22:12:03,964 WARNING [optim.py:503] Scaling gradients by 0.037981707602739334, model_norm_threshold=26711297097728.0
2024-10-08 22:12:04,100 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.206e+29, grad_sumsq=3.796e+31, orig_rms_sq=5.813e-03
2024-10-08 22:12:04,277 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=2780.6666666666665, ans=0.095725
2024-10-08 22:12:05,183 WARNING [optim.py:503] Scaling gradients by 0.0026393390726298094, model_norm_threshold=26711297097728.0
2024-10-08 22:12:05,325 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.553e+31, grad_sumsq=9.552e+33, orig_rms_sq=5.813e-03
2024-10-08 22:12:12,813 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.56 vs. limit=6.3919999999999995
2024-10-08 22:12:13,229 INFO [train.py:1154] Epoch 2, batch 1500, loss[loss=1.17, simple_loss=0.6014, pruned_loss=0.6726, ctc_loss=0.9822, over 4751.00 frames. ], tot_loss[loss=1.268, simple_loss=0.6427, pruned_loss=0.7374, ctc_loss=1.045, over 966209.40 frames. ], batch size: 26, lr: 3.61e-02,
2024-10-08 22:12:14,238 WARNING [optim.py:503] Scaling gradients by 0.0032366688828915358, model_norm_threshold=26711297097728.0
2024-10-08 22:12:14,377 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.868e+31, grad_sumsq=8.365e+30, orig_rms_sq=3.428e+00
2024-10-08 22:12:15,976 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.55 vs. limit=5.696
2024-10-08 22:12:18,817 WARNING [optim.py:503] Scaling gradients by 0.013525126501917839, model_norm_threshold=26711297097728.0
2024-10-08 22:12:18,955 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.828e+29, grad_sumsq=1.739e+32, orig_rms_sq=5.650e-03
2024-10-08 22:12:24,122 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=54.86 vs. limit=8.54525
2024-10-08 22:12:24,636 WARNING [optim.py:503] Scaling gradients by 0.012893539853394032, model_norm_threshold=26711297097728.0
2024-10-08 22:12:24,776 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.152e+29, grad_sumsq=1.442e+32, orig_rms_sq=5.654e-03
2024-10-08 22:12:28,334 WARNING [optim.py:503] Scaling gradients by 0.04912976920604706, model_norm_threshold=26711297097728.0
2024-10-08 22:12:28,473 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.489e+28, grad_sumsq=2.848e+30, orig_rms_sq=2.629e-02
2024-10-08 22:12:35,185 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.ff2_skip_rate, batch_count=2790.6666666666665, ans=0.03720999999999999
2024-10-08 22:12:38,235 WARNING [optim.py:503] Scaling gradients by 0.01674646884202957, model_norm_threshold=26711297097728.0
2024-10-08 22:12:38,375 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.69, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.756e+30, grad_sumsq=3.009e+32, orig_rms_sq=5.835e-03
2024-10-08 22:12:39,445 WARNING [optim.py:503] Scaling gradients by 0.09768970310688019, model_norm_threshold=26711297097728.0
2024-10-08 22:12:39,585 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.804e+28, grad_sumsq=6.519e+30, orig_rms_sq=5.835e-03
2024-10-08 22:12:41,384 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=34.92 vs. limit=8.5465
2024-10-08 22:12:41,872 WARNING [optim.py:503] Scaling gradients by 0.01632823422551155, model_norm_threshold=26711297097728.0
2024-10-08 22:12:42,011 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.776e+29, grad_sumsq=5.392e+28, orig_rms_sq=8.857e+00
2024-10-08 22:12:45,481 WARNING [optim.py:503] Scaling gradients by 0.05151311680674553, model_norm_threshold=26711297097728.0
2024-10-08 22:12:45,619 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.089e+28, grad_sumsq=3.068e+30, orig_rms_sq=2.637e-02
2024-10-08 22:12:47,099 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=31.29 vs. limit=9.5955
2024-10-08 22:12:47,716 WARNING [optim.py:503] Scaling gradients by 0.0032363086938858032, model_norm_threshold=26711297097728.0
2024-10-08 22:12:47,856 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.249e+31, grad_sumsq=3.849e+33, orig_rms_sq=5.844e-03
2024-10-08 22:12:55,667 WARNING [optim.py:503] Scaling gradients by 0.00797151681035757, model_norm_threshold=26711297097728.0
2024-10-08 22:12:55,806 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.129e+30, grad_sumsq=6.348e+29, orig_rms_sq=3.354e+00
2024-10-08 22:12:58,230 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.attention_skip_rate, batch_count=2797.3333333333335, ans=0.09509999999999999
2024-10-08 22:12:59,058 WARNING [optim.py:503] Scaling gradients by 0.049682628363370895, model_norm_threshold=26711297097728.0
2024-10-08 22:12:59,198 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.105e+28, grad_sumsq=2.348e+30, orig_rms_sq=2.600e-02
2024-10-08 22:13:00,260 WARNING [optim.py:503] Scaling gradients by 0.06107822060585022, model_norm_threshold=26711297097728.0
2024-10-08 22:13:00,401 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.989e+28, grad_sumsq=1.179e+28, orig_rms_sq=3.383e+00
2024-10-08 22:13:04,805 WARNING [optim.py:503] Scaling gradients by 0.021169422194361687, model_norm_threshold=26711297097728.0
2024-10-08 22:13:04,943 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.330e+29, grad_sumsq=1.281e+31, orig_rms_sq=2.599e-02
2024-10-08 22:13:07,118 WARNING [optim.py:503] Scaling gradients by 0.020309315994381905, model_norm_threshold=26711297097728.0
2024-10-08 22:13:07,258 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.281e+29, grad_sumsq=1.263e+31, orig_rms_sq=2.599e-02
2024-10-08 22:13:08,562 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.292e+10 4.923e+12 2.766e+13 5.185e+14 4.428e+16, threshold=5.532e+13, percent-clipped=51.0
2024-10-08 22:13:08,749 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=1.021e+05
2024-10-08 22:13:09,597 WARNING [optim.py:503] Scaling gradients by 0.07099838554859161, model_norm_threshold=55324065136640.0
2024-10-08 22:13:09,733 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.376e+29, grad_sumsq=5.290e+30, orig_rms_sq=2.602e-02
2024-10-08 22:13:10,956 INFO [train.py:1154] Epoch 2, batch 1550, loss[loss=1.332, simple_loss=0.6817, pruned_loss=0.7705, ctc_loss=1.101, over 4840.00 frames. ], tot_loss[loss=1.267, simple_loss=0.6428, pruned_loss=0.7363, ctc_loss=1.045, over 966004.73 frames. ], batch size: 31, lr: 3.60e-02,
2024-10-08 22:13:11,500 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=21.93 vs. limit=9.6005
2024-10-08 22:13:11,981 WARNING [optim.py:503] Scaling gradients by 0.0897284671664238, model_norm_threshold=55324065136640.0
2024-10-08 22:13:12,119 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.809e+28, grad_sumsq=3.386e+30, orig_rms_sq=2.602e-02
2024-10-08 22:13:12,555 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=37.95 vs. limit=8.55025
2024-10-08 22:13:23,726 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.98 vs. limit=8.5515
2024-10-08 22:13:23,801 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=30.88 vs. limit=9.603
2024-10-08 22:13:25,342 WARNING [optim.py:503] Scaling gradients by 0.0009706991841085255, model_norm_threshold=55324065136640.0
2024-10-08 22:13:25,481 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.908e+32, grad_sumsq=2.373e+32, orig_rms_sq=3.332e+00
2024-10-08 22:13:26,612 WARNING [optim.py:503] Scaling gradients by 0.024339105933904648, model_norm_threshold=55324065136640.0
2024-10-08 22:13:26,751 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.566e+30, grad_sumsq=5.996e+31, orig_rms_sq=2.611e-02
2024-10-08 22:13:26,967 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=2804.0, ans=0.3685625
2024-10-08 22:13:38,073 WARNING [optim.py:503] Scaling gradients by 0.062489476054906845, model_norm_threshold=55324065136640.0
2024-10-08 22:13:38,212 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.539e+29, grad_sumsq=5.737e+30, orig_rms_sq=2.682e-02
2024-10-08 22:13:45,850 WARNING [optim.py:503] Scaling gradients by 0.0012019703863188624, model_norm_threshold=55324065136640.0
2024-10-08 22:13:45,987 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.217e+32, grad_sumsq=1.359e+32, orig_rms_sq=3.839e+00
2024-10-08 22:13:46,519 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.whiten.whitening_limit, batch_count=2810.6666666666665, ans=8.554
2024-10-08 22:13:46,699 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten.whitening_limit, batch_count=2810.6666666666665, ans=8.554
2024-10-08 22:13:48,497 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_skip_rate, batch_count=2810.6666666666665, ans=0.0946
2024-10-08 22:13:48,681 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=16.77 vs. limit=8.554
2024-10-08 22:13:53,363 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.26 vs. limit=8.554
2024-10-08 22:13:55,773 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=13.55 vs. limit=8.555250000000001
2024-10-08 22:13:56,650 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=4.88 vs. limit=5.1256
2024-10-08 22:13:59,714 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2814.0, ans=0.27186
2024-10-08 22:14:05,098 WARNING [optim.py:503] Scaling gradients by 0.03337620198726654, model_norm_threshold=55324065136640.0
2024-10-08 22:14:05,238 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.947e+29, grad_sumsq=2.238e+31, orig_rms_sq=2.658e-02
2024-10-08 22:14:07,695 INFO [train.py:1154] Epoch 2, batch 1600, loss[loss=1.367, simple_loss=0.6937, pruned_loss=0.8009, ctc_loss=1.094, over 4799.00 frames. ], tot_loss[loss=1.269, simple_loss=0.6442, pruned_loss=0.7378, ctc_loss=1.045, over 966251.12 frames. ], batch size: 25, lr: 3.59e-02,
2024-10-08 22:14:10,936 WARNING [optim.py:503] Scaling gradients by 0.00035733188269659877, model_norm_threshold=55324065136640.0
2024-10-08 22:14:11,076 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.094e+33, grad_sumsq=2.702e+35, orig_rms_sq=2.625e-02
2024-10-08 22:14:11,285 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=2817.3333333333335, ans=0.14783333333333332
2024-10-08 22:14:15,183 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=34.74 vs. limit=8.5565
2024-10-08 22:14:16,076 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=23.72 vs. limit=8.5565
2024-10-08 22:14:17,021 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2817.3333333333335, ans=0.27182666666666666
2024-10-08 22:14:17,942 WARNING [optim.py:503] Scaling gradients by 0.0013572152238339186, model_norm_threshold=55324065136640.0
2024-10-08 22:14:18,080 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.671e+32, grad_sumsq=1.323e+35, orig_rms_sq=5.798e-03
2024-10-08 22:14:18,532 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=18.89 vs. limit=9.6155
2024-10-08 22:14:19,211 WARNING [optim.py:503] Scaling gradients by 0.07544469833374023, model_norm_threshold=55324065136640.0
2024-10-08 22:14:19,350 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.159e+29, grad_sumsq=1.999e+31, orig_rms_sq=5.798e-03
2024-10-08 22:14:24,433 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.79 vs. limit=6.410333333333333
2024-10-08 22:14:24,757 WARNING [optim.py:503] Scaling gradients by 0.023476187139749527, model_norm_threshold=55324065136640.0
2024-10-08 22:14:24,896 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.310e+30, grad_sumsq=4.961e+31, orig_rms_sq=2.641e-02
2024-10-08 22:14:25,111 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module1.balancer2.prob, batch_count=2820.6666666666665, ans=0.36778125
2024-10-08 22:14:28,166 WARNING [optim.py:503] Scaling gradients by 0.0011701727053150535, model_norm_threshold=55324065136640.0
2024-10-08 22:14:28,305 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.104e+32, grad_sumsq=2.692e+34, orig_rms_sq=2.639e-02
2024-10-08 22:14:39,430 WARNING [optim.py:503] Scaling gradients by 0.016216058284044266, model_norm_threshold=55324065136640.0
2024-10-08 22:14:39,569 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.498e+30, grad_sumsq=1.301e+32, orig_rms_sq=2.689e-02
2024-10-08 22:14:41,850 WARNING [optim.py:503] Scaling gradients by 0.0459333099424839, model_norm_threshold=55324065136640.0
2024-10-08 22:14:41,989 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.336e+29, grad_sumsq=1.612e+31, orig_rms_sq=2.689e-02
2024-10-08 22:14:42,904 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.69 vs. limit=3.4241
2024-10-08 22:14:43,056 WARNING [optim.py:503] Scaling gradients by 0.006731964647769928, model_norm_threshold=55324065136640.0
2024-10-08 22:14:43,196 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.045e+31, grad_sumsq=3.517e+33, orig_rms_sq=5.816e-03
2024-10-08 22:14:46,112 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=7.32 vs. limit=5.130933333333333
2024-10-08 22:14:48,770 WARNING [optim.py:503] Scaling gradients by 0.021829843521118164, model_norm_threshold=55324065136640.0
2024-10-08 22:14:48,908 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.624e+30, grad_sumsq=6.060e+31, orig_rms_sq=2.680e-02
2024-10-08 22:15:00,398 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=2830.6666666666665, ans=0.3673125
2024-10-08 22:15:01,219 WARNING [optim.py:503] Scaling gradients by 0.08917053788900375, model_norm_threshold=55324065136640.0
2024-10-08 22:15:01,357 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.027e+29, grad_sumsq=2.745e+28, orig_rms_sq=3.743e+00
2024-10-08 22:15:02,466 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.301e+11 6.719e+12 3.308e+13 3.517e+14 1.548e+17, threshold=6.617e+13, percent-clipped=42.0
2024-10-08 22:15:04,123 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.84 vs. limit=3.4251
2024-10-08 22:15:04,239 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=31.52 vs. limit=8.56275
2024-10-08 22:15:04,600 WARNING [optim.py:503] Scaling gradients by 0.0023997314274311066, model_norm_threshold=66165896380416.0
2024-10-08 22:15:04,739 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.034e+32, grad_sumsq=7.663e+33, orig_rms_sq=2.654e-02
2024-10-08 22:15:04,785 INFO [train.py:1154] Epoch 2, batch 1650, loss[loss=1.301, simple_loss=0.6569, pruned_loss=0.7571, ctc_loss=1.079, over 4780.00 frames. ], tot_loss[loss=1.268, simple_loss=0.6444, pruned_loss=0.7372, ctc_loss=1.043, over 966771.45 frames. ], batch size: 29, lr: 3.59e-02,
2024-10-08 22:15:05,946 WARNING [optim.py:503] Scaling gradients by 0.04278227686882019, model_norm_threshold=66165896380416.0
2024-10-08 22:15:06,084 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.357e+29, grad_sumsq=1.443e+29, orig_rms_sq=3.711e+00
2024-10-08 22:15:06,846 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=5.50 vs. limit=5.1335999999999995
2024-10-08 22:15:07,175 WARNING [optim.py:503] Scaling gradients by 0.00435312744230032, model_norm_threshold=66165896380416.0
2024-10-08 22:15:07,316 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.244e+31, grad_sumsq=1.977e+33, orig_rms_sq=2.653e-02
2024-10-08 22:15:09,497 WARNING [optim.py:503] Scaling gradients by 0.05341210588812828, model_norm_threshold=66165896380416.0
2024-10-08 22:15:09,637 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.461e+29, grad_sumsq=1.217e+29, orig_rms_sq=3.667e+00
2024-10-08 22:15:10,605 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.whiten, num_groups=1, num_channels=192, metric=4.63 vs. limit=5.1335999999999995
2024-10-08 22:15:12,959 WARNING [optim.py:503] Scaling gradients by 0.0550529807806015, model_norm_threshold=66165896380416.0
2024-10-08 22:15:13,095 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.53, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.633e+29, grad_sumsq=1.419e+32, orig_rms_sq=5.379e-03
2024-10-08 22:15:14,397 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2834.0, ans=0.27166
2024-10-08 22:15:14,888 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.25 vs. limit=8.56275
2024-10-08 22:15:20,148 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=2837.3333333333335, ans=0.8006933333333334
2024-10-08 22:15:21,807 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.58 vs. limit=5.709333333333333
2024-10-08 22:15:23,519 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=2837.3333333333335, ans=0.367
2024-10-08 22:15:24,411 WARNING [optim.py:503] Scaling gradients by 0.013617420569062233, model_norm_threshold=66165896380416.0
2024-10-08 22:15:24,550 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.103e+30, grad_sumsq=3.384e+32, orig_rms_sq=2.690e-02
2024-10-08 22:15:32,371 WARNING [optim.py:503] Scaling gradients by 0.022674867883324623, model_norm_threshold=66165896380416.0
2024-10-08 22:15:32,509 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.700e+30, grad_sumsq=4.905e+32, orig_rms_sq=5.505e-03
2024-10-08 22:15:32,706 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2840.6666666666665, ans=0.27159333333333335
2024-10-08 22:15:34,960 WARNING [optim.py:503] Scaling gradients by 0.04747467115521431, model_norm_threshold=66165896380416.0
2024-10-08 22:15:35,099 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.094e+29, grad_sumsq=9.243e+31, orig_rms_sq=5.511e-03
2024-10-08 22:15:37,385 WARNING [optim.py:503] Scaling gradients by 0.05474497005343437, model_norm_threshold=66165896380416.0
2024-10-08 22:15:37,525 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.234e+29, grad_sumsq=7.682e+31, orig_rms_sq=5.511e-03
2024-10-08 22:15:41,945 WARNING [optim.py:503] Scaling gradients by 0.0943637415766716, model_norm_threshold=66165896380416.0
2024-10-08 22:15:42,086 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.833e+29, grad_sumsq=3.261e+31, orig_rms_sq=5.621e-03
2024-10-08 22:15:42,681 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=37.20 vs. limit=8.5665
2024-10-08 22:15:43,179 WARNING [optim.py:503] Scaling gradients by 0.05693710967898369, model_norm_threshold=66165896380416.0
2024-10-08 22:15:43,321 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.596e+29, grad_sumsq=1.370e+31, orig_rms_sq=2.625e-02
2024-10-08 22:15:45,862 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=2844.0, ans=0.3666875
2024-10-08 22:15:46,950 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=2844.0, ans=0.090025
2024-10-08 22:15:48,471 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=64.22 vs. limit=8.5665
2024-10-08 22:15:51,270 WARNING [optim.py:503] Scaling gradients by 0.009185468778014183, model_norm_threshold=66165896380416.0
2024-10-08 22:15:51,410 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.223e+31, grad_sumsq=4.671e+32, orig_rms_sq=2.619e-02
2024-10-08 22:15:52,478 WARNING [optim.py:503] Scaling gradients by 0.08008426427841187, model_norm_threshold=66165896380416.0
2024-10-08 22:15:52,619 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.746e+29, grad_sumsq=3.026e+31, orig_rms_sq=5.772e-03
2024-10-08 22:15:56,852 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.90 vs. limit=6.423666666666667
2024-10-08 22:15:59,867 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.56 vs. limit=5.711833333333334
2024-10-08 22:16:02,314 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.09 vs. limit=9.638
2024-10-08 22:16:02,757 INFO [train.py:1154] Epoch 2, batch 1700, loss[loss=1.25, simple_loss=0.6138, pruned_loss=0.739, ctc_loss=1.022, over 4940.00 frames. ], tot_loss[loss=1.268, simple_loss=0.6428, pruned_loss=0.7378, ctc_loss=1.043, over 966817.97 frames. ], batch size: 19, lr: 3.58e-02,
2024-10-08 22:16:03,094 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.62 vs. limit=9.638
2024-10-08 22:16:03,923 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=2850.6666666666665, ans=0.08965000000000001
2024-10-08 22:16:08,405 WARNING [optim.py:503] Scaling gradients by 0.010456586256623268, model_norm_threshold=66165896380416.0
2024-10-08 22:16:08,543 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.665e+31, grad_sumsq=6.242e+32, orig_rms_sq=2.667e-02
2024-10-08 22:16:09,189 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.51 vs. limit=5.712666666666666
2024-10-08 22:16:13,019 WARNING [optim.py:503] Scaling gradients by 0.09370147436857224, model_norm_threshold=66165896380416.0
2024-10-08 22:16:13,156 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.529e+29, grad_sumsq=5.740e+30, orig_rms_sq=2.664e-02
2024-10-08 22:16:19,885 WARNING [optim.py:503] Scaling gradients by 0.0007520996150560677, model_norm_threshold=66165896380416.0
2024-10-08 22:16:20,024 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.613e+33, grad_sumsq=6.088e+34, orig_rms_sq=2.649e-02
2024-10-08 22:16:21,602 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.27 vs. limit=8.57025
2024-10-08 22:16:23,750 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=2854.0, ans=0.5
2024-10-08 22:16:25,751 WARNING [optim.py:503] Scaling gradients by 0.06890276074409485, model_norm_threshold=66165896380416.0
2024-10-08 22:16:25,889 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.533e+29, grad_sumsq=9.607e+30, orig_rms_sq=2.636e-02
2024-10-08 22:16:28,577 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.41 vs. limit=5.714333333333333
2024-10-08 22:16:33,681 WARNING [optim.py:503] Scaling gradients by 0.029371390119194984, model_norm_threshold=66165896380416.0
2024-10-08 22:16:33,820 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.703e+29, grad_sumsq=3.656e+31, orig_rms_sq=2.654e-02
2024-10-08 22:16:35,616 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.79 vs. limit=3.4286
2024-10-08 22:16:35,975 WARNING [optim.py:503] Scaling gradients by 0.022352293133735657, model_norm_threshold=66165896380416.0
2024-10-08 22:16:36,115 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.650e+30, grad_sumsq=9.977e+31, orig_rms_sq=2.656e-02
2024-10-08 22:16:38,973 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=10.52 vs. limit=5.715166666666667
2024-10-08 22:16:38,982 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=19.79 vs. limit=8.57275
2024-10-08 22:16:52,766 WARNING [optim.py:503] Scaling gradients by 0.0008730997215025127, model_norm_threshold=66165896380416.0
2024-10-08 22:16:52,906 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.742e+33, grad_sumsq=6.740e+34, orig_rms_sq=2.585e-02
2024-10-08 22:16:55,699 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.81 vs. limit=3.4295999999999998
2024-10-08 22:16:57,731 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.306e+10 2.899e+12 2.748e+13 3.145e+14 8.797e+16, threshold=5.497e+13, percent-clipped=40.0
2024-10-08 22:16:57,732 WARNING [optim.py:503] Scaling gradients by 0.0008081876439973712, model_norm_threshold=54968358797312.0
2024-10-08 22:16:57,872 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.303e+33, grad_sumsq=8.925e+34, orig_rms_sq=2.580e-02
2024-10-08 22:17:00,101 WARNING [optim.py:503] Scaling gradients by 0.01286538690328598, model_norm_threshold=54968358797312.0
2024-10-08 22:17:00,239 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.045e+30, grad_sumsq=1.914e+30, orig_rms_sq=1.591e+00
2024-10-08 22:17:00,284 INFO [train.py:1154] Epoch 2, batch 1750, loss[loss=1.211, simple_loss=0.6037, pruned_loss=0.7111, ctc_loss=0.989, over 4959.00 frames. ], tot_loss[loss=1.267, simple_loss=0.6419, pruned_loss=0.7372, ctc_loss=1.042, over 967075.83 frames. ], batch size: 19, lr: 3.58e-02,
2024-10-08 22:17:03,827 WARNING [optim.py:503] Scaling gradients by 0.0363420695066452, model_norm_threshold=54968358797312.0
2024-10-08 22:17:03,966 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.511e+29, grad_sumsq=1.745e+31, orig_rms_sq=2.585e-02
2024-10-08 22:17:04,568 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=30.91 vs. limit=9.650500000000001
2024-10-08 22:17:11,705 WARNING [optim.py:503] Scaling gradients by 0.008120025508105755, model_norm_threshold=54968358797312.0
2024-10-08 22:17:11,844 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.616e+30, grad_sumsq=4.156e+32, orig_rms_sq=2.073e-02
2024-10-08 22:17:13,101 WARNING [optim.py:503] Scaling gradients by 0.038232121616601944, model_norm_threshold=54968358797312.0
2024-10-08 22:17:13,240 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.210e+29, grad_sumsq=8.909e+28, orig_rms_sq=3.603e+00
2024-10-08 22:17:14,308 WARNING [optim.py:503] Scaling gradients by 0.06843647360801697, model_norm_threshold=54968358797312.0
2024-10-08 22:17:14,447 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.904e+29, grad_sumsq=5.175e+31, orig_rms_sq=5.612e-03
2024-10-08 22:17:24,704 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.balancer1.prob, batch_count=2874.0, ans=0.36528125
2024-10-08 22:17:25,853 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.balancer2.prob, batch_count=2874.0, ans=0.36528125
2024-10-08 22:17:26,687 WARNING [optim.py:503] Scaling gradients by 0.006698617246001959, model_norm_threshold=54968358797312.0
2024-10-08 22:17:26,825 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.364e+31, grad_sumsq=9.442e+32, orig_rms_sq=2.503e-02
2024-10-08 22:17:27,864 WARNING [optim.py:503] Scaling gradients by 0.01917484775185585, model_norm_threshold=54968358797312.0
2024-10-08 22:17:28,004 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.078e+30, grad_sumsq=2.320e+29, orig_rms_sq=8.957e+00
2024-10-08 22:17:28,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=2874.0, ans=0.14075
2024-10-08 22:17:30,197 WARNING [optim.py:503] Scaling gradients by 0.020434560254216194, model_norm_threshold=54968358797312.0
2024-10-08 22:17:30,335 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.588e+30, grad_sumsq=6.344e+31, orig_rms_sq=2.503e-02
2024-10-08 22:17:32,494 WARNING [optim.py:503] Scaling gradients by 0.0006343908607959747, model_norm_threshold=54968358797312.0
2024-10-08 22:17:32,633 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.159e+33, grad_sumsq=3.709e+35, orig_rms_sq=5.820e-03
2024-10-08 22:17:38,435 WARNING [optim.py:503] Scaling gradients by 0.00014445220585912466, model_norm_threshold=54968358797312.0
2024-10-08 22:17:38,575 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.588e+34, grad_sumsq=1.036e+36, orig_rms_sq=2.499e-02
2024-10-08 22:17:39,214 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=56.63 vs. limit=8.579
2024-10-08 22:17:40,241 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=30.32 vs. limit=9.658
2024-10-08 22:17:42,161 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2877.3333333333335, ans=0.36512500000000003
2024-10-08 22:17:48,401 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2.whitening_limit, batch_count=2880.6666666666665, ans=6.440333333333333
2024-10-08 22:17:49,000 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=2880.6666666666665, ans=0.36496875
2024-10-08 22:17:51,805 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=52.55 vs. limit=8.58025
2024-10-08 22:17:53,377 WARNING [optim.py:503] Scaling gradients by 0.024257564917206764, model_norm_threshold=54968358797312.0
2024-10-08 22:17:53,515 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.013e+29, grad_sumsq=3.186e+31, orig_rms_sq=2.515e-02
2024-10-08 22:17:54,005 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.87 vs. limit=3.4321
2024-10-08 22:17:54,149 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=21.76 vs. limit=9.660499999999999
2024-10-08 22:17:55,420 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=19.83 vs. limit=8.58025
2024-10-08 22:17:57,989 WARNING [optim.py:503] Scaling gradients by 0.0499022975564003, model_norm_threshold=54968358797312.0
2024-10-08 22:17:58,130 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.196e+29, grad_sumsq=8.743e+30, orig_rms_sq=2.511e-02
2024-10-08 22:17:59,335 INFO [train.py:1154] Epoch 2, batch 1800, loss[loss=1.28, simple_loss=0.6395, pruned_loss=0.7459, ctc_loss=1.07, over 4842.00 frames. ], tot_loss[loss=1.27, simple_loss=0.644, pruned_loss=0.7388, ctc_loss=1.044, over 967791.06 frames. ], batch size: 23, lr: 3.57e-02,
2024-10-08 22:18:03,769 WARNING [optim.py:503] Scaling gradients by 0.029857372865080833, model_norm_threshold=54968358797312.0
2024-10-08 22:18:03,907 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.833e+29, grad_sumsq=3.101e+31, orig_rms_sq=2.526e-02
2024-10-08 22:18:04,994 WARNING [optim.py:503] Scaling gradients by 0.0025801798328757286, model_norm_threshold=54968358797312.0
2024-10-08 22:18:05,133 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.978e+31, grad_sumsq=3.950e+33, orig_rms_sq=2.526e-02
2024-10-08 22:18:07,310 WARNING [optim.py:503] Scaling gradients by 0.008390967734158039, model_norm_threshold=54968358797312.0
2024-10-08 22:18:07,449 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.449e+31, grad_sumsq=5.735e+32, orig_rms_sq=2.526e-02
2024-10-08 22:18:12,932 WARNING [optim.py:503] Scaling gradients by 0.03529324382543564, model_norm_threshold=54968358797312.0
2024-10-08 22:18:13,071 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.475e+29, grad_sumsq=1.211e+32, orig_rms_sq=6.172e-03
2024-10-08 22:18:14,124 WARNING [optim.py:503] Scaling gradients by 0.06397940218448639, model_norm_threshold=54968358797312.0
2024-10-08 22:18:14,264 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.035e+29, grad_sumsq=7.905e+30, orig_rms_sq=2.574e-02
2024-10-08 22:18:17,985 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=2887.3333333333335, ans=0.2711266666666666
2024-10-08 22:18:21,072 WARNING [optim.py:503] Scaling gradients by 0.0016792949754744768, model_norm_threshold=54968358797312.0
2024-10-08 22:18:21,211 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.927e+32, grad_sumsq=7.592e+31, orig_rms_sq=3.856e+00
2024-10-08 22:18:21,444 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=2890.6666666666665, ans=0.7988266666666667
2024-10-08 22:18:22,279 WARNING [optim.py:503] Scaling gradients by 9.500652959104627e-05, model_norm_threshold=54968358797312.0
2024-10-08 22:18:22,420 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.339e+34, grad_sumsq=2.429e+34, orig_rms_sq=3.844e+00
2024-10-08 22:18:23,582 WARNING [optim.py:503] Scaling gradients by 0.06846990436315536, model_norm_threshold=54968358797312.0
2024-10-08 22:18:23,723 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.156e+29, grad_sumsq=7.091e+28, orig_rms_sq=1.630e+00
2024-10-08 22:18:28,762 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=22.36 vs. limit=9.668
2024-10-08 22:18:30,986 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=12.25 vs. limit=6.445333333333333
2024-10-08 22:18:36,192 WARNING [optim.py:503] Scaling gradients by 4.383415580377914e-05, model_norm_threshold=54968358797312.0
2024-10-08 22:18:36,332 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.55, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.701e+35, grad_sumsq=3.376e+37, orig_rms_sq=2.577e-02
2024-10-08 22:18:40,708 WARNING [optim.py:503] Scaling gradients by 0.00170026789419353, model_norm_threshold=54968358797312.0
2024-10-08 22:18:40,845 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.524e+32, grad_sumsq=9.549e+31, orig_rms_sq=3.690e+00
2024-10-08 22:18:41,634 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=2.82 vs. limit=8.58525
2024-10-08 22:18:47,851 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer1.prob, batch_count=2897.3333333333335, ans=0.3641875
2024-10-08 22:18:48,737 WARNING [optim.py:503] Scaling gradients by 0.03856241703033447, model_norm_threshold=54968358797312.0
2024-10-08 22:18:48,875 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.583e+29, grad_sumsq=7.685e+31, orig_rms_sq=5.964e-03
2024-10-08 22:18:50,251 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2897.3333333333335, ans=0.27102666666666664
2024-10-08 22:18:53,141 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.55 vs. limit=6.448666666666667
2024-10-08 22:18:54,033 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=7.63 vs. limit=8.586500000000001
2024-10-08 22:18:54,809 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.573e+11 8.418e+12 5.393e+13 5.417e+14 1.254e+18, threshold=1.079e+14, percent-clipped=49.0
2024-10-08 22:18:54,810 WARNING [optim.py:503] Scaling gradients by 0.05653788894414902, model_norm_threshold=107857265557504.0
2024-10-08 22:18:54,948 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.593e+30, grad_sumsq=4.300e+29, orig_rms_sq=3.706e+00
2024-10-08 22:18:55,555 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=7.06 vs. limit=5.158933333333334
2024-10-08 22:18:57,196 INFO [train.py:1154] Epoch 2, batch 1850, loss[loss=1.352, simple_loss=0.6837, pruned_loss=0.7853, ctc_loss=1.124, over 4736.00 frames. ], tot_loss[loss=1.272, simple_loss=0.6447, pruned_loss=0.74, ctc_loss=1.046, over 967990.35 frames. ], batch size: 26, lr: 3.57e-02,
2024-10-08 22:18:58,224 WARNING [optim.py:503] Scaling gradients by 0.0012967790244147182, model_norm_threshold=107857265557504.0
2024-10-08 22:18:58,363 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.270e+33, grad_sumsq=8.824e+32, orig_rms_sq=3.706e+00
2024-10-08 22:19:04,954 WARNING [optim.py:503] Scaling gradients by 0.05666153505444527, model_norm_threshold=107857265557504.0
2024-10-08 22:19:05,094 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.871e+29, grad_sumsq=4.152e+31, orig_rms_sq=2.377e-02
2024-10-08 22:19:06,141 WARNING [optim.py:503] Scaling gradients by 0.054665036499500275, model_norm_threshold=107857265557504.0
2024-10-08 22:19:06,278 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.542e+29, grad_sumsq=9.636e+28, orig_rms_sq=8.865e+00
2024-10-08 22:19:06,438 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=2900.6666666666665, ans=0.36403125000000003
2024-10-08 22:19:07,616 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward3.hidden_balancer.prob, batch_count=2904.0, ans=0.363875
2024-10-08 22:19:08,525 WARNING [optim.py:503] Scaling gradients by 0.05514809489250183, model_norm_threshold=107857265557504.0
2024-10-08 22:19:08,664 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.014e+30, grad_sumsq=1.677e+32, orig_rms_sq=6.047e-03
2024-10-08 22:19:11,542 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=35.63 vs. limit=8.589
2024-10-08 22:19:12,747 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=24.87 vs. limit=9.678
2024-10-08 22:19:13,157 WARNING [optim.py:503] Scaling gradients by 0.011509167030453682, model_norm_threshold=107857265557504.0
2024-10-08 22:19:13,298 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.923e+31, grad_sumsq=3.159e+33, orig_rms_sq=6.087e-03
2024-10-08 22:19:22,158 WARNING [optim.py:503] Scaling gradients by 0.0028035121504217386, model_norm_threshold=107857265557504.0
2024-10-08 22:19:22,296 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.681e+32, grad_sumsq=2.755e+34, orig_rms_sq=2.425e-02
2024-10-08 22:19:25,068 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=5.19 vs. limit=5.162933333333333
2024-10-08 22:19:29,950 WARNING [optim.py:503] Scaling gradients by 0.09323874115943909, model_norm_threshold=107857265557504.0
2024-10-08 22:19:30,089 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.524e+29, grad_sumsq=9.348e+28, orig_rms_sq=3.769e+00
2024-10-08 22:19:30,977 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.17 vs. limit=8.5915
2024-10-08 22:19:33,693 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=2910.6666666666665, ans=0.3635625
2024-10-08 22:19:33,726 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=2910.6666666666665, ans=0.7791066666666666
2024-10-08 22:19:34,412 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=5.50 vs. limit=5.727666666666667
2024-10-08 22:19:38,172 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2910.6666666666665, ans=0.2708933333333333
2024-10-08 22:19:44,344 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=10.03 vs. limit=5.7285
2024-10-08 22:19:51,542 WARNING [optim.py:503] Scaling gradients by 0.007130485959351063, model_norm_threshold=107857265557504.0
2024-10-08 22:19:51,680 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.032e+31, grad_sumsq=4.832e+30, orig_rms_sq=8.343e+00
2024-10-08 22:19:53,882 INFO [train.py:1154] Epoch 2, batch 1900, loss[loss=1.228, simple_loss=0.6184, pruned_loss=0.71, ctc_loss=1.043, over 4788.00 frames. ], tot_loss[loss=1.267, simple_loss=0.6425, pruned_loss=0.7366, ctc_loss=1.045, over 967833.03 frames. ], batch size: 29, lr: 3.56e-02,
2024-10-08 22:19:54,573 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=11.72 vs. limit=8.594
2024-10-08 22:19:56,167 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=2917.3333333333335, ans=0.36324999999999996
2024-10-08 22:19:58,938 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.27 vs. limit=9.688
2024-10-08 22:19:59,803 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.56 vs. limit=8.594
2024-10-08 22:20:03,724 WARNING [optim.py:503] Scaling gradients by 0.01497188862413168, model_norm_threshold=107857265557504.0
2024-10-08 22:20:03,862 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.574e+31, grad_sumsq=6.836e+32, orig_rms_sq=2.302e-02
2024-10-08 22:20:09,375 WARNING [optim.py:503] Scaling gradients by 0.02324257604777813, model_norm_threshold=107857265557504.0
2024-10-08 22:20:09,515 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.264e+30, grad_sumsq=1.213e+30, orig_rms_sq=3.516e+00
2024-10-08 22:20:11,682 WARNING [optim.py:503] Scaling gradients by 0.0006966726505197585, model_norm_threshold=107857265557504.0
2024-10-08 22:20:11,821 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.131e+33, grad_sumsq=6.275e+32, orig_rms_sq=8.177e+00
2024-10-08 22:20:12,882 WARNING [optim.py:503] Scaling gradients by 0.0004232880019117147, model_norm_threshold=107857265557504.0
2024-10-08 22:20:13,022 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.502e+34, grad_sumsq=6.574e+35, orig_rms_sq=2.285e-02
2024-10-08 22:20:15,203 WARNING [optim.py:503] Scaling gradients by 0.026046961545944214, model_norm_threshold=107857265557504.0
2024-10-08 22:20:15,343 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.889e+30, grad_sumsq=6.985e+32, orig_rms_sq=5.568e-03
2024-10-08 22:20:16,082 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=16.77 vs. limit=8.5965
2024-10-08 22:20:18,620 WARNING [optim.py:503] Scaling gradients by 0.0025278599932789803, model_norm_threshold=107857265557504.0
2024-10-08 22:20:18,761 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.807e+32, grad_sumsq=2.541e+34, orig_rms_sq=2.285e-02
2024-10-08 22:20:19,829 WARNING [optim.py:503] Scaling gradients by 0.0012534265406429768, model_norm_threshold=107857265557504.0
2024-10-08 22:20:19,970 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.756e+33, grad_sumsq=7.684e+34, orig_rms_sq=2.285e-02
2024-10-08 22:20:22,119 WARNING [optim.py:503] Scaling gradients by 0.04462219402194023, model_norm_threshold=107857265557504.0
2024-10-08 22:20:22,259 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.012e+30, grad_sumsq=4.440e+31, orig_rms_sq=2.280e-02
2024-10-08 22:20:24,473 WARNING [optim.py:503] Scaling gradients by 0.023121759295463562, model_norm_threshold=107857265557504.0
2024-10-08 22:20:24,612 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.678e+30, grad_sumsq=2.052e+32, orig_rms_sq=2.280e-02
2024-10-08 22:20:26,320 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.10 vs. limit=9.693
2024-10-08 22:20:32,603 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=2927.3333333333335, ans=0.36278125
2024-10-08 22:20:35,879 WARNING [optim.py:503] Scaling gradients by 0.06652636080980301, model_norm_threshold=107857265557504.0
2024-10-08 22:20:36,016 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.689e+29, grad_sumsq=5.606e+28, orig_rms_sq=8.365e+00
2024-10-08 22:20:37,109 WARNING [optim.py:503] Scaling gradients by 0.06088792905211449, model_norm_threshold=107857265557504.0
2024-10-08 22:20:37,250 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.165e+30, grad_sumsq=3.157e+29, orig_rms_sq=3.689e+00
2024-10-08 22:20:40,475 WARNING [optim.py:503] Scaling gradients by 0.0019500929629430175, model_norm_threshold=107857265557504.0
2024-10-08 22:20:40,617 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.666e+32, grad_sumsq=2.108e+32, orig_rms_sq=3.636e+00
2024-10-08 22:20:42,069 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=16.44 vs. limit=6.465333333333334
2024-10-08 22:20:42,670 WARNING [optim.py:503] Scaling gradients by 0.0018027168698608875, model_norm_threshold=107857265557504.0
2024-10-08 22:20:42,808 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.718e+32, grad_sumsq=3.318e+34, orig_rms_sq=2.326e-02
2024-10-08 22:20:43,893 WARNING [optim.py:503] Scaling gradients by 0.017995426431298256, model_norm_threshold=107857265557504.0
2024-10-08 22:20:44,032 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.353e+30, grad_sumsq=2.046e+30, orig_rms_sq=3.593e+00
2024-10-08 22:20:45,088 WARNING [optim.py:503] Scaling gradients by 0.030140677466988564, model_norm_threshold=107857265557504.0
2024-10-08 22:20:45,226 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.284e+30, grad_sumsq=8.132e+32, orig_rms_sq=5.267e-03
2024-10-08 22:20:48,759 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.664e+11 3.218e+12 4.313e+13 9.592e+14 2.548e+17, threshold=8.626e+13, percent-clipped=43.0
2024-10-08 22:20:49,821 WARNING [optim.py:503] Scaling gradients by 0.023989081382751465, model_norm_threshold=86261331132416.0
2024-10-08 22:20:49,960 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.307e+30, grad_sumsq=2.693e+29, orig_rms_sq=8.568e+00
2024-10-08 22:20:51,161 INFO [train.py:1154] Epoch 2, batch 1950, loss[loss=1.387, simple_loss=0.6826, pruned_loss=0.8207, ctc_loss=1.123, over 4866.00 frames. ], tot_loss[loss=1.27, simple_loss=0.6444, pruned_loss=0.7381, ctc_loss=1.047, over 966712.50 frames. ], batch size: 20, lr: 3.55e-02,
2024-10-08 22:20:51,262 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=2934.0, ans=0.7973100000000001
2024-10-08 22:20:52,157 WARNING [optim.py:503] Scaling gradients by 0.003468422219157219, model_norm_threshold=86261331132416.0
2024-10-08 22:20:52,297 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.908e+32, grad_sumsq=5.648e+34, orig_rms_sq=5.148e-03
2024-10-08 22:20:55,897 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=2934.0, ans=0.13324999999999998
2024-10-08 22:20:56,765 WARNING [optim.py:503] Scaling gradients by 0.058786459267139435, model_norm_threshold=86261331132416.0
2024-10-08 22:20:56,904 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.922e+29, grad_sumsq=9.609e+31, orig_rms_sq=5.122e-03
2024-10-08 22:20:58,215 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.hidden_balancer.prob, batch_count=2934.0, ans=0.36246875
2024-10-08 22:20:59,381 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2934.0, ans=0.27066
2024-10-08 22:21:00,232 WARNING [optim.py:503] Scaling gradients by 0.08247146755456924, model_norm_threshold=86261331132416.0
2024-10-08 22:21:00,369 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.444e+29, grad_sumsq=8.676e+31, orig_rms_sq=5.122e-03
2024-10-08 22:21:01,489 WARNING [optim.py:503] Scaling gradients by 0.03888941556215286, model_norm_threshold=86261331132416.0
2024-10-08 22:21:01,628 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.090e+30, grad_sumsq=4.840e+31, orig_rms_sq=2.252e-02
2024-10-08 22:21:02,727 WARNING [optim.py:503] Scaling gradients by 0.010325629264116287, model_norm_threshold=86261331132416.0
2024-10-08 22:21:02,866 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.597e+31, grad_sumsq=4.544e+30, orig_rms_sq=3.516e+00
2024-10-08 22:21:03,894 WARNING [optim.py:503] Scaling gradients by 0.008408060297369957, model_norm_threshold=86261331132416.0
2024-10-08 22:21:04,033 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.829e+31, grad_sumsq=7.434e+33, orig_rms_sq=5.151e-03
2024-10-08 22:21:04,262 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=2937.3333333333335, ans=0.7971933333333334
2024-10-08 22:21:06,494 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.bypass.scale_min, batch_count=2937.3333333333335, ans=0.7971933333333334
2024-10-08 22:21:12,300 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2.whitening_limit, batch_count=2937.3333333333335, ans=6.468666666666667
2024-10-08 22:21:12,815 WARNING [optim.py:503] Scaling gradients by 0.04440060630440712, model_norm_threshold=86261331132416.0
2024-10-08 22:21:12,956 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.108e+29, grad_sumsq=4.093e+31, orig_rms_sq=2.225e-02
2024-10-08 22:21:22,405 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.26 vs. limit=6.4703333333333335
2024-10-08 22:21:23,406 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.48 vs. limit=8.60275
2024-10-08 22:21:23,831 WARNING [optim.py:503] Scaling gradients by 0.01579286716878414, model_norm_threshold=86261331132416.0
2024-10-08 22:21:23,975 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.583e+30, grad_sumsq=6.515e+29, orig_rms_sq=8.569e+00
2024-10-08 22:21:24,438 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=19.19 vs. limit=9.708
2024-10-08 22:21:25,263 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer2.prob, batch_count=2944.0, ans=0.362
2024-10-08 22:21:26,079 WARNING [optim.py:503] Scaling gradients by 0.08215048909187317, model_norm_threshold=86261331132416.0
2024-10-08 22:21:26,216 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.755e+29, grad_sumsq=1.262e+31, orig_rms_sq=2.183e-02
2024-10-08 22:21:28,406 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=9.10 vs. limit=8.604
2024-10-08 22:21:35,336 WARNING [optim.py:503] Scaling gradients by 0.031693872064352036, model_norm_threshold=86261331132416.0
2024-10-08 22:21:35,476 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.974e+30, grad_sumsq=8.984e+31, orig_rms_sq=2.198e-02
2024-10-08 22:21:36,547 WARNING [optim.py:503] Scaling gradients by 0.0013824183261021972, model_norm_threshold=86261331132416.0
2024-10-08 22:21:36,686 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.827e+32, grad_sumsq=4.472e+34, orig_rms_sq=2.198e-02
2024-10-08 22:21:38,349 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=512, metric=4.17 vs. limit=8.60525
2024-10-08 22:21:38,818 WARNING [optim.py:503] Scaling gradients by 0.005432359874248505, model_norm_threshold=86261331132416.0
2024-10-08 22:21:38,956 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.810e+31, grad_sumsq=3.525e+33, orig_rms_sq=2.216e-02
2024-10-08 22:21:39,994 WARNING [optim.py:503] Scaling gradients by 0.0016286172904074192, model_norm_threshold=86261331132416.0
2024-10-08 22:21:40,133 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.506e+32, grad_sumsq=1.759e+32, orig_rms_sq=3.699e+00
2024-10-08 22:21:42,336 WARNING [optim.py:503] Scaling gradients by 0.056132908910512924, model_norm_threshold=86261331132416.0
2024-10-08 22:21:42,473 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.366e+29, grad_sumsq=9.431e+31, orig_rms_sq=5.690e-03
2024-10-08 22:21:46,118 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=2947.3333333333335, ans=0.08157916666666667
2024-10-08 22:21:48,188 INFO [train.py:1154] Epoch 2, batch 2000, loss[loss=1.268, simple_loss=0.6293, pruned_loss=0.7467, ctc_loss=1.034, over 4959.00 frames. ], tot_loss[loss=1.271, simple_loss=0.6452, pruned_loss=0.7385, ctc_loss=1.049, over 966453.06 frames. ], batch size: 19, lr: 3.55e-02,
2024-10-08 22:21:49,176 WARNING [optim.py:503] Scaling gradients by 0.052002567797899246, model_norm_threshold=86261331132416.0
2024-10-08 22:21:49,315 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.293e+29, grad_sumsq=2.158e+31, orig_rms_sq=1.990e-02
2024-10-08 22:21:50,527 WARNING [optim.py:503] Scaling gradients by 0.07675080746412277, model_norm_threshold=86261331132416.0
2024-10-08 22:21:50,666 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.892e+29, grad_sumsq=8.560e+30, orig_rms_sq=2.211e-02
2024-10-08 22:21:54,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=2950.6666666666665, ans=0.3616875
2024-10-08 22:21:54,521 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.63 vs. limit=9.713000000000001
2024-10-08 22:21:58,325 WARNING [optim.py:503] Scaling gradients by 0.03991853818297386, model_norm_threshold=86261331132416.0
2024-10-08 22:21:58,464 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.350e+30, grad_sumsq=2.430e+32, orig_rms_sq=5.556e-03
2024-10-08 22:22:03,741 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.94 vs. limit=9.7155
2024-10-08 22:22:05,428 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=2954.0, ans=0.36153124999999997
2024-10-08 22:22:05,984 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=8.07 vs. limit=6.477
2024-10-08 22:22:09,123 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=8.65 vs. limit=5.1815999999999995
2024-10-08 22:22:09,528 WARNING [optim.py:503] Scaling gradients by 0.003798944875597954, model_norm_threshold=86261331132416.0
2024-10-08 22:22:09,670 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.178e+32, grad_sumsq=5.493e+33, orig_rms_sq=2.145e-02
2024-10-08 22:22:15,564 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2957.3333333333335, ans=0.27042666666666665
2024-10-08 22:22:17,470 WARNING [optim.py:503] Scaling gradients by 0.023770244792103767, model_norm_threshold=86261331132416.0
2024-10-08 22:22:17,610 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.038e+30, grad_sumsq=1.149e+30, orig_rms_sq=3.513e+00
2024-10-08 22:22:18,622 WARNING [optim.py:503] Scaling gradients by 0.00045608653454110026, model_norm_threshold=86261331132416.0
2024-10-08 22:22:18,761 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.561e+34, grad_sumsq=2.932e+36, orig_rms_sq=5.324e-03
2024-10-08 22:22:19,829 WARNING [optim.py:503] Scaling gradients by 0.0009591425769031048, model_norm_threshold=86261331132416.0
2024-10-08 22:22:19,968 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.761e+33, grad_sumsq=8.312e+34, orig_rms_sq=2.119e-02
2024-10-08 22:22:20,094 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff2_skip_rate, batch_count=2957.3333333333335, ans=0.033460000000000004
2024-10-08 22:22:24,409 WARNING [optim.py:503] Scaling gradients by 0.002897496335208416, model_norm_threshold=86261331132416.0
2024-10-08 22:22:24,547 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.445e+32, grad_sumsq=6.949e+31, orig_rms_sq=3.519e+00
2024-10-08 22:22:25,603 WARNING [optim.py:503] Scaling gradients by 0.025226708501577377, model_norm_threshold=86261331132416.0
2024-10-08 22:22:25,742 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.534e+30, grad_sumsq=6.530e+32, orig_rms_sq=5.411e-03
2024-10-08 22:22:28,776 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=6.51 vs. limit=5.184266666666667
2024-10-08 22:22:30,153 WARNING [optim.py:503] Scaling gradients by 0.017878178507089615, model_norm_threshold=86261331132416.0
2024-10-08 22:22:30,293 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.130e+30, grad_sumsq=4.578e+29, orig_rms_sq=9.022e+00
2024-10-08 22:22:32,464 WARNING [optim.py:503] Scaling gradients by 0.031019486486911774, model_norm_threshold=86261331132416.0
2024-10-08 22:22:32,604 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.839e+30, grad_sumsq=8.620e+31, orig_rms_sq=2.133e-02
2024-10-08 22:22:34,920 WARNING [optim.py:503] Scaling gradients by 0.034521833062171936, model_norm_threshold=86261331132416.0
2024-10-08 22:22:35,060 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.286e+30, grad_sumsq=5.989e+31, orig_rms_sq=2.147e-02
2024-10-08 22:22:38,794 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=3.431e+03
2024-10-08 22:22:41,399 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.67 vs. limit=6.482
2024-10-08 22:22:42,801 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.901e+11 6.762e+12 7.478e+13 1.124e+15 1.891e+17, threshold=1.496e+14, percent-clipped=46.0
2024-10-08 22:22:43,935 WARNING [optim.py:503] Scaling gradients by 0.05941261351108551, model_norm_threshold=149567404244992.0
2024-10-08 22:22:44,072 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.453e+30, grad_sumsq=4.140e+32, orig_rms_sq=5.925e-03
2024-10-08 22:22:45,326 INFO [train.py:1154] Epoch 2, batch 2050, loss[loss=1.237, simple_loss=0.6243, pruned_loss=0.7219, ctc_loss=1.013, over 4914.00 frames. ], tot_loss[loss=1.269, simple_loss=0.6445, pruned_loss=0.7372, ctc_loss=1.046, over 966856.73 frames. ], batch size: 19, lr: 3.54e-02,
2024-10-08 22:22:49,799 WARNING [optim.py:503] Scaling gradients by 0.048736073076725006, model_norm_threshold=149567404244992.0
2024-10-08 22:22:49,938 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.184e+30, grad_sumsq=1.006e+32, orig_rms_sq=2.171e-02
2024-10-08 22:22:53,281 WARNING [optim.py:503] Scaling gradients by 0.05526774004101753, model_norm_threshold=149567404244992.0
2024-10-08 22:22:53,417 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.120e+30, grad_sumsq=2.389e+29, orig_rms_sq=8.875e+00
2024-10-08 22:23:01,660 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.min_positive, batch_count=2970.6666666666665, ans=0.04071666666666667
2024-10-08 22:23:03,695 WARNING [optim.py:503] Scaling gradients by 0.006148306652903557, model_norm_threshold=149567404244992.0
2024-10-08 22:23:03,833 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.205e+32, grad_sumsq=5.615e+33, orig_rms_sq=2.145e-02
2024-10-08 22:23:05,961 WARNING [optim.py:503] Scaling gradients by 0.06977779418230057, model_norm_threshold=149567404244992.0
2024-10-08 22:23:06,100 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.967e+30, grad_sumsq=3.437e+32, orig_rms_sq=5.722e-03
2024-10-08 22:23:06,487 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=14.20 vs. limit=8.614
2024-10-08 22:23:13,419 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=52.04 vs. limit=8.61525
2024-10-08 22:23:13,950 WARNING [optim.py:503] Scaling gradients by 0.056257087737321854, model_norm_threshold=149567404244992.0
2024-10-08 22:23:14,088 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.438e+30, grad_sumsq=1.138e+32, orig_rms_sq=2.141e-02
2024-10-08 22:23:14,596 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.34 vs. limit=8.61525
2024-10-08 22:23:15,164 WARNING [optim.py:503] Scaling gradients by 0.006079033017158508, model_norm_threshold=149567404244992.0
2024-10-08 22:23:15,304 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.397e+32, grad_sumsq=6.520e+33, orig_rms_sq=2.143e-02
2024-10-08 22:23:16,827 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten.whitening_limit, batch_count=2974.0, ans=8.61525
2024-10-08 22:23:16,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=29.49 vs. limit=9.7305
2024-10-08 22:23:18,535 WARNING [optim.py:503] Scaling gradients by 0.00020621174189727753, model_norm_threshold=149567404244992.0
2024-10-08 22:23:18,673 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.278e+35, grad_sumsq=3.441e+34, orig_rms_sq=3.715e+00
2024-10-08 22:23:21,216 WARNING [optim.py:503] Scaling gradients by 6.875671533634886e-05, model_norm_threshold=149567404244992.0
2024-10-08 22:23:21,353 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.680e+36, grad_sumsq=3.051e+38, orig_rms_sq=5.506e-03
2024-10-08 22:23:22,420 WARNING [optim.py:503] Scaling gradients by 0.017533553764224052, model_norm_threshold=149567404244992.0
2024-10-08 22:23:22,557 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.370e+31, grad_sumsq=1.640e+30, orig_rms_sq=8.356e+00
2024-10-08 22:23:23,050 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=15.27 vs. limit=8.6165
2024-10-08 22:23:23,182 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=64.54 vs. limit=9.733
2024-10-08 22:23:34,864 WARNING [optim.py:503] Scaling gradients by 0.04385549947619438, model_norm_threshold=149567404244992.0
2024-10-08 22:23:35,003 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.552e+30, grad_sumsq=4.482e+32, orig_rms_sq=5.695e-03
2024-10-08 22:23:37,171 WARNING [optim.py:503] Scaling gradients by 0.01809852570295334, model_norm_threshold=149567404244992.0
2024-10-08 22:23:37,311 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.14, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.699e+30, grad_sumsq=4.653e+32, orig_rms_sq=2.084e-02
2024-10-08 22:23:39,133 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=43.39 vs. limit=8.617750000000001
2024-10-08 22:23:42,870 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.53 vs. limit=9.7355
2024-10-08 22:23:43,567 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.18 vs. limit=8.619
2024-10-08 22:23:43,819 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=29.69 vs. limit=9.738
2024-10-08 22:23:43,883 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=20.71 vs. limit=9.738
2024-10-08 22:23:44,363 INFO [train.py:1154] Epoch 2, batch 2100, loss[loss=1.328, simple_loss=0.6569, pruned_loss=0.7793, ctc_loss=1.101, over 4842.00 frames. ], tot_loss[loss=1.267, simple_loss=0.6432, pruned_loss=0.7367, ctc_loss=1.045, over 967073.81 frames. ], batch size: 21, lr: 3.54e-02,
2024-10-08 22:23:56,635 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=2987.3333333333335, ans=0.12658333333333333
2024-10-08 22:23:57,558 WARNING [optim.py:503] Scaling gradients by 0.00807207077741623, model_norm_threshold=149567404244992.0
2024-10-08 22:23:57,697 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.217e+32, grad_sumsq=2.169e+34, orig_rms_sq=5.612e-03
2024-10-08 22:24:01,003 WARNING [optim.py:503] Scaling gradients by 0.09486974030733109, model_norm_threshold=149567404244992.0
2024-10-08 22:24:01,141 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.599e+29, grad_sumsq=1.839e+29, orig_rms_sq=3.589e+00
2024-10-08 22:24:02,571 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=1.377e-01
2024-10-08 22:24:04,624 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2987.3333333333335, ans=0.2701266666666666
2024-10-08 22:24:05,051 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=32.14 vs. limit=9.7405
2024-10-08 22:24:08,783 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2.whitening_limit, batch_count=2990.6666666666665, ans=6.495333333333333
2024-10-08 22:24:13,022 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2.whitening_limit, batch_count=2990.6666666666665, ans=6.495333333333333
2024-10-08 22:24:13,546 WARNING [optim.py:503] Scaling gradients by 0.0734686404466629, model_norm_threshold=149567404244992.0
2024-10-08 22:24:13,686 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.214e+30, grad_sumsq=6.099e+31, orig_rms_sq=1.991e-02
2024-10-08 22:24:16,228 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=2990.6666666666665, ans=0.1261666666666667
2024-10-08 22:24:16,254 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.out_combiner.scale_min, batch_count=2990.6666666666665, ans=0.7953266666666667
2024-10-08 22:24:17,327 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=2994.0, ans=0.27005999999999997
2024-10-08 22:24:18,070 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=7.58 vs. limit=8.62275
2024-10-08 22:24:22,426 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=12.87 vs. limit=8.62275
2024-10-08 22:24:24,236 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.08 vs. limit=9.7455
2024-10-08 22:24:24,758 WARNING [optim.py:503] Scaling gradients by 0.03232549875974655, model_norm_threshold=149567404244992.0
2024-10-08 22:24:24,897 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.388e+30, grad_sumsq=2.714e+32, orig_rms_sq=1.985e-02
2024-10-08 22:24:29,380 WARNING [optim.py:503] Scaling gradients by 0.013372508808970451, model_norm_threshold=149567404244992.0
2024-10-08 22:24:29,518 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.417e+31, grad_sumsq=2.935e+30, orig_rms_sq=8.236e+00
2024-10-08 22:24:30,861 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=2997.3333333333335, ans=0.27002666666666664
2024-10-08 22:24:31,719 WARNING [optim.py:503] Scaling gradients by 0.0061608427204191685, model_norm_threshold=149567404244992.0
2024-10-08 22:24:31,857 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.128e+32, grad_sumsq=2.020e+34, orig_rms_sq=5.583e-03
2024-10-08 22:24:32,880 WARNING [optim.py:503] Scaling gradients by 0.0026544383727014065, model_norm_threshold=149567404244992.0
2024-10-08 22:24:33,018 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.157e+32, grad_sumsq=3.104e+34, orig_rms_sq=1.984e-02
2024-10-08 22:24:35,898 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=26.09 vs. limit=9.748000000000001
2024-10-08 22:24:37,941 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=2997.3333333333335, ans=0.7950933333333333
2024-10-08 22:24:38,878 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.259e+11 2.501e+13 1.730e+14 8.242e+14 2.175e+18, threshold=3.459e+14, percent-clipped=52.0
2024-10-08 22:24:40,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten.whitening_limit, batch_count=3000.6666666666665, ans=8.62525
2024-10-08 22:24:41,155 INFO [train.py:1154] Epoch 2, batch 2150, loss[loss=1.289, simple_loss=0.6425, pruned_loss=0.758, ctc_loss=1.049, over 4869.00 frames. ], tot_loss[loss=1.266, simple_loss=0.642, pruned_loss=0.7366, ctc_loss=1.042, over 967927.80 frames. ], batch size: 20, lr: 3.53e-02,
2024-10-08 22:24:43,425 WARNING [optim.py:503] Scaling gradients by 0.009706013835966587, model_norm_threshold=345914719862784.0
2024-10-08 22:24:43,563 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.808e+32, grad_sumsq=8.161e+34, orig_rms_sq=5.891e-03
2024-10-08 22:24:46,686 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.93 vs. limit=5.750166666666667
2024-10-08 22:24:51,724 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass_mid.scale_min, batch_count=3004.0, ans=0.79486
2024-10-08 22:24:54,703 WARNING [optim.py:503] Scaling gradients by 0.020423218607902527, model_norm_threshold=345914719862784.0
2024-10-08 22:24:54,842 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.483e+31, grad_sumsq=4.814e+33, orig_rms_sq=1.970e-02
2024-10-08 22:24:57,802 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=8.86 vs. limit=6.502
2024-10-08 22:24:59,686 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=3004.0, ans=0.08735
2024-10-08 22:25:00,843 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff3_skip_rate, batch_count=3004.0, ans=0.03241000000000001
2024-10-08 22:25:06,870 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=28.55 vs. limit=9.7555
2024-10-08 22:25:10,134 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=47.81 vs. limit=8.62775
2024-10-08 22:25:13,809 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=43.55 vs. limit=8.62775
2024-10-08 22:25:14,429 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.const_attention_rate, batch_count=3010.6666666666665, ans=0.08065
2024-10-08 22:25:17,953 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer1.prob, batch_count=3010.6666666666665, ans=0.358875
2024-10-08 22:25:21,934 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=19.69 vs. limit=9.758
2024-10-08 22:25:32,424 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3014.0, ans=0.26986
2024-10-08 22:25:37,459 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.77 vs. limit=9.763
2024-10-08 22:25:37,896 INFO [train.py:1154] Epoch 2, batch 2200, loss[loss=1.161, simple_loss=0.5943, pruned_loss=0.6708, ctc_loss=0.9647, over 4746.00 frames. ], tot_loss[loss=1.263, simple_loss=0.6404, pruned_loss=0.7345, ctc_loss=1.04, over 967667.89 frames. ], batch size: 26, lr: 3.52e-02,
2024-10-08 22:25:42,177 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.nonlin_attention.whiten1.whitening_limit, batch_count=3017.3333333333335, ans=5.754333333333333
2024-10-08 22:25:42,560 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass_mid.scale_min, batch_count=3017.3333333333335, ans=0.7943933333333334
2024-10-08 22:25:46,224 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=24.73 vs. limit=9.763
2024-10-08 22:25:46,805 WARNING [optim.py:503] Scaling gradients by 0.02257324755191803, model_norm_threshold=345914719862784.0
2024-10-08 22:25:46,943 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.381e+31, grad_sumsq=4.944e+30, orig_rms_sq=8.862e+00
2024-10-08 22:25:52,147 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=15.01 vs. limit=8.63275
2024-10-08 22:25:53,249 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.39 vs. limit=3.4531
2024-10-08 22:25:57,179 WARNING [optim.py:503] Scaling gradients by 0.025223588570952415, model_norm_threshold=345914719862784.0
2024-10-08 22:25:57,321 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.852e+31, grad_sumsq=1.888e+31, orig_rms_sq=3.629e+00
2024-10-08 22:25:58,391 WARNING [optim.py:503] Scaling gradients by 0.058348845690488815, model_norm_threshold=345914719862784.0
2024-10-08 22:25:58,534 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.641e+30, grad_sumsq=2.106e+30, orig_rms_sq=3.629e+00
2024-10-08 22:26:00,638 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.21 vs. limit=5.756
2024-10-08 22:26:03,210 WARNING [optim.py:503] Scaling gradients by 0.027595551684498787, model_norm_threshold=345914719862784.0
2024-10-08 22:26:03,350 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.014e+31, grad_sumsq=6.935e+33, orig_rms_sq=5.788e-03
2024-10-08 22:26:04,203 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.79 vs. limit=9.768
2024-10-08 22:26:05,572 WARNING [optim.py:503] Scaling gradients by 0.036958903074264526, model_norm_threshold=345914719862784.0
2024-10-08 22:26:05,711 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.647e+31, grad_sumsq=8.532e+32, orig_rms_sq=1.930e-02
2024-10-08 22:26:06,958 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=3024.0, ans=0.35825
2024-10-08 22:26:12,281 WARNING [optim.py:503] Scaling gradients by 0.0018616901943460107, model_norm_threshold=345914719862784.0
2024-10-08 22:26:12,418 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.473e+33, grad_sumsq=3.796e+35, orig_rms_sq=1.969e-02
2024-10-08 22:26:13,766 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=3027.3333333333335, ans=0.5
2024-10-08 22:26:16,874 WARNING [optim.py:503] Scaling gradients by 0.010495579801499844, model_norm_threshold=345914719862784.0
2024-10-08 22:26:17,013 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.456e+32, grad_sumsq=1.213e+34, orig_rms_sq=2.024e-02
2024-10-08 22:26:24,574 WARNING [optim.py:503] Scaling gradients by 0.06383944302797318, model_norm_threshold=345914719862784.0
2024-10-08 22:26:24,712 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.133e+31, grad_sumsq=5.495e+32, orig_rms_sq=2.062e-02
2024-10-08 22:26:25,770 WARNING [optim.py:503] Scaling gradients by 0.04251965135335922, model_norm_threshold=345914719862784.0
2024-10-08 22:26:25,909 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.861e+31, grad_sumsq=2.913e+33, orig_rms_sq=6.389e-03
2024-10-08 22:26:28,096 WARNING [optim.py:503] Scaling gradients by 0.004099096171557903, model_norm_threshold=345914719862784.0
2024-10-08 22:26:28,235 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.388e+33, grad_sumsq=3.395e+32, orig_rms_sq=4.090e+00
2024-10-08 22:26:28,366 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3030.6666666666665, ans=0.26969333333333334
2024-10-08 22:26:31,894 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=13.55 vs. limit=6.515333333333333
2024-10-08 22:26:32,615 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.644e+11 3.745e+12 2.484e+13 3.073e+14 1.858e+17, threshold=4.968e+13, percent-clipped=23.0
2024-10-08 22:26:33,660 WARNING [optim.py:503] Scaling gradients by 0.015816200524568558, model_norm_threshold=49678762639360.0
2024-10-08 22:26:33,797 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.752e+30, grad_sumsq=4.542e+32, orig_rms_sq=6.058e-03
2024-10-08 22:26:34,889 WARNING [optim.py:503] Scaling gradients by 0.03753792122006416, model_norm_threshold=49678762639360.0
2024-10-08 22:26:35,027 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.492e+29, grad_sumsq=1.718e+31, orig_rms_sq=2.032e-02
2024-10-08 22:26:35,074 INFO [train.py:1154] Epoch 2, batch 2250, loss[loss=1.295, simple_loss=0.6489, pruned_loss=0.7456, ctc_loss=1.122, over 4876.00 frames. ], tot_loss[loss=1.263, simple_loss=0.6405, pruned_loss=0.7349, ctc_loss=1.041, over 967733.55 frames. ], batch size: 22, lr: 3.52e-02,
2024-10-08 22:26:35,606 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.00 vs. limit=9.775500000000001
2024-10-08 22:26:35,865 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2.whitening_limit, batch_count=3034.0, ans=6.5169999999999995
2024-10-08 22:26:36,724 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=19.90 vs. limit=8.63775
2024-10-08 22:26:39,004 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys.whitening_limit, batch_count=3034.0, ans=3.4551
2024-10-08 22:26:39,380 WARNING [optim.py:503] Scaling gradients by 1.266832714463817e-05, model_norm_threshold=49678762639360.0
2024-10-08 22:26:39,520 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.457e+36, grad_sumsq=1.684e+38, orig_rms_sq=2.053e-02
2024-10-08 22:26:39,644 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=3034.0, ans=0.79381
2024-10-08 22:26:41,746 WARNING [optim.py:503] Scaling gradients by 0.09390614181756973, model_norm_threshold=49678762639360.0
2024-10-08 22:26:41,885 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.107e+29, grad_sumsq=1.960e+31, orig_rms_sq=5.648e-03
2024-10-08 22:26:42,066 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=3034.0, ans=6.89625
2024-10-08 22:26:52,140 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.2.self_attn_weights, loss-sum=1.889e+03
2024-10-08 22:26:55,086 WARNING [optim.py:503] Scaling gradients by 0.0026175477541983128, model_norm_threshold=49678762639360.0
2024-10-08 22:26:55,224 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.016e+31, grad_sumsq=7.846e+30, orig_rms_sq=8.941e+00
2024-10-08 22:26:58,709 WARNING [optim.py:503] Scaling gradients by 0.08707670122385025, model_norm_threshold=49678762639360.0
2024-10-08 22:26:58,850 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.565e+28, grad_sumsq=4.134e+28, orig_rms_sq=1.346e+00
2024-10-08 22:27:00,648 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=33.29 vs. limit=8.64025
2024-10-08 22:27:04,681 WARNING [optim.py:503] Scaling gradients by 0.06500223278999329, model_norm_threshold=49678762639360.0
2024-10-08 22:27:04,820 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.298e+29, grad_sumsq=4.287e+31, orig_rms_sq=5.360e-03
2024-10-08 22:27:09,298 WARNING [optim.py:503] Scaling gradients by 0.09048587083816528, model_norm_threshold=49678762639360.0
2024-10-08 22:27:09,436 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.221e+28, grad_sumsq=8.033e+27, orig_rms_sq=8.990e+00
2024-10-08 22:27:10,625 WARNING [optim.py:503] Scaling gradients by 0.022723009809851646, model_norm_threshold=49678762639360.0
2024-10-08 22:27:10,763 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.706e+30, grad_sumsq=8.651e+31, orig_rms_sq=1.972e-02
2024-10-08 22:27:12,022 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3044.0, ans=0.26955999999999997
2024-10-08 22:27:15,322 WARNING [optim.py:503] Scaling gradients by 0.011754997074604034, model_norm_threshold=49678762639360.0
2024-10-08 22:27:15,461 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.059e+30, grad_sumsq=1.117e+33, orig_rms_sq=5.423e-03
2024-10-08 22:27:18,301 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.63 vs. limit=8.6415
2024-10-08 22:27:20,882 WARNING [optim.py:503] Scaling gradients by 0.004531981889158487, model_norm_threshold=49678762639360.0
2024-10-08 22:27:21,022 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.214e+31, grad_sumsq=2.470e+30, orig_rms_sq=8.965e+00
2024-10-08 22:27:21,687 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.32 vs. limit=8.64275
2024-10-08 22:27:23,396 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.min_abs, batch_count=3047.3333333333335, ans=0.24571
2024-10-08 22:27:24,289 WARNING [optim.py:503] Scaling gradients by 0.0033823344856500626, model_norm_threshold=49678762639360.0
2024-10-08 22:27:24,427 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.145e+31, grad_sumsq=3.581e+33, orig_rms_sq=1.995e-02
2024-10-08 22:27:24,550 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=3047.3333333333335, ans=0.35715625
2024-10-08 22:27:25,222 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.18 vs. limit=9.785499999999999
2024-10-08 22:27:28,993 WARNING [optim.py:503] Scaling gradients by 0.012314862571656704, model_norm_threshold=49678762639360.0
2024-10-08 22:27:29,131 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.724e+30, grad_sumsq=1.032e+33, orig_rms_sq=5.549e-03
2024-10-08 22:27:32,523 INFO [train.py:1154] Epoch 2, batch 2300, loss[loss=1.212, simple_loss=0.6055, pruned_loss=0.7103, ctc_loss=0.9934, over 4883.00 frames. ], tot_loss[loss=1.261, simple_loss=0.639, pruned_loss=0.7338, ctc_loss=1.039, over 968263.98 frames. ], batch size: 19, lr: 3.51e-02,
2024-10-08 22:27:33,572 WARNING [optim.py:503] Scaling gradients by 0.023206934332847595, model_norm_threshold=49678762639360.0
2024-10-08 22:27:33,712 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.454e+29, grad_sumsq=4.201e+31, orig_rms_sq=2.013e-02
2024-10-08 22:27:36,040 WARNING [optim.py:503] Scaling gradients by 0.07952880859375, model_norm_threshold=49678762639360.0
2024-10-08 22:27:36,178 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.093e+28, grad_sumsq=1.448e+31, orig_rms_sq=5.591e-03
2024-10-08 22:27:36,617 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=12.88 vs. limit=8.644
2024-10-08 22:27:40,304 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.whiten.whitening_limit, batch_count=3050.6666666666665, ans=5.220266666666666
2024-10-08 22:27:43,621 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.77 vs. limit=6.527
2024-10-08 22:27:44,204 WARNING [optim.py:503] Scaling gradients by 0.00026929532759822905, model_norm_threshold=49678762639360.0
2024-10-08 22:27:44,344 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.242e+33, grad_sumsq=1.106e+36, orig_rms_sq=5.642e-03
2024-10-08 22:27:46,539 WARNING [optim.py:503] Scaling gradients by 0.05331501364707947, model_norm_threshold=49678762639360.0
2024-10-08 22:27:46,677 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.203e+29, grad_sumsq=1.089e+31, orig_rms_sq=2.024e-02
2024-10-08 22:27:47,312 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=21.08 vs. limit=9.7905
2024-10-08 22:27:47,341 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=29.90 vs. limit=9.7905
2024-10-08 22:27:47,791 WARNING [optim.py:503] Scaling gradients by 0.028999367728829384, model_norm_threshold=49678762639360.0
2024-10-08 22:27:47,929 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.917e+29, grad_sumsq=1.401e+32, orig_rms_sq=5.651e-03
2024-10-08 22:27:49,102 WARNING [optim.py:503] Scaling gradients by 0.018171923235058784, model_norm_threshold=49678762639360.0
2024-10-08 22:27:49,244 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.080e+30, grad_sumsq=3.680e+32, orig_rms_sq=5.651e-03
2024-10-08 22:27:51,146 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=34.59 vs. limit=9.7905
2024-10-08 22:27:52,048 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=11.43 vs. limit=8.64525
2024-10-08 22:27:57,366 WARNING [optim.py:503] Scaling gradients by 0.029502587392926216, model_norm_threshold=49678762639360.0
2024-10-08 22:27:57,504 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.101e+29, grad_sumsq=1.243e+32, orig_rms_sq=5.712e-03
2024-10-08 22:27:58,523 WARNING [optim.py:503] Scaling gradients by 0.0025057855527848005, model_norm_threshold=49678762639360.0
2024-10-08 22:27:58,662 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.324e+31, grad_sumsq=1.457e+34, orig_rms_sq=5.712e-03
2024-10-08 22:28:00,874 WARNING [optim.py:503] Scaling gradients by 0.0674879252910614, model_norm_threshold=49678762639360.0
2024-10-08 22:28:01,012 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.392e+29, grad_sumsq=2.432e+31, orig_rms_sq=5.724e-03
2024-10-08 22:28:04,571 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module2.balancer2.prob, batch_count=3057.3333333333335, ans=0.3566875
2024-10-08 22:28:05,391 WARNING [optim.py:503] Scaling gradients by 0.02867012284696102, model_norm_threshold=49678762639360.0
2024-10-08 22:28:05,530 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.649e+29, grad_sumsq=2.763e+31, orig_rms_sq=2.044e-02
2024-10-08 22:28:08,427 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=25.31 vs. limit=9.7955
2024-10-08 22:28:08,903 WARNING [optim.py:503] Scaling gradients by 0.02668995037674904, model_norm_threshold=49678762639360.0
2024-10-08 22:28:09,044 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.56, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.932e+30, grad_sumsq=3.362e+32, orig_rms_sq=5.746e-03
2024-10-08 22:28:09,750 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.88 vs. limit=9.7955
2024-10-08 22:28:12,234 WARNING [optim.py:503] Scaling gradients by 0.032240886241197586, model_norm_threshold=49678762639360.0
2024-10-08 22:28:12,372 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.076e+29, grad_sumsq=1.054e+32, orig_rms_sq=5.766e-03
2024-10-08 22:28:13,121 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.80 vs. limit=9.7955
2024-10-08 22:28:15,820 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.min_abs, batch_count=3060.6666666666665, ans=0.24591000000000002
2024-10-08 22:28:16,294 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.54 vs. limit=6.530333333333333
2024-10-08 22:28:17,743 WARNING [optim.py:503] Scaling gradients by 0.023455284535884857, model_norm_threshold=49678762639360.0
2024-10-08 22:28:17,881 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.259e+29, grad_sumsq=8.085e+28, orig_rms_sq=8.978e+00
2024-10-08 22:28:18,924 WARNING [optim.py:503] Scaling gradients by 0.060066238045692444, model_norm_threshold=49678762639360.0
2024-10-08 22:28:19,060 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.395e+29, grad_sumsq=6.763e+30, orig_rms_sq=2.063e-02
2024-10-08 22:28:20,110 WARNING [optim.py:503] Scaling gradients by 0.0022252039052546024, model_norm_threshold=49678762639360.0
2024-10-08 22:28:20,248 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.061e+32, grad_sumsq=1.851e+34, orig_rms_sq=5.731e-03
2024-10-08 22:28:25,999 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=3064.0, ans=0.79276
2024-10-08 22:28:28,203 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.555e+10 1.081e+13 6.791e+13 6.247e+14 3.921e+18, threshold=1.358e+14, percent-clipped=53.0
2024-10-08 22:28:28,762 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=26.62 vs. limit=9.798
2024-10-08 22:28:29,273 WARNING [optim.py:503] Scaling gradients by 0.03771522641181946, model_norm_threshold=135829062156288.0
2024-10-08 22:28:29,424 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.754e+30, grad_sumsq=4.849e+32, orig_rms_sq=5.679e-03
2024-10-08 22:28:30,602 INFO [train.py:1154] Epoch 2, batch 2350, loss[loss=1.295, simple_loss=0.6334, pruned_loss=0.7596, ctc_loss=1.096, over 4844.00 frames. ], tot_loss[loss=1.259, simple_loss=0.6378, pruned_loss=0.7325, ctc_loss=1.038, over 968291.76 frames. ], batch size: 23, lr: 3.51e-02,
2024-10-08 22:28:31,618 WARNING [optim.py:503] Scaling gradients by 0.002256054198369384, model_norm_threshold=135829062156288.0
2024-10-08 22:28:31,756 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.58, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.108e+33, grad_sumsq=3.712e+35, orig_rms_sq=5.679e-03
2024-10-08 22:28:31,978 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.const_attention_rate, batch_count=3067.3333333333335, ans=0.07746249999999999
2024-10-08 22:28:32,260 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.55 vs. limit=8.65025
2024-10-08 22:28:33,490 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=30.08 vs. limit=8.65025
2024-10-08 22:28:35,319 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=3067.3333333333335, ans=0.084975
2024-10-08 22:28:37,329 WARNING [optim.py:503] Scaling gradients by 0.057280272245407104, model_norm_threshold=135829062156288.0
2024-10-08 22:28:37,468 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.421e+30, grad_sumsq=2.522e+32, orig_rms_sq=5.634e-03
2024-10-08 22:28:41,147 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3070.6666666666665, ans=0.26929333333333333
2024-10-08 22:28:41,971 WARNING [optim.py:503] Scaling gradients by 0.028517117723822594, model_norm_threshold=135829062156288.0
2024-10-08 22:28:42,110 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.045e+30, grad_sumsq=1.317e+30, orig_rms_sq=3.829e+00
2024-10-08 22:28:42,893 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=256, metric=1.81 vs. limit=8.6515
2024-10-08 22:28:44,256 WARNING [optim.py:503] Scaling gradients by 0.04830200970172882, model_norm_threshold=135829062156288.0
2024-10-08 22:28:44,394 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.819e+30, grad_sumsq=4.750e+29, orig_rms_sq=3.829e+00
2024-10-08 22:28:48,097 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=3070.6666666666665, ans=9.803
2024-10-08 22:28:53,095 WARNING [optim.py:503] Scaling gradients by 0.008926622569561005, model_norm_threshold=135829062156288.0
2024-10-08 22:28:53,234 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.020e+31, grad_sumsq=2.400e+33, orig_rms_sq=2.091e-02
2024-10-08 22:28:55,052 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=25.31 vs. limit=9.8055
2024-10-08 22:28:56,517 WARNING [optim.py:503] Scaling gradients by 0.0033798145595937967, model_norm_threshold=135829062156288.0
2024-10-08 22:28:56,654 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.746e+32, grad_sumsq=6.900e+34, orig_rms_sq=5.429e-03
2024-10-08 22:29:06,333 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=44.55 vs. limit=8.654
2024-10-08 22:29:07,928 WARNING [optim.py:503] Scaling gradients by 0.07377885282039642, model_norm_threshold=135829062156288.0
2024-10-08 22:29:08,068 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.808e+29, grad_sumsq=1.626e+32, orig_rms_sq=5.415e-03
2024-10-08 22:29:10,217 WARNING [optim.py:503] Scaling gradients by 0.04622978717088699, model_norm_threshold=135829062156288.0
2024-10-08 22:29:10,354 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.532e+30, grad_sumsq=2.825e+32, orig_rms_sq=5.424e-03
2024-10-08 22:29:11,589 WARNING [optim.py:503] Scaling gradients by 0.009584402665495872, model_norm_threshold=135829062156288.0
2024-10-08 22:29:11,727 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.427e+31, grad_sumsq=3.795e+30, orig_rms_sq=9.030e+00
2024-10-08 22:29:13,991 WARNING [optim.py:503] Scaling gradients by 0.030290931463241577, model_norm_threshold=135829062156288.0
2024-10-08 22:29:14,128 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.683e+30, grad_sumsq=1.232e+33, orig_rms_sq=5.424e-03
2024-10-08 22:29:14,718 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=26.46 vs. limit=9.808
2024-10-08 22:29:16,329 WARNING [optim.py:503] Scaling gradients by 0.010248062200844288, model_norm_threshold=135829062156288.0
2024-10-08 22:29:16,467 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.350e+31, grad_sumsq=2.129e+33, orig_rms_sq=2.043e-02
2024-10-08 22:29:16,979 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.32 vs. limit=9.810500000000001
2024-10-08 22:29:28,329 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.42 vs. limit=9.813
2024-10-08 22:29:28,812 INFO [train.py:1154] Epoch 2, batch 2400, loss[loss=1.249, simple_loss=0.6146, pruned_loss=0.734, ctc_loss=1.038, over 4755.00 frames. ], tot_loss[loss=1.261, simple_loss=0.6387, pruned_loss=0.7339, ctc_loss=1.04, over 967510.32 frames. ], batch size: 19, lr: 3.50e-02,
2024-10-08 22:29:32,009 WARNING [optim.py:503] Scaling gradients by 0.017272498458623886, model_norm_threshold=135829062156288.0
2024-10-08 22:29:32,147 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.106e+31, grad_sumsq=5.307e+32, orig_rms_sq=2.085e-02
2024-10-08 22:29:36,601 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=3084.0, ans=0.35543749999999996
2024-10-08 22:29:40,189 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=3087.3333333333335, ans=0.35528125
2024-10-08 22:29:49,183 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=24.29 vs. limit=9.8155
2024-10-08 22:29:49,482 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.94 vs. limit=9.8155
2024-10-08 22:29:53,768 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=37.36 vs. limit=8.659
2024-10-08 22:29:56,272 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=20.79 vs. limit=8.659
2024-10-08 22:29:57,557 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.49 vs. limit=5.772666666666667
2024-10-08 22:29:58,531 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=10.50 vs. limit=8.659
2024-10-08 22:30:00,045 WARNING [optim.py:503] Scaling gradients by 0.006062410771846771, model_norm_threshold=135829062156288.0
2024-10-08 22:30:00,185 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.277e+32, grad_sumsq=2.366e+34, orig_rms_sq=5.396e-03
2024-10-08 22:30:02,303 WARNING [optim.py:503] Scaling gradients by 0.002599595347419381, model_norm_threshold=135829062156288.0
2024-10-08 22:30:02,440 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.666e+32, grad_sumsq=8.519e+31, orig_rms_sq=8.999e+00
2024-10-08 22:30:08,998 WARNING [optim.py:503] Scaling gradients by 0.0004717099654953927, model_norm_threshold=135829062156288.0
2024-10-08 22:30:09,136 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.880e+34, grad_sumsq=8.818e+35, orig_rms_sq=2.132e-02
2024-10-08 22:30:18,655 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=14.85 vs. limit=8.6615
2024-10-08 22:30:20,428 WARNING [optim.py:503] Scaling gradients by 0.006970403250306845, model_norm_threshold=135829062156288.0
2024-10-08 22:30:20,568 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.608e+32, grad_sumsq=7.511e+33, orig_rms_sq=2.141e-02
2024-10-08 22:30:22,650 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.682e+11 8.204e+12 4.380e+13 4.474e+14 2.880e+17, threshold=8.761e+13, percent-clipped=37.0
2024-10-08 22:30:24,210 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=10.64 vs. limit=8.662749999999999
2024-10-08 22:30:24,951 INFO [train.py:1154] Epoch 2, batch 2450, loss[loss=1.277, simple_loss=0.6494, pruned_loss=0.7454, ctc_loss=1.036, over 4886.00 frames. ], tot_loss[loss=1.264, simple_loss=0.641, pruned_loss=0.7353, ctc_loss=1.043, over 966929.13 frames. ], batch size: 22, lr: 3.50e-02,
2024-10-08 22:30:26,162 WARNING [optim.py:503] Scaling gradients by 0.06905677914619446, model_norm_threshold=87605572009984.0
2024-10-08 22:30:26,301 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.732e+29, grad_sumsq=9.582e+28, orig_rms_sq=3.895e+00
2024-10-08 22:30:30,767 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.balancer_ff3.min_abs, batch_count=3100.6666666666665, ans=0.15503333333333333
2024-10-08 22:30:32,766 WARNING [optim.py:503] Scaling gradients by 0.00022921431809663773, model_norm_threshold=87605572009984.0
2024-10-08 22:30:32,907 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.520e+34, grad_sumsq=4.839e+36, orig_rms_sq=5.209e-03
2024-10-08 22:30:33,965 WARNING [optim.py:503] Scaling gradients by 0.0006361792329698801, model_norm_threshold=87605572009984.0
2024-10-08 22:30:34,102 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.161e+33, grad_sumsq=1.183e+36, orig_rms_sq=5.209e-03
2024-10-08 22:30:39,001 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.43 vs. limit=6.552
2024-10-08 22:30:41,747 WARNING [optim.py:503] Scaling gradients by 0.010518895462155342, model_norm_threshold=87605572009984.0
2024-10-08 22:30:41,885 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.473e+31, grad_sumsq=3.708e+30, orig_rms_sq=3.973e+00
2024-10-08 22:30:49,758 WARNING [optim.py:503] Scaling gradients by 0.009595128707587719, model_norm_threshold=87605572009984.0
2024-10-08 22:30:49,897 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.452e+31, grad_sumsq=4.631e+33, orig_rms_sq=5.294e-03
2024-10-08 22:30:56,488 WARNING [optim.py:503] Scaling gradients by 0.011852347292006016, model_norm_threshold=87605572009984.0
2024-10-08 22:30:56,628 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.681e+31, grad_sumsq=3.161e+33, orig_rms_sq=5.319e-03
2024-10-08 22:30:56,786 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_skip_rate, batch_count=3107.3333333333335, ans=0.083475
2024-10-08 22:30:58,871 WARNING [optim.py:503] Scaling gradients by 0.004596472252160311, model_norm_threshold=87605572009984.0
2024-10-08 22:30:59,009 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.525e+31, grad_sumsq=2.384e+31, orig_rms_sq=3.995e+00
2024-10-08 22:31:05,114 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=27.73 vs. limit=9.833
2024-10-08 22:31:05,654 WARNING [optim.py:503] Scaling gradients by 0.0006920174928382039, model_norm_threshold=87605572009984.0
2024-10-08 22:31:05,795 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.743e+33, grad_sumsq=1.761e+35, orig_rms_sq=2.126e-02
2024-10-08 22:31:08,035 WARNING [optim.py:503] Scaling gradients by 0.0006547746015712619, model_norm_threshold=87605572009984.0
2024-10-08 22:31:08,172 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.773e+33, grad_sumsq=1.314e+35, orig_rms_sq=2.110e-02
2024-10-08 22:31:09,548 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=3114.0, ans=0.35403125
2024-10-08 22:31:11,492 WARNING [optim.py:503] Scaling gradients by 0.005068649537861347, model_norm_threshold=87605572009984.0
2024-10-08 22:31:11,631 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.812e+31, grad_sumsq=3.228e+33, orig_rms_sq=2.110e-02
2024-10-08 22:31:17,382 WARNING [optim.py:503] Scaling gradients by 0.0019734555389732122, model_norm_threshold=87605572009984.0
2024-10-08 22:31:17,521 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.353e+32, grad_sumsq=2.536e+34, orig_rms_sq=2.111e-02
2024-10-08 22:31:19,739 WARNING [optim.py:503] Scaling gradients by 0.011003651656210423, model_norm_threshold=87605572009984.0
2024-10-08 22:31:19,880 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.721e+31, grad_sumsq=3.283e+33, orig_rms_sq=5.241e-03
2024-10-08 22:31:22,281 INFO [train.py:1154] Epoch 2, batch 2500, loss[loss=1.238, simple_loss=0.6336, pruned_loss=0.7204, ctc_loss=1.005, over 4741.00 frames. ], tot_loss[loss=1.263, simple_loss=0.6404, pruned_loss=0.7342, ctc_loss=1.042, over 966616.28 frames. ], batch size: 26, lr: 3.49e-02,
2024-10-08 22:31:23,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3117.3333333333335, ans=0.26882666666666666
2024-10-08 22:31:29,338 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=3117.3333333333335, ans=0.0831
2024-10-08 22:31:30,218 WARNING [optim.py:503] Scaling gradients by 0.024135762825608253, model_norm_threshold=87605572009984.0
2024-10-08 22:31:30,358 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.555e+30, grad_sumsq=1.194e+32, orig_rms_sq=2.139e-02
2024-10-08 22:31:31,092 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=15.57 vs. limit=8.669
2024-10-08 22:31:31,427 WARNING [optim.py:503] Scaling gradients by 0.022889168933033943, model_norm_threshold=87605572009984.0
2024-10-08 22:31:31,569 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.058e+30, grad_sumsq=3.409e+29, orig_rms_sq=8.969e+00
2024-10-08 22:31:37,152 WARNING [optim.py:503] Scaling gradients by 0.001478142337873578, model_norm_threshold=87605572009984.0
2024-10-08 22:31:37,294 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.098e+33, grad_sumsq=2.110e+35, orig_rms_sq=5.204e-03
2024-10-08 22:31:40,519 WARNING [optim.py:503] Scaling gradients by 0.002151080872863531, model_norm_threshold=87605572009984.0
2024-10-08 22:31:40,657 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.255e+32, grad_sumsq=1.504e+34, orig_rms_sq=2.165e-02
2024-10-08 22:31:41,043 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=16.59 vs. limit=8.67025
2024-10-08 22:31:47,271 WARNING [optim.py:503] Scaling gradients by 0.0812624841928482, model_norm_threshold=87605572009984.0
2024-10-08 22:31:47,407 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.903e+29, grad_sumsq=7.096e+28, orig_rms_sq=4.092e+00
2024-10-08 22:32:00,953 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3127.3333333333335, ans=0.26872666666666667
2024-10-08 22:32:04,690 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=31.99 vs. limit=8.67275
2024-10-08 22:32:07,077 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=13.45 vs. limit=5.782666666666667
2024-10-08 22:32:16,611 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.529e+11 4.320e+12 2.172e+13 2.247e+14 3.822e+17, threshold=4.344e+13, percent-clipped=31.0
2024-10-08 22:32:18,843 INFO [train.py:1154] Epoch 2, batch 2550, loss[loss=1.231, simple_loss=0.6185, pruned_loss=0.7233, ctc_loss=0.9948, over 4959.00 frames. ], tot_loss[loss=1.262, simple_loss=0.6401, pruned_loss=0.7335, ctc_loss=1.041, over 967028.66 frames. ], batch size: 19, lr: 3.48e-02,
2024-10-08 22:32:25,242 WARNING [optim.py:503] Scaling gradients by 0.026599446311593056, model_norm_threshold=43438259044352.0
2024-10-08 22:32:25,379 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.112e+30, grad_sumsq=5.329e+31, orig_rms_sq=2.087e-02
2024-10-08 22:32:25,547 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3134.0, ans=0.26866
2024-10-08 22:32:27,579 WARNING [optim.py:503] Scaling gradients by 0.0025384086184203625, model_norm_threshold=43438259044352.0
2024-10-08 22:32:27,719 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.718e+31, grad_sumsq=3.219e+33, orig_rms_sq=2.087e-02
2024-10-08 22:32:29,865 WARNING [optim.py:503] Scaling gradients by 0.020450666546821594, model_norm_threshold=43438259044352.0
2024-10-08 22:32:30,004 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.760e+29, grad_sumsq=9.798e+28, orig_rms_sq=8.940e+00
2024-10-08 22:32:33,210 WARNING [optim.py:503] Scaling gradients by 0.01581369712948799, model_norm_threshold=43438259044352.0
2024-10-08 22:32:33,346 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.654e+30, grad_sumsq=5.261e+32, orig_rms_sq=5.045e-03
2024-10-08 22:32:33,975 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=9.77 vs. limit=5.7843333333333335
2024-10-08 22:32:37,799 WARNING [optim.py:503] Scaling gradients by 0.009224571287631989, model_norm_threshold=43438259044352.0
2024-10-08 22:32:37,941 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.119e+30, grad_sumsq=1.611e+33, orig_rms_sq=5.041e-03
2024-10-08 22:32:38,689 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.76 vs. limit=9.853
2024-10-08 22:32:39,008 WARNING [optim.py:503] Scaling gradients by 0.0009123379131779075, model_norm_threshold=43438259044352.0
2024-10-08 22:32:39,147 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.208e+32, grad_sumsq=1.438e+35, orig_rms_sq=5.012e-03
2024-10-08 22:32:41,353 WARNING [optim.py:503] Scaling gradients by 0.0009622756624594331, model_norm_threshold=43438259044352.0
2024-10-08 22:32:41,491 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.959e+32, grad_sumsq=7.899e+34, orig_rms_sq=5.012e-03
2024-10-08 22:32:42,243 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.49 vs. limit=9.8555
2024-10-08 22:32:42,593 WARNING [optim.py:503] Scaling gradients by 0.03939179703593254, model_norm_threshold=43438259044352.0
2024-10-08 22:32:42,732 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.068e+29, grad_sumsq=1.211e+32, orig_rms_sq=5.012e-03
2024-10-08 22:32:44,440 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=6.98 vs. limit=5.256266666666667
2024-10-08 22:32:44,897 WARNING [optim.py:503] Scaling gradients by 0.0006762428674846888, model_norm_threshold=43438259044352.0
2024-10-08 22:32:45,034 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.52, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.133e+33, grad_sumsq=4.283e+35, orig_rms_sq=4.979e-03
2024-10-08 22:32:46,115 WARNING [optim.py:503] Scaling gradients by 0.0011550092604011297, model_norm_threshold=43438259044352.0
2024-10-08 22:32:46,252 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.637e+32, grad_sumsq=5.296e+34, orig_rms_sq=4.979e-03
2024-10-08 22:32:49,454 WARNING [optim.py:503] Scaling gradients by 0.0031614392064511776, model_norm_threshold=43438259044352.0
2024-10-08 22:32:49,596 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.768e+31, grad_sumsq=1.753e+34, orig_rms_sq=5.000e-03
2024-10-08 22:32:50,327 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.21 vs. limit=9.8555
2024-10-08 22:32:55,966 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.73 vs. limit=6.572
2024-10-08 22:32:56,982 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.57 vs. limit=5.786
2024-10-08 22:32:58,077 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=16.59 vs. limit=8.679
2024-10-08 22:33:01,948 WARNING [optim.py:503] Scaling gradients by 0.03410610184073448, model_norm_threshold=43438259044352.0
2024-10-08 22:33:02,085 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.736e+29, grad_sumsq=1.800e+31, orig_rms_sq=2.075e-02
2024-10-08 22:33:02,269 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3144.0, ans=0.26856
2024-10-08 22:33:03,264 WARNING [optim.py:503] Scaling gradients by 0.00710697378963232, model_norm_threshold=43438259044352.0
2024-10-08 22:33:03,404 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.783e+30, grad_sumsq=4.715e+32, orig_rms_sq=2.075e-02
2024-10-08 22:33:07,881 WARNING [optim.py:503] Scaling gradients by 0.00028184865368530154, model_norm_threshold=43438259044352.0
2024-10-08 22:33:08,021 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.757e+33, grad_sumsq=5.337e+32, orig_rms_sq=8.913e+00
2024-10-08 22:33:09,671 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=28.34 vs. limit=9.8605
2024-10-08 22:33:10,456 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module2.balancer1.min_positive, batch_count=3147.3333333333335, ans=0.04016458333333334
2024-10-08 22:33:11,353 WARNING [optim.py:503] Scaling gradients by 0.021124588325619698, model_norm_threshold=43438259044352.0
2024-10-08 22:33:11,491 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.53, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.261e+30, grad_sumsq=4.380e+32, orig_rms_sq=5.161e-03
2024-10-08 22:33:16,024 INFO [train.py:1154] Epoch 2, batch 2600, loss[loss=1.323, simple_loss=0.6738, pruned_loss=0.7727, ctc_loss=1.068, over 4852.00 frames. ], tot_loss[loss=1.261, simple_loss=0.64, pruned_loss=0.7325, ctc_loss=1.04, over 966506.11 frames. ], batch size: 20, lr: 3.48e-02,
2024-10-08 22:33:17,142 WARNING [optim.py:503] Scaling gradients by 0.031813375651836395, model_norm_threshold=43438259044352.0
2024-10-08 22:33:17,280 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.298e+29, grad_sumsq=1.017e+32, orig_rms_sq=5.212e-03
2024-10-08 22:33:18,328 WARNING [optim.py:503] Scaling gradients by 0.06542220711708069, model_norm_threshold=43438259044352.0
2024-10-08 22:33:18,464 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.116e+28, grad_sumsq=4.311e+30, orig_rms_sq=2.115e-02
2024-10-08 22:33:21,733 WARNING [optim.py:503] Scaling gradients by 0.04839427024126053, model_norm_threshold=43438259044352.0
2024-10-08 22:33:21,873 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.623e+29, grad_sumsq=4.982e+31, orig_rms_sq=5.265e-03
2024-10-08 22:33:25,064 WARNING [optim.py:503] Scaling gradients by 0.008111807517707348, model_norm_threshold=43438259044352.0
2024-10-08 22:33:25,204 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.57, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.624e+31, grad_sumsq=3.049e+33, orig_rms_sq=5.326e-03
2024-10-08 22:33:30,770 WARNING [optim.py:503] Scaling gradients by 0.024172434583306313, model_norm_threshold=43438259044352.0
2024-10-08 22:33:30,909 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.342e+30, grad_sumsq=2.484e+32, orig_rms_sq=5.400e-03
2024-10-08 22:33:31,103 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.attention_skip_rate, batch_count=3154.0, ans=0.08172499999999999
2024-10-08 22:33:33,038 WARNING [optim.py:503] Scaling gradients by 0.0007287093321792781, model_norm_threshold=43438259044352.0
2024-10-08 22:33:33,177 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.163e+33, grad_sumsq=5.375e+34, orig_rms_sq=2.165e-02
2024-10-08 22:33:34,250 WARNING [optim.py:503] Scaling gradients by 0.026190895587205887, model_norm_threshold=43438259044352.0
2024-10-08 22:33:34,394 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.260e+29, grad_sumsq=3.772e+31, orig_rms_sq=2.190e-02
2024-10-08 22:33:34,617 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.hidden_balancer.prob, batch_count=3154.0, ans=0.35215625
2024-10-08 22:33:35,437 WARNING [optim.py:503] Scaling gradients by 0.023472582921385765, model_norm_threshold=43438259044352.0
2024-10-08 22:33:35,578 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.273e+29, grad_sumsq=1.335e+32, orig_rms_sq=5.447e-03
2024-10-08 22:33:36,645 WARNING [optim.py:503] Scaling gradients by 0.09780024737119675, model_norm_threshold=43438259044352.0
2024-10-08 22:33:36,784 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.353e+28, grad_sumsq=5.940e+27, orig_rms_sq=9.011e+00
2024-10-08 22:33:38,777 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.62 vs. limit=9.868
2024-10-08 22:33:40,162 WARNING [optim.py:503] Scaling gradients by 0.0002663348277565092, model_norm_threshold=43438259044352.0
2024-10-08 22:33:40,301 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.439e+33, grad_sumsq=8.128e+35, orig_rms_sq=5.462e-03
2024-10-08 22:33:41,449 WARNING [optim.py:503] Scaling gradients by 0.09301932901144028, model_norm_threshold=43438259044352.0
2024-10-08 22:33:41,586 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.510e+28, grad_sumsq=1.375e+31, orig_rms_sq=5.462e-03
2024-10-08 22:33:43,567 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.57 vs. limit=5.789333333333333
2024-10-08 22:33:46,128 WARNING [optim.py:503] Scaling gradients by 0.014180431142449379, model_norm_threshold=43438259044352.0
2024-10-08 22:33:46,267 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.490e+30, grad_sumsq=1.119e+32, orig_rms_sq=2.225e-02
2024-10-08 22:33:46,468 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.1.self_attn_weights, loss-sum=2.901e+05
2024-10-08 22:33:51,642 WARNING [optim.py:503] Scaling gradients by 0.007673587184399366, model_norm_threshold=43438259044352.0
2024-10-08 22:33:51,782 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.234e+31, grad_sumsq=2.248e+33, orig_rms_sq=5.487e-03
2024-10-08 22:33:52,509 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=17.56 vs. limit=8.68525
2024-10-08 22:33:53,926 WARNING [optim.py:503] Scaling gradients by 0.002379330340772867, model_norm_threshold=43438259044352.0
2024-10-08 22:33:54,064 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.037e+32, grad_sumsq=4.742e+33, orig_rms_sq=2.187e-02
2024-10-08 22:33:55,394 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.prob, batch_count=3160.6666666666665, ans=0.35184375
2024-10-08 22:34:00,243 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=13.30 vs. limit=8.68525
2024-10-08 22:34:02,063 WARNING [optim.py:503] Scaling gradients by 0.03330230340361595, model_norm_threshold=43438259044352.0
2024-10-08 22:34:02,203 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.58, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.913e+29, grad_sumsq=1.879e+32, orig_rms_sq=5.276e-03
2024-10-08 22:34:03,852 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=10.37 vs. limit=5.791
2024-10-08 22:34:08,681 WARNING [optim.py:503] Scaling gradients by 0.027620969340205193, model_norm_threshold=43438259044352.0
2024-10-08 22:34:08,819 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.875e+29, grad_sumsq=7.394e+31, orig_rms_sq=5.241e-03
2024-10-08 22:34:11,038 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.382e+11 5.845e+12 8.054e+13 1.365e+15 1.631e+17, threshold=1.611e+14, percent-clipped=56.0
2024-10-08 22:34:11,039 WARNING [optim.py:503] Scaling gradients by 0.022740336135029793, model_norm_threshold=161080382849024.0
2024-10-08 22:34:11,195 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.660e+31, grad_sumsq=3.173e+33, orig_rms_sq=5.233e-03
2024-10-08 22:34:11,410 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=3164.0, ans=0.24746
2024-10-08 22:34:13,368 INFO [train.py:1154] Epoch 2, batch 2650, loss[loss=1.267, simple_loss=0.6463, pruned_loss=0.7272, ctc_loss=1.081, over 4824.00 frames. ], tot_loss[loss=1.259, simple_loss=0.6388, pruned_loss=0.7312, ctc_loss=1.04, over 966162.46 frames. ], batch size: 38, lr: 3.47e-02,
2024-10-08 22:34:14,588 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=3167.3333333333335, ans=0.08020416666666667
2024-10-08 22:34:35,020 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=21.38 vs. limit=9.8805
2024-10-08 22:34:39,045 WARNING [optim.py:503] Scaling gradients by 0.08283919841051102, model_norm_threshold=161080382849024.0
2024-10-08 22:34:39,183 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.761e+30, grad_sumsq=3.194e+32, orig_rms_sq=5.515e-03
2024-10-08 22:34:49,227 WARNING [optim.py:503] Scaling gradients by 0.004570105578750372, model_norm_threshold=161080382849024.0
2024-10-08 22:34:49,365 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.829e+32, grad_sumsq=7.330e+31, orig_rms_sq=3.860e+00
2024-10-08 22:34:51,524 WARNING [optim.py:503] Scaling gradients by 0.0109907491132617, model_norm_threshold=161080382849024.0
2024-10-08 22:34:51,663 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.609e+31, grad_sumsq=2.166e+33, orig_rms_sq=2.128e-02
2024-10-08 22:34:57,063 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.76 vs. limit=9.883
2024-10-08 22:34:58,448 WARNING [optim.py:503] Scaling gradients by 0.041127659380435944, model_norm_threshold=161080382849024.0
2024-10-08 22:34:58,587 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.913e+30, grad_sumsq=9.901e+32, orig_rms_sq=4.962e-03
2024-10-08 22:35:01,767 WARNING [optim.py:503] Scaling gradients by 0.040308911353349686, model_norm_threshold=161080382849024.0
2024-10-08 22:35:01,907 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.988e+30, grad_sumsq=6.048e+32, orig_rms_sq=4.941e-03
2024-10-08 22:35:05,322 WARNING [optim.py:503] Scaling gradients by 0.08679527044296265, model_norm_threshold=161080382849024.0
2024-10-08 22:35:05,461 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.003e+29, grad_sumsq=3.767e+31, orig_rms_sq=2.124e-02
2024-10-08 22:35:09,863 INFO [train.py:1154] Epoch 2, batch 2700, loss[loss=1.192, simple_loss=0.6042, pruned_loss=0.6866, ctc_loss=1.017, over 4865.00 frames. ], tot_loss[loss=1.26, simple_loss=0.6399, pruned_loss=0.7317, ctc_loss=1.041, over 966280.62 frames. ], batch size: 28, lr: 3.47e-02,
2024-10-08 22:35:11,978 WARNING [optim.py:503] Scaling gradients by 0.031789299100637436, model_norm_threshold=161080382849024.0
2024-10-08 22:35:12,117 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.024e+30, grad_sumsq=1.651e+33, orig_rms_sq=4.859e-03
2024-10-08 22:35:17,769 WARNING [optim.py:503] Scaling gradients by 0.005281959660351276, model_norm_threshold=161080382849024.0
2024-10-08 22:35:17,911 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.73, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.801e+32, grad_sumsq=1.407e+35, orig_rms_sq=4.833e-03
2024-10-08 22:35:23,498 WARNING [optim.py:503] Scaling gradients by 0.0011588820489123464, model_norm_threshold=161080382849024.0
2024-10-08 22:35:23,636 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.844e+33, grad_sumsq=1.873e+33, orig_rms_sq=3.654e+00
2024-10-08 22:35:27,578 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=36.10 vs. limit=9.8905
2024-10-08 22:35:27,583 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.33 vs. limit=5.796833333333334
2024-10-08 22:35:30,446 WARNING [optim.py:503] Scaling gradients by 0.00164808111730963, model_norm_threshold=161080382849024.0
2024-10-08 22:35:30,584 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.944e+33, grad_sumsq=3.977e+35, orig_rms_sq=4.887e-03
2024-10-08 22:35:31,663 WARNING [optim.py:503] Scaling gradients by 0.025256523862481117, model_norm_threshold=161080382849024.0
2024-10-08 22:35:31,803 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.326e+31, grad_sumsq=2.713e+33, orig_rms_sq=4.887e-03
2024-10-08 22:35:38,499 WARNING [optim.py:503] Scaling gradients by 0.0726957842707634, model_norm_threshold=161080382849024.0
2024-10-08 22:35:38,639 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.830e+30, grad_sumsq=3.530e+32, orig_rms_sq=5.184e-03
2024-10-08 22:35:40,415 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.13 vs. limit=8.6965
2024-10-08 22:35:41,876 WARNING [optim.py:503] Scaling gradients by 0.006189174018800259, model_norm_threshold=161080382849024.0
2024-10-08 22:35:42,014 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.867e+32, grad_sumsq=8.332e+33, orig_rms_sq=2.241e-02
2024-10-08 22:35:47,575 WARNING [optim.py:503] Scaling gradients by 0.005686675664037466, model_norm_threshold=161080382849024.0
2024-10-08 22:35:47,713 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.235e+32, grad_sumsq=4.260e+34, orig_rms_sq=5.247e-03
2024-10-08 22:35:47,973 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3194.0, ans=0.26805999999999996
2024-10-08 22:35:48,256 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.05 vs. limit=8.69775
2024-10-08 22:35:51,123 WARNING [optim.py:503] Scaling gradients by 0.00720518734306097, model_norm_threshold=161080382849024.0
2024-10-08 22:35:51,261 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.812e+32, grad_sumsq=7.924e+33, orig_rms_sq=2.287e-02
2024-10-08 22:35:56,850 WARNING [optim.py:503] Scaling gradients by 0.07251820713281631, model_norm_threshold=161080382849024.0
2024-10-08 22:35:56,989 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.045e+29, grad_sumsq=1.562e+32, orig_rms_sq=5.150e-03
2024-10-08 22:35:58,065 WARNING [optim.py:503] Scaling gradients by 0.09217950701713562, model_norm_threshold=161080382849024.0
2024-10-08 22:35:58,204 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.631e+29, grad_sumsq=2.521e+31, orig_rms_sq=2.233e-02
2024-10-08 22:36:01,543 WARNING [optim.py:503] Scaling gradients by 0.004626265726983547, model_norm_threshold=161080382849024.0
2024-10-08 22:36:01,682 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.667e+32, grad_sumsq=1.664e+34, orig_rms_sq=2.204e-02
2024-10-08 22:36:02,746 WARNING [optim.py:503] Scaling gradients by 0.0002417489158688113, model_norm_threshold=161080382849024.0
2024-10-08 22:36:02,885 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.637e+35, grad_sumsq=3.205e+37, orig_rms_sq=5.106e-03
2024-10-08 22:36:03,916 WARNING [optim.py:503] Scaling gradients by 0.0039122686721384525, model_norm_threshold=161080382849024.0
2024-10-08 22:36:04,054 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.177e+32, grad_sumsq=3.256e+34, orig_rms_sq=2.204e-02
2024-10-08 22:36:04,214 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=3197.3333333333335, ans=0.350125
2024-10-08 22:36:06,613 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.332e+11 6.924e+12 4.898e+13 7.802e+14 6.663e+17, threshold=9.797e+13, percent-clipped=44.0
2024-10-08 22:36:08,956 INFO [train.py:1154] Epoch 2, batch 2750, loss[loss=1.291, simple_loss=0.6594, pruned_loss=0.7568, ctc_loss=1.025, over 4800.00 frames. ], tot_loss[loss=1.26, simple_loss=0.6396, pruned_loss=0.7318, ctc_loss=1.04, over 966928.59 frames. ], batch size: 19, lr: 3.46e-02,
2024-10-08 22:36:09,142 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.scale_min, batch_count=3200.6666666666665, ans=0.7879766666666668
2024-10-08 22:36:12,198 WARNING [optim.py:503] Scaling gradients by 0.006531764753162861, model_norm_threshold=97967717482496.0
2024-10-08 22:36:12,336 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.523e+31, grad_sumsq=1.302e+34, orig_rms_sq=5.011e-03
2024-10-08 22:36:12,971 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=8.87 vs. limit=5.280266666666667
2024-10-08 22:36:20,097 WARNING [optim.py:503] Scaling gradients by 0.0572751946747303, model_norm_threshold=97967717482496.0
2024-10-08 22:36:20,236 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.244e+29, grad_sumsq=7.961e+28, orig_rms_sq=9.100e+00
2024-10-08 22:36:21,334 WARNING [optim.py:503] Scaling gradients by 0.004098828416317701, model_norm_threshold=97967717482496.0
2024-10-08 22:36:21,472 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.868e+32, grad_sumsq=3.716e+34, orig_rms_sq=5.026e-03
2024-10-08 22:36:23,638 WARNING [optim.py:503] Scaling gradients by 0.0017521744593977928, model_norm_threshold=97967717482496.0
2024-10-08 22:36:23,777 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.108e+33, grad_sumsq=2.204e+35, orig_rms_sq=5.026e-03
2024-10-08 22:36:25,906 WARNING [optim.py:503] Scaling gradients by 0.006911130156368017, model_norm_threshold=97967717482496.0
2024-10-08 22:36:26,046 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.613e+31, grad_sumsq=1.664e+33, orig_rms_sq=2.171e-02
2024-10-08 22:36:30,524 WARNING [optim.py:503] Scaling gradients by 0.017123712226748466, model_norm_threshold=97967717482496.0
2024-10-08 22:36:30,662 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.694e+30, grad_sumsq=1.320e+33, orig_rms_sq=5.072e-03
2024-10-08 22:36:30,840 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=3207.3333333333335, ans=0.34965625
2024-10-08 22:36:31,223 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=6.02 vs. limit=8.70275
2024-10-08 22:36:31,695 WARNING [optim.py:503] Scaling gradients by 0.013400565832853317, model_norm_threshold=97967717482496.0
2024-10-08 22:36:31,836 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.467e+30, grad_sumsq=1.044e+30, orig_rms_sq=9.068e+00
2024-10-08 22:36:33,001 WARNING [optim.py:503] Scaling gradients by 0.05902232229709625, model_norm_threshold=97967717482496.0
2024-10-08 22:36:33,140 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.114e+30, grad_sumsq=2.196e+32, orig_rms_sq=5.072e-03
2024-10-08 22:36:33,746 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=65.90 vs. limit=8.70275
2024-10-08 22:36:34,391 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward2.hidden_balancer.prob, batch_count=3207.3333333333335, ans=0.34965625
2024-10-08 22:36:41,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.64 vs. limit=3.4811
2024-10-08 22:36:42,528 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.attention_skip_rate, batch_count=3210.6666666666665, ans=0.0796
2024-10-08 22:36:46,605 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=16.54 vs. limit=8.704
2024-10-08 22:36:48,212 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3210.6666666666665, ans=0.2678933333333333
2024-10-08 22:36:48,717 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=13.17 vs. limit=8.704
2024-10-08 22:36:52,308 WARNING [optim.py:503] Scaling gradients by 0.0003676794876810163, model_norm_threshold=97967717482496.0
2024-10-08 22:36:52,447 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.418e+34, grad_sumsq=4.997e+36, orig_rms_sq=4.839e-03
2024-10-08 22:36:53,273 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.01 vs. limit=9.908
2024-10-08 22:36:55,911 WARNING [optim.py:503] Scaling gradients by 0.0007568205473944545, model_norm_threshold=97967717482496.0
2024-10-08 22:36:56,048 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.642e+33, grad_sumsq=1.545e+33, orig_rms_sq=3.652e+00
2024-10-08 22:36:59,918 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.77 vs. limit=9.910499999999999
2024-10-08 22:37:06,151 INFO [train.py:1154] Epoch 2, batch 2800, loss[loss=1.296, simple_loss=0.6755, pruned_loss=0.7331, ctc_loss=1.125, over 4755.00 frames. ], tot_loss[loss=1.26, simple_loss=0.6401, pruned_loss=0.7322, ctc_loss=1.041, over 967102.57 frames. ], batch size: 53, lr: 3.46e-02,
2024-10-08 22:37:07,129 WARNING [optim.py:503] Scaling gradients by 0.03810586407780647, model_norm_threshold=97967717482496.0
2024-10-08 22:37:07,271 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.375e+30, grad_sumsq=2.831e+32, orig_rms_sq=4.856e-03
2024-10-08 22:37:09,474 WARNING [optim.py:503] Scaling gradients by 0.003107246709987521, model_norm_threshold=97967717482496.0
2024-10-08 22:37:09,613 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.128e+32, grad_sumsq=5.606e+32, orig_rms_sq=3.796e-01
2024-10-08 22:37:10,727 WARNING [optim.py:503] Scaling gradients by 0.0019541149958968163, model_norm_threshold=97967717482496.0
2024-10-08 22:37:10,870 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.763e+32, grad_sumsq=2.766e+34, orig_rms_sq=2.083e-02
2024-10-08 22:37:12,990 WARNING [optim.py:503] Scaling gradients by 0.0977252945303917, model_norm_threshold=97967717482496.0
2024-10-08 22:37:13,130 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.969e+29, grad_sumsq=9.449e+30, orig_rms_sq=2.083e-02
2024-10-08 22:37:15,498 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3217.3333333333335, ans=0.26782666666666666
2024-10-08 22:37:20,424 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=41.60 vs. limit=8.70775
2024-10-08 22:37:23,084 WARNING [optim.py:503] Scaling gradients by 0.00030931876972317696, model_norm_threshold=97967717482496.0
2024-10-08 22:37:23,222 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.145e+34, grad_sumsq=6.283e+36, orig_rms_sq=5.006e-03
2024-10-08 22:37:25,421 WARNING [optim.py:503] Scaling gradients by 0.013753343373537064, model_norm_threshold=97967717482496.0
2024-10-08 22:37:25,561 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.132e+31, grad_sumsq=5.360e+32, orig_rms_sq=2.112e-02
2024-10-08 22:37:27,125 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=16.34 vs. limit=8.70775
2024-10-08 22:37:29,919 WARNING [optim.py:503] Scaling gradients by 0.03306816890835762, model_norm_threshold=97967717482496.0
2024-10-08 22:37:30,057 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.040e+30, grad_sumsq=4.040e+32, orig_rms_sq=5.050e-03
2024-10-08 22:37:31,087 WARNING [optim.py:503] Scaling gradients by 0.022492174059152603, model_norm_threshold=97967717482496.0
2024-10-08 22:37:31,224 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.777e+30, grad_sumsq=2.259e+32, orig_rms_sq=2.115e-02
2024-10-08 22:37:32,962 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=17.77 vs. limit=9.918
2024-10-08 22:37:33,347 WARNING [optim.py:503] Scaling gradients by 0.03683902695775032, model_norm_threshold=97967717482496.0
2024-10-08 22:37:33,489 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.782e+30, grad_sumsq=8.416e+31, orig_rms_sq=2.118e-02
2024-10-08 22:37:37,280 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=19.98 vs. limit=5.806
2024-10-08 22:37:37,737 WARNING [optim.py:503] Scaling gradients by 0.06242890655994415, model_norm_threshold=97967717482496.0
2024-10-08 22:37:37,876 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.885e+29, grad_sumsq=2.769e+31, orig_rms_sq=2.126e-02
2024-10-08 22:37:38,997 WARNING [optim.py:503] Scaling gradients by 0.00412948289886117, model_norm_threshold=97967717482496.0
2024-10-08 22:37:39,136 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.338e+32, grad_sumsq=6.296e+33, orig_rms_sq=2.126e-02
2024-10-08 22:37:39,564 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=34.44 vs. limit=8.71025
2024-10-08 22:37:45,777 WARNING [optim.py:503] Scaling gradients by 0.03328864276409149, model_norm_threshold=97967717482496.0
2024-10-08 22:37:45,917 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.641e+30, grad_sumsq=7.697e+31, orig_rms_sq=2.132e-02
2024-10-08 22:37:50,535 WARNING [optim.py:503] Scaling gradients by 0.015528878197073936, model_norm_threshold=97967717482496.0
2024-10-08 22:37:50,674 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.297e+31, grad_sumsq=1.479e+30, orig_rms_sq=8.770e+00
2024-10-08 22:37:50,823 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=2.042e+06
2024-10-08 22:37:52,809 WARNING [optim.py:503] Scaling gradients by 0.045400556176900864, model_norm_threshold=97967717482496.0
2024-10-08 22:37:52,948 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.303e+30, grad_sumsq=6.054e+31, orig_rms_sq=2.152e-02
2024-10-08 22:37:54,091 WARNING [optim.py:503] Scaling gradients by 0.024578621610999107, model_norm_threshold=97967717482496.0
2024-10-08 22:37:54,229 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.673e+30, grad_sumsq=7.325e+32, orig_rms_sq=5.014e-03
2024-10-08 22:37:56,586 WARNING [optim.py:503] Scaling gradients by 0.0009105782373808324, model_norm_threshold=97967717482496.0
2024-10-08 22:37:56,724 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.155e+33, grad_sumsq=1.459e+35, orig_rms_sq=2.163e-02
2024-10-08 22:37:59,424 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.72 vs. limit=3.4846
2024-10-08 22:38:01,187 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.633e+10 4.772e+12 5.468e+13 1.569e+15 3.167e+17, threshold=1.094e+14, percent-clipped=44.0
2024-10-08 22:38:03,491 INFO [train.py:1154] Epoch 2, batch 2850, loss[loss=1.308, simple_loss=0.6668, pruned_loss=0.7639, ctc_loss=1.056, over 4938.00 frames. ], tot_loss[loss=1.261, simple_loss=0.6413, pruned_loss=0.7315, ctc_loss=1.042, over 966836.47 frames. ], batch size: 20, lr: 3.45e-02,
2024-10-08 22:38:04,523 WARNING [optim.py:503] Scaling gradients by 0.021413935348391533, model_norm_threshold=109367802200064.0
2024-10-08 22:38:04,662 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.047e+31, grad_sumsq=2.088e+33, orig_rms_sq=5.014e-03
2024-10-08 22:38:05,755 WARNING [optim.py:503] Scaling gradients by 0.04173048213124275, model_norm_threshold=109367802200064.0
2024-10-08 22:38:05,894 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.738e+30, grad_sumsq=8.029e+31, orig_rms_sq=2.165e-02
2024-10-08 22:38:06,938 WARNING [optim.py:503] Scaling gradients by 2.7269348720437847e-05, model_norm_threshold=109367802200064.0
2024-10-08 22:38:07,077 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.534e+36, grad_sumsq=2.094e+38, orig_rms_sq=2.165e-02
2024-10-08 22:38:08,147 WARNING [optim.py:503] Scaling gradients by 0.07562714070081711, model_norm_threshold=109367802200064.0
2024-10-08 22:38:08,284 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.051e+29, grad_sumsq=2.795e+31, orig_rms_sq=2.165e-02
2024-10-08 22:38:09,363 WARNING [optim.py:503] Scaling gradients by 0.01188031304627657, model_norm_threshold=109367802200064.0
2024-10-08 22:38:09,504 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.155e+31, grad_sumsq=9.955e+32, orig_rms_sq=2.165e-02
2024-10-08 22:38:13,130 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=3234.0, ans=0.34840625000000003
2024-10-08 22:38:13,459 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=28.85 vs. limit=9.9255
2024-10-08 22:38:17,387 WARNING [optim.py:503] Scaling gradients by 0.0018194775329902768, model_norm_threshold=109367802200064.0
2024-10-08 22:38:17,529 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.384e+33, grad_sumsq=6.431e+34, orig_rms_sq=2.152e-02
2024-10-08 22:38:18,741 WARNING [optim.py:503] Scaling gradients by 0.003825969062745571, model_norm_threshold=109367802200064.0
2024-10-08 22:38:18,884 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.595e+32, grad_sumsq=6.979e+31, orig_rms_sq=3.719e+00
2024-10-08 22:38:28,575 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=14.95 vs. limit=8.71525
2024-10-08 22:38:30,058 WARNING [optim.py:503] Scaling gradients by 0.0076353419572114944, model_norm_threshold=109367802200064.0
2024-10-08 22:38:30,200 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.918e+31, grad_sumsq=1.552e+31, orig_rms_sq=3.812e+00
2024-10-08 22:38:31,261 WARNING [optim.py:503] Scaling gradients by 0.0059440722689032555, model_norm_threshold=109367802200064.0
2024-10-08 22:38:31,399 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.197e+31, grad_sumsq=3.881e+33, orig_rms_sq=2.112e-02
2024-10-08 22:38:33,645 WARNING [optim.py:503] Scaling gradients by 6.822888099122792e-05, model_norm_threshold=109367802200064.0
2024-10-08 22:38:33,785 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.800e+35, grad_sumsq=2.022e+35, orig_rms_sq=3.858e+00
2024-10-08 22:38:34,892 WARNING [optim.py:503] Scaling gradients by 0.06531034409999847, model_norm_threshold=109367802200064.0
2024-10-08 22:38:35,031 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.765e+29, grad_sumsq=1.105e+32, orig_rms_sq=5.218e-03
2024-10-08 22:38:36,141 WARNING [optim.py:503] Scaling gradients by 0.007219436578452587, model_norm_threshold=109367802200064.0
2024-10-08 22:38:36,282 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.766e+31, grad_sumsq=2.727e+33, orig_rms_sq=2.115e-02
2024-10-08 22:38:37,754 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.93 vs. limit=9.933
2024-10-08 22:38:43,000 WARNING [optim.py:503] Scaling gradients by 0.00032960818498395383, model_norm_threshold=109367802200064.0
2024-10-08 22:38:43,139 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.881e+34, grad_sumsq=1.378e+36, orig_rms_sq=2.092e-02
2024-10-08 22:38:45,545 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=3244.0, ans=0.26756
2024-10-08 22:38:46,417 WARNING [optim.py:503] Scaling gradients by 0.06429312378168106, model_norm_threshold=109367802200064.0
2024-10-08 22:38:46,557 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.395e+29, grad_sumsq=1.352e+30, orig_rms_sq=3.992e-01
2024-10-08 22:38:51,653 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=30.36 vs. limit=9.935500000000001
2024-10-08 22:38:52,329 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.min_abs, batch_count=3247.3333333333335, ans=0.24871000000000001
2024-10-08 22:38:53,468 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_skip_rate, batch_count=3247.3333333333335, ans=0.07822499999999999
2024-10-08 22:39:00,120 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.min_abs, batch_count=3250.6666666666665, ans=0.24876
2024-10-08 22:39:01,124 INFO [train.py:1154] Epoch 2, batch 2900, loss[loss=1.294, simple_loss=0.6571, pruned_loss=0.7627, ctc_loss=1.015, over 4747.00 frames. ], tot_loss[loss=1.264, simple_loss=0.6433, pruned_loss=0.7339, ctc_loss=1.042, over 965928.43 frames. ], batch size: 20, lr: 3.45e-02,
2024-10-08 22:39:06,614 WARNING [optim.py:503] Scaling gradients by 0.005739319603890181, model_norm_threshold=109367802200064.0
2024-10-08 22:39:06,751 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.55, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.980e+32, grad_sumsq=4.037e+34, orig_rms_sq=4.906e-03
2024-10-08 22:39:07,998 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=3250.6666666666665, ans=0.02686000000000001
2024-10-08 22:39:11,452 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer1.prob, batch_count=3254.0, ans=0.34746875
2024-10-08 22:39:23,052 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=6.70 vs. limit=8.7215
2024-10-08 22:39:35,750 WARNING [optim.py:503] Scaling gradients by 0.021382318809628487, model_norm_threshold=109367802200064.0
2024-10-08 22:39:35,889 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.297e+30, grad_sumsq=3.003e+32, orig_rms_sq=2.097e-02
2024-10-08 22:39:37,114 WARNING [optim.py:503] Scaling gradients by 0.0014137598918750882, model_norm_threshold=109367802200064.0
2024-10-08 22:39:37,252 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.033e+33, grad_sumsq=4.502e+34, orig_rms_sq=2.294e-02
2024-10-08 22:39:38,029 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=13.23 vs. limit=8.72275
2024-10-08 22:39:39,199 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=7.18 vs. limit=6.630333333333333
2024-10-08 22:39:41,644 WARNING [optim.py:503] Scaling gradients by 0.00367954489775002, model_norm_threshold=109367802200064.0
2024-10-08 22:39:41,782 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.164e+32, grad_sumsq=8.413e+31, orig_rms_sq=3.760e+00
2024-10-08 22:39:43,525 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=19.13 vs. limit=5.304266666666667
2024-10-08 22:39:43,954 WARNING [optim.py:503] Scaling gradients by 0.013173900544643402, model_norm_threshold=109367802200064.0
2024-10-08 22:39:44,094 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.609e+31, grad_sumsq=7.574e+32, orig_rms_sq=2.124e-02
2024-10-08 22:39:49,670 WARNING [optim.py:503] Scaling gradients by 0.07971148937940598, model_norm_threshold=109367802200064.0
2024-10-08 22:39:49,808 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.826e+29, grad_sumsq=1.238e+32, orig_rms_sq=4.705e-03
2024-10-08 22:39:52,022 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=5.48 vs. limit=4.6528
2024-10-08 22:39:54,356 WARNING [optim.py:503] Scaling gradients by 0.03565178066492081, model_norm_threshold=109367802200064.0
2024-10-08 22:39:54,494 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.252e+30, grad_sumsq=4.934e+32, orig_rms_sq=4.564e-03
2024-10-08 22:39:55,310 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=70.92 vs. limit=8.724
2024-10-08 22:39:55,456 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=8.78 vs. limit=8.724
2024-10-08 22:39:55,773 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.731e+11 1.127e+13 1.175e+14 7.692e+14 4.011e+18, threshold=2.350e+14, percent-clipped=51.0
2024-10-08 22:39:56,824 WARNING [optim.py:503] Scaling gradients by 0.0007266422617249191, model_norm_threshold=234969490259968.0
2024-10-08 22:39:56,961 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.860e+34, grad_sumsq=5.391e+33, orig_rms_sq=3.451e+00
2024-10-08 22:39:57,144 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=3267.3333333333335, ans=0.07957916666666667
2024-10-08 22:39:58,099 INFO [train.py:1154] Epoch 2, batch 2950, loss[loss=1.291, simple_loss=0.6483, pruned_loss=0.7565, ctc_loss=1.052, over 4799.00 frames. ], tot_loss[loss=1.264, simple_loss=0.643, pruned_loss=0.7348, ctc_loss=1.04, over 966518.31 frames. ], batch size: 19, lr: 3.44e-02,
2024-10-08 22:39:59,088 WARNING [optim.py:503] Scaling gradients by 0.08302648365497589, model_norm_threshold=234969490259968.0
2024-10-08 22:39:59,226 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.772e+30, grad_sumsq=6.237e+32, orig_rms_sq=4.444e-03
2024-10-08 22:40:04,871 WARNING [optim.py:503] Scaling gradients by 0.005997858475893736, model_norm_threshold=234969490259968.0
2024-10-08 22:40:05,011 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.197e+32, grad_sumsq=7.050e+34, orig_rms_sq=4.534e-03
2024-10-08 22:40:07,097 WARNING [optim.py:503] Scaling gradients by 0.0041891420260071754, model_norm_threshold=234969490259968.0
2024-10-08 22:40:07,235 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.848e+32, grad_sumsq=2.876e+34, orig_rms_sq=2.033e-02
2024-10-08 22:40:12,921 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=3270.6666666666665, ans=0.7855266666666667
2024-10-08 22:40:20,677 WARNING [optim.py:503] Scaling gradients by 0.014246475882828236, model_norm_threshold=234969490259968.0
2024-10-08 22:40:20,816 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.183e+31, grad_sumsq=1.074e+34, orig_rms_sq=4.828e-03
2024-10-08 22:40:20,943 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.nonlin_attention.balancer.min_positive, batch_count=3274.0, ans=0.21726
2024-10-08 22:40:21,851 WARNING [optim.py:503] Scaling gradients by 0.010585415177047253, model_norm_threshold=234969490259968.0
2024-10-08 22:40:21,991 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.186e+32, grad_sumsq=1.042e+34, orig_rms_sq=2.099e-02
2024-10-08 22:40:22,601 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=7.82 vs. limit=5.3096
2024-10-08 22:40:27,071 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=24.75 vs. limit=9.9555
2024-10-08 22:40:28,603 WARNING [optim.py:503] Scaling gradients by 0.09344319254159927, model_norm_threshold=234969490259968.0
2024-10-08 22:40:30,041 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.552e+30, grad_sumsq=3.204e+32, orig_rms_sq=4.843e-03
2024-10-08 22:40:36,711 WARNING [optim.py:503] Scaling gradients by 0.020502837374806404, model_norm_threshold=234969490259968.0
2024-10-08 22:40:36,850 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.610e+31, grad_sumsq=7.466e+33, orig_rms_sq=4.835e-03
2024-10-08 22:40:42,984 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=48.43 vs. limit=8.729
2024-10-08 22:40:44,611 WARNING [optim.py:503] Scaling gradients by 0.014729413203895092, model_norm_threshold=234969490259968.0
2024-10-08 22:40:44,751 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.295e+31, grad_sumsq=2.578e+33, orig_rms_sq=2.054e-02
2024-10-08 22:40:45,811 WARNING [optim.py:503] Scaling gradients by 0.09967422485351562, model_norm_threshold=234969490259968.0
2024-10-08 22:40:45,951 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.609e+30, grad_sumsq=5.434e+32, orig_rms_sq=4.802e-03
2024-10-08 22:40:46,989 WARNING [optim.py:503] Scaling gradients by 0.0431465283036232, model_norm_threshold=234969490259968.0
2024-10-08 22:40:47,150 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.379e+30, grad_sumsq=7.728e+29, orig_rms_sq=8.254e+00
2024-10-08 22:40:50,624 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.hidden_balancer.prob, batch_count=3280.6666666666665, ans=0.34621875
2024-10-08 22:40:50,833 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.3.self_attn_weights, loss-sum=9.703e+02
2024-10-08 22:40:52,878 WARNING [optim.py:503] Scaling gradients by 0.01252614427357912, model_norm_threshold=234969490259968.0
2024-10-08 22:40:53,016 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.762e+32, grad_sumsq=3.748e+34, orig_rms_sq=4.703e-03
2024-10-08 22:40:55,400 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=3284.0, ans=0.7850600000000001
2024-10-08 22:40:55,704 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.74 vs. limit=3.4926
2024-10-08 22:40:56,471 INFO [train.py:1154] Epoch 2, batch 3000, loss[loss=1.205, simple_loss=0.6072, pruned_loss=0.7034, ctc_loss=0.9896, over 4834.00 frames. ], tot_loss[loss=1.263, simple_loss=0.6427, pruned_loss=0.7341, ctc_loss=1.04, over 967210.30 frames. ], batch size: 21, lr: 3.43e-02,
2024-10-08 22:40:56,472 INFO [train.py:1177] Computing validation loss
2024-10-08 22:41:01,931 INFO [train.py:1186] Epoch 2, validation: loss=1.332, simple_loss=0.6848, pruned_loss=0.7701, ctc_loss=1.095, over 90464.00 frames.
2024-10-08 22:41:01,931 INFO [train.py:1187] Maximum memory allocated so far is 5927MB
2024-10-08 22:41:02,947 WARNING [optim.py:503] Scaling gradients by 0.008519216440618038, model_norm_threshold=234969490259968.0
2024-10-08 22:41:03,086 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.959e+32, grad_sumsq=9.455e+33, orig_rms_sq=2.072e-02
2024-10-08 22:41:04,175 WARNING [optim.py:503] Scaling gradients by 0.062568798661232, model_norm_threshold=234969490259968.0
2024-10-08 22:41:04,314 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.382e+30, grad_sumsq=2.642e+29, orig_rms_sq=9.017e+00
2024-10-08 22:41:06,269 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.68 vs. limit=3.4926
2024-10-08 22:41:09,117 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.ff3_skip_rate, batch_count=3284.0, ans=0.026109999999999994
2024-10-08 22:41:09,186 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.const_attention_rate, batch_count=3284.0, ans=0.065275
2024-10-08 22:41:11,635 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.33 vs. limit=6.6419999999999995
2024-10-08 22:41:12,138 WARNING [optim.py:503] Scaling gradients by 0.026684317737817764, model_norm_threshold=234969490259968.0
2024-10-08 22:41:12,276 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.162e+31, grad_sumsq=1.022e+33, orig_rms_sq=2.116e-02
2024-10-08 22:41:13,326 WARNING [optim.py:503] Scaling gradients by 0.005531312897801399, model_norm_threshold=234969490259968.0
2024-10-08 22:41:13,464 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.285e+32, grad_sumsq=2.025e+34, orig_rms_sq=2.116e-02
2024-10-08 22:41:18,139 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.bypass.skip_rate, batch_count=3287.3333333333335, ans=0.5
2024-10-08 22:41:19,672 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.41 vs. limit=6.643666666666666
2024-10-08 22:41:20,540 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=64.02 vs. limit=8.73275
2024-10-08 22:41:21,874 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=18.09 vs. limit=9.9655
2024-10-08 22:41:22,986 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=9.42 vs. limit=5.821833333333333
2024-10-08 22:41:25,482 WARNING [optim.py:503] Scaling gradients by 0.025277137756347656, model_norm_threshold=234969490259968.0
2024-10-08 22:41:25,626 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.096e+31, grad_sumsq=9.793e+32, orig_rms_sq=2.141e-02
2024-10-08 22:41:28,977 WARNING [optim.py:503] Scaling gradients by 0.021206744015216827, model_norm_threshold=234969490259968.0
2024-10-08 22:41:29,128 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.937e+31, grad_sumsq=1.076e+31, orig_rms_sq=3.660e+00
2024-10-08 22:41:38,240 WARNING [optim.py:503] Scaling gradients by 0.00012791996414307505, model_norm_threshold=234969490259968.0
2024-10-08 22:41:38,379 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.715e+36, grad_sumsq=4.784e+35, orig_rms_sq=3.586e+00
2024-10-08 22:41:39,475 WARNING [optim.py:503] Scaling gradients by 0.0025829242076724768, model_norm_threshold=234969490259968.0
2024-10-08 22:41:39,615 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.514e+33, grad_sumsq=5.462e+35, orig_rms_sq=4.604e-03
2024-10-08 22:41:39,754 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=3294.0, ans=0.07647499999999999
2024-10-08 22:41:40,693 WARNING [optim.py:503] Scaling gradients by 0.0279373936355114, model_norm_threshold=234969490259968.0
2024-10-08 22:41:40,831 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.912e+31, grad_sumsq=8.742e+32, orig_rms_sq=2.186e-02
2024-10-08 22:41:41,886 WARNING [optim.py:503] Scaling gradients by 0.0006090151146054268, model_norm_threshold=234969490259968.0
2024-10-08 22:41:42,023 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.305e+34, grad_sumsq=2.037e+34, orig_rms_sq=3.586e+00
2024-10-08 22:41:42,291 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward2.hidden_balancer.prob, batch_count=3294.0, ans=0.34559375000000003
2024-10-08 22:41:43,125 WARNING [optim.py:503] Scaling gradients by 0.0014799968339502811, model_norm_threshold=234969490259968.0
2024-10-08 22:41:43,263 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.138e+34, grad_sumsq=3.187e+33, orig_rms_sq=3.571e+00
2024-10-08 22:41:45,446 WARNING [optim.py:503] Scaling gradients by 0.05726565048098564, model_norm_threshold=234969490259968.0
2024-10-08 22:41:45,583 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.372e+30, grad_sumsq=2.345e+30, orig_rms_sq=3.571e+00
2024-10-08 22:41:46,107 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=14.16 vs. limit=8.73525
2024-10-08 22:41:52,040 WARNING [optim.py:503] Scaling gradients by 0.012302614748477936, model_norm_threshold=234969490259968.0
2024-10-08 22:41:52,179 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.216e+31, grad_sumsq=1.644e+34, orig_rms_sq=4.390e-03
2024-10-08 22:41:52,892 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=6.62 vs. limit=5.318933333333334
2024-10-08 22:41:54,857 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.29 vs. limit=6.648666666666667
2024-10-08 22:41:55,420 WARNING [optim.py:503] Scaling gradients by 0.006160305347293615, model_norm_threshold=234969490259968.0
2024-10-08 22:41:55,559 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.445e+32, grad_sumsq=9.803e+31, orig_rms_sq=3.515e+00
2024-10-08 22:41:56,390 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=18.56 vs. limit=8.7365
2024-10-08 22:41:56,748 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.160e+11 1.018e+13 1.072e+14 2.515e+15 1.837e+18, threshold=2.144e+14, percent-clipped=41.0
2024-10-08 22:41:57,942 WARNING [optim.py:503] Scaling gradients by 0.035499121993780136, model_norm_threshold=214362304282624.0
2024-10-08 22:41:58,078 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.056e+31, grad_sumsq=2.403e+33, orig_rms_sq=4.397e-03
2024-10-08 22:41:59,108 WARNING [optim.py:503] Scaling gradients by 0.00914083980023861, model_norm_threshold=214362304282624.0
2024-10-08 22:41:59,245 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.68, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.728e+32, grad_sumsq=8.479e+34, orig_rms_sq=4.397e-03
2024-10-08 22:41:59,290 INFO [train.py:1154] Epoch 2, batch 3050, loss[loss=1.332, simple_loss=0.67, pruned_loss=0.7859, ctc_loss=1.057, over 4749.00 frames. ], tot_loss[loss=1.262, simple_loss=0.6427, pruned_loss=0.733, ctc_loss=1.041, over 966712.08 frames. ], batch size: 19, lr: 3.43e-02,
2024-10-08 22:42:01,389 WARNING [optim.py:503] Scaling gradients by 0.002898066071793437, model_norm_threshold=214362304282624.0
2024-10-08 22:42:01,526 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.306e+33, grad_sumsq=6.291e+34, orig_rms_sq=2.077e-02
2024-10-08 22:42:05,431 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.83 vs. limit=8.73775
2024-10-08 22:42:06,873 WARNING [optim.py:503] Scaling gradients by 0.0019632207695394754, model_norm_threshold=214362304282624.0
2024-10-08 22:42:07,010 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.099e+33, grad_sumsq=8.940e+32, orig_rms_sq=3.467e+00
2024-10-08 22:42:07,537 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.56 vs. limit=5.825166666666666
2024-10-08 22:42:08,118 WARNING [optim.py:503] Scaling gradients by 6.756450602551922e-05, model_norm_threshold=214362304282624.0
2024-10-08 22:42:08,255 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.888e+36, grad_sumsq=8.330e+35, orig_rms_sq=3.467e+00
2024-10-08 22:42:08,489 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer1.min_positive, batch_count=3300.6666666666665, ans=0.03968541666666667
2024-10-08 22:42:09,500 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=3304.0, ans=0.345125
2024-10-08 22:42:09,510 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.const_attention_rate, batch_count=3304.0, ans=0.06415000000000001
2024-10-08 22:42:13,103 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=20.03 vs. limit=9.978
2024-10-08 22:42:17,007 WARNING [optim.py:503] Scaling gradients by 0.010030367411673069, model_norm_threshold=214362304282624.0
2024-10-08 22:42:17,143 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.898e+32, grad_sumsq=4.319e+34, orig_rms_sq=4.395e-03
2024-10-08 22:42:20,403 WARNING [optim.py:503] Scaling gradients by 0.08929348737001419, model_norm_threshold=214362304282624.0
2024-10-08 22:42:20,541 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.672e+30, grad_sumsq=7.735e+29, orig_rms_sq=3.454e+00
2024-10-08 22:42:20,718 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer1.prob, batch_count=3307.3333333333335, ans=0.34496875
2024-10-08 22:42:22,909 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=3307.3333333333335, ans=0.34496875
2024-10-08 22:42:23,776 WARNING [optim.py:503] Scaling gradients by 0.0004681813297793269, model_norm_threshold=214362304282624.0
2024-10-08 22:42:23,913 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.883e+34, grad_sumsq=1.998e+34, orig_rms_sq=3.445e+00
2024-10-08 22:42:26,204 WARNING [optim.py:503] Scaling gradients by 0.048461638391017914, model_norm_threshold=214362304282624.0
2024-10-08 22:42:26,340 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.372e+30, grad_sumsq=2.140e+30, orig_rms_sq=3.445e+00
2024-10-08 22:42:29,692 WARNING [optim.py:503] Scaling gradients by 0.033130303025245667, model_norm_threshold=214362304282624.0
2024-10-08 22:42:29,830 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.791e+30, grad_sumsq=2.863e+30, orig_rms_sq=3.419e+00
2024-10-08 22:42:30,474 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.96 vs. limit=3.4961
2024-10-08 22:42:31,214 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=3307.3333333333335, ans=0.07597499999999999
2024-10-08 22:42:31,675 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=14.52 vs. limit=8.74025
2024-10-08 22:42:32,001 WARNING [optim.py:503] Scaling gradients by 0.0008564955787733197, model_norm_threshold=214362304282624.0
2024-10-08 22:42:32,144 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.418e+34, grad_sumsq=6.808e+35, orig_rms_sq=2.082e-02
2024-10-08 22:42:32,713 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.16 vs. limit=9.983
2024-10-08 22:42:33,844 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=12.41 vs. limit=6.655333333333333
2024-10-08 22:42:34,300 WARNING [optim.py:503] Scaling gradients by 0.01771804504096508, model_norm_threshold=214362304282624.0
2024-10-08 22:42:34,438 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.644e+31, grad_sumsq=7.801e+30, orig_rms_sq=3.389e+00
2024-10-08 22:42:46,879 WARNING [optim.py:503] Scaling gradients by 0.08752680569887161, model_norm_threshold=214362304282624.0
2024-10-08 22:42:47,018 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.linear_pos.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.176e+30, grad_sumsq=1.304e+29, orig_rms_sq=9.015e+00
2024-10-08 22:42:49,152 WARNING [optim.py:503] Scaling gradients by 0.006077616009861231, model_norm_threshold=214362304282624.0
2024-10-08 22:42:49,290 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.627e+32, grad_sumsq=9.198e+32, orig_rms_sq=3.943e-01
2024-10-08 22:42:50,498 WARNING [optim.py:503] Scaling gradients by 0.0037496427539736032, model_norm_threshold=214362304282624.0
2024-10-08 22:42:50,637 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.403e+32, grad_sumsq=2.065e+35, orig_rms_sq=4.554e-03
2024-10-08 22:42:54,264 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward3.hidden_balancer.prob, batch_count=3314.0, ans=0.34465625
2024-10-08 22:42:55,090 WARNING [optim.py:503] Scaling gradients by 0.0004379914316814393, model_norm_threshold=214362304282624.0
2024-10-08 22:42:55,229 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.731e+34, grad_sumsq=1.225e+37, orig_rms_sq=4.678e-03
2024-10-08 22:42:56,415 INFO [train.py:1154] Epoch 2, batch 3100, loss[loss=1.229, simple_loss=0.6331, pruned_loss=0.6989, ctc_loss=1.066, over 4817.00 frames. ], tot_loss[loss=1.266, simple_loss=0.6437, pruned_loss=0.7357, ctc_loss=1.041, over 966447.38 frames. ], batch size: 38, lr: 3.42e-02,
2024-10-08 22:42:58,197 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=83.36 vs. limit=8.744
2024-10-08 22:43:03,151 WARNING [optim.py:503] Scaling gradients by 0.03531583398580551, model_norm_threshold=214362304282624.0
2024-10-08 22:43:03,289 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.689e+30, grad_sumsq=5.657e+30, orig_rms_sq=1.182e+00
2024-10-08 22:43:04,508 WARNING [optim.py:503] Scaling gradients by 0.09947695583105087, model_norm_threshold=214362304282624.0
2024-10-08 22:43:04,645 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.228e+29, grad_sumsq=4.325e+31, orig_rms_sq=2.133e-02
2024-10-08 22:43:05,404 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.45 vs. limit=9.988
2024-10-08 22:43:05,801 WARNING [optim.py:503] Scaling gradients by 0.06302053481340408, model_norm_threshold=214362304282624.0
2024-10-08 22:43:05,941 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.535e+30, grad_sumsq=7.375e+32, orig_rms_sq=4.793e-03
2024-10-08 22:43:07,292 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_skip_rate, batch_count=3320.6666666666665, ans=0.075475
2024-10-08 22:43:08,583 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=12.15 vs. limit=6.660333333333333
2024-10-08 22:43:11,137 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=11.65 vs. limit=8.74525
2024-10-08 22:43:11,406 WARNING [optim.py:503] Scaling gradients by 0.0009800560073927045, model_norm_threshold=214362304282624.0
2024-10-08 22:43:11,543 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.387e+33, grad_sumsq=4.360e+35, orig_rms_sq=2.153e-02
2024-10-08 22:43:13,709 WARNING [optim.py:503] Scaling gradients by 0.013390026986598969, model_norm_threshold=214362304282624.0
2024-10-08 22:43:13,847 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.681e+31, grad_sumsq=1.263e+31, orig_rms_sq=3.707e+00
2024-10-08 22:43:19,364 WARNING [optim.py:503] Scaling gradients by 0.002731521613895893, model_norm_threshold=214362304282624.0
2024-10-08 22:43:19,503 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.098e+33, grad_sumsq=5.029e+34, orig_rms_sq=2.184e-02
2024-10-08 22:43:26,106 WARNING [optim.py:503] Scaling gradients by 0.07457036525011063, model_norm_threshold=214362304282624.0
2024-10-08 22:43:26,242 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.037e+30, grad_sumsq=2.758e+29, orig_rms_sq=7.385e+00
2024-10-08 22:43:30,103 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.68 vs. limit=5.831833333333334
2024-10-08 22:43:31,669 WARNING [optim.py:503] Scaling gradients by 0.002677833428606391, model_norm_threshold=214362304282624.0
2024-10-08 22:43:31,809 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.098e+33, grad_sumsq=4.334e+35, orig_rms_sq=4.840e-03
2024-10-08 22:43:33,640 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.48 vs. limit=5.831833333333334
2024-10-08 22:43:35,379 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.prob, batch_count=3327.3333333333335, ans=0.34403125
2024-10-08 22:43:36,208 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.81 vs. limit=9.9955
2024-10-08 22:43:37,446 WARNING [optim.py:503] Scaling gradients by 0.0016674582147970796, model_norm_threshold=214362304282624.0
2024-10-08 22:43:37,593 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.755e+33, grad_sumsq=1.239e+35, orig_rms_sq=2.224e-02
2024-10-08 22:43:38,642 WARNING [optim.py:503] Scaling gradients by 0.03977144509553909, model_norm_threshold=214362304282624.0
2024-10-08 22:43:38,778 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.941e+30, grad_sumsq=3.121e+32, orig_rms_sq=2.224e-02
2024-10-08 22:43:45,590 WARNING [optim.py:503] Scaling gradients by 0.09316457062959671, model_norm_threshold=214362304282624.0
2024-10-08 22:43:45,726 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.066e+30, grad_sumsq=2.258e+32, orig_rms_sq=4.722e-03
2024-10-08 22:43:48,674 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=15.80 vs. limit=8.749
2024-10-08 22:43:49,765 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=18.25 vs. limit=9.998000000000001
2024-10-08 22:43:51,052 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.47 vs. limit=8.749
2024-10-08 22:43:51,086 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.64 vs. limit=3.4996
2024-10-08 22:43:51,556 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 5.966e+11 2.559e+13 1.964e+14 2.401e+15 3.173e+18, threshold=3.929e+14, percent-clipped=48.0
2024-10-08 22:43:53,746 INFO [train.py:1154] Epoch 2, batch 3150, loss[loss=1.235, simple_loss=0.6332, pruned_loss=0.7115, ctc_loss=1.034, over 4796.00 frames. ], tot_loss[loss=1.267, simple_loss=0.644, pruned_loss=0.737, ctc_loss=1.041, over 966706.57 frames. ], batch size: 40, lr: 3.42e-02,
2024-10-08 22:43:54,166 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=111.69 vs. limit=8.75025
2024-10-08 22:43:54,941 WARNING [optim.py:503] Scaling gradients by 0.010899830609560013, model_norm_threshold=392853578579968.0
2024-10-08 22:43:55,078 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.290e+32, grad_sumsq=2.800e+34, orig_rms_sq=2.246e-02
2024-10-08 22:43:55,227 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.prob, batch_count=3334.0, ans=0.34371874999999996
2024-10-08 22:43:55,297 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=3334.0, ans=0.08324999999999999
2024-10-08 22:43:56,580 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=16.39 vs. limit=10.0005
2024-10-08 22:43:57,258 WARNING [optim.py:503] Scaling gradients by 0.07649130374193192, model_norm_threshold=392853578579968.0
2024-10-08 22:43:57,398 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.297e+30, grad_sumsq=7.132e+29, orig_rms_sq=7.427e+00
2024-10-08 22:44:01,364 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=10.18 vs. limit=5.3336
2024-10-08 22:44:02,998 WARNING [optim.py:503] Scaling gradients by 0.03131403401494026, model_norm_threshold=392853578579968.0
2024-10-08 22:44:03,136 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.075e+31, grad_sumsq=4.129e+30, orig_rms_sq=7.448e+00
2024-10-08 22:44:05,689 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=3337.3333333333335, ans=0.06227499999999997
2024-10-08 22:44:15,759 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward3.hidden_balancer.prob, batch_count=3340.6666666666665, ans=0.34340625
2024-10-08 22:44:23,531 WARNING [optim.py:503] Scaling gradients by 0.04006838798522949, model_norm_threshold=392853578579968.0
2024-10-08 22:44:23,671 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.196e+31, grad_sumsq=5.314e+33, orig_rms_sq=4.133e-03
2024-10-08 22:44:30,358 WARNING [optim.py:503] Scaling gradients by 0.03643910214304924, model_norm_threshold=392853578579968.0
2024-10-08 22:44:30,496 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.537e+31, grad_sumsq=1.218e+33, orig_rms_sq=2.084e-02
2024-10-08 22:44:35,388 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=29.38 vs. limit=10.008
2024-10-08 22:44:36,257 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.prob, batch_count=3344.0, ans=0.34325
2024-10-08 22:44:37,155 WARNING [optim.py:503] Scaling gradients by 0.009716430678963661, model_norm_threshold=392853578579968.0
2024-10-08 22:44:37,292 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.444e+32, grad_sumsq=8.465e+34, orig_rms_sq=4.068e-03
2024-10-08 22:44:39,728 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=3347.3333333333335, ans=0.07447499999999999
2024-10-08 22:44:40,633 WARNING [optim.py:503] Scaling gradients by 0.005441030487418175, model_norm_threshold=392853578579968.0
2024-10-08 22:44:40,771 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.603e+32, grad_sumsq=2.361e+35, orig_rms_sq=4.068e-03
2024-10-08 22:44:42,476 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.80 vs. limit=6.673666666666667
2024-10-08 22:44:42,846 WARNING [optim.py:503] Scaling gradients by 0.03611605241894722, model_norm_threshold=392853578579968.0
2024-10-08 22:44:42,984 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.207e+31, grad_sumsq=1.276e+34, orig_rms_sq=4.080e-03
2024-10-08 22:44:44,084 WARNING [optim.py:503] Scaling gradients by 0.02214416116476059, model_norm_threshold=392853578579968.0
2024-10-08 22:44:44,220 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.775e+31, grad_sumsq=3.711e+33, orig_rms_sq=2.095e-02
2024-10-08 22:44:49,826 WARNING [optim.py:503] Scaling gradients by 0.017459625378251076, model_norm_threshold=392853578579968.0
2024-10-08 22:44:49,966 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.637e+32, grad_sumsq=4.839e+31, orig_rms_sq=3.383e+00
2024-10-08 22:44:51,099 INFO [train.py:1154] Epoch 2, batch 3200, loss[loss=1.347, simple_loss=0.6778, pruned_loss=0.7916, ctc_loss=1.085, over 4758.00 frames. ], tot_loss[loss=1.268, simple_loss=0.6443, pruned_loss=0.7378, ctc_loss=1.04, over 967163.60 frames. ], batch size: 20, lr: 3.41e-02,
2024-10-08 22:44:53,770 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=33.87 vs. limit=10.013
2024-10-08 22:44:54,784 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=19.08 vs. limit=10.013
2024-10-08 22:44:56,481 WARNING [optim.py:503] Scaling gradients by 0.002761286683380604, model_norm_threshold=392853578579968.0
2024-10-08 22:44:56,620 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.531e+33, grad_sumsq=3.080e+35, orig_rms_sq=2.120e-02
2024-10-08 22:44:59,463 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=3350.6666666666665, ans=10.013
2024-10-08 22:45:00,393 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.19 vs. limit=3.5026
2024-10-08 22:45:01,059 WARNING [optim.py:503] Scaling gradients by 0.00708313612267375, model_norm_threshold=392853578579968.0
2024-10-08 22:45:01,197 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.295e+33, grad_sumsq=3.721e+32, orig_rms_sq=3.481e+00
2024-10-08 22:45:01,743 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.84 vs. limit=8.75775
2024-10-08 22:45:03,961 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.62 vs. limit=6.677
2024-10-08 22:45:04,045 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.82 vs. limit=6.677
2024-10-08 22:45:06,866 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=3354.0, ans=0.07422499999999999
2024-10-08 22:45:07,993 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=3354.0, ans=0.25031000000000003
2024-10-08 22:45:08,389 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.81 vs. limit=6.677
2024-10-08 22:45:08,849 WARNING [optim.py:503] Scaling gradients by 0.0042734029702842236, model_norm_threshold=392853578579968.0
2024-10-08 22:45:08,988 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.256e+33, grad_sumsq=1.660e+33, orig_rms_sq=1.359e+00
2024-10-08 22:45:10,777 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=19.93 vs. limit=8.75775
2024-10-08 22:45:15,863 WARNING [optim.py:503] Scaling gradients by 0.018484802916646004, model_norm_threshold=392853578579968.0
2024-10-08 22:45:16,003 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.166e+32, grad_sumsq=2.702e+34, orig_rms_sq=4.315e-03
2024-10-08 22:45:21,352 WARNING [optim.py:503] Scaling gradients by 0.05299696698784828, model_norm_threshold=392853578579968.0
2024-10-08 22:45:21,490 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.292e+31, grad_sumsq=1.557e+30, orig_rms_sq=8.297e+00
2024-10-08 22:45:22,189 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=10.47 vs. limit=6.6786666666666665
2024-10-08 22:45:25,942 WARNING [optim.py:503] Scaling gradients by 0.0010075941681861877, model_norm_threshold=392853578579968.0
2024-10-08 22:45:26,081 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.210e+34, grad_sumsq=9.089e+33, orig_rms_sq=3.532e+00
2024-10-08 22:45:30,563 WARNING [optim.py:503] Scaling gradients by 0.0005550001515075564, model_norm_threshold=392853578579968.0
2024-10-08 22:45:30,701 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.451e+35, grad_sumsq=4.054e+34, orig_rms_sq=3.580e+00
2024-10-08 22:45:30,988 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_skip_rate, batch_count=3360.6666666666665, ans=0.07397499999999999
2024-10-08 22:45:31,812 WARNING [optim.py:503] Scaling gradients by 0.009192651137709618, model_norm_threshold=392853578579968.0
2024-10-08 22:45:31,951 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.110e+32, grad_sumsq=6.902e+34, orig_rms_sq=4.506e-03
2024-10-08 22:45:32,379 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=35.04 vs. limit=10.0205
2024-10-08 22:45:38,615 WARNING [optim.py:503] Scaling gradients by 0.0008832248859107494, model_norm_threshold=392853578579968.0
2024-10-08 22:45:38,754 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.484e+34, grad_sumsq=2.515e+36, orig_rms_sq=2.181e-02
2024-10-08 22:45:40,928 WARNING [optim.py:503] Scaling gradients by 0.01561304647475481, model_norm_threshold=392853578579968.0
2024-10-08 22:45:41,066 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.115e+32, grad_sumsq=2.777e+32, orig_rms_sq=4.015e-01
2024-10-08 22:45:43,747 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.67 vs. limit=5.841
2024-10-08 22:45:43,854 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.15 vs. limit=6.682
2024-10-08 22:45:44,339 WARNING [optim.py:503] Scaling gradients by 0.00040529691614210606, model_norm_threshold=392853578579968.0
2024-10-08 22:45:45,736 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.730e+35, grad_sumsq=7.808e+34, orig_rms_sq=3.497e+00
2024-10-08 22:45:46,756 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.704e+11 1.906e+13 2.509e+14 2.843e+15 9.693e+17, threshold=5.019e+14, percent-clipped=40.0
2024-10-08 22:45:46,756 WARNING [optim.py:503] Scaling gradients by 0.03617152199149132, model_norm_threshold=501874344067072.0
2024-10-08 22:45:46,892 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.938e+31, grad_sumsq=1.620e+34, orig_rms_sq=4.282e-03
2024-10-08 22:45:47,668 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.18 vs. limit=6.682
2024-10-08 22:45:48,675 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=6.55 vs. limit=5.346933333333333
2024-10-08 22:45:49,158 INFO [train.py:1154] Epoch 2, batch 3250, loss[loss=1.364, simple_loss=0.6953, pruned_loss=0.7943, ctc_loss=1.109, over 4860.00 frames. ], tot_loss[loss=1.267, simple_loss=0.6441, pruned_loss=0.7367, ctc_loss=1.039, over 967266.74 frames. ], batch size: 24, lr: 3.41e-02,
2024-10-08 22:45:54,072 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.min_positive, batch_count=3367.3333333333335, ans=0.07895416666666667
2024-10-08 22:45:54,579 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.64 vs. limit=8.76275
2024-10-08 22:45:57,173 WARNING [optim.py:503] Scaling gradients by 0.004349918104708195, model_norm_threshold=501874344067072.0
2024-10-08 22:45:57,311 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.777e+33, grad_sumsq=1.765e+35, orig_rms_sq=2.140e-02
2024-10-08 22:45:57,862 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=28.08 vs. limit=8.76275
2024-10-08 22:45:59,268 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.36 vs. limit=10.025500000000001
2024-10-08 22:45:59,998 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.97 vs. limit=8.764
2024-10-08 22:46:01,741 WARNING [optim.py:503] Scaling gradients by 0.07642358541488647, model_norm_threshold=501874344067072.0
2024-10-08 22:46:01,878 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.073e+31, grad_sumsq=2.596e+33, orig_rms_sq=4.132e-03
2024-10-08 22:46:05,405 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=3370.6666666666665, ans=0.07866666666666666
2024-10-08 22:46:05,918 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.08 vs. limit=5.842666666666666
2024-10-08 22:46:07,846 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.05 vs. limit=8.764
2024-10-08 22:46:09,210 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.90 vs. limit=6.685333333333333
2024-10-08 22:46:11,401 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.88 vs. limit=8.76525
2024-10-08 22:46:14,969 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.00 vs. limit=8.76525
2024-10-08 22:46:16,357 WARNING [optim.py:503] Scaling gradients by 0.014803254045546055, model_norm_threshold=501874344067072.0
2024-10-08 22:46:16,495 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.446e+32, grad_sumsq=1.182e+34, orig_rms_sq=2.070e-02
2024-10-08 22:46:17,721 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer1.prob, batch_count=3374.0, ans=0.34184375
2024-10-08 22:46:20,884 WARNING [optim.py:503] Scaling gradients by 0.08086300641298294, model_norm_threshold=501874344067072.0
2024-10-08 22:46:21,024 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.008e+30, grad_sumsq=4.312e+32, orig_rms_sq=2.089e-02
2024-10-08 22:46:21,187 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.prob, batch_count=3374.0, ans=0.34184375
2024-10-08 22:46:22,081 WARNING [optim.py:503] Scaling gradients by 0.018542474135756493, model_norm_threshold=501874344067072.0
2024-10-08 22:46:22,217 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.650e+32, grad_sumsq=1.269e+34, orig_rms_sq=2.089e-02
2024-10-08 22:46:23,313 WARNING [optim.py:503] Scaling gradients by 0.0046718548983335495, model_norm_threshold=501874344067072.0
2024-10-08 22:46:23,453 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.045e+33, grad_sumsq=9.282e+32, orig_rms_sq=3.280e+00
2024-10-08 22:46:31,112 WARNING [optim.py:503] Scaling gradients by 0.08432404696941376, model_norm_threshold=501874344067072.0
2024-10-08 22:46:31,251 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.548e+30, grad_sumsq=2.277e+30, orig_rms_sq=3.315e+00
2024-10-08 22:46:33,487 WARNING [optim.py:503] Scaling gradients by 0.020339490845799446, model_norm_threshold=501874344067072.0
2024-10-08 22:46:33,624 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.024e+32, grad_sumsq=4.577e+34, orig_rms_sq=4.422e-03
2024-10-08 22:46:45,897 INFO [train.py:1154] Epoch 2, batch 3300, loss[loss=1.276, simple_loss=0.6645, pruned_loss=0.733, ctc_loss=1.052, over 4840.00 frames. ], tot_loss[loss=1.263, simple_loss=0.6419, pruned_loss=0.7354, ctc_loss=1.035, over 967749.25 frames. ], batch size: 43, lr: 3.40e-02,
2024-10-08 22:46:50,247 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=18.95 vs. limit=8.769
2024-10-08 22:46:50,826 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=7.65 vs. limit=5.3536
2024-10-08 22:46:51,301 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=13.20 vs. limit=10.038
2024-10-08 22:46:51,533 WARNING [optim.py:503] Scaling gradients by 0.07443057000637054, model_norm_threshold=501874344067072.0
2024-10-08 22:46:51,671 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.076e+31, grad_sumsq=4.950e+32, orig_rms_sq=2.173e-02
2024-10-08 22:46:52,203 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.79 vs. limit=3.5076
2024-10-08 22:46:52,722 WARNING [optim.py:503] Scaling gradients by 0.014837970025837421, model_norm_threshold=501874344067072.0
2024-10-08 22:46:52,859 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.166e+32, grad_sumsq=1.127e+32, orig_rms_sq=3.698e+00
2024-10-08 22:46:53,866 WARNING [optim.py:503] Scaling gradients by 0.0020437578205019236, model_norm_threshold=501874344067072.0
2024-10-08 22:46:54,004 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.900e+34, grad_sumsq=8.743e+35, orig_rms_sq=2.173e-02
2024-10-08 22:47:00,292 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.self_attn1.whiten.whitening_limit, batch_count=3387.3333333333335, ans=10.0405
2024-10-08 22:47:02,000 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=3387.3333333333335, ans=0.34121875
2024-10-08 22:47:04,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.const_attention_rate, batch_count=3387.3333333333335, ans=0.059462499999999974
2024-10-08 22:47:08,252 WARNING [optim.py:503] Scaling gradients by 0.034172285348176956, model_norm_threshold=501874344067072.0
2024-10-08 22:47:08,391 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.494e+31, grad_sumsq=1.262e+31, orig_rms_sq=3.561e+00
2024-10-08 22:47:09,865 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=18.10 vs. limit=6.695333333333333
2024-10-08 22:47:15,017 WARNING [optim.py:503] Scaling gradients by 0.03456048667430878, model_norm_threshold=501874344067072.0
2024-10-08 22:47:15,157 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.707e+31, grad_sumsq=1.088e+34, orig_rms_sq=4.328e-03
2024-10-08 22:47:15,962 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=7.28 vs. limit=5.3562666666666665
2024-10-08 22:47:16,922 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=18.42 vs. limit=8.7715
2024-10-08 22:47:18,675 WARNING [optim.py:503] Scaling gradients by 0.019353991374373436, model_norm_threshold=501874344067072.0
2024-10-08 22:47:18,814 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.578e+32, grad_sumsq=6.087e+34, orig_rms_sq=4.236e-03
2024-10-08 22:47:20,788 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=11.39 vs. limit=8.77275
2024-10-08 22:47:26,632 WARNING [optim.py:503] Scaling gradients by 0.01034596748650074, model_norm_threshold=501874344067072.0
2024-10-08 22:47:26,768 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.085e+33, grad_sumsq=2.482e+35, orig_rms_sq=4.371e-03
2024-10-08 22:47:32,326 WARNING [optim.py:503] Scaling gradients by 0.046598900109529495, model_norm_threshold=501874344067072.0
2024-10-08 22:47:32,467 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.618e+31, grad_sumsq=2.168e+33, orig_rms_sq=2.131e-02
2024-10-08 22:47:33,696 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=3397.3333333333335, ans=0.04949747468305833
2024-10-08 22:47:40,581 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.652e+11 1.568e+13 1.466e+14 2.594e+15 2.456e+17, threshold=2.932e+14, percent-clipped=38.0
2024-10-08 22:47:41,357 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.01 vs. limit=3.5096
2024-10-08 22:47:42,865 WARNING [optim.py:503] Scaling gradients by 0.012758667580783367, model_norm_threshold=293230100873216.0
2024-10-08 22:47:43,005 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.497e+32, grad_sumsq=3.313e+34, orig_rms_sq=4.519e-03
2024-10-08 22:47:43,051 INFO [train.py:1154] Epoch 2, batch 3350, loss[loss=1.244, simple_loss=0.6436, pruned_loss=0.7092, ctc_loss=1.066, over 4799.00 frames. ], tot_loss[loss=1.262, simple_loss=0.6422, pruned_loss=0.7337, ctc_loss=1.037, over 967044.17 frames. ], batch size: 40, lr: 3.40e-02,
2024-10-08 22:47:44,054 WARNING [optim.py:503] Scaling gradients by 0.03852340206503868, model_norm_threshold=293230100873216.0
2024-10-08 22:47:44,192 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.567e+31, grad_sumsq=4.117e+30, orig_rms_sq=3.805e+00
2024-10-08 22:47:45,260 WARNING [optim.py:503] Scaling gradients by 0.0030656293965876102, model_norm_threshold=293230100873216.0
2024-10-08 22:47:45,399 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.461e+33, grad_sumsq=1.191e+35, orig_rms_sq=2.066e-02
2024-10-08 22:47:45,891 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=14.54 vs. limit=8.77525
2024-10-08 22:47:47,270 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.81 vs. limit=6.700333333333333
2024-10-08 22:47:48,190 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=11.93 vs. limit=6.700333333333333
2024-10-08 22:47:53,621 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=256, metric=7.95 vs. limit=8.7765
2024-10-08 22:47:57,570 WARNING [optim.py:503] Scaling gradients by 0.08843084424734116, model_norm_threshold=293230100873216.0
2024-10-08 22:47:57,707 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.918e+30, grad_sumsq=1.448e+32, orig_rms_sq=2.016e-02
2024-10-08 22:48:00,724 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=17.10 vs. limit=8.7765
2024-10-08 22:48:00,770 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.62 vs. limit=10.053
2024-10-08 22:48:02,233 WARNING [optim.py:503] Scaling gradients by 0.043743424117565155, model_norm_threshold=293230100873216.0
2024-10-08 22:48:02,370 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.821e+31, grad_sumsq=4.821e+30, orig_rms_sq=3.778e+00
2024-10-08 22:48:02,531 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=3404.0, ans=0.078725
2024-10-08 22:48:07,992 WARNING [optim.py:503] Scaling gradients by 0.024112829938530922, model_norm_threshold=293230100873216.0
2024-10-08 22:48:08,132 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.290e+31, grad_sumsq=1.571e+33, orig_rms_sq=2.093e-02
2024-10-08 22:48:09,225 WARNING [optim.py:503] Scaling gradients by 0.01058389712125063, model_norm_threshold=293230100873216.0
2024-10-08 22:48:09,365 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.540e+32, grad_sumsq=7.357e+33, orig_rms_sq=2.093e-02
2024-10-08 22:48:10,058 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.60 vs. limit=10.0555
2024-10-08 22:48:11,477 WARNING [optim.py:503] Scaling gradients by 0.09641897678375244, model_norm_threshold=293230100873216.0
2024-10-08 22:48:11,613 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.941e+30, grad_sumsq=9.272e+31, orig_rms_sq=2.093e-02
2024-10-08 22:48:11,764 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer2.prob, batch_count=3407.3333333333335, ans=0.34028125
2024-10-08 22:48:15,461 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=13.97 vs. limit=6.703666666666667
2024-10-08 22:48:17,372 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer2.min_abs, batch_count=3410.6666666666665, ans=0.25116
2024-10-08 22:48:18,167 WARNING [optim.py:503] Scaling gradients by 0.00034939267789013684, model_norm_threshold=293230100873216.0
2024-10-08 22:48:18,306 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.513e+35, grad_sumsq=1.183e+37, orig_rms_sq=2.125e-02
2024-10-08 22:48:21,407 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.21 vs. limit=10.058
2024-10-08 22:48:23,482 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.85 vs. limit=3.5116
2024-10-08 22:48:26,802 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=53.58 vs. limit=8.779
2024-10-08 22:48:27,301 WARNING [optim.py:503] Scaling gradients by 0.0061390274204313755, model_norm_threshold=293230100873216.0
2024-10-08 22:48:27,440 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.164e+33, grad_sumsq=2.517e+35, orig_rms_sq=4.625e-03
2024-10-08 22:48:29,639 WARNING [optim.py:503] Scaling gradients by 0.0017935645300894976, model_norm_threshold=293230100873216.0
2024-10-08 22:48:29,777 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.62, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.648e+34, grad_sumsq=3.563e+36, orig_rms_sq=4.625e-03
2024-10-08 22:48:30,871 WARNING [optim.py:503] Scaling gradients by 0.0011413233587518334, model_norm_threshold=293230100873216.0
2024-10-08 22:48:31,008 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.539e+34, grad_sumsq=7.200e+35, orig_rms_sq=2.138e-02
2024-10-08 22:48:31,176 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass_mid.scale_min, batch_count=3414.0, ans=0.78051
2024-10-08 22:48:33,833 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.50 vs. limit=8.78025
2024-10-08 22:48:40,235 INFO [train.py:1154] Epoch 2, batch 3400, loss[loss=1.272, simple_loss=0.6322, pruned_loss=0.7464, ctc_loss=1.048, over 4959.00 frames. ], tot_loss[loss=1.26, simple_loss=0.6413, pruned_loss=0.7322, ctc_loss=1.036, over 966834.69 frames. ], batch size: 19, lr: 3.39e-02,
2024-10-08 22:48:41,548 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=3417.3333333333335, ans=0.33981249999999996
2024-10-08 22:48:45,735 WARNING [optim.py:503] Scaling gradients by 3.8932044844841585e-05, model_norm_threshold=293230100873216.0
2024-10-08 22:48:45,874 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.367e+37, grad_sumsq=inf, orig_rms_sq=2.114e-02
2024-10-08 22:48:51,670 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3420.6666666666665, ans=0.2657933333333333
2024-10-08 22:49:01,509 WARNING [optim.py:503] Scaling gradients by 0.003502656938508153, model_norm_threshold=293230100873216.0
2024-10-08 22:49:01,648 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.206e+33, grad_sumsq=4.690e+35, orig_rms_sq=4.703e-03
2024-10-08 22:49:03,846 WARNING [optim.py:503] Scaling gradients by 0.004271689336746931, model_norm_threshold=293230100873216.0
2024-10-08 22:49:03,985 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.057e+33, grad_sumsq=4.935e+34, orig_rms_sq=2.142e-02
2024-10-08 22:49:05,543 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.69 vs. limit=10.068
2024-10-08 22:49:09,629 WARNING [optim.py:503] Scaling gradients by 0.016697842627763748, model_norm_threshold=293230100873216.0
2024-10-08 22:49:09,765 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.723e+31, grad_sumsq=1.855e+34, orig_rms_sq=4.703e-03
2024-10-08 22:49:11,915 WARNING [optim.py:503] Scaling gradients by 0.05604097247123718, model_norm_threshold=293230100873216.0
2024-10-08 22:49:12,053 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.181e+30, grad_sumsq=4.274e+32, orig_rms_sq=2.148e-02
2024-10-08 22:49:17,156 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=14.76 vs. limit=8.78525
2024-10-08 22:49:18,222 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=15.90 vs. limit=8.78525
2024-10-08 22:49:23,299 WARNING [optim.py:503] Scaling gradients by 0.021163886412978172, model_norm_threshold=293230100873216.0
2024-10-08 22:49:23,438 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.098e+31, grad_sumsq=2.381e+33, orig_rms_sq=2.141e-02
2024-10-08 22:49:31,866 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=7.77 vs. limit=5.3722666666666665
2024-10-08 22:49:33,676 WARNING [optim.py:503] Scaling gradients by 0.017046546563506126, model_norm_threshold=293230100873216.0
2024-10-08 22:49:33,814 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.274e+31, grad_sumsq=3.454e+33, orig_rms_sq=2.106e-02
2024-10-08 22:49:34,862 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.790e+11 1.480e+13 1.198e+14 1.360e+15 7.532e+18, threshold=2.396e+14, percent-clipped=38.0
2024-10-08 22:49:35,268 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=12.83 vs. limit=10.073
2024-10-08 22:49:37,150 INFO [train.py:1154] Epoch 2, batch 3450, loss[loss=1.256, simple_loss=0.6439, pruned_loss=0.7166, ctc_loss=1.089, over 4836.00 frames. ], tot_loss[loss=1.26, simple_loss=0.641, pruned_loss=0.7322, ctc_loss=1.037, over 967029.08 frames. ], batch size: 43, lr: 3.39e-02,
2024-10-08 22:49:38,179 WARNING [optim.py:503] Scaling gradients by 0.0699671134352684, model_norm_threshold=239605286699008.0
2024-10-08 22:49:38,320 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.109e+30, grad_sumsq=1.013e+32, orig_rms_sq=2.083e-02
2024-10-08 22:49:38,573 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.ff3_skip_rate, batch_count=3434.0, ans=0.02273499999999999
2024-10-08 22:49:38,973 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=13.05 vs. limit=8.787749999999999
2024-10-08 22:49:42,829 WARNING [optim.py:503] Scaling gradients by 0.0013531807344406843, model_norm_threshold=239605286699008.0
2024-10-08 22:49:42,966 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.672e+33, grad_sumsq=4.200e+35, orig_rms_sq=2.065e-02
2024-10-08 22:49:45,081 WARNING [optim.py:503] Scaling gradients by 0.08806643635034561, model_norm_threshold=239605286699008.0
2024-10-08 22:49:45,219 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.563e+30, grad_sumsq=7.640e+31, orig_rms_sq=2.046e-02
2024-10-08 22:49:45,783 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.45 vs. limit=6.7170000000000005
2024-10-08 22:49:46,303 WARNING [optim.py:503] Scaling gradients by 0.025836806744337082, model_norm_threshold=239605286699008.0
2024-10-08 22:49:46,441 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.030e+31, grad_sumsq=5.773e+30, orig_rms_sq=3.516e+00
2024-10-08 22:49:47,060 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.63 vs. limit=3.5151
2024-10-08 22:49:48,633 WARNING [optim.py:503] Scaling gradients by 0.05780632793903351, model_norm_threshold=239605286699008.0
2024-10-08 22:49:48,770 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.095e+30, grad_sumsq=2.018e+32, orig_rms_sq=2.029e-02
2024-10-08 22:49:49,544 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=8.19 vs. limit=5.859333333333334
2024-10-08 22:49:49,802 WARNING [optim.py:503] Scaling gradients by 0.00204665819182992, model_norm_threshold=239605286699008.0
2024-10-08 22:49:49,942 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.687e+33, grad_sumsq=1.817e+35, orig_rms_sq=2.029e-02
2024-10-08 22:49:52,396 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=3437.3333333333335, ans=0.0703333333333333
2024-10-08 22:49:53,276 WARNING [optim.py:503] Scaling gradients by 0.016382010653614998, model_norm_threshold=239605286699008.0
2024-10-08 22:49:53,415 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.230e+31, grad_sumsq=3.085e+33, orig_rms_sq=2.020e-02
2024-10-08 22:49:56,608 WARNING [optim.py:503] Scaling gradients by 0.007409228477627039, model_norm_threshold=239605286699008.0
2024-10-08 22:49:56,746 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.440e+32, grad_sumsq=4.963e+31, orig_rms_sq=8.945e+00
2024-10-08 22:49:57,955 WARNING [optim.py:503] Scaling gradients by 0.04348120838403702, model_norm_threshold=239605286699008.0
2024-10-08 22:49:58,093 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.13, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.055e+30, grad_sumsq=2.018e+32, orig_rms_sq=2.010e-02
2024-10-08 22:50:03,716 WARNING [optim.py:503] Scaling gradients by 0.013454937376081944, model_norm_threshold=239605286699008.0
2024-10-08 22:50:03,856 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.389e+31, grad_sumsq=3.173e+33, orig_rms_sq=2.014e-02
2024-10-08 22:50:07,119 WARNING [optim.py:503] Scaling gradients by 0.0008825344848446548, model_norm_threshold=239605286699008.0
2024-10-08 22:50:07,257 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.546e+34, grad_sumsq=7.678e+35, orig_rms_sq=2.014e-02
2024-10-08 22:50:09,704 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer1.prob, batch_count=3440.6666666666665, ans=0.33871875
2024-10-08 22:50:13,989 WARNING [optim.py:503] Scaling gradients by 0.00027561208116821945, model_norm_threshold=239605286699008.0
2024-10-08 22:50:14,128 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.557e+35, grad_sumsq=8.390e+37, orig_rms_sq=4.240e-03
2024-10-08 22:50:16,412 WARNING [optim.py:503] Scaling gradients by 0.020556924864649773, model_norm_threshold=239605286699008.0
2024-10-08 22:50:16,550 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.393e+31, grad_sumsq=1.696e+33, orig_rms_sq=2.000e-02
2024-10-08 22:50:17,905 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.min_abs, batch_count=3444.0, ans=0.25166
2024-10-08 22:50:22,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=3447.3333333333335, ans=7.154583333333333
2024-10-08 22:50:23,709 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.30 vs. limit=10.0855
2024-10-08 22:50:25,434 WARNING [optim.py:503] Scaling gradients by 0.0003674479085020721, model_norm_threshold=239605286699008.0
2024-10-08 22:50:25,571 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.419e+35, grad_sumsq=3.364e+37, orig_rms_sq=4.217e-03
2024-10-08 22:50:27,389 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=19.55 vs. limit=8.79275
2024-10-08 22:50:28,885 WARNING [optim.py:503] Scaling gradients by 0.001359860529191792, model_norm_threshold=239605286699008.0
2024-10-08 22:50:29,023 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.567e+33, grad_sumsq=4.291e+35, orig_rms_sq=1.997e-02
2024-10-08 22:50:29,636 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.22 vs. limit=6.7236666666666665
2024-10-08 22:50:34,679 INFO [train.py:1154] Epoch 2, batch 3500, loss[loss=1.291, simple_loss=0.6469, pruned_loss=0.7598, ctc_loss=1.038, over 4883.00 frames. ], tot_loss[loss=1.256, simple_loss=0.639, pruned_loss=0.7296, ctc_loss=1.034, over 967387.59 frames. ], batch size: 19, lr: 3.38e-02,
2024-10-08 22:50:35,695 WARNING [optim.py:503] Scaling gradients by 0.020223787054419518, model_norm_threshold=239605286699008.0
2024-10-08 22:50:35,833 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.491e+31, grad_sumsq=1.063e+34, orig_rms_sq=4.225e-03
2024-10-08 22:50:41,178 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.00 vs. limit=10.088000000000001
2024-10-08 22:50:41,599 WARNING [optim.py:503] Scaling gradients by 0.011588575318455696, model_norm_threshold=239605286699008.0
2024-10-08 22:50:41,735 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.844e+32, grad_sumsq=4.419e+34, orig_rms_sq=4.173e-03
2024-10-08 22:50:42,793 WARNING [optim.py:503] Scaling gradients by 0.00023438446805812418, model_norm_threshold=239605286699008.0
2024-10-08 22:50:42,931 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.69, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.184e+35, grad_sumsq=1.722e+38, orig_rms_sq=4.173e-03
2024-10-08 22:50:43,983 WARNING [optim.py:503] Scaling gradients by 0.0014446834102272987, model_norm_threshold=239605286699008.0
2024-10-08 22:50:44,122 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.117e+33, grad_sumsq=3.501e+35, orig_rms_sq=2.033e-02
2024-10-08 22:50:49,073 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=44.82 vs. limit=10.0905
2024-10-08 22:50:49,681 WARNING [optim.py:503] Scaling gradients by 0.0673522874712944, model_norm_threshold=239605286699008.0
2024-10-08 22:50:49,819 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.525e+30, grad_sumsq=1.737e+32, orig_rms_sq=2.030e-02
2024-10-08 22:50:55,428 WARNING [optim.py:503] Scaling gradients by 0.018830176442861557, model_norm_threshold=239605286699008.0
2024-10-08 22:50:55,569 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.887e+31, grad_sumsq=1.672e+34, orig_rms_sq=4.119e-03
2024-10-08 22:51:00,767 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.03 vs. limit=3.5186
2024-10-08 22:51:03,008 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=19.17 vs. limit=8.7965
2024-10-08 22:51:04,517 WARNING [optim.py:503] Scaling gradients by 0.012890259735286236, model_norm_threshold=239605286699008.0
2024-10-08 22:51:04,657 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.112e+31, grad_sumsq=3.527e+33, orig_rms_sq=2.017e-02
2024-10-08 22:51:05,720 WARNING [optim.py:503] Scaling gradients by 0.012110056355595589, model_norm_threshold=239605286699008.0
2024-10-08 22:51:05,860 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.441e+31, grad_sumsq=3.689e+33, orig_rms_sq=2.017e-02
2024-10-08 22:51:13,831 WARNING [optim.py:503] Scaling gradients by 0.004028039518743753, model_norm_threshold=239605286699008.0
2024-10-08 22:51:13,980 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.044e+33, grad_sumsq=5.171e+34, orig_rms_sq=2.020e-02
2024-10-08 22:51:15,021 WARNING [optim.py:503] Scaling gradients by 8.789591811364517e-05, model_norm_threshold=239605286699008.0
2024-10-08 22:51:15,167 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.873e+36, grad_sumsq=1.423e+38, orig_rms_sq=2.020e-02
2024-10-08 22:51:17,437 WARNING [optim.py:503] Scaling gradients by 0.0878617912530899, model_norm_threshold=239605286699008.0
2024-10-08 22:51:17,578 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.054e+30, grad_sumsq=1.019e+32, orig_rms_sq=2.015e-02
2024-10-08 22:51:18,829 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=3460.6666666666665, ans=7.162916666666666
2024-10-08 22:51:18,861 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=3460.6666666666665, ans=0.2653933333333333
2024-10-08 22:51:18,888 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff3_skip_rate, batch_count=3460.6666666666665, ans=0.022135000000000002
2024-10-08 22:51:20,893 WARNING [optim.py:503] Scaling gradients by 0.0006912125973030925, model_norm_threshold=239605286699008.0
2024-10-08 22:51:21,043 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.615e+34, grad_sumsq=1.298e+36, orig_rms_sq=2.015e-02
2024-10-08 22:51:21,575 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.24 vs. limit=5.866
2024-10-08 22:51:23,659 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.attention_skip_rate, batch_count=3464.0, ans=0.0701
2024-10-08 22:51:25,163 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=32.96 vs. limit=10.097999999999999
2024-10-08 22:51:26,248 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=31.41 vs. limit=8.799
2024-10-08 22:51:29,808 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=12.81 vs. limit=10.097999999999999
2024-10-08 22:51:30,092 WARNING [optim.py:503] Scaling gradients by 0.0010913260048255324, model_norm_threshold=239605286699008.0
2024-10-08 22:51:30,231 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.077e+34, grad_sumsq=5.474e+35, orig_rms_sq=1.968e-02
2024-10-08 22:51:31,555 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.264e+12 4.362e+13 1.905e+14 3.557e+15 2.726e+18, threshold=3.810e+14, percent-clipped=48.0
2024-10-08 22:51:32,830 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.ff2_skip_rate, batch_count=3467.3333333333335, ans=0.02198499999999999
2024-10-08 22:51:33,706 WARNING [optim.py:503] Scaling gradients by 0.026772262528538704, model_norm_threshold=381044431781888.0
2024-10-08 22:51:33,861 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.295e+31, grad_sumsq=9.815e+33, orig_rms_sq=4.376e-03
2024-10-08 22:51:33,907 INFO [train.py:1154] Epoch 2, batch 3550, loss[loss=1.179, simple_loss=0.5982, pruned_loss=0.6784, ctc_loss=1.01, over 4795.00 frames. ], tot_loss[loss=1.26, simple_loss=0.6411, pruned_loss=0.7321, ctc_loss=1.036, over 967440.72 frames. ], batch size: 29, lr: 3.37e-02,
2024-10-08 22:51:35,097 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=3467.3333333333335, ans=0.33746875
2024-10-08 22:51:40,768 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.bypass.skip_rate, batch_count=3467.3333333333335, ans=0.07
2024-10-08 22:51:42,738 WARNING [optim.py:503] Scaling gradients by 0.0603940412402153, model_norm_threshold=381044431781888.0
2024-10-08 22:51:42,877 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.152e+31, grad_sumsq=2.631e+33, orig_rms_sq=4.381e-03
2024-10-08 22:51:48,787 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.bypass.skip_rate, batch_count=3470.6666666666665, ans=0.09899494936611666
2024-10-08 22:51:51,268 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=9.56 vs. limit=5.867666666666667
2024-10-08 22:51:51,921 WARNING [optim.py:503] Scaling gradients by 0.0177720095962286, model_norm_threshold=381044431781888.0
2024-10-08 22:51:52,060 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.949e+31, grad_sumsq=5.121e+33, orig_rms_sq=1.943e-02
2024-10-08 22:51:55,316 WARNING [optim.py:503] Scaling gradients by 8.009598241187632e-05, model_norm_threshold=381044431781888.0
2024-10-08 22:51:55,453 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.303e+36, grad_sumsq=2.724e+38, orig_rms_sq=1.947e-02
2024-10-08 22:51:56,508 WARNING [optim.py:503] Scaling gradients by 0.0113534452393651, model_norm_threshold=381044431781888.0
2024-10-08 22:51:56,647 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.514e+32, grad_sumsq=6.998e+31, orig_rms_sq=3.592e+00
2024-10-08 22:51:57,327 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=27.68 vs. limit=10.1055
2024-10-08 22:52:01,132 WARNING [optim.py:503] Scaling gradients by 0.0008161933510564268, model_norm_threshold=381044431781888.0
2024-10-08 22:52:01,270 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.48, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.050e+35, grad_sumsq=2.893e+34, orig_rms_sq=3.631e+00
2024-10-08 22:52:03,551 WARNING [optim.py:503] Scaling gradients by 0.0008104981388896704, model_norm_threshold=381044431781888.0
2024-10-08 22:52:03,690 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.749e+34, grad_sumsq=1.583e+34, orig_rms_sq=3.631e+00
2024-10-08 22:52:04,716 WARNING [optim.py:503] Scaling gradients by 0.02017158456146717, model_norm_threshold=381044431781888.0
2024-10-08 22:52:04,855 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.56, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.000e+32, grad_sumsq=5.509e+31, orig_rms_sq=3.631e+00
2024-10-08 22:52:10,886 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.92 vs. limit=8.804
2024-10-08 22:52:12,578 WARNING [optim.py:503] Scaling gradients by 0.04149273410439491, model_norm_threshold=381044431781888.0
2024-10-08 22:52:12,717 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.802e+31, grad_sumsq=9.374e+32, orig_rms_sq=1.922e-02
2024-10-08 22:52:13,743 WARNING [optim.py:503] Scaling gradients by 0.07582984864711761, model_norm_threshold=381044431781888.0
2024-10-08 22:52:13,883 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.067e+30, grad_sumsq=2.636e+32, orig_rms_sq=1.922e-02
2024-10-08 22:52:16,143 WARNING [optim.py:503] Scaling gradients by 0.056956056505441666, model_norm_threshold=381044431781888.0
2024-10-08 22:52:16,280 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.051e+31, grad_sumsq=2.268e+33, orig_rms_sq=4.634e-03
2024-10-08 22:52:17,318 WARNING [optim.py:503] Scaling gradients by 0.0006823830190114677, model_norm_threshold=381044431781888.0
2024-10-08 22:52:17,457 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.907e+34, grad_sumsq=5.145e+36, orig_rms_sq=1.925e-02
2024-10-08 22:52:31,073 WARNING [optim.py:503] Scaling gradients by 0.0025663336273282766, model_norm_threshold=381044431781888.0
2024-10-08 22:52:31,214 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.655e+33, grad_sumsq=2.458e+35, orig_rms_sq=1.894e-02
2024-10-08 22:52:31,260 INFO [train.py:1154] Epoch 2, batch 3600, loss[loss=1.332, simple_loss=0.6656, pruned_loss=0.7838, ctc_loss=1.079, over 4925.00 frames. ], tot_loss[loss=1.261, simple_loss=0.6416, pruned_loss=0.733, ctc_loss=1.037, over 967477.28 frames. ], batch size: 20, lr: 3.37e-02,
2024-10-08 22:52:33,369 WARNING [optim.py:503] Scaling gradients by 0.003825779538601637, model_norm_threshold=381044431781888.0
2024-10-08 22:52:33,507 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.688e+33, grad_sumsq=7.289e+32, orig_rms_sq=3.687e+00
2024-10-08 22:52:34,227 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.83 vs. limit=3.5225999999999997
2024-10-08 22:52:37,199 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.balancer.prob, batch_count=3484.0, ans=0.33668750000000003
2024-10-08 22:52:38,287 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=1.517e+01
2024-10-08 22:52:38,902 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=6.70 vs. limit=6.742
2024-10-08 22:52:44,508 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.62 vs. limit=10.1155
2024-10-08 22:52:45,936 WARNING [optim.py:503] Scaling gradients by 0.011102457530796528, model_norm_threshold=381044431781888.0
2024-10-08 22:52:46,075 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.342e+32, grad_sumsq=1.208e+34, orig_rms_sq=1.939e-02
2024-10-08 22:52:47,172 WARNING [optim.py:503] Scaling gradients by 0.0006423419108614326, model_norm_threshold=381044431781888.0
2024-10-08 22:52:47,309 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.071e+35, grad_sumsq=5.526e+36, orig_rms_sq=1.939e-02
2024-10-08 22:52:50,463 WARNING [optim.py:503] Scaling gradients by 0.009129207581281662, model_norm_threshold=381044431781888.0
2024-10-08 22:52:50,600 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.417e+32, grad_sumsq=1.756e+34, orig_rms_sq=1.945e-02
2024-10-08 22:52:52,721 WARNING [optim.py:503] Scaling gradients by 0.0014658495783805847, model_norm_threshold=381044431781888.0
2024-10-08 22:52:52,861 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.551e+34, grad_sumsq=7.971e+35, orig_rms_sq=1.945e-02
2024-10-08 22:52:57,248 WARNING [optim.py:503] Scaling gradients by 0.021407868713140488, model_norm_threshold=381044431781888.0
2024-10-08 22:52:57,385 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.570e+31, grad_sumsq=4.442e+33, orig_rms_sq=1.929e-02
2024-10-08 22:52:57,643 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=3490.6666666666665, ans=0.336375
2024-10-08 22:52:59,652 WARNING [optim.py:503] Scaling gradients by 0.0035535399802029133, model_norm_threshold=381044431781888.0
2024-10-08 22:52:59,788 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.816e+33, grad_sumsq=2.026e+32, orig_rms_sq=8.965e+00
2024-10-08 22:53:02,255 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_skip_rate, batch_count=3490.6666666666665, ans=0.0691
2024-10-08 22:53:06,503 WARNING [optim.py:503] Scaling gradients by 0.010428929701447487, model_norm_threshold=381044431781888.0
2024-10-08 22:53:06,642 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.57, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.673e+32, grad_sumsq=1.685e+35, orig_rms_sq=4.554e-03
2024-10-08 22:53:07,722 WARNING [optim.py:503] Scaling gradients by 0.014334309846162796, model_norm_threshold=381044431781888.0
2024-10-08 22:53:07,861 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.272e+32, grad_sumsq=6.532e+33, orig_rms_sq=1.947e-02
2024-10-08 22:53:16,185 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.63 vs. limit=8.8115
2024-10-08 22:53:17,926 WARNING [optim.py:503] Scaling gradients by 0.08623103052377701, model_norm_threshold=381044431781888.0
2024-10-08 22:53:18,063 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.290e+30, grad_sumsq=1.161e+30, orig_rms_sq=3.694e+00
2024-10-08 22:53:18,290 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=3497.3333333333335, ans=0.3360625
2024-10-08 22:53:18,305 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=3497.3333333333335, ans=0.3360625
2024-10-08 22:53:20,510 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.1.encoder.layers.1.self_attn_weights, loss-sum=1.674e+06
2024-10-08 22:53:21,385 WARNING [optim.py:503] Scaling gradients by 0.001061373739503324, model_norm_threshold=381044431781888.0
2024-10-08 22:53:21,524 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.269e+34, grad_sumsq=2.151e+36, orig_rms_sq=1.984e-02
2024-10-08 22:53:22,571 WARNING [optim.py:503] Scaling gradients by 0.011779386550188065, model_norm_threshold=381044431781888.0
2024-10-08 22:53:22,708 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.486e+32, grad_sumsq=1.253e+34, orig_rms_sq=1.984e-02
2024-10-08 22:53:23,744 WARNING [optim.py:503] Scaling gradients by 0.013871045783162117, model_norm_threshold=381044431781888.0
2024-10-08 22:53:23,883 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.845e+32, grad_sumsq=8.122e+34, orig_rms_sq=4.734e-03
2024-10-08 22:53:26,039 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.955e+11 2.701e+13 2.771e+14 5.025e+15 4.757e+18, threshold=5.542e+14, percent-clipped=45.0
2024-10-08 22:53:27,330 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=3500.6666666666665, ans=0.06872500000000001
2024-10-08 22:53:28,418 INFO [train.py:1154] Epoch 2, batch 3650, loss[loss=1.317, simple_loss=0.6804, pruned_loss=0.7595, ctc_loss=1.085, over 4851.00 frames. ], tot_loss[loss=1.264, simple_loss=0.6429, pruned_loss=0.7352, ctc_loss=1.037, over 967979.47 frames. ], batch size: 31, lr: 3.36e-02,
2024-10-08 22:53:29,423 WARNING [optim.py:503] Scaling gradients by 0.03320782631635666, model_norm_threshold=554231840899072.0
2024-10-08 22:53:29,561 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.151e+31, grad_sumsq=3.617e+33, orig_rms_sq=1.977e-02
2024-10-08 22:53:29,801 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module2.balancer2.prob, batch_count=3500.6666666666665, ans=0.33590624999999996
2024-10-08 22:53:31,634 WARNING [optim.py:503] Scaling gradients by 0.05718513950705528, model_norm_threshold=554231840899072.0
2024-10-08 22:53:31,772 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.789e+31, grad_sumsq=6.154e+33, orig_rms_sq=4.532e-03
2024-10-08 22:53:42,697 WARNING [optim.py:503] Scaling gradients by 0.004354427568614483, model_norm_threshold=554231840899072.0
2024-10-08 22:53:42,835 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.873e+33, grad_sumsq=2.005e+35, orig_rms_sq=1.932e-02
2024-10-08 22:53:43,354 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.95 vs. limit=10.128
2024-10-08 22:53:45,055 WARNING [optim.py:503] Scaling gradients by 0.008899382315576077, model_norm_threshold=554231840899072.0
2024-10-08 22:53:45,194 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.678e+32, grad_sumsq=1.571e+35, orig_rms_sq=4.251e-03
2024-10-08 22:53:46,302 WARNING [optim.py:503] Scaling gradients by 0.01642957702279091, model_norm_threshold=554231840899072.0
2024-10-08 22:53:46,438 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.372e+32, grad_sumsq=5.580e+34, orig_rms_sq=4.251e-03
2024-10-08 22:53:47,071 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.72 vs. limit=3.5256
2024-10-08 22:53:50,401 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.91 vs. limit=5.876833333333334
2024-10-08 22:54:01,997 WARNING [optim.py:503] Scaling gradients by 0.05299825593829155, model_norm_threshold=554231840899072.0
2024-10-08 22:54:02,138 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.781e+31, grad_sumsq=8.591e+33, orig_rms_sq=4.401e-03
2024-10-08 22:54:03,176 WARNING [optim.py:503] Scaling gradients by 0.04708937555551529, model_norm_threshold=554231840899072.0
2024-10-08 22:54:03,314 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.766e+31, grad_sumsq=1.310e+34, orig_rms_sq=4.401e-03
2024-10-08 22:54:04,429 WARNING [optim.py:503] Scaling gradients by 0.04380803927779198, model_norm_threshold=554231840899072.0
2024-10-08 22:54:04,567 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.305e+31, grad_sumsq=3.663e+30, orig_rms_sq=9.021e+00
2024-10-08 22:54:07,898 WARNING [optim.py:503] Scaling gradients by 0.019492797553539276, model_norm_threshold=554231840899072.0
2024-10-08 22:54:08,037 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.968e+32, grad_sumsq=4.480e+34, orig_rms_sq=4.393e-03
2024-10-08 22:54:08,403 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=9.85 vs. limit=5.4042666666666666
2024-10-08 22:54:14,557 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=7.05 vs. limit=5.4056
2024-10-08 22:54:17,059 WARNING [optim.py:503] Scaling gradients by 0.023655975237488747, model_norm_threshold=554231840899072.0
2024-10-08 22:54:17,198 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.181e+32, grad_sumsq=1.317e+31, orig_rms_sq=8.973e+00
2024-10-08 22:54:18,264 WARNING [optim.py:503] Scaling gradients by 0.008328985422849655, model_norm_threshold=554231840899072.0
2024-10-08 22:54:18,403 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.52, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.312e+33, grad_sumsq=5.250e+35, orig_rms_sq=4.404e-03
2024-10-08 22:54:21,773 WARNING [optim.py:503] Scaling gradients by 5.756442988058552e-05, model_norm_threshold=554231840899072.0
2024-10-08 22:54:21,914 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.051e+37, grad_sumsq=inf, orig_rms_sq=1.982e-02
2024-10-08 22:54:22,951 WARNING [optim.py:503] Scaling gradients by 0.006961558945477009, model_norm_threshold=554231840899072.0
2024-10-08 22:54:23,107 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.561e+33, grad_sumsq=7.876e+34, orig_rms_sq=1.982e-02
2024-10-08 22:54:25,376 WARNING [optim.py:503] Scaling gradients by 0.014805798418819904, model_norm_threshold=554231840899072.0
2024-10-08 22:54:25,514 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.581e+32, grad_sumsq=8.057e+34, orig_rms_sq=4.445e-03
2024-10-08 22:54:25,560 INFO [train.py:1154] Epoch 2, batch 3700, loss[loss=1.28, simple_loss=0.6352, pruned_loss=0.7463, ctc_loss=1.081, over 4853.00 frames. ], tot_loss[loss=1.268, simple_loss=0.6443, pruned_loss=0.7374, ctc_loss=1.04, over 967367.50 frames. ], batch size: 24, lr: 3.36e-02,
2024-10-08 22:54:30,021 WARNING [optim.py:503] Scaling gradients by 0.014378289692103863, model_norm_threshold=554231840899072.0
2024-10-08 22:54:30,159 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.578e+32, grad_sumsq=1.483e+35, orig_rms_sq=4.434e-03
2024-10-08 22:54:35,002 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=3517.3333333333335, ans=0.25276
2024-10-08 22:54:38,767 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=15.24 vs. limit=5.880166666666667
2024-10-08 22:54:40,378 WARNING [optim.py:503] Scaling gradients by 0.003145260736346245, model_norm_threshold=554231840899072.0
2024-10-08 22:54:40,516 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.060e+33, grad_sumsq=2.689e+35, orig_rms_sq=1.882e-02
2024-10-08 22:54:46,024 WARNING [optim.py:503] Scaling gradients by 0.02069615013897419, model_norm_threshold=554231840899072.0
2024-10-08 22:54:46,163 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.649e+32, grad_sumsq=8.770e+33, orig_rms_sq=1.880e-02
2024-10-08 22:54:47,453 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=3524.0, ans=0.26476
2024-10-08 22:54:49,536 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer1.min_positive, batch_count=3524.0, ans=0.0389875
2024-10-08 22:54:49,746 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=32.62 vs. limit=10.143
2024-10-08 22:54:50,925 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=6.95 vs. limit=6.7620000000000005
2024-10-08 22:54:52,568 WARNING [optim.py:503] Scaling gradients by 0.006108855828642845, model_norm_threshold=554231840899072.0
2024-10-08 22:54:52,706 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.234e+33, grad_sumsq=1.183e+35, orig_rms_sq=1.889e-02
2024-10-08 22:55:05,524 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten.whitening_limit, batch_count=3527.3333333333335, ans=8.82275
2024-10-08 22:55:05,577 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.86 vs. limit=10.1455
2024-10-08 22:55:07,032 WARNING [optim.py:503] Scaling gradients by 0.0029730319511145353, model_norm_threshold=554231840899072.0
2024-10-08 22:55:07,172 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.935e+33, grad_sumsq=6.546e+32, orig_rms_sq=9.067e+00
2024-10-08 22:55:09,562 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.min_positive, batch_count=3530.6666666666665, ans=0.07793333333333334
2024-10-08 22:55:17,438 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=3530.6666666666665, ans=0.7764266666666667
2024-10-08 22:55:17,870 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=30.68 vs. limit=10.148
2024-10-08 22:55:19,641 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 9.955e+11 2.920e+13 1.618e+14 2.565e+15 9.628e+18, threshold=3.235e+14, percent-clipped=38.0
2024-10-08 22:55:21,936 INFO [train.py:1154] Epoch 2, batch 3750, loss[loss=1.193, simple_loss=0.6026, pruned_loss=0.6988, ctc_loss=0.9671, over 4959.00 frames. ], tot_loss[loss=1.267, simple_loss=0.6441, pruned_loss=0.737, ctc_loss=1.039, over 967811.19 frames. ], batch size: 19, lr: 3.35e-02,
2024-10-08 22:55:22,959 WARNING [optim.py:503] Scaling gradients by 0.035556402057409286, model_norm_threshold=323516062760960.0
2024-10-08 22:55:23,097 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.549e+31, grad_sumsq=7.866e+32, orig_rms_sq=1.969e-02
2024-10-08 22:55:24,220 WARNING [optim.py:503] Scaling gradients by 0.0004951537121087313, model_norm_threshold=323516062760960.0
2024-10-08 22:55:24,357 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.487e+35, grad_sumsq=1.664e+34, orig_rms_sq=8.938e+00
2024-10-08 22:55:27,703 WARNING [optim.py:503] Scaling gradients by 0.08202020078897476, model_norm_threshold=323516062760960.0
2024-10-08 22:55:27,841 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.713e+30, grad_sumsq=1.239e+33, orig_rms_sq=4.609e-03
2024-10-08 22:55:28,541 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=21.96 vs. limit=8.82525
2024-10-08 22:55:31,620 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=52.59 vs. limit=10.150500000000001
2024-10-08 22:55:32,257 WARNING [optim.py:503] Scaling gradients by 0.0008991000941023231, model_norm_threshold=323516062760960.0
2024-10-08 22:55:32,397 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.520e+34, grad_sumsq=9.180e+33, orig_rms_sq=3.834e+00
2024-10-08 22:55:34,222 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=33.73 vs. limit=10.153
2024-10-08 22:55:34,266 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.82 vs. limit=5.884333333333333
2024-10-08 22:55:34,690 WARNING [optim.py:503] Scaling gradients by 0.010069211944937706, model_norm_threshold=323516062760960.0
2024-10-08 22:55:34,827 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.996e+32, grad_sumsq=2.253e+31, orig_rms_sq=8.861e+00
2024-10-08 22:55:35,264 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=24.33 vs. limit=10.153
2024-10-08 22:55:35,931 WARNING [optim.py:503] Scaling gradients by 0.016468392685055733, model_norm_threshold=323516062760960.0
2024-10-08 22:55:36,070 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.039e+32, grad_sumsq=2.716e+31, orig_rms_sq=3.826e+00
2024-10-08 22:55:37,563 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=5.96 vs. limit=5.884333333333333
2024-10-08 22:55:39,464 WARNING [optim.py:503] Scaling gradients by 0.018336260691285133, model_norm_threshold=323516062760960.0
2024-10-08 22:55:39,601 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.228e+31, grad_sumsq=2.010e+34, orig_rms_sq=4.591e-03
2024-10-08 22:55:44,064 WARNING [optim.py:503] Scaling gradients by 0.0024950497318059206, model_norm_threshold=323516062760960.0
2024-10-08 22:55:44,202 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.735e+33, grad_sumsq=8.083e+35, orig_rms_sq=4.621e-03
2024-10-08 22:55:44,776 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=13.19 vs. limit=5.885166666666667
2024-10-08 22:55:45,279 WARNING [optim.py:503] Scaling gradients by 0.0011353699956089258, model_norm_threshold=323516062760960.0
2024-10-08 22:55:45,417 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.877e+34, grad_sumsq=7.470e+33, orig_rms_sq=3.851e+00
2024-10-08 22:55:48,261 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=23.80 vs. limit=8.82775
2024-10-08 22:55:48,859 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=3540.6666666666665, ans=0.06722500000000001
2024-10-08 22:55:54,310 WARNING [optim.py:503] Scaling gradients by 0.007140724454075098, model_norm_threshold=323516062760960.0
2024-10-08 22:55:54,449 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.335e+32, grad_sumsq=2.252e+34, orig_rms_sq=1.925e-02
2024-10-08 22:55:59,498 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten.whitening_limit, batch_count=3544.0, ans=8.829
2024-10-08 22:56:03,571 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer1.max_abs, batch_count=3544.0, ans=7.215
2024-10-08 22:56:04,577 WARNING [optim.py:503] Scaling gradients by 0.01585320569574833, model_norm_threshold=323516062760960.0
2024-10-08 22:56:04,717 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.266e+32, grad_sumsq=6.397e+33, orig_rms_sq=1.980e-02
2024-10-08 22:56:09,318 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=3547.3333333333335, ans=0.7758433333333333
2024-10-08 22:56:10,976 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=25.75 vs. limit=10.160499999999999
2024-10-08 22:56:11,324 WARNING [optim.py:503] Scaling gradients by 0.013365846127271652, model_norm_threshold=323516062760960.0
2024-10-08 22:56:11,465 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.847e+32, grad_sumsq=9.175e+33, orig_rms_sq=2.013e-02
2024-10-08 22:56:13,601 WARNING [optim.py:503] Scaling gradients by 6.351544288918376e-05, model_norm_threshold=323516062760960.0
2024-10-08 22:56:13,739 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.182e+37, grad_sumsq=inf, orig_rms_sq=4.921e-03
2024-10-08 22:56:15,495 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=28.07 vs. limit=10.160499999999999
2024-10-08 22:56:20,645 INFO [train.py:1154] Epoch 2, batch 3800, loss[loss=1.26, simple_loss=0.6456, pruned_loss=0.7314, ctc_loss=1.03, over 4774.00 frames. ], tot_loss[loss=1.262, simple_loss=0.6416, pruned_loss=0.734, ctc_loss=1.036, over 967504.90 frames. ], batch size: 26, lr: 3.35e-02,
2024-10-08 22:56:21,473 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=15.46 vs. limit=8.8315
2024-10-08 22:56:24,207 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module2.balancer2.min_abs, batch_count=3550.6666666666665, ans=0.25326
2024-10-08 22:56:25,205 WARNING [optim.py:503] Scaling gradients by 0.0051247053779661655, model_norm_threshold=323516062760960.0
2024-10-08 22:56:25,342 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.765e+32, grad_sumsq=2.045e+35, orig_rms_sq=4.776e-03
2024-10-08 22:56:25,493 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.balancer1.prob, batch_count=3550.6666666666665, ans=0.3335625
2024-10-08 22:56:26,407 WARNING [optim.py:503] Scaling gradients by 0.01033441536128521, model_norm_threshold=323516062760960.0
2024-10-08 22:56:26,545 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.290e+32, grad_sumsq=1.114e+34, orig_rms_sq=2.055e-02
2024-10-08 22:56:27,016 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=11.48 vs. limit=6.775333333333333
2024-10-08 22:56:27,708 WARNING [optim.py:503] Scaling gradients by 0.0889408066868782, model_norm_threshold=323516062760960.0
2024-10-08 22:56:27,845 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.340e+30, grad_sumsq=8.646e+30, orig_rms_sq=3.864e-01
2024-10-08 22:56:29,051 WARNING [optim.py:503] Scaling gradients by 0.025855856016278267, model_norm_threshold=323516062760960.0
2024-10-08 22:56:29,188 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.107e+31, grad_sumsq=1.512e+33, orig_rms_sq=2.055e-02
2024-10-08 22:56:30,287 WARNING [optim.py:503] Scaling gradients by 0.011897090822458267, model_norm_threshold=323516062760960.0
2024-10-08 22:56:30,425 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.815e+32, grad_sumsq=8.847e+33, orig_rms_sq=2.052e-02
2024-10-08 22:56:31,516 WARNING [optim.py:503] Scaling gradients by 0.039758335798978806, model_norm_threshold=323516062760960.0
2024-10-08 22:56:31,654 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.514e+31, grad_sumsq=7.380e+32, orig_rms_sq=2.052e-02
2024-10-08 22:56:34,338 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.32 vs. limit=10.1655
2024-10-08 22:56:41,541 WARNING [optim.py:503] Scaling gradients by 0.0012918049469590187, model_norm_threshold=323516062760960.0
2024-10-08 22:56:41,680 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.637e+34, grad_sumsq=1.817e+33, orig_rms_sq=9.009e+00
2024-10-08 22:56:43,922 WARNING [optim.py:503] Scaling gradients by 0.00033394130878150463, model_norm_threshold=323516062760960.0
2024-10-08 22:56:44,060 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.219e+35, grad_sumsq=2.452e+34, orig_rms_sq=9.048e+00
2024-10-08 22:56:46,184 WARNING [optim.py:503] Scaling gradients by 0.005323686171323061, model_norm_threshold=323516062760960.0
2024-10-08 22:56:46,325 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.14, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.283e+32, grad_sumsq=1.318e+33, orig_rms_sq=4.010e-01
2024-10-08 22:56:50,714 WARNING [optim.py:503] Scaling gradients by 0.0002517156535759568, model_norm_threshold=323516062760960.0
2024-10-08 22:56:50,851 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.659e+35, grad_sumsq=2.265e+37, orig_rms_sq=2.057e-02
2024-10-08 22:56:54,794 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=27.99 vs. limit=10.1705
2024-10-08 22:56:55,531 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.const_attention_rate, batch_count=3560.6666666666665, ans=0.04971249999999999
2024-10-08 22:57:05,091 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=17.59 vs. limit=10.1705
2024-10-08 22:57:08,697 WARNING [optim.py:503] Scaling gradients by 0.01714319735765457, model_norm_threshold=323516062760960.0
2024-10-08 22:57:08,834 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.612e+31, grad_sumsq=3.789e+33, orig_rms_sq=2.009e-02
2024-10-08 22:57:09,885 WARNING [optim.py:503] Scaling gradients by 0.01145798061043024, model_norm_threshold=323516062760960.0
2024-10-08 22:57:10,024 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.433e+32, grad_sumsq=1.596e+31, orig_rms_sq=8.979e+00
2024-10-08 22:57:15,688 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.007e+11 2.295e+13 1.901e+14 3.637e+15 5.094e+18, threshold=3.802e+14, percent-clipped=44.0
2024-10-08 22:57:18,001 INFO [train.py:1154] Epoch 2, batch 3850, loss[loss=1.18, simple_loss=0.6138, pruned_loss=0.6765, ctc_loss=0.9852, over 4821.00 frames. ], tot_loss[loss=1.26, simple_loss=0.6406, pruned_loss=0.7332, ctc_loss=1.034, over 967557.35 frames. ], batch size: 38, lr: 3.34e-02,
2024-10-08 22:57:18,630 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=27.77 vs. limit=8.83775
2024-10-08 22:57:20,410 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=3567.3333333333335, ans=0.33278125000000003
2024-10-08 22:57:21,429 WARNING [optim.py:503] Scaling gradients by 0.056129515171051025, model_norm_threshold=380244259241984.0
2024-10-08 22:57:21,567 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.202e+30, grad_sumsq=2.333e+30, orig_rms_sq=3.515e+00
2024-10-08 22:57:23,315 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=75.92 vs. limit=8.83775
2024-10-08 22:57:23,856 WARNING [optim.py:503] Scaling gradients by 0.003425922943279147, model_norm_threshold=380244259241984.0
2024-10-08 22:57:23,997 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.635e+33, grad_sumsq=1.056e+36, orig_rms_sq=4.387e-03
2024-10-08 22:57:27,489 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3567.3333333333335, ans=0.26432666666666665
2024-10-08 22:57:28,772 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.1.self_attn_weights, loss-sum=5.484e+01
2024-10-08 22:57:28,783 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer2.min_abs, batch_count=3570.6666666666665, ans=0.25356
2024-10-08 22:57:29,025 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.88 vs. limit=5.892666666666667
2024-10-08 22:57:31,937 WARNING [optim.py:503] Scaling gradients by 0.008061747997999191, model_norm_threshold=380244259241984.0
2024-10-08 22:57:32,075 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.817e+32, grad_sumsq=5.335e+31, orig_rms_sq=9.030e+00
2024-10-08 22:57:32,647 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=512, metric=27.93 vs. limit=10.178
2024-10-08 22:57:33,095 WARNING [optim.py:503] Scaling gradients by 0.022203417494893074, model_norm_threshold=380244259241984.0
2024-10-08 22:57:33,234 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.387e+31, grad_sumsq=2.073e+34, orig_rms_sq=4.528e-03
2024-10-08 22:57:33,891 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=6.68 vs. limit=5.428266666666667
2024-10-08 22:57:36,476 WARNING [optim.py:503] Scaling gradients by 0.0002846668066922575, model_norm_threshold=380244259241984.0
2024-10-08 22:57:36,613 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.827e+35, grad_sumsq=8.686e+34, orig_rms_sq=9.011e+00
2024-10-08 22:57:38,989 WARNING [optim.py:503] Scaling gradients by 0.008659476414322853, model_norm_threshold=380244259241984.0
2024-10-08 22:57:39,126 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.437e+32, grad_sumsq=1.562e+35, orig_rms_sq=4.762e-03
2024-10-08 22:57:41,330 WARNING [optim.py:503] Scaling gradients by 0.026000507175922394, model_norm_threshold=380244259241984.0
2024-10-08 22:57:41,469 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.697e+31, grad_sumsq=2.929e+33, orig_rms_sq=1.945e-02
2024-10-08 22:57:42,101 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.89 vs. limit=3.5361000000000002
2024-10-08 22:57:43,567 WARNING [optim.py:503] Scaling gradients by 0.09870442003011703, model_norm_threshold=380244259241984.0
2024-10-08 22:57:43,706 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.177e+30, grad_sumsq=1.624e+32, orig_rms_sq=1.957e-02
2024-10-08 22:57:46,188 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=2.938e+04
2024-10-08 22:57:47,007 WARNING [optim.py:503] Scaling gradients by 0.027208391577005386, model_norm_threshold=380244259241984.0
2024-10-08 22:57:47,146 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.864e+31, grad_sumsq=8.009e+33, orig_rms_sq=4.825e-03
2024-10-08 22:57:49,171 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.78 vs. limit=5.8934999999999995
2024-10-08 22:57:50,056 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=35.40 vs. limit=8.84025
2024-10-08 22:57:53,177 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=10.12 vs. limit=5.430933333333334
2024-10-08 22:57:55,003 WARNING [optim.py:503] Scaling gradients by 0.00687657855451107, model_norm_threshold=380244259241984.0
2024-10-08 22:57:55,141 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.262e+32, grad_sumsq=1.589e+32, orig_rms_sq=3.941e+00
2024-10-08 22:57:56,210 WARNING [optim.py:503] Scaling gradients by 0.036142606288194656, model_norm_threshold=380244259241984.0
2024-10-08 22:57:56,350 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.564e+31, grad_sumsq=6.504e+30, orig_rms_sq=3.941e+00
2024-10-08 22:57:59,957 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.min_positive, batch_count=3577.3333333333335, ans=0.21422666666666668
2024-10-08 22:58:05,332 WARNING [optim.py:503] Scaling gradients by 0.03616516664624214, model_norm_threshold=380244259241984.0
2024-10-08 22:58:05,469 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.348e+31, grad_sumsq=5.874e+31, orig_rms_sq=3.997e-01
2024-10-08 22:58:08,826 WARNING [optim.py:503] Scaling gradients by 0.0007568641449324787, model_norm_threshold=380244259241984.0
2024-10-08 22:58:08,963 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.380e+34, grad_sumsq=2.207e+36, orig_rms_sq=1.985e-02
2024-10-08 22:58:12,849 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.whiten.whitening_limit, batch_count=3580.6666666666665, ans=5.432266666666667
2024-10-08 22:58:15,774 INFO [train.py:1154] Epoch 2, batch 3900, loss[loss=1.269, simple_loss=0.6518, pruned_loss=0.7361, ctc_loss=1.034, over 4751.00 frames. ], tot_loss[loss=1.261, simple_loss=0.641, pruned_loss=0.7332, ctc_loss=1.035, over 967006.82 frames. ], batch size: 26, lr: 3.34e-02,
2024-10-08 22:58:16,773 WARNING [optim.py:503] Scaling gradients by 0.03190330043435097, model_norm_threshold=380244259241984.0
2024-10-08 22:58:16,915 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.061e+31, grad_sumsq=8.825e+33, orig_rms_sq=4.601e-03
2024-10-08 22:58:22,075 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=13.63 vs. limit=8.844
2024-10-08 22:58:22,479 WARNING [optim.py:503] Scaling gradients by 0.06701766699552536, model_norm_threshold=380244259241984.0
2024-10-08 22:58:22,618 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.380e+30, grad_sumsq=1.599e+33, orig_rms_sq=4.615e-03
2024-10-08 22:58:23,626 WARNING [optim.py:503] Scaling gradients by 0.010298533365130424, model_norm_threshold=380244259241984.0
2024-10-08 22:58:23,764 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.383e+32, grad_sumsq=1.181e+32, orig_rms_sq=3.710e+00
2024-10-08 22:58:25,200 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten.whitening_limit, batch_count=3584.0, ans=8.844
2024-10-08 22:58:30,304 WARNING [optim.py:503] Scaling gradients by 0.0262138731777668, model_norm_threshold=380244259241984.0
2024-10-08 22:58:30,443 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.661e+31, grad_sumsq=1.657e+34, orig_rms_sq=4.623e-03
2024-10-08 22:58:32,556 WARNING [optim.py:503] Scaling gradients by 0.005553027614951134, model_norm_threshold=380244259241984.0
2024-10-08 22:58:32,695 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.323e+33, grad_sumsq=2.863e+35, orig_rms_sq=4.623e-03
2024-10-08 22:58:36,034 WARNING [optim.py:503] Scaling gradients by 0.023358656093478203, model_norm_threshold=380244259241984.0
2024-10-08 22:58:36,173 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.960e+31, grad_sumsq=1.799e+32, orig_rms_sq=3.869e-01
2024-10-08 22:58:37,208 WARNING [optim.py:503] Scaling gradients by 0.04350609332323074, model_norm_threshold=380244259241984.0
2024-10-08 22:58:37,347 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.860e+31, grad_sumsq=9.554e+32, orig_rms_sq=1.947e-02
2024-10-08 22:58:37,993 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=9.68 vs. limit=5.436266666666667
2024-10-08 22:58:39,559 WARNING [optim.py:503] Scaling gradients by 0.0002434673224342987, model_norm_threshold=380244259241984.0
2024-10-08 22:58:39,698 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.157e+35, grad_sumsq=3.674e+37, orig_rms_sq=1.948e-02
2024-10-08 22:58:41,526 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.whiten, num_groups=1, num_channels=512, metric=9.71 vs. limit=5.436266666666667
2024-10-08 22:58:43,124 WARNING [optim.py:503] Scaling gradients by 0.020410284399986267, model_norm_threshold=380244259241984.0
2024-10-08 22:58:43,264 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.700e+31, grad_sumsq=8.872e+30, orig_rms_sq=8.679e+00
2024-10-08 22:58:44,944 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=15.75 vs. limit=6.795333333333334
2024-10-08 22:58:45,387 WARNING [optim.py:503] Scaling gradients by 0.0004580155364237726, model_norm_threshold=380244259241984.0
2024-10-08 22:58:45,525 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.861e+35, grad_sumsq=9.540e+36, orig_rms_sq=1.951e-02
2024-10-08 22:58:46,246 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=16.47 vs. limit=8.8465
2024-10-08 22:58:46,644 WARNING [optim.py:503] Scaling gradients by 0.0004590625176206231, model_norm_threshold=380244259241984.0
2024-10-08 22:58:46,780 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.620e+35, grad_sumsq=8.305e+36, orig_rms_sq=1.951e-02
2024-10-08 22:58:46,958 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.attention_skip_rate, batch_count=3590.6666666666665, ans=0.06535000000000002
2024-10-08 22:58:49,098 WARNING [optim.py:503] Scaling gradients by 0.0005924541037529707, model_norm_threshold=380244259241984.0
2024-10-08 22:58:49,237 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.060e+35, grad_sumsq=5.422e+36, orig_rms_sq=1.954e-02
2024-10-08 22:58:51,407 WARNING [optim.py:503] Scaling gradients by 0.05080127343535423, model_norm_threshold=380244259241984.0
2024-10-08 22:58:51,546 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.558e+31, grad_sumsq=4.348e+30, orig_rms_sq=3.583e+00
2024-10-08 22:58:54,472 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=3594.0, ans=10.1955
2024-10-08 22:58:58,522 WARNING [optim.py:503] Scaling gradients by 0.0009223566739819944, model_norm_threshold=380244259241984.0
2024-10-08 22:58:58,664 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.358e+34, grad_sumsq=2.793e+36, orig_rms_sq=1.918e-02
2024-10-08 22:58:59,736 WARNING [optim.py:503] Scaling gradients by 0.03983623534440994, model_norm_threshold=380244259241984.0
2024-10-08 22:58:59,873 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.597e+31, grad_sumsq=1.354e+33, orig_rms_sq=1.918e-02
2024-10-08 22:59:02,057 WARNING [optim.py:503] Scaling gradients by 0.013678709976375103, model_norm_threshold=380244259241984.0
2024-10-08 22:59:02,196 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.977e+32, grad_sumsq=1.046e+34, orig_rms_sq=1.890e-02
2024-10-08 22:59:03,248 WARNING [optim.py:503] Scaling gradients by 0.0025280031841248274, model_norm_threshold=380244259241984.0
2024-10-08 22:59:03,388 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.163e+33, grad_sumsq=6.020e+32, orig_rms_sq=8.576e+00
2024-10-08 22:59:05,092 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=14.51 vs. limit=5.899333333333334
2024-10-08 22:59:06,291 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=35.71 vs. limit=10.198
2024-10-08 22:59:10,024 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=12.62 vs. limit=10.198
2024-10-08 22:59:11,684 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.123e+11 3.712e+13 4.396e+14 9.545e+15 1.562e+18, threshold=8.793e+14, percent-clipped=51.0
2024-10-08 22:59:12,890 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.scale_min, batch_count=3600.6666666666665, ans=0.7739766666666668
2024-10-08 22:59:13,820 INFO [train.py:1154] Epoch 2, batch 3950, loss[loss=1.337, simple_loss=0.6916, pruned_loss=0.7728, ctc_loss=1.091, over 4837.00 frames. ], tot_loss[loss=1.257, simple_loss=0.6382, pruned_loss=0.7314, ctc_loss=1.031, over 967330.14 frames. ], batch size: 36, lr: 3.33e-02,
2024-10-08 22:59:20,005 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=17.20 vs. limit=8.850249999999999
2024-10-08 22:59:20,403 WARNING [optim.py:503] Scaling gradients by 6.615898018935695e-05, model_norm_threshold=879277448888320.0
2024-10-08 22:59:20,545 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.638e+37, grad_sumsq=inf, orig_rms_sq=1.884e-02
2024-10-08 22:59:21,651 WARNING [optim.py:503] Scaling gradients by 0.002739260671660304, model_norm_threshold=879277448888320.0
2024-10-08 22:59:21,790 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.113e+34, grad_sumsq=1.122e+36, orig_rms_sq=1.884e-02
2024-10-08 22:59:29,697 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=3604.0, ans=0.04949999999999999
2024-10-08 22:59:30,580 WARNING [optim.py:503] Scaling gradients by 0.011651844717562199, model_norm_threshold=879277448888320.0
2024-10-08 22:59:30,719 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.248e+33, grad_sumsq=6.206e+32, orig_rms_sq=3.622e+00
2024-10-08 22:59:31,875 WARNING [optim.py:503] Scaling gradients by 0.0016822973266243935, model_norm_threshold=879277448888320.0
2024-10-08 22:59:32,015 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.280e+34, grad_sumsq=1.734e+34, orig_rms_sq=3.622e+00
2024-10-08 22:59:35,302 WARNING [optim.py:503] Scaling gradients by 0.07462745904922485, model_norm_threshold=879277448888320.0
2024-10-08 22:59:35,442 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.267e+31, grad_sumsq=1.178e+33, orig_rms_sq=1.924e-02
2024-10-08 22:59:36,722 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_skip_rate, batch_count=3607.3333333333335, ans=0.06472499999999998
2024-10-08 22:59:43,950 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=17.52 vs. limit=8.85275
2024-10-08 22:59:44,360 WARNING [optim.py:503] Scaling gradients by 0.0007099590147845447, model_norm_threshold=879277448888320.0
2024-10-08 22:59:44,499 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.348e+35, grad_sumsq=3.733e+34, orig_rms_sq=8.970e+00
2024-10-08 22:59:46,291 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=36.82 vs. limit=8.85275
2024-10-08 22:59:49,331 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module1.balancer2.prob, batch_count=3610.6666666666665, ans=0.33075
2024-10-08 22:59:49,385 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=3610.6666666666665, ans=0.04866666666666669
2024-10-08 22:59:54,049 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.43 vs. limit=8.854
2024-10-08 22:59:54,946 WARNING [optim.py:503] Scaling gradients by 0.03891363367438316, model_norm_threshold=879277448888320.0
2024-10-08 22:59:55,087 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.026e+32, grad_sumsq=5.901e+31, orig_rms_sq=3.433e+00
2024-10-08 22:59:56,783 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.78 vs. limit=3.5416
2024-10-08 23:00:03,972 WARNING [optim.py:503] Scaling gradients by 0.00045913708163425326, model_norm_threshold=879277448888320.0
2024-10-08 23:00:04,111 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.030e+36, grad_sumsq=5.454e+37, orig_rms_sq=1.889e-02
2024-10-08 23:00:06,068 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=15.10 vs. limit=8.85525
2024-10-08 23:00:08,102 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=14.00 vs. limit=8.85525
2024-10-08 23:00:08,765 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.balancer2.prob, batch_count=3614.0, ans=0.33059375
2024-10-08 23:00:09,298 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.41 vs. limit=3.5421
2024-10-08 23:00:09,458 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.conv_module2.whiten, num_groups=1, num_channels=192, metric=3.64 vs. limit=8.85525
2024-10-08 23:00:10,825 INFO [train.py:1154] Epoch 2, batch 4000, loss[loss=1.231, simple_loss=0.6178, pruned_loss=0.7225, ctc_loss=0.9957, over 4815.00 frames. ], tot_loss[loss=1.259, simple_loss=0.6392, pruned_loss=0.733, ctc_loss=1.033, over 967288.92 frames. ], batch size: 19, lr: 3.33e-02,
2024-10-08 23:00:11,872 WARNING [optim.py:503] Scaling gradients by 0.09730391204357147, model_norm_threshold=879277448888320.0
2024-10-08 23:00:12,009 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.660e+31, grad_sumsq=3.451e+33, orig_rms_sq=4.809e-03
2024-10-08 23:00:13,081 WARNING [optim.py:503] Scaling gradients by 0.027837025001645088, model_norm_threshold=879277448888320.0
2024-10-08 23:00:13,221 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.727e+32, grad_sumsq=1.487e+34, orig_rms_sq=1.835e-02
2024-10-08 23:00:17,103 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.15 vs. limit=5.904333333333334
2024-10-08 23:00:18,519 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=16.47 vs. limit=8.8565
2024-10-08 23:00:22,049 WARNING [optim.py:503] Scaling gradients by 0.0977579727768898, model_norm_threshold=879277448888320.0
2024-10-08 23:00:22,186 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.781e+31, grad_sumsq=3.745e+33, orig_rms_sq=4.756e-03
2024-10-08 23:00:26,413 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn2.whiten, num_groups=1, num_channels=192, metric=13.85 vs. limit=10.2155
2024-10-08 23:00:27,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.34 vs. limit=10.2155
2024-10-08 23:00:28,722 WARNING [optim.py:503] Scaling gradients by 0.000852858298458159, model_norm_threshold=879277448888320.0
2024-10-08 23:00:28,860 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.301e+35, grad_sumsq=1.155e+35, orig_rms_sq=3.723e+00
2024-10-08 23:00:30,238 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass_mid.scale_min, batch_count=3620.6666666666665, ans=0.7732766666666667
2024-10-08 23:00:31,090 WARNING [optim.py:503] Scaling gradients by 0.003514559706673026, model_norm_threshold=879277448888320.0
2024-10-08 23:00:31,230 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.501e+34, grad_sumsq=4.031e+33, orig_rms_sq=3.723e+00
2024-10-08 23:00:32,606 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.min_positive, batch_count=3624.0, ans=0.07735
2024-10-08 23:00:40,432 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=3624.0, ans=0.330125
2024-10-08 23:00:41,885 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=42.95 vs. limit=8.859
2024-10-08 23:00:42,414 WARNING [optim.py:503] Scaling gradients by 0.02345268800854683, model_norm_threshold=879277448888320.0
2024-10-08 23:00:42,554 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.314e+32, grad_sumsq=8.718e+34, orig_rms_sq=4.949e-03
2024-10-08 23:00:44,746 WARNING [optim.py:503] Scaling gradients by 0.01772909052670002, model_norm_threshold=879277448888320.0
2024-10-08 23:00:44,883 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.335e+33, grad_sumsq=7.257e+34, orig_rms_sq=1.840e-02
2024-10-08 23:00:45,371 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.49 vs. limit=10.2205
2024-10-08 23:00:52,637 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=3627.3333333333335, ans=0.04596249999999999
2024-10-08 23:00:59,269 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=3630.6666666666665, ans=0.018309999999999993
2024-10-08 23:01:05,995 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.747e+11 3.924e+13 1.862e+14 3.056e+15 1.329e+19, threshold=3.724e+14, percent-clipped=37.0
2024-10-08 23:01:08,276 INFO [train.py:1154] Epoch 2, batch 4050, loss[loss=1.327, simple_loss=0.6992, pruned_loss=0.7546, ctc_loss=1.112, over 4768.00 frames. ], tot_loss[loss=1.255, simple_loss=0.6371, pruned_loss=0.7301, ctc_loss=1.03, over 967673.41 frames. ], batch size: 53, lr: 3.32e-02,
2024-10-08 23:01:10,134 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=9.19 vs. limit=6.817
2024-10-08 23:01:12,371 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=11.09 vs. limit=5.9085
2024-10-08 23:01:14,406 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.95 vs. limit=5.9085
2024-10-08 23:01:15,360 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=22.71 vs. limit=8.86275
2024-10-08 23:01:17,237 WARNING [optim.py:503] Scaling gradients by 0.09521712362766266, model_norm_threshold=372364772638720.0
2024-10-08 23:01:17,373 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.060e+30, grad_sumsq=1.046e+30, orig_rms_sq=3.882e+00
2024-10-08 23:01:20,607 WARNING [optim.py:503] Scaling gradients by 0.011540775187313557, model_norm_threshold=372364772638720.0
2024-10-08 23:01:20,744 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.452e+32, grad_sumsq=5.162e+34, orig_rms_sq=4.750e-03
2024-10-08 23:01:21,899 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=3637.3333333333335, ans=0.3295
2024-10-08 23:01:28,474 WARNING [optim.py:503] Scaling gradients by 0.06860734522342682, model_norm_threshold=372364772638720.0
2024-10-08 23:01:28,612 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.063e+30, grad_sumsq=4.628e+32, orig_rms_sq=1.742e-02
2024-10-08 23:01:29,670 WARNING [optim.py:503] Scaling gradients by 0.01666042022407055, model_norm_threshold=372364772638720.0
2024-10-08 23:01:29,809 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.946e+31, grad_sumsq=2.056e+34, orig_rms_sq=4.838e-03
2024-10-08 23:01:33,081 WARNING [optim.py:503] Scaling gradients by 0.0032174037769436836, model_norm_threshold=372364772638720.0
2024-10-08 23:01:33,222 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.167e+33, grad_sumsq=4.444e+35, orig_rms_sq=4.876e-03
2024-10-08 23:01:34,077 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=144, metric=7.84 vs. limit=5.910166666666667
2024-10-08 23:01:35,298 WARNING [optim.py:503] Scaling gradients by 0.00019622602849267423, model_norm_threshold=372364772638720.0
2024-10-08 23:01:35,446 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.265e+35, grad_sumsq=5.294e+37, orig_rms_sq=1.750e-02
2024-10-08 23:01:38,693 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.whiten.whitening_limit, batch_count=3640.6666666666665, ans=5.456266666666666
2024-10-08 23:01:41,775 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=12.65 vs. limit=8.8665
2024-10-08 23:01:42,334 WARNING [optim.py:503] Scaling gradients by 0.003291194560006261, model_norm_threshold=372364772638720.0
2024-10-08 23:01:42,473 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.902e+33, grad_sumsq=1.647e+35, orig_rms_sq=1.762e-02
2024-10-08 23:01:43,514 WARNING [optim.py:503] Scaling gradients by 0.0006683479878120124, model_norm_threshold=372364772638720.0
2024-10-08 23:01:43,654 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.977e+34, grad_sumsq=3.960e+36, orig_rms_sq=1.762e-02
2024-10-08 23:01:44,704 WARNING [optim.py:503] Scaling gradients by 0.018127119168639183, model_norm_threshold=372364772638720.0
2024-10-08 23:01:44,841 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.364e+31, grad_sumsq=2.075e+31, orig_rms_sq=4.031e+00
2024-10-08 23:01:47,134 WARNING [optim.py:503] Scaling gradients by 0.043299295008182526, model_norm_threshold=372364772638720.0
2024-10-08 23:01:47,273 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.617e+31, grad_sumsq=4.030e+30, orig_rms_sq=4.012e+00
2024-10-08 23:01:48,334 WARNING [optim.py:503] Scaling gradients by 0.0016643516719341278, model_norm_threshold=372364772638720.0
2024-10-08 23:01:48,472 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.296e+34, grad_sumsq=3.232e+33, orig_rms_sq=4.012e+00
2024-10-08 23:01:49,655 WARNING [optim.py:503] Scaling gradients by 0.005826144479215145, model_norm_threshold=372364772638720.0
2024-10-08 23:01:49,794 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.208e+33, grad_sumsq=6.834e+34, orig_rms_sq=1.767e-02
2024-10-08 23:01:52,050 WARNING [optim.py:503] Scaling gradients by 0.004739457741379738, model_norm_threshold=372364772638720.0
2024-10-08 23:01:52,190 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.703e+33, grad_sumsq=4.289e+32, orig_rms_sq=3.969e+00
2024-10-08 23:01:57,592 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.25 vs. limit=10.2355
2024-10-08 23:02:01,811 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=25.09 vs. limit=8.867750000000001
2024-10-08 23:02:02,320 WARNING [optim.py:503] Scaling gradients by 0.0, model_norm_threshold=372364772638720.0
2024-10-08 23:02:02,352 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out_norm.log_scale with proportion 0.00, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.409e+34, grad_sumsq=6.409e+36, orig_rms_sq=1.000e-02
2024-10-08 23:02:03,440 WARNING [optim.py:503] Scaling gradients by 0.07143694162368774, model_norm_threshold=372364772638720.0
2024-10-08 23:02:03,581 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.531e+30, grad_sumsq=2.175e+30, orig_rms_sq=3.922e+00
2024-10-08 23:02:05,780 WARNING [optim.py:503] Scaling gradients by 5.5548473028466105e-05, model_norm_threshold=372364772638720.0
2024-10-08 23:02:05,918 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.007e+37, grad_sumsq=2.577e+36, orig_rms_sq=3.910e+00
2024-10-08 23:02:05,963 INFO [train.py:1154] Epoch 2, batch 4100, loss[loss=1.284, simple_loss=0.6563, pruned_loss=0.739, ctc_loss=1.084, over 4868.00 frames. ], tot_loss[loss=1.255, simple_loss=0.6382, pruned_loss=0.7298, ctc_loss=1.03, over 967077.11 frames. ], batch size: 31, lr: 3.32e-02,
2024-10-08 23:02:06,111 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=3650.6666666666665, ans=0.26349333333333336
2024-10-08 23:02:08,898 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=12.48 vs. limit=8.869
2024-10-08 23:02:10,352 WARNING [optim.py:503] Scaling gradients by 0.062229059636592865, model_norm_threshold=372364772638720.0
2024-10-08 23:02:10,491 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.222e+30, grad_sumsq=2.369e+30, orig_rms_sq=3.893e+00
2024-10-08 23:02:13,621 WARNING [optim.py:503] Scaling gradients by 0.0003913913678843528, model_norm_threshold=372364772638720.0
2024-10-08 23:02:13,761 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.57, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.187e+35, grad_sumsq=1.080e+38, orig_rms_sq=4.804e-03
2024-10-08 23:02:14,869 WARNING [optim.py:503] Scaling gradients by 0.0003953737032134086, model_norm_threshold=372364772638720.0
2024-10-08 23:02:15,007 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.536e+35, grad_sumsq=5.283e+37, orig_rms_sq=4.800e-03
2024-10-08 23:02:22,704 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=10.23 vs. limit=10.2405
2024-10-08 23:02:22,912 WARNING [optim.py:503] Scaling gradients by 0.010675540193915367, model_norm_threshold=372364772638720.0
2024-10-08 23:02:23,048 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.648e+32, grad_sumsq=2.021e+34, orig_rms_sq=1.805e-02
2024-10-08 23:02:28,047 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.04 vs. limit=5.914333333333333
2024-10-08 23:02:30,792 WARNING [optim.py:503] Scaling gradients by 0.045796431601047516, model_norm_threshold=372364772638720.0
2024-10-08 23:02:30,931 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.236e+31, grad_sumsq=3.158e+30, orig_rms_sq=3.916e+00
2024-10-08 23:02:32,541 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=32.37 vs. limit=10.243
2024-10-08 23:02:36,589 WARNING [optim.py:503] Scaling gradients by 0.0012436718679964542, model_norm_threshold=372364772638720.0
2024-10-08 23:02:36,734 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.187e+34, grad_sumsq=5.507e+33, orig_rms_sq=3.971e+00
2024-10-08 23:02:38,116 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_proj.dropout_p, batch_count=3657.3333333333335, ans=0.26342666666666664
2024-10-08 23:02:43,533 WARNING [optim.py:503] Scaling gradients by 0.00039367523277178407, model_norm_threshold=372364772638720.0
2024-10-08 23:02:43,672 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.312e+35, grad_sumsq=1.287e+37, orig_rms_sq=1.797e-02
2024-10-08 23:02:45,814 WARNING [optim.py:503] Scaling gradients by 0.0012503160396590829, model_norm_threshold=372364772638720.0
2024-10-08 23:02:45,952 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.782e+34, grad_sumsq=9.920e+35, orig_rms_sq=1.797e-02
2024-10-08 23:02:48,750 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=11.61 vs. limit=8.87275
2024-10-08 23:02:49,547 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=6.376e+06
2024-10-08 23:02:56,019 WARNING [optim.py:503] Scaling gradients by 3.1780993595020846e-05, model_norm_threshold=372364772638720.0
2024-10-08 23:02:56,159 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.179e+37, grad_sumsq=4.632e+36, orig_rms_sq=9.021e+00
2024-10-08 23:02:58,782 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=20.57 vs. limit=10.248000000000001
2024-10-08 23:03:00,923 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.842e+11 3.007e+13 1.853e+14 3.911e+15 inf, threshold=3.706e+14, percent-clipped=41.0
2024-10-08 23:03:03,163 WARNING [optim.py:503] Scaling gradients by 0.03656861186027527, model_norm_threshold=370584575803392.0
2024-10-08 23:03:03,302 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.942e+31, grad_sumsq=1.628e+33, orig_rms_sq=1.807e-02
2024-10-08 23:03:03,347 INFO [train.py:1154] Epoch 2, batch 4150, loss[loss=1.324, simple_loss=0.6616, pruned_loss=0.7732, ctc_loss=1.1, over 4749.00 frames. ], tot_loss[loss=1.257, simple_loss=0.6397, pruned_loss=0.7315, ctc_loss=1.03, over 967122.98 frames. ], batch size: 20, lr: 3.31e-02,
2024-10-08 23:03:06,640 WARNING [optim.py:503] Scaling gradients by 0.0015223696827888489, model_norm_threshold=370584575803392.0
2024-10-08 23:03:06,780 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.508e+34, grad_sumsq=3.782e+33, orig_rms_sq=3.987e+00
2024-10-08 23:03:09,215 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=3667.3333333333335, ans=0.062474999999999975
2024-10-08 23:03:13,591 WARNING [optim.py:503] Scaling gradients by 0.021761180832982063, model_norm_threshold=370584575803392.0
2024-10-08 23:03:13,727 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.166e+31, grad_sumsq=1.656e+34, orig_rms_sq=4.932e-03
2024-10-08 23:03:14,147 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=11.23 vs. limit=6.835333333333333
2024-10-08 23:03:15,045 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=3670.6666666666665, ans=0.3279375
2024-10-08 23:03:17,077 WARNING [optim.py:503] Scaling gradients by 4.628492024494335e-05, model_norm_threshold=370584575803392.0
2024-10-08 23:03:17,216 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.661e+37, grad_sumsq=inf, orig_rms_sq=1.796e-02
2024-10-08 23:03:18,320 WARNING [optim.py:503] Scaling gradients by 0.003236981574445963, model_norm_threshold=370584575803392.0
2024-10-08 23:03:18,458 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.451e+33, grad_sumsq=7.040e+35, orig_rms_sq=4.902e-03
2024-10-08 23:03:23,509 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=41.18 vs. limit=8.8765
2024-10-08 23:03:26,367 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=3674.0, ans=0.32778125
2024-10-08 23:03:29,469 WARNING [optim.py:503] Scaling gradients by 0.012050691060721874, model_norm_threshold=370584575803392.0
2024-10-08 23:03:29,607 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.889e+32, grad_sumsq=3.938e+34, orig_rms_sq=4.797e-03
2024-10-08 23:03:36,369 WARNING [optim.py:503] Scaling gradients by 0.06159665435552597, model_norm_threshold=370584575803392.0
2024-10-08 23:03:36,505 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.061e+30, grad_sumsq=1.463e+33, orig_rms_sq=4.826e-03
2024-10-08 23:03:38,892 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.balancer2.prob, batch_count=3677.3333333333335, ans=0.327625
2024-10-08 23:03:39,482 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.36 vs. limit=3.5516
2024-10-08 23:03:39,745 WARNING [optim.py:503] Scaling gradients by 0.055868905037641525, model_norm_threshold=370584575803392.0
2024-10-08 23:03:39,884 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.042e+31, grad_sumsq=5.827e+32, orig_rms_sq=1.788e-02
2024-10-08 23:03:42,983 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=6.11 vs. limit=5.470933333333333
2024-10-08 23:03:44,400 WARNING [optim.py:503] Scaling gradients by 0.0008300246554426849, model_norm_threshold=370584575803392.0
2024-10-08 23:03:44,538 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.599e+34, grad_sumsq=1.169e+34, orig_rms_sq=3.935e+00
2024-10-08 23:03:46,347 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=5.43 vs. limit=5.470933333333333
2024-10-08 23:03:46,667 WARNING [optim.py:503] Scaling gradients by 0.025769861415028572, model_norm_threshold=370584575803392.0
2024-10-08 23:03:46,805 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.515e+31, grad_sumsq=1.573e+34, orig_rms_sq=4.778e-03
2024-10-08 23:03:47,833 WARNING [optim.py:503] Scaling gradients by 6.973862764425576e-05, model_norm_threshold=370584575803392.0
2024-10-08 23:03:47,971 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.064e+36, grad_sumsq=inf, orig_rms_sq=4.778e-03
2024-10-08 23:03:49,822 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.84 vs. limit=10.2605
2024-10-08 23:03:51,311 WARNING [optim.py:503] Scaling gradients by 0.0014563672011718154, model_norm_threshold=370584575803392.0
2024-10-08 23:03:51,448 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.029e+34, grad_sumsq=5.707e+35, orig_rms_sq=1.804e-02
2024-10-08 23:03:52,339 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=9.68 vs. limit=8.88025
2024-10-08 23:03:55,756 WARNING [optim.py:503] Scaling gradients by 0.013736969791352749, model_norm_threshold=370584575803392.0
2024-10-08 23:03:55,896 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.509e+32, grad_sumsq=6.604e+31, orig_rms_sq=3.799e+00
2024-10-08 23:04:00,280 WARNING [optim.py:503] Scaling gradients by 0.08636951446533203, model_norm_threshold=370584575803392.0
2024-10-08 23:04:00,418 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.607e+30, grad_sumsq=9.640e+29, orig_rms_sq=3.742e+00
2024-10-08 23:04:00,464 INFO [train.py:1154] Epoch 2, batch 4200, loss[loss=1.215, simple_loss=0.6125, pruned_loss=0.7032, ctc_loss=1.026, over 4830.00 frames. ], tot_loss[loss=1.257, simple_loss=0.6403, pruned_loss=0.7314, ctc_loss=1.03, over 967292.55 frames. ], batch size: 31, lr: 3.31e-02,
2024-10-08 23:04:00,573 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=3684.0, ans=0.3273125
2024-10-08 23:04:02,332 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=18.12 vs. limit=8.881499999999999
2024-10-08 23:04:02,619 WARNING [optim.py:503] Scaling gradients by 0.04449164867401123, model_norm_threshold=370584575803392.0
2024-10-08 23:04:02,757 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.602e+31, grad_sumsq=8.141e+32, orig_rms_sq=1.967e-02
2024-10-08 23:04:05,742 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=15.40 vs. limit=8.881499999999999
2024-10-08 23:04:18,171 WARNING [optim.py:503] Scaling gradients by 0.06414627283811569, model_norm_threshold=370584575803392.0
2024-10-08 23:04:18,310 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.122e+30, grad_sumsq=2.396e+30, orig_rms_sq=3.806e+00
2024-10-08 23:04:19,042 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.67 vs. limit=6.843666666666667
2024-10-08 23:04:23,878 WARNING [optim.py:503] Scaling gradients by 0.0054332842119038105, model_norm_threshold=370584575803392.0
2024-10-08 23:04:24,015 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.526e+33, grad_sumsq=8.040e+34, orig_rms_sq=1.899e-02
2024-10-08 23:04:24,141 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3690.6666666666665, ans=0.26309333333333335
2024-10-08 23:04:24,574 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.93 vs. limit=3.5536
2024-10-08 23:04:26,229 WARNING [optim.py:503] Scaling gradients by 0.002319431398063898, model_norm_threshold=370584575803392.0
2024-10-08 23:04:26,368 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.112e+33, grad_sumsq=6.801e+32, orig_rms_sq=8.987e+00
2024-10-08 23:04:27,501 WARNING [optim.py:503] Scaling gradients by 0.059193894267082214, model_norm_threshold=370584575803392.0
2024-10-08 23:04:27,637 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.118e+30, grad_sumsq=2.366e+30, orig_rms_sq=3.854e+00
2024-10-08 23:04:29,533 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=7.05 vs. limit=6.8453333333333335
2024-10-08 23:04:29,820 WARNING [optim.py:503] Scaling gradients by 0.07366174459457397, model_norm_threshold=370584575803392.0
2024-10-08 23:04:29,959 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.776e+30, grad_sumsq=2.018e+30, orig_rms_sq=3.854e+00
2024-10-08 23:04:32,207 WARNING [optim.py:503] Scaling gradients by 0.008042624220252037, model_norm_threshold=370584575803392.0
2024-10-08 23:04:32,343 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.688e+32, grad_sumsq=1.981e+32, orig_rms_sq=3.881e+00
2024-10-08 23:04:38,150 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.max_abs, batch_count=3694.0, ans=7.30875
2024-10-08 23:04:38,631 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten.whitening_limit, batch_count=3694.0, ans=8.88525
2024-10-08 23:04:41,136 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=11.41 vs. limit=8.88525
2024-10-08 23:04:46,698 WARNING [optim.py:503] Scaling gradients by 0.07936090975999832, model_norm_threshold=370584575803392.0
2024-10-08 23:04:46,837 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.919e+30, grad_sumsq=1.494e+33, orig_rms_sq=4.630e-03
2024-10-08 23:04:49,789 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=8.43 vs. limit=5.924333333333333
2024-10-08 23:04:50,106 WARNING [optim.py:503] Scaling gradients by 0.031249716877937317, model_norm_threshold=370584575803392.0
2024-10-08 23:04:50,245 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.407e+31, grad_sumsq=1.681e+31, orig_rms_sq=3.811e+00
2024-10-08 23:04:51,223 INFO [scaling.py:1024] Whitening: name=encoder_embed.out_whiten, num_groups=1, num_channels=192, metric=12.76 vs. limit=4.739466666666667
2024-10-08 23:04:52,102 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.79 vs. limit=8.8865
2024-10-08 23:04:54,874 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.353e+11 2.278e+13 1.781e+14 3.179e+15 8.007e+18, threshold=3.561e+14, percent-clipped=43.0
2024-10-08 23:04:55,038 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer2.min_positive, batch_count=3697.3333333333335, ans=0.07689166666666666
2024-10-08 23:04:57,218 WARNING [optim.py:503] Scaling gradients by 0.0662228986620903, model_norm_threshold=356142312062976.0
2024-10-08 23:04:57,355 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.273e+30, grad_sumsq=2.759e+32, orig_rms_sq=1.911e-02
2024-10-08 23:04:57,400 INFO [train.py:1154] Epoch 2, batch 4250, loss[loss=1.257, simple_loss=0.6283, pruned_loss=0.7384, ctc_loss=1.023, over 4745.00 frames. ], tot_loss[loss=1.254, simple_loss=0.6384, pruned_loss=0.7298, ctc_loss=1.026, over 967191.80 frames. ], batch size: 19, lr: 3.30e-02,
2024-10-08 23:04:59,147 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.whiten, num_groups=1, num_channels=384, metric=6.93 vs. limit=5.480266666666667
2024-10-08 23:04:59,852 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.const_attention_rate, batch_count=3700.6666666666665, ans=0.0418375
2024-10-08 23:05:02,294 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=28.54 vs. limit=8.88775
2024-10-08 23:05:08,905 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3704.0, ans=0.26295999999999997
2024-10-08 23:05:10,731 WARNING [optim.py:503] Scaling gradients by 0.0006538802990689874, model_norm_threshold=356142312062976.0
2024-10-08 23:05:10,869 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.742e+34, grad_sumsq=1.422e+34, orig_rms_sq=4.037e+00
2024-10-08 23:05:11,452 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=11.19 vs. limit=5.926
2024-10-08 23:05:11,648 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=9.95 vs. limit=8.889
2024-10-08 23:05:12,132 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=3704.0, ans=0.32637499999999997
2024-10-08 23:05:14,141 WARNING [optim.py:503] Scaling gradients by 0.0008943523280322552, model_norm_threshold=356142312062976.0
2024-10-08 23:05:14,279 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.422e+34, grad_sumsq=2.293e+36, orig_rms_sq=1.928e-02
2024-10-08 23:05:15,325 WARNING [optim.py:503] Scaling gradients by 0.0008658014121465385, model_norm_threshold=356142312062976.0
2024-10-08 23:05:15,465 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.361e+34, grad_sumsq=1.057e+35, orig_rms_sq=4.126e-01
2024-10-08 23:05:16,509 WARNING [optim.py:503] Scaling gradients by 0.015878183767199516, model_norm_threshold=356142312062976.0
2024-10-08 23:05:16,647 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.192e+32, grad_sumsq=4.415e+34, orig_rms_sq=4.964e-03
2024-10-08 23:05:19,734 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.62 vs. limit=10.278
2024-10-08 23:05:20,907 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=5.84 vs. limit=5.482933333333333
2024-10-08 23:05:20,968 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=23.03 vs. limit=8.89025
2024-10-08 23:05:22,804 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=20.96 vs. limit=8.89025
2024-10-08 23:05:23,525 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=3707.3333333333335, ans=0.060974999999999974
2024-10-08 23:05:31,219 WARNING [optim.py:503] Scaling gradients by 0.0072083380073308945, model_norm_threshold=356142312062976.0
2024-10-08 23:05:31,357 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.103e+32, grad_sumsq=1.258e+32, orig_rms_sq=4.057e+00
2024-10-08 23:05:32,448 WARNING [optim.py:503] Scaling gradients by 0.038161493837833405, model_norm_threshold=356142312062976.0
2024-10-08 23:05:32,587 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.322e+31, grad_sumsq=4.601e+33, orig_rms_sq=5.047e-03
2024-10-08 23:05:33,748 WARNING [optim.py:503] Scaling gradients by 0.020143194124102592, model_norm_threshold=356142312062976.0
2024-10-08 23:05:33,885 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.409e+31, grad_sumsq=4.524e+33, orig_rms_sq=1.859e-02
2024-10-08 23:05:38,344 WARNING [optim.py:503] Scaling gradients by 0.007031643763184547, model_norm_threshold=356142312062976.0
2024-10-08 23:05:38,482 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.905e+32, grad_sumsq=3.722e+34, orig_rms_sq=1.855e-02
2024-10-08 23:05:38,672 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_module2.balancer2.prob, batch_count=3710.6666666666665, ans=0.32606250000000003
2024-10-08 23:05:40,665 WARNING [optim.py:503] Scaling gradients by 0.0016322884475812316, model_norm_threshold=356142312062976.0
2024-10-08 23:05:40,804 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.385e+34, grad_sumsq=2.650e+36, orig_rms_sq=5.227e-03
2024-10-08 23:05:44,157 WARNING [optim.py:503] Scaling gradients by 0.014869073405861855, model_norm_threshold=356142312062976.0
2024-10-08 23:05:44,295 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.159e+32, grad_sumsq=1.151e+34, orig_rms_sq=1.875e-02
2024-10-08 23:05:47,682 WARNING [optim.py:503] Scaling gradients by 0.006114321295171976, model_norm_threshold=356142312062976.0
2024-10-08 23:05:47,818 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.321e+33, grad_sumsq=2.386e+35, orig_rms_sq=5.538e-03
2024-10-08 23:05:48,839 WARNING [optim.py:503] Scaling gradients by 0.00045935341040603817, model_norm_threshold=356142312062976.0
2024-10-08 23:05:48,981 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.569e+35, grad_sumsq=1.738e+34, orig_rms_sq=9.031e+00
2024-10-08 23:05:50,067 WARNING [optim.py:503] Scaling gradients by 0.00033869725302793086, model_norm_threshold=356142312062976.0
2024-10-08 23:05:50,208 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.407e+35, grad_sumsq=4.346e+37, orig_rms_sq=5.538e-03
2024-10-08 23:05:50,381 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer2.prob, batch_count=3714.0, ans=0.32590625
2024-10-08 23:05:51,318 WARNING [optim.py:503] Scaling gradients by 0.005742629524320364, model_norm_threshold=356142312062976.0
2024-10-08 23:05:51,456 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.232e+33, grad_sumsq=6.464e+34, orig_rms_sq=1.906e-02
2024-10-08 23:05:51,634 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=3714.0, ans=0.01643499999999999
2024-10-08 23:05:52,999 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=34.19 vs. limit=10.285499999999999
2024-10-08 23:05:53,615 WARNING [optim.py:503] Scaling gradients by 0.010274280793964863, model_norm_threshold=356142312062976.0
2024-10-08 23:05:53,754 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.103e+32, grad_sumsq=3.742e+34, orig_rms_sq=5.619e-03
2024-10-08 23:05:56,026 INFO [train.py:1154] Epoch 2, batch 4300, loss[loss=1.323, simple_loss=0.6673, pruned_loss=0.7748, ctc_loss=1.075, over 4839.00 frames. ], tot_loss[loss=1.253, simple_loss=0.6376, pruned_loss=0.7293, ctc_loss=1.025, over 967435.15 frames. ], batch size: 21, lr: 3.30e-02,
2024-10-08 23:06:01,573 WARNING [optim.py:503] Scaling gradients by 0.008486739359796047, model_norm_threshold=356142312062976.0
2024-10-08 23:06:01,711 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.572e+32, grad_sumsq=1.067e+32, orig_rms_sq=4.284e+00
2024-10-08 23:06:02,768 WARNING [optim.py:503] Scaling gradients by 0.00040590143180452287, model_norm_threshold=356142312062976.0
2024-10-08 23:06:02,907 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.377e+35, grad_sumsq=3.378e+35, orig_rms_sq=4.077e-01
2024-10-08 23:06:08,445 WARNING [optim.py:503] Scaling gradients by 0.0019988459534943104, model_norm_threshold=356142312062976.0
2024-10-08 23:06:08,583 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.57, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.794e+34, grad_sumsq=3.260e+36, orig_rms_sq=5.502e-03
2024-10-08 23:06:08,762 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=3720.6666666666665, ans=0.016285000000000008
2024-10-08 23:06:09,910 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.conv_module2.balancer1.max_abs, batch_count=3720.6666666666665, ans=7.3254166666666665
2024-10-08 23:06:13,771 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=15.36 vs. limit=8.89525
2024-10-08 23:06:14,492 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=16.69 vs. limit=10.2905
2024-10-08 23:06:14,589 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=21.85 vs. limit=8.89525
2024-10-08 23:06:15,176 WARNING [optim.py:503] Scaling gradients by 0.0003679563233163208, model_norm_threshold=356142312062976.0
2024-10-08 23:06:15,315 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.240e+35, grad_sumsq=1.136e+37, orig_rms_sq=1.972e-02
2024-10-08 23:06:17,765 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.const_attention_rate, batch_count=3724.0, ans=0.040525000000000005
2024-10-08 23:06:26,592 WARNING [optim.py:503] Scaling gradients by 0.00048692620475776494, model_norm_threshold=356142312062976.0
2024-10-08 23:06:26,732 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.942e+35, grad_sumsq=2.167e+34, orig_rms_sq=8.963e+00
2024-10-08 23:06:27,231 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=3.23 vs. limit=8.8965
2024-10-08 23:06:34,424 WARNING [optim.py:503] Scaling gradients by 0.004872411023825407, model_norm_threshold=356142312062976.0
2024-10-08 23:06:34,560 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.338e+33, grad_sumsq=6.534e+34, orig_rms_sq=2.047e-02
2024-10-08 23:06:39,625 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=45.04 vs. limit=8.89775
2024-10-08 23:06:40,130 WARNING [optim.py:503] Scaling gradients by 0.0014421733794733882, model_norm_threshold=356142312062976.0
2024-10-08 23:06:40,269 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.057e+34, grad_sumsq=2.297e+33, orig_rms_sq=8.953e+00
2024-10-08 23:06:42,724 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass.scale_min, batch_count=3730.6666666666665, ans=0.7694266666666667
2024-10-08 23:06:43,640 WARNING [optim.py:503] Scaling gradients by 0.03437191992998123, model_norm_threshold=356142312062976.0
2024-10-08 23:06:43,778 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.513e+31, grad_sumsq=2.188e+33, orig_rms_sq=2.063e-02
2024-10-08 23:06:44,329 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.76 vs. limit=6.865333333333333
2024-10-08 23:06:45,896 WARNING [optim.py:503] Scaling gradients by 0.02516133151948452, model_norm_threshold=356142312062976.0
2024-10-08 23:06:46,033 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.730e+31, grad_sumsq=7.421e+33, orig_rms_sq=5.026e-03
2024-10-08 23:06:46,609 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=4.32 vs. limit=3.5596
2024-10-08 23:06:47,081 WARNING [optim.py:503] Scaling gradients by 2.495694934623316e-05, model_norm_threshold=356142312062976.0
2024-10-08 23:06:47,219 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.825e+37, grad_sumsq=inf, orig_rms_sq=2.062e-02
2024-10-08 23:06:49,393 WARNING [optim.py:503] Scaling gradients by 0.039907604455947876, model_norm_threshold=356142312062976.0
2024-10-08 23:06:49,530 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.941e+31, grad_sumsq=2.175e+30, orig_rms_sq=8.924e+00
2024-10-08 23:06:50,909 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.120e+11 6.592e+13 5.113e+14 9.333e+15 1.427e+19, threshold=1.023e+15, percent-clipped=54.0
2024-10-08 23:06:53,247 INFO [train.py:1154] Epoch 2, batch 4350, loss[loss=1.163, simple_loss=0.5881, pruned_loss=0.6783, ctc_loss=0.9509, over 4840.00 frames. ], tot_loss[loss=1.254, simple_loss=0.6378, pruned_loss=0.7299, ctc_loss=1.027, over 966282.73 frames. ], batch size: 21, lr: 3.29e-02,
2024-10-08 23:06:56,624 WARNING [optim.py:503] Scaling gradients by 0.018134530633687973, model_norm_threshold=1022585609388032.0
2024-10-08 23:06:56,763 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.311e+33, grad_sumsq=2.683e+35, orig_rms_sq=4.884e-03
2024-10-08 23:07:00,352 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=3734.0, ans=0.76931
2024-10-08 23:07:00,382 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer2.prob, batch_count=3734.0, ans=0.32496875000000003
2024-10-08 23:07:03,125 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.27 vs. limit=10.3005
2024-10-08 23:07:06,872 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward1.hidden_balancer.prob, batch_count=3737.3333333333335, ans=0.3248125
2024-10-08 23:07:07,829 WARNING [optim.py:503] Scaling gradients by 0.0, model_norm_threshold=1022585609388032.0
2024-10-08 23:07:07,861 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out_norm.log_scale with proportion 0.00, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.445e+35, grad_sumsq=7.445e+37, orig_rms_sq=1.000e-02
2024-10-08 23:07:08,030 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3737.3333333333335, ans=0.2626266666666667
2024-10-08 23:07:08,073 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.ff3_skip_rate, batch_count=3737.3333333333335, ans=0.015909999999999994
2024-10-08 23:07:10,892 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.30 vs. limit=10.303
2024-10-08 23:07:13,091 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.73 vs. limit=6.868666666666667
2024-10-08 23:07:14,787 WARNING [optim.py:503] Scaling gradients by 0.000916292832698673, model_norm_threshold=1022585609388032.0
2024-10-08 23:07:14,924 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.443e+35, grad_sumsq=1.285e+37, orig_rms_sq=1.902e-02
2024-10-08 23:07:16,346 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.out_combiner.scale_min, batch_count=3740.6666666666665, ans=0.7690766666666667
2024-10-08 23:07:17,180 WARNING [optim.py:503] Scaling gradients by 0.033157430589199066, model_norm_threshold=1022585609388032.0
2024-10-08 23:07:17,319 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.615e+32, grad_sumsq=1.375e+34, orig_rms_sq=1.902e-02
2024-10-08 23:07:18,862 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.16 vs. limit=8.90275
2024-10-08 23:07:25,321 WARNING [optim.py:503] Scaling gradients by 0.00010726969776442274, model_norm_threshold=1022585609388032.0
2024-10-08 23:07:25,460 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.007e+37, grad_sumsq=6.955e+36, orig_rms_sq=4.324e+00
2024-10-08 23:07:28,371 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=28.78 vs. limit=8.904
2024-10-08 23:07:28,809 WARNING [optim.py:503] Scaling gradients by 0.0023449789732694626, model_norm_threshold=1022585609388032.0
2024-10-08 23:07:28,947 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.726e+34, grad_sumsq=2.489e+36, orig_rms_sq=1.899e-02
2024-10-08 23:07:31,584 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.whiten, num_groups=1, num_channels=256, metric=12.25 vs. limit=5.4976
2024-10-08 23:07:31,650 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=32.85 vs. limit=10.308
2024-10-08 23:07:36,720 WARNING [optim.py:503] Scaling gradients by 0.005132500547915697, model_norm_threshold=1022585609388032.0
2024-10-08 23:07:36,858 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.755e+33, grad_sumsq=4.117e+35, orig_rms_sq=1.884e-02
2024-10-08 23:07:39,115 WARNING [optim.py:503] Scaling gradients by 0.0010288567282259464, model_norm_threshold=1022585609388032.0
2024-10-08 23:07:39,253 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.136e+35, grad_sumsq=1.134e+37, orig_rms_sq=1.884e-02
2024-10-08 23:07:41,438 WARNING [optim.py:503] Scaling gradients by 0.02960740029811859, model_norm_threshold=1022585609388032.0
2024-10-08 23:07:41,577 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.401e+32, grad_sumsq=2.656e+31, orig_rms_sq=9.043e+00
2024-10-08 23:07:45,971 WARNING [optim.py:503] Scaling gradients by 0.023106947541236877, model_norm_threshold=1022585609388032.0
2024-10-08 23:07:46,111 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.063e+32, grad_sumsq=1.995e+35, orig_rms_sq=4.542e-03
2024-10-08 23:07:50,663 INFO [train.py:1154] Epoch 2, batch 4400, loss[loss=1.274, simple_loss=0.6446, pruned_loss=0.741, ctc_loss=1.056, over 4748.00 frames. ], tot_loss[loss=1.254, simple_loss=0.6379, pruned_loss=0.7296, ctc_loss=1.026, over 965909.51 frames. ], batch size: 26, lr: 3.29e-02,
2024-10-08 23:07:53,816 WARNING [optim.py:503] Scaling gradients by 0.00010508877312531695, model_norm_threshold=1022585609388032.0
2024-10-08 23:07:53,956 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.844e+37, grad_sumsq=inf, orig_rms_sq=1.846e-02
2024-10-08 23:07:54,213 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=3750.6666666666665, ans=0.059350000000000014
2024-10-08 23:07:57,722 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=8.28 vs. limit=8.9065
2024-10-08 23:07:59,962 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=10.75 vs. limit=8.9065
2024-10-08 23:08:03,924 WARNING [optim.py:503] Scaling gradients by 0.0022606397978961468, model_norm_threshold=1022585609388032.0
2024-10-08 23:08:04,065 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.824e+34, grad_sumsq=6.474e+33, orig_rms_sq=8.996e+00
2024-10-08 23:08:06,051 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.whiten1.whitening_limit, batch_count=3754.0, ans=5.9385
2024-10-08 23:08:06,625 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.balancer1.prob, batch_count=3754.0, ans=0.32403125
2024-10-08 23:08:09,638 WARNING [optim.py:503] Scaling gradients by 0.01082183513790369, model_norm_threshold=1022585609388032.0
2024-10-08 23:08:09,776 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.832e+33, grad_sumsq=6.612e+35, orig_rms_sq=4.283e-03
2024-10-08 23:08:10,002 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=3754.0, ans=0.26245999999999997
2024-10-08 23:08:10,832 WARNING [optim.py:503] Scaling gradients by 0.00022474276192951947, model_norm_threshold=1022585609388032.0
2024-10-08 23:08:10,971 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.061e+37, grad_sumsq=inf, orig_rms_sq=4.283e-03
2024-10-08 23:08:16,533 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.conv_module1.balancer2.prob, batch_count=3757.3333333333335, ans=0.323875
2024-10-08 23:08:19,870 WARNING [optim.py:503] Scaling gradients by 0.0238045547157526, model_norm_threshold=1022585609388032.0
2024-10-08 23:08:20,008 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.112e+32, grad_sumsq=1.321e+32, orig_rms_sq=3.869e+00
2024-10-08 23:08:22,205 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.66 vs. limit=3.5636
2024-10-08 23:08:24,577 WARNING [optim.py:503] Scaling gradients by 0.05396571755409241, model_norm_threshold=1022585609388032.0
2024-10-08 23:08:24,714 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.189e+32, grad_sumsq=2.759e+34, orig_rms_sq=4.311e-03
2024-10-08 23:08:24,915 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.self_attn_weights.pos_emb_skip_rate, batch_count=3760.6666666666665, ans=0.029916666666666702
2024-10-08 23:08:26,386 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.53 vs. limit=3.5641
2024-10-08 23:08:28,801 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=23.90 vs. limit=10.3205
2024-10-08 23:08:29,198 WARNING [optim.py:503] Scaling gradients by 0.027419425547122955, model_norm_threshold=1022585609388032.0
2024-10-08 23:08:29,336 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.113e+32, grad_sumsq=1.678e+34, orig_rms_sq=1.855e-02
2024-10-08 23:08:30,670 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=3760.6666666666665, ans=0.03846250000000001
2024-10-08 23:08:32,821 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.prob, batch_count=3760.6666666666665, ans=0.32371875
2024-10-08 23:08:33,674 WARNING [optim.py:503] Scaling gradients by 0.03494836762547493, model_norm_threshold=1022585609388032.0
2024-10-08 23:08:33,848 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.685e+32, grad_sumsq=3.680e+34, orig_rms_sq=4.578e-03
2024-10-08 23:08:36,072 WARNING [optim.py:503] Scaling gradients by 0.002196148969233036, model_norm_threshold=1022585609388032.0
2024-10-08 23:08:36,213 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.262e+34, grad_sumsq=1.341e+37, orig_rms_sq=4.668e-03
2024-10-08 23:08:39,111 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.97 vs. limit=8.9115
2024-10-08 23:08:40,566 WARNING [optim.py:503] Scaling gradients by 0.03200493007898331, model_norm_threshold=1022585609388032.0
2024-10-08 23:08:40,704 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.496e+32, grad_sumsq=1.148e+35, orig_rms_sq=4.786e-03
2024-10-08 23:08:43,980 WARNING [optim.py:503] Scaling gradients by 0.021361546590924263, model_norm_threshold=1022585609388032.0
2024-10-08 23:08:44,116 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.974e+32, grad_sumsq=2.180e+34, orig_rms_sq=1.823e-02
2024-10-08 23:08:44,890 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.38 vs. limit=8.9115
2024-10-08 23:08:45,260 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 4.464e+11 5.915e+13 5.288e+14 7.437e+15 inf, threshold=1.058e+15, percent-clipped=41.0
2024-10-08 23:08:47,659 INFO [train.py:1154] Epoch 2, batch 4450, loss[loss=1.306, simple_loss=0.6576, pruned_loss=0.7709, ctc_loss=1.032, over 4883.00 frames. ], tot_loss[loss=1.253, simple_loss=0.6381, pruned_loss=0.7283, ctc_loss=1.026, over 966227.33 frames. ], batch size: 19, lr: 3.28e-02,
2024-10-08 23:08:48,668 WARNING [optim.py:503] Scaling gradients by 0.008532029576599598, model_norm_threshold=1057566775836672.0
2024-10-08 23:08:48,808 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.425e+33, grad_sumsq=1.045e+33, orig_rms_sq=4.235e+00
2024-10-08 23:08:53,136 WARNING [optim.py:503] Scaling gradients by 0.010299943387508392, model_norm_threshold=1057566775836672.0
2024-10-08 23:08:53,276 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.318e+33, grad_sumsq=1.265e+35, orig_rms_sq=1.833e-02
2024-10-08 23:08:54,567 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=3767.3333333333335, ans=0.32340625
2024-10-08 23:08:56,626 WARNING [optim.py:503] Scaling gradients by 0.006262963637709618, model_norm_threshold=1057566775836672.0
2024-10-08 23:08:56,766 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.397e+33, grad_sumsq=1.528e+36, orig_rms_sq=4.840e-03
2024-10-08 23:08:58,589 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=6.01 vs. limit=5.508266666666667
2024-10-08 23:08:58,945 WARNING [optim.py:503] Scaling gradients by 0.0007528971182182431, model_norm_threshold=1057566775836672.0
2024-10-08 23:08:59,085 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.308e+35, grad_sumsq=3.643e+34, orig_rms_sq=9.079e+00
2024-10-08 23:09:07,062 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.5.prob, batch_count=3770.6666666666665, ans=0.32325000000000004
2024-10-08 23:09:09,233 WARNING [optim.py:503] Scaling gradients by 0.0010892434511333704, model_norm_threshold=1057566775836672.0
2024-10-08 23:09:09,371 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.925e+35, grad_sumsq=1.569e+37, orig_rms_sq=1.865e-02
2024-10-08 23:09:10,423 WARNING [optim.py:503] Scaling gradients by 0.038804057985544205, model_norm_threshold=1057566775836672.0
2024-10-08 23:09:10,561 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.294e+32, grad_sumsq=6.940e+33, orig_rms_sq=1.865e-02
2024-10-08 23:09:13,548 WARNING [optim.py:503] Scaling gradients by 0.002691039117053151, model_norm_threshold=1057566775836672.0
2024-10-08 23:09:13,688 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.309e+34, grad_sumsq=1.764e+36, orig_rms_sq=1.876e-02
2024-10-08 23:09:14,678 WARNING [optim.py:503] Scaling gradients by 0.040364716202020645, model_norm_threshold=1057566775836672.0
2024-10-08 23:09:14,816 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.708e+32, grad_sumsq=3.410e+34, orig_rms_sq=5.009e-03
2024-10-08 23:09:15,896 WARNING [optim.py:503] Scaling gradients by 0.02982170693576336, model_norm_threshold=1057566775836672.0
2024-10-08 23:09:16,033 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.248e+32, grad_sumsq=4.488e+34, orig_rms_sq=5.009e-03
2024-10-08 23:09:17,311 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.conv_skip_rate, batch_count=3774.0, ans=0.05847499999999997
2024-10-08 23:09:17,327 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=3774.0, ans=0.038206250000000004
2024-10-08 23:09:18,136 WARNING [optim.py:503] Scaling gradients by 0.0013846635119989514, model_norm_threshold=1057566775836672.0
2024-10-08 23:09:18,276 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.173e+35, grad_sumsq=1.287e+34, orig_rms_sq=9.118e+00
2024-10-08 23:09:20,389 WARNING [optim.py:503] Scaling gradients by 0.009790580719709396, model_norm_threshold=1057566775836672.0
2024-10-08 23:09:20,526 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.906e+33, grad_sumsq=5.808e+35, orig_rms_sq=5.003e-03
2024-10-08 23:09:21,593 WARNING [optim.py:503] Scaling gradients by 0.0033887289464473724, model_norm_threshold=1057566775836672.0
2024-10-08 23:09:21,730 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.923e+34, grad_sumsq=3.844e+36, orig_rms_sq=5.003e-03
2024-10-08 23:09:23,629 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.64 vs. limit=10.333
2024-10-08 23:09:31,247 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten.whitening_limit, batch_count=3777.3333333333335, ans=8.9165
2024-10-08 23:09:32,950 WARNING [optim.py:503] Scaling gradients by 0.03158145397901535, model_norm_threshold=1057566775836672.0
2024-10-08 23:09:33,089 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.446e+32, grad_sumsq=7.168e+34, orig_rms_sq=4.808e-03
2024-10-08 23:09:37,000 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=10.95 vs. limit=6.890333333333333
2024-10-08 23:09:37,087 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=25.63 vs. limit=10.3355
2024-10-08 23:09:43,377 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.balancer2.prob, batch_count=3780.6666666666665, ans=0.32278125
2024-10-08 23:09:45,205 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.47 vs. limit=10.338000000000001
2024-10-08 23:09:45,372 WARNING [optim.py:503] Scaling gradients by 0.021613944321870804, model_norm_threshold=1057566775836672.0
2024-10-08 23:09:45,510 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.323e+32, grad_sumsq=1.297e+32, orig_rms_sq=4.104e+00
2024-10-08 23:09:45,554 INFO [train.py:1154] Epoch 2, batch 4500, loss[loss=1.253, simple_loss=0.6403, pruned_loss=0.7289, ctc_loss=1.019, over 4845.00 frames. ], tot_loss[loss=1.252, simple_loss=0.6382, pruned_loss=0.728, ctc_loss=1.026, over 966157.38 frames. ], batch size: 28, lr: 3.28e-02,
2024-10-08 23:09:52,355 WARNING [optim.py:503] Scaling gradients by 0.04609911888837814, model_norm_threshold=1057566775836672.0
2024-10-08 23:09:52,495 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.136e+32, grad_sumsq=6.112e+33, orig_rms_sq=1.859e-02
2024-10-08 23:09:53,572 WARNING [optim.py:503] Scaling gradients by 0.013976525515317917, model_norm_threshold=1057566775836672.0
2024-10-08 23:09:53,711 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.806e+33, grad_sumsq=9.713e+34, orig_rms_sq=1.859e-02
2024-10-08 23:09:54,300 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=44.35 vs. limit=8.919
2024-10-08 23:09:58,186 WARNING [optim.py:503] Scaling gradients by 0.0010118539212271571, model_norm_threshold=1057566775836672.0
2024-10-08 23:09:58,325 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.463e+35, grad_sumsq=1.852e+37, orig_rms_sq=1.870e-02
2024-10-08 23:10:02,278 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=18.55 vs. limit=10.3405
2024-10-08 23:10:05,767 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.02 vs. limit=10.3405
2024-10-08 23:10:06,018 WARNING [optim.py:503] Scaling gradients by 0.04115193709731102, model_norm_threshold=1057566775836672.0
2024-10-08 23:10:06,157 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.113e+32, grad_sumsq=2.575e+31, orig_rms_sq=4.324e+00
2024-10-08 23:10:12,066 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.attention_skip_rate, batch_count=3790.6666666666665, ans=0.05785000000000001
2024-10-08 23:10:12,897 WARNING [optim.py:503] Scaling gradients by 0.038401179015636444, model_norm_threshold=1057566775836672.0
2024-10-08 23:10:13,036 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.644e+32, grad_sumsq=3.842e+31, orig_rms_sq=4.280e+00
2024-10-08 23:10:13,175 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.attention_skip_rate, batch_count=3790.6666666666665, ans=0.05785000000000001
2024-10-08 23:10:15,479 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=3790.6666666666665, ans=0.07
2024-10-08 23:10:19,181 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=14.40 vs. limit=5.9485
2024-10-08 23:10:21,482 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=512, metric=28.58 vs. limit=10.3455
2024-10-08 23:10:21,961 WARNING [optim.py:503] Scaling gradients by 0.012839661911129951, model_norm_threshold=1057566775836672.0
2024-10-08 23:10:22,102 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.920e+33, grad_sumsq=4.438e+32, orig_rms_sq=4.326e+00
2024-10-08 23:10:23,145 WARNING [optim.py:503] Scaling gradients by 0.031866706907749176, model_norm_threshold=1057566775836672.0
2024-10-08 23:10:23,287 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.983e+32, grad_sumsq=6.895e+31, orig_rms_sq=4.326e+00
2024-10-08 23:10:24,328 WARNING [optim.py:503] Scaling gradients by 0.028736405074596405, model_norm_threshold=1057566775836672.0
2024-10-08 23:10:24,466 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.125e+32, grad_sumsq=1.416e+32, orig_rms_sq=4.326e+00
2024-10-08 23:10:32,569 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=3797.3333333333335, ans=0.7670933333333334
2024-10-08 23:10:32,920 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=69.00 vs. limit=8.924
2024-10-08 23:10:34,460 WARNING [optim.py:503] Scaling gradients by 0.07672558724880219, model_norm_threshold=1057566775836672.0
2024-10-08 23:10:34,601 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.437e+31, grad_sumsq=1.513e+34, orig_rms_sq=4.915e-03
2024-10-08 23:10:36,311 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=20.10 vs. limit=8.924
2024-10-08 23:10:38,506 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.09 vs. limit=3.5696
2024-10-08 23:10:40,654 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.243e+12 6.813e+13 7.246e+14 8.808e+15 1.405e+18, threshold=1.449e+15, percent-clipped=42.0
2024-10-08 23:10:41,671 WARNING [optim.py:503] Scaling gradients by 0.004617643542587757, model_norm_threshold=1449296717152256.0
2024-10-08 23:10:41,808 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.55, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.378e+34, grad_sumsq=1.247e+34, orig_rms_sq=4.312e+00
2024-10-08 23:10:42,986 INFO [train.py:1154] Epoch 2, batch 4550, loss[loss=1.083, simple_loss=0.539, pruned_loss=0.626, ctc_loss=0.9395, over 4848.00 frames. ], tot_loss[loss=1.255, simple_loss=0.639, pruned_loss=0.7299, ctc_loss=1.028, over 965944.90 frames. ], batch size: 20, lr: 3.27e-02,
2024-10-08 23:10:47,479 WARNING [optim.py:503] Scaling gradients by 0.0021857249084860086, model_norm_threshold=1449296717152256.0
2024-10-08 23:10:47,618 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.501e+34, grad_sumsq=5.028e+36, orig_rms_sq=1.890e-02
2024-10-08 23:10:52,179 WARNING [optim.py:503] Scaling gradients by 0.07751646637916565, model_norm_threshold=1449296717152256.0
2024-10-08 23:10:52,317 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.383e+31, grad_sumsq=1.098e+34, orig_rms_sq=4.903e-03
2024-10-08 23:10:53,392 WARNING [optim.py:503] Scaling gradients by 0.0036163772456347942, model_norm_threshold=1449296717152256.0
2024-10-08 23:10:53,533 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.849e+34, grad_sumsq=2.027e+36, orig_rms_sq=1.899e-02
2024-10-08 23:10:55,847 WARNING [optim.py:503] Scaling gradients by 0.0016327652847394347, model_norm_threshold=1449296717152256.0
2024-10-08 23:10:55,985 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.14, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.128e+35, grad_sumsq=2.573e+34, orig_rms_sq=4.385e+00
2024-10-08 23:10:56,614 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=21.93 vs. limit=10.353
2024-10-08 23:11:01,500 WARNING [optim.py:503] Scaling gradients by 0.0004689059278462082, model_norm_threshold=1449296717152256.0
2024-10-08 23:11:01,638 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.591e+36, grad_sumsq=5.947e+35, orig_rms_sq=4.357e+00
2024-10-08 23:11:07,373 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_proj.dropout_p, batch_count=3807.3333333333335, ans=0.26192666666666664
2024-10-08 23:11:08,298 WARNING [optim.py:503] Scaling gradients by 0.09694104641675949, model_norm_threshold=1449296717152256.0
2024-10-08 23:11:08,437 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.617e+31, grad_sumsq=1.650e+34, orig_rms_sq=4.616e-03
2024-10-08 23:11:11,758 WARNING [optim.py:503] Scaling gradients by 0.005666766781359911, model_norm_threshold=1449296717152256.0
2024-10-08 23:11:11,896 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.458e+34, grad_sumsq=7.855e+35, orig_rms_sq=1.856e-02
2024-10-08 23:11:13,207 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=5.468e+05
2024-10-08 23:11:14,050 WARNING [optim.py:503] Scaling gradients by 0.0017341229831799865, model_norm_threshold=1449296717152256.0
2024-10-08 23:11:14,191 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.168e+35, grad_sumsq=1.306e+34, orig_rms_sq=8.948e+00
2024-10-08 23:11:14,246 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=3807.3333333333335, ans=0.16478216666666667
2024-10-08 23:11:18,676 WARNING [optim.py:503] Scaling gradients by 0.09502115100622177, model_norm_threshold=1449296717152256.0
2024-10-08 23:11:18,813 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.524e+31, grad_sumsq=1.411e+34, orig_rms_sq=4.625e-03
2024-10-08 23:11:23,490 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=10.23 vs. limit=8.929
2024-10-08 23:11:34,507 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.scale_min, batch_count=3814.0, ans=0.76651
2024-10-08 23:11:36,739 WARNING [optim.py:503] Scaling gradients by 0.04438610002398491, model_norm_threshold=1449296717152256.0
2024-10-08 23:11:36,880 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.790e+32, grad_sumsq=3.082e+31, orig_rms_sq=9.053e+00
2024-10-08 23:11:39,065 WARNING [optim.py:503] Scaling gradients by 0.05323592200875282, model_norm_threshold=1449296717152256.0
2024-10-08 23:11:39,204 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.611e+32, grad_sumsq=8.583e+33, orig_rms_sq=1.877e-02
2024-10-08 23:11:39,676 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=19.16 vs. limit=10.363
2024-10-08 23:11:40,447 INFO [train.py:1154] Epoch 2, batch 4600, loss[loss=1.287, simple_loss=0.6753, pruned_loss=0.7355, ctc_loss=1.071, over 4759.00 frames. ], tot_loss[loss=1.256, simple_loss=0.6394, pruned_loss=0.7304, ctc_loss=1.028, over 966274.07 frames. ], batch size: 45, lr: 3.27e-02,
2024-10-08 23:11:43,699 WARNING [optim.py:503] Scaling gradients by 0.06529561430215836, model_norm_threshold=1449296717152256.0
2024-10-08 23:11:43,838 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.372e+32, grad_sumsq=3.026e+34, orig_rms_sq=4.533e-03
2024-10-08 23:11:44,493 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.feed_forward3.out_whiten.whitening_limit, batch_count=3817.3333333333335, ans=8.9315
2024-10-08 23:11:47,159 WARNING [optim.py:503] Scaling gradients by 0.04906664043664932, model_norm_threshold=1449296717152256.0
2024-10-08 23:11:47,297 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.822e+32, grad_sumsq=6.350e+34, orig_rms_sq=4.444e-03
2024-10-08 23:11:47,425 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=3817.3333333333335, ans=0.32106250000000003
2024-10-08 23:11:50,552 WARNING [optim.py:503] Scaling gradients by 0.0021752920001745224, model_norm_threshold=1449296717152256.0
2024-10-08 23:11:50,695 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.183e+35, grad_sumsq=2.745e+37, orig_rms_sq=4.309e-03
2024-10-08 23:11:52,242 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=11.25 vs. limit=8.93275
2024-10-08 23:11:56,496 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass.skip_rate, batch_count=3820.6666666666665, ans=0.04949747468305833
2024-10-08 23:11:58,548 WARNING [optim.py:503] Scaling gradients by 0.050321970134973526, model_norm_threshold=1449296717152256.0
2024-10-08 23:11:58,687 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.938e+32, grad_sumsq=1.034e+34, orig_rms_sq=1.874e-02
2024-10-08 23:12:03,275 WARNING [optim.py:503] Scaling gradients by 0.011547976173460484, model_norm_threshold=1449296717152256.0
2024-10-08 23:12:03,417 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.737e+33, grad_sumsq=9.286e+32, orig_rms_sq=4.024e+00
2024-10-08 23:12:07,938 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.attention_skip_rate, batch_count=3824.0, ans=0.056599999999999984
2024-10-08 23:12:14,002 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=16.47 vs. limit=6.913666666666667
2024-10-08 23:12:21,426 WARNING [optim.py:503] Scaling gradients by 0.0026898710057139397, model_norm_threshold=1449296717152256.0
2024-10-08 23:12:21,564 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.338e+35, grad_sumsq=3.261e+34, orig_rms_sq=4.105e+00
2024-10-08 23:12:23,742 WARNING [optim.py:503] Scaling gradients by 0.012031476013362408, model_norm_threshold=1449296717152256.0
2024-10-08 23:12:23,897 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.520e+33, grad_sumsq=1.901e+35, orig_rms_sq=1.852e-02
2024-10-08 23:12:24,113 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=3827.3333333333335, ans=0.7882733333333334
2024-10-08 23:12:25,772 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=14.94 vs. limit=8.9365
2024-10-08 23:12:27,184 WARNING [optim.py:503] Scaling gradients by 0.09343443065881729, model_norm_threshold=1449296717152256.0
2024-10-08 23:12:27,323 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.647e+31, grad_sumsq=3.066e+33, orig_rms_sq=1.842e-02
2024-10-08 23:12:31,287 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=22.31 vs. limit=8.9365
2024-10-08 23:12:33,160 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer1.prob, batch_count=3830.6666666666665, ans=0.3204375
2024-10-08 23:12:35,226 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.481e+11 6.629e+13 3.755e+14 5.202e+15 3.091e+18, threshold=7.510e+14, percent-clipped=37.0
2024-10-08 23:12:35,450 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=3830.6666666666665, ans=0.021166666666666667
2024-10-08 23:12:37,509 INFO [train.py:1154] Epoch 2, batch 4650, loss[loss=1.289, simple_loss=0.6703, pruned_loss=0.7462, ctc_loss=1.038, over 4819.00 frames. ], tot_loss[loss=1.254, simple_loss=0.6389, pruned_loss=0.729, ctc_loss=1.027, over 965720.76 frames. ], batch size: 36, lr: 3.26e-02,
2024-10-08 23:12:39,877 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module2.balancer2.prob, batch_count=3834.0, ans=0.32028124999999996
2024-10-08 23:12:42,103 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_module1.balancer1.prob, batch_count=3834.0, ans=0.32028124999999996
2024-10-08 23:12:44,222 WARNING [optim.py:503] Scaling gradients by 0.06238691881299019, model_norm_threshold=750994627493888.0
2024-10-08 23:12:44,358 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.211e+31, grad_sumsq=4.604e+33, orig_rms_sq=4.803e-03
2024-10-08 23:12:45,394 WARNING [optim.py:503] Scaling gradients by 0.06734739989042282, model_norm_threshold=750994627493888.0
2024-10-08 23:12:45,534 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.814e+31, grad_sumsq=1.523e+33, orig_rms_sq=1.847e-02
2024-10-08 23:12:51,034 WARNING [optim.py:503] Scaling gradients by 0.04488968849182129, model_norm_threshold=750994627493888.0
2024-10-08 23:12:51,173 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.072e+32, grad_sumsq=2.220e+34, orig_rms_sq=4.831e-03
2024-10-08 23:12:52,226 WARNING [optim.py:503] Scaling gradients by 4.188645107205957e-05, model_norm_threshold=750994627493888.0
2024-10-08 23:12:52,365 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.911e+37, grad_sumsq=inf, orig_rms_sq=1.850e-02
2024-10-08 23:12:53,418 WARNING [optim.py:503] Scaling gradients by 0.0006064603221602738, model_norm_threshold=750994627493888.0
2024-10-08 23:12:53,558 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.489e+35, grad_sumsq=1.545e+38, orig_rms_sq=4.846e-03
2024-10-08 23:12:54,820 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer1.prob, batch_count=3837.3333333333335, ans=0.320125
2024-10-08 23:13:02,471 WARNING [optim.py:503] Scaling gradients by 0.00076946901390329, model_norm_threshold=750994627493888.0
2024-10-08 23:13:02,609 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.492e+35, grad_sumsq=7.246e+37, orig_rms_sq=4.819e-03
2024-10-08 23:13:03,674 WARNING [optim.py:503] Scaling gradients by 0.01696433313190937, model_norm_threshold=750994627493888.0
2024-10-08 23:13:03,812 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.440e+32, grad_sumsq=2.913e+34, orig_rms_sq=1.867e-02
2024-10-08 23:13:05,036 WARNING [optim.py:503] Scaling gradients by 0.09302926063537598, model_norm_threshold=750994627493888.0
2024-10-08 23:13:05,174 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.487e+31, grad_sumsq=5.162e+33, orig_rms_sq=4.819e-03
2024-10-08 23:13:08,341 WARNING [optim.py:503] Scaling gradients by 0.045424677431583405, model_norm_threshold=750994627493888.0
2024-10-08 23:13:08,480 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.984e+31, grad_sumsq=2.661e+33, orig_rms_sq=1.873e-02
2024-10-08 23:13:09,804 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.attention_skip_rate, batch_count=3840.6666666666665, ans=0.055975
2024-10-08 23:13:14,227 WARNING [optim.py:503] Scaling gradients by 0.08496764302253723, model_norm_threshold=750994627493888.0
2024-10-08 23:13:14,367 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.771e+31, grad_sumsq=9.420e+32, orig_rms_sq=1.880e-02
2024-10-08 23:13:18,115 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=31.69 vs. limit=10.383
2024-10-08 23:13:19,328 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=26.92 vs. limit=10.383
2024-10-08 23:13:20,066 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.balancer1.prob, batch_count=3844.0, ans=0.3198125
2024-10-08 23:13:20,330 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module1.whiten, num_groups=1, num_channels=384, metric=2.43 vs. limit=8.9415
2024-10-08 23:13:20,894 WARNING [optim.py:503] Scaling gradients by 0.00028900697361677885, model_norm_threshold=750994627493888.0
2024-10-08 23:13:21,032 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.116e+36, grad_sumsq=1.250e+35, orig_rms_sq=8.931e+00
2024-10-08 23:13:26,592 WARNING [optim.py:503] Scaling gradients by 0.027121651917696, model_norm_threshold=750994627493888.0
2024-10-08 23:13:26,732 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.645e+32, grad_sumsq=4.180e+32, orig_rms_sq=3.936e-01
2024-10-08 23:13:27,993 WARNING [optim.py:503] Scaling gradients by 0.012468910776078701, model_norm_threshold=750994627493888.0
2024-10-08 23:13:28,132 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.52, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.874e+33, grad_sumsq=4.012e+35, orig_rms_sq=4.672e-03
2024-10-08 23:13:29,525 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=31.56 vs. limit=8.94275
2024-10-08 23:13:31,076 WARNING [optim.py:503] Scaling gradients by 9.386154124513268e-05, model_norm_threshold=750994627493888.0
2024-10-08 23:13:31,213 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.809e+37, grad_sumsq=inf, orig_rms_sq=1.884e-02
2024-10-08 23:13:33,266 WARNING [optim.py:503] Scaling gradients by 0.023864232003688812, model_norm_threshold=750994627493888.0
2024-10-08 23:13:33,405 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.455e+32, grad_sumsq=1.303e+34, orig_rms_sq=1.884e-02
2024-10-08 23:13:34,483 WARNING [optim.py:503] Scaling gradients by 0.0008970333146862686, model_norm_threshold=750994627493888.0
2024-10-08 23:13:34,624 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.042e+35, grad_sumsq=1.084e+37, orig_rms_sq=1.884e-02
2024-10-08 23:13:35,724 WARNING [optim.py:503] Scaling gradients by 0.0022050829138606787, model_norm_threshold=750994627493888.0
2024-10-08 23:13:35,862 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.931e+34, grad_sumsq=1.549e+36, orig_rms_sq=1.892e-02
2024-10-08 23:13:35,908 INFO [train.py:1154] Epoch 2, batch 4700, loss[loss=1.15, simple_loss=0.5744, pruned_loss=0.6731, ctc_loss=0.948, over 4940.00 frames. ], tot_loss[loss=1.252, simple_loss=0.6376, pruned_loss=0.7277, ctc_loss=1.025, over 965802.64 frames. ], batch size: 19, lr: 3.26e-02,
2024-10-08 23:13:36,869 WARNING [optim.py:503] Scaling gradients by 0.018443383276462555, model_norm_threshold=750994627493888.0
2024-10-08 23:13:37,005 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.758e+32, grad_sumsq=1.458e+34, orig_rms_sq=1.892e-02
2024-10-08 23:13:40,159 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=17.77 vs. limit=8.943999999999999
2024-10-08 23:13:41,755 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.conv_module2.balancer2.prob, batch_count=3850.6666666666665, ans=0.3195
2024-10-08 23:13:42,271 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=29.35 vs. limit=10.388
2024-10-08 23:13:46,243 WARNING [optim.py:503] Scaling gradients by 0.005413522478193045, model_norm_threshold=750994627493888.0
2024-10-08 23:13:46,380 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.343e+33, grad_sumsq=4.363e+35, orig_rms_sq=1.912e-02
2024-10-08 23:13:47,443 WARNING [optim.py:503] Scaling gradients by 0.0009643909288570285, model_norm_threshold=750994627493888.0
2024-10-08 23:13:47,581 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.746e+35, grad_sumsq=9.130e+36, orig_rms_sq=1.912e-02
2024-10-08 23:13:49,487 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=15.52 vs. limit=8.94525
2024-10-08 23:13:52,099 WARNING [optim.py:503] Scaling gradients by 0.008770654909312725, model_norm_threshold=750994627493888.0
2024-10-08 23:13:52,237 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.258e+33, grad_sumsq=1.175e+35, orig_rms_sq=1.921e-02
2024-10-08 23:13:54,312 WARNING [optim.py:503] Scaling gradients by 0.0013773179380223155, model_norm_threshold=750994627493888.0
2024-10-08 23:13:54,450 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.129e+34, grad_sumsq=3.700e+36, orig_rms_sq=1.926e-02
2024-10-08 23:13:54,624 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=3854.0, ans=0.01824999999999999
2024-10-08 23:13:55,513 WARNING [optim.py:503] Scaling gradients by 0.094366155564785, model_norm_threshold=750994627493888.0
2024-10-08 23:13:55,652 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.376e+31, grad_sumsq=5.110e+33, orig_rms_sq=4.650e-03
2024-10-08 23:13:57,824 WARNING [optim.py:503] Scaling gradients by 0.06735694408416748, model_norm_threshold=750994627493888.0
2024-10-08 23:13:57,964 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.604e+31, grad_sumsq=1.352e+33, orig_rms_sq=1.926e-02
2024-10-08 23:14:09,174 WARNING [optim.py:503] Scaling gradients by 0.0012508757645264268, model_norm_threshold=750994627493888.0
2024-10-08 23:14:09,312 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.987e+34, grad_sumsq=3.148e+36, orig_rms_sq=1.902e-02
2024-10-08 23:14:09,553 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.bypass.skip_rate, batch_count=3860.6666666666665, ans=0.07
2024-10-08 23:14:10,366 WARNING [optim.py:503] Scaling gradients by 0.062233004719018936, model_norm_threshold=750994627493888.0
2024-10-08 23:14:10,505 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.014e+31, grad_sumsq=4.495e+30, orig_rms_sq=8.930e+00
2024-10-08 23:14:12,414 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=10.49 vs. limit=5.965166666666667
2024-10-08 23:14:12,825 WARNING [optim.py:503] Scaling gradients by 0.002879910869523883, model_norm_threshold=750994627493888.0
2024-10-08 23:14:12,962 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.14, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.232e+33, grad_sumsq=1.966e+36, orig_rms_sq=4.695e-03
2024-10-08 23:14:16,352 WARNING [optim.py:503] Scaling gradients by 0.0559675432741642, model_norm_threshold=750994627493888.0
2024-10-08 23:14:16,492 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.806e+31, grad_sumsq=2.015e+33, orig_rms_sq=1.889e-02
2024-10-08 23:14:17,602 WARNING [optim.py:503] Scaling gradients by 0.0037687241565436125, model_norm_threshold=750994627493888.0
2024-10-08 23:14:17,740 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.061e+33, grad_sumsq=4.271e+35, orig_rms_sq=1.887e-02
2024-10-08 23:14:20,000 WARNING [optim.py:503] Scaling gradients by 0.06916682422161102, model_norm_threshold=750994627493888.0
2024-10-08 23:14:20,141 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.102e+31, grad_sumsq=1.114e+33, orig_rms_sq=1.887e-02
2024-10-08 23:14:22,806 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=9.08 vs. limit=5.5456
2024-10-08 23:14:23,062 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=19.02 vs. limit=5.966
2024-10-08 23:14:23,680 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.bypass_mid.scale_min, batch_count=3864.0, ans=0.76476
2024-10-08 23:14:25,215 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=12.17 vs. limit=6.932
2024-10-08 23:14:25,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.04 vs. limit=5.966
2024-10-08 23:14:26,801 WARNING [optim.py:503] Scaling gradients by 0.05281972885131836, model_norm_threshold=750994627493888.0
2024-10-08 23:14:26,941 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.935e+31, grad_sumsq=2.100e+33, orig_rms_sq=1.874e-02
2024-10-08 23:14:31,308 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.248e+12 3.570e+13 4.013e+14 1.204e+16 1.793e+19, threshold=8.026e+14, percent-clipped=41.0
2024-10-08 23:14:33,473 WARNING [optim.py:503] Scaling gradients by 0.015402250923216343, model_norm_threshold=802577520263168.0
2024-10-08 23:14:33,610 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.529e+32, grad_sumsq=6.096e+31, orig_rms_sq=9.071e+00
2024-10-08 23:14:33,655 INFO [train.py:1154] Epoch 2, batch 4750, loss[loss=1.258, simple_loss=0.6569, pruned_loss=0.7184, ctc_loss=1.053, over 4725.00 frames. ], tot_loss[loss=1.253, simple_loss=0.6387, pruned_loss=0.7283, ctc_loss=1.025, over 965609.32 frames. ], batch size: 45, lr: 3.25e-02,
2024-10-08 23:14:34,270 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=8, num_channels=256, metric=3.70 vs. limit=3.5801
2024-10-08 23:14:36,889 WARNING [optim.py:503] Scaling gradients by 0.016735630109906197, model_norm_threshold=802577520263168.0
2024-10-08 23:14:37,028 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.503e+32, grad_sumsq=2.406e+34, orig_rms_sq=1.872e-02
2024-10-08 23:14:38,383 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=3867.3333333333335, ans=0.01658333333333334
2024-10-08 23:14:40,341 WARNING [optim.py:503] Scaling gradients by 0.010032503865659237, model_norm_threshold=802577520263168.0
2024-10-08 23:14:40,480 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.307e+33, grad_sumsq=2.917e+35, orig_rms_sq=4.482e-03
2024-10-08 23:14:43,566 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.feed_forward1.out_whiten.whitening_limit, batch_count=3867.3333333333335, ans=8.95025
2024-10-08 23:14:48,282 WARNING [optim.py:503] Scaling gradients by 0.007434822153300047, model_norm_threshold=802577520263168.0
2024-10-08 23:14:48,421 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.403e+33, grad_sumsq=7.660e+35, orig_rms_sq=4.442e-03
2024-10-08 23:14:48,957 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.whiten, num_groups=1, num_channels=512, metric=6.50 vs. limit=5.548266666666667
2024-10-08 23:14:51,071 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=32.57 vs. limit=8.9515
2024-10-08 23:14:51,703 WARNING [optim.py:503] Scaling gradients by 0.007446263451129198, model_norm_threshold=802577520263168.0
2024-10-08 23:14:51,841 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.090e+33, grad_sumsq=7.044e+35, orig_rms_sq=4.387e-03
2024-10-08 23:14:55,163 WARNING [optim.py:503] Scaling gradients by 0.0027929090429097414, model_norm_threshold=802577520263168.0
2024-10-08 23:14:55,301 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.597e+34, grad_sumsq=3.704e+36, orig_rms_sq=4.312e-03
2024-10-08 23:14:55,867 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=9.62 vs. limit=6.937
2024-10-08 23:14:59,652 WARNING [optim.py:503] Scaling gradients by 0.012221736833453178, model_norm_threshold=802577520263168.0
2024-10-08 23:14:59,790 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.066e+33, grad_sumsq=2.473e+35, orig_rms_sq=4.309e-03
2024-10-08 23:14:59,951 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=3874.0, ans=0.26126
2024-10-08 23:15:00,288 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.10 vs. limit=5.9685
2024-10-08 23:15:02,572 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=9.27 vs. limit=5.5496
2024-10-08 23:15:03,066 WARNING [optim.py:503] Scaling gradients by 0.0, model_norm_threshold=802577520263168.0
2024-10-08 23:15:03,098 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out_norm.log_scale with proportion 0.00, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.908e+35, grad_sumsq=2.908e+37, orig_rms_sq=1.000e-02
2024-10-08 23:15:04,264 WARNING [optim.py:503] Scaling gradients by 0.011610832065343857, model_norm_threshold=802577520263168.0
2024-10-08 23:15:04,402 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.777e+33, grad_sumsq=4.718e+32, orig_rms_sq=3.765e+00
2024-10-08 23:15:11,276 WARNING [optim.py:503] Scaling gradients by 0.018305839970707893, model_norm_threshold=802577520263168.0
2024-10-08 23:15:11,414 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.361e+32, grad_sumsq=4.893e+31, orig_rms_sq=8.913e+00
2024-10-08 23:15:12,507 WARNING [optim.py:503] Scaling gradients by 0.0032051559537649155, model_norm_threshold=802577520263168.0
2024-10-08 23:15:12,647 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.113e+34, grad_sumsq=1.118e+36, orig_rms_sq=1.890e-02
2024-10-08 23:15:14,919 WARNING [optim.py:503] Scaling gradients by 4.9928414227906615e-05, model_norm_threshold=802577520263168.0
2024-10-08 23:15:15,058 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.446e+37, grad_sumsq=inf, orig_rms_sq=1.890e-02
2024-10-08 23:15:22,132 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=24.05 vs. limit=8.95525
2024-10-08 23:15:22,810 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff2_skip_rate, batch_count=3880.6666666666665, ans=0.012684999999999988
2024-10-08 23:15:25,988 WARNING [optim.py:503] Scaling gradients by 0.07868295162916183, model_norm_threshold=802577520263168.0
2024-10-08 23:15:26,126 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.36, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.735e+31, grad_sumsq=2.021e+33, orig_rms_sq=1.848e-02
2024-10-08 23:15:30,631 INFO [train.py:1154] Epoch 2, batch 4800, loss[loss=1.18, simple_loss=0.5878, pruned_loss=0.6839, ctc_loss=1.01, over 4888.00 frames. ], tot_loss[loss=1.249, simple_loss=0.6361, pruned_loss=0.7259, ctc_loss=1.024, over 966010.38 frames. ], batch size: 22, lr: 3.25e-02,
2024-10-08 23:15:33,946 WARNING [optim.py:503] Scaling gradients by 0.04086456447839737, model_norm_threshold=802577520263168.0
2024-10-08 23:15:34,085 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.218e+31, grad_sumsq=4.957e+33, orig_rms_sq=1.860e-02
2024-10-08 23:15:35,453 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=3884.0, ans=7.4275
2024-10-08 23:15:35,741 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=33.62 vs. limit=6.942
2024-10-08 23:15:38,009 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.70 vs. limit=3.5826000000000002
2024-10-08 23:15:41,417 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.08 vs. limit=8.95775
2024-10-08 23:15:41,861 WARNING [optim.py:503] Scaling gradients by 0.002466256031766534, model_norm_threshold=802577520263168.0
2024-10-08 23:15:41,999 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.897e+34, grad_sumsq=7.963e+36, orig_rms_sq=4.894e-03
2024-10-08 23:15:43,071 WARNING [optim.py:503] Scaling gradients by 0.005346396472305059, model_norm_threshold=802577520263168.0
2024-10-08 23:15:43,208 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.715e+33, grad_sumsq=3.012e+35, orig_rms_sq=1.898e-02
2024-10-08 23:15:44,998 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=13.06 vs. limit=8.95775
2024-10-08 23:15:50,105 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=3887.3333333333335, ans=0.2611266666666667
2024-10-08 23:15:53,336 WARNING [optim.py:503] Scaling gradients by 0.03557160124182701, model_norm_threshold=802577520263168.0
2024-10-08 23:15:53,473 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.926e+32, grad_sumsq=9.909e+33, orig_rms_sq=1.944e-02
2024-10-08 23:15:57,131 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=3890.6666666666665, ans=0.05410000000000001
2024-10-08 23:15:57,180 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=3890.6666666666665, ans=0.317625
2024-10-08 23:15:58,068 WARNING [optim.py:503] Scaling gradients by 0.013147415593266487, model_norm_threshold=802577520263168.0
2024-10-08 23:15:58,207 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.032e+33, grad_sumsq=1.160e+32, orig_rms_sq=8.897e+00
2024-10-08 23:15:58,420 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.ff2_skip_rate, batch_count=3890.6666666666665, ans=0.012459999999999999
2024-10-08 23:15:59,234 WARNING [optim.py:503] Scaling gradients by 0.0047130160965025425, model_norm_threshold=802577520263168.0
2024-10-08 23:15:59,373 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.715e+33, grad_sumsq=1.613e+36, orig_rms_sq=4.782e-03
2024-10-08 23:16:00,518 WARNING [optim.py:503] Scaling gradients by 0.0009685615659691393, model_norm_threshold=802577520263168.0
2024-10-08 23:16:00,655 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.753e+35, grad_sumsq=9.010e+36, orig_rms_sq=1.945e-02
2024-10-08 23:16:01,316 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=8.83 vs. limit=8.959
2024-10-08 23:16:02,732 WARNING [optim.py:503] Scaling gradients by 0.0006676046177744865, model_norm_threshold=802577520263168.0
2024-10-08 23:16:02,872 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.231e+35, grad_sumsq=5.580e+34, orig_rms_sq=3.998e+00
2024-10-08 23:16:03,980 WARNING [optim.py:503] Scaling gradients by 0.022330893203616142, model_norm_threshold=802577520263168.0
2024-10-08 23:16:04,119 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.636e+32, grad_sumsq=1.926e+34, orig_rms_sq=1.888e-02
2024-10-08 23:16:05,180 WARNING [optim.py:503] Scaling gradients by 0.0025356989353895187, model_norm_threshold=802577520263168.0
2024-10-08 23:16:05,318 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.838e+34, grad_sumsq=6.044e+36, orig_rms_sq=4.695e-03
2024-10-08 23:16:05,877 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=73.69 vs. limit=8.96025
2024-10-08 23:16:06,379 WARNING [optim.py:503] Scaling gradients by 0.0004863742215093225, model_norm_threshold=802577520263168.0
2024-10-08 23:16:06,519 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.745e+35, grad_sumsq=1.223e+38, orig_rms_sq=4.695e-03
2024-10-08 23:16:08,639 WARNING [optim.py:503] Scaling gradients by 0.015816275030374527, model_norm_threshold=802577520263168.0
2024-10-08 23:16:08,778 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.489e+32, grad_sumsq=1.167e+35, orig_rms_sq=4.706e-03
2024-10-08 23:16:10,918 WARNING [optim.py:503] Scaling gradients by 0.08680327236652374, model_norm_threshold=802577520263168.0
2024-10-08 23:16:11,056 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.178e+31, grad_sumsq=1.169e+33, orig_rms_sq=1.863e-02
2024-10-08 23:16:11,566 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=6.98 vs. limit=5.5576
2024-10-08 23:16:16,551 WARNING [optim.py:503] Scaling gradients by 0.003184051252901554, model_norm_threshold=802577520263168.0
2024-10-08 23:16:16,692 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.358e+34, grad_sumsq=7.261e+35, orig_rms_sq=1.871e-02
2024-10-08 23:16:18,895 WARNING [optim.py:503] Scaling gradients by 0.008694896474480629, model_norm_threshold=802577520263168.0
2024-10-08 23:16:19,032 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.455e+33, grad_sumsq=1.312e+35, orig_rms_sq=1.871e-02
2024-10-08 23:16:20,618 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=384, metric=8.04 vs. limit=8.961500000000001
2024-10-08 23:16:20,902 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.66 vs. limit=10.423
2024-10-08 23:16:21,044 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=192, metric=14.59 vs. limit=8.961500000000001
2024-10-08 23:16:24,707 WARNING [optim.py:503] Scaling gradients by 7.436601299559698e-05, model_norm_threshold=802577520263168.0
2024-10-08 23:16:24,845 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.342e+37, grad_sumsq=inf, orig_rms_sq=1.881e-02
2024-10-08 23:16:26,022 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.246e+11 5.951e+13 9.532e+14 3.594e+16 inf, threshold=1.906e+15, percent-clipped=51.0
2024-10-08 23:16:28,140 WARNING [optim.py:503] Scaling gradients by 0.06357245147228241, model_norm_threshold=1906328884740096.0
2024-10-08 23:16:28,279 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.727e+32, grad_sumsq=5.831e+34, orig_rms_sq=4.676e-03
2024-10-08 23:16:28,324 INFO [train.py:1154] Epoch 2, batch 4850, loss[loss=1.222, simple_loss=0.6073, pruned_loss=0.7073, ctc_loss=1.056, over 4854.00 frames. ], tot_loss[loss=1.253, simple_loss=0.6386, pruned_loss=0.7289, ctc_loss=1.025, over 966638.87 frames. ], batch size: 28, lr: 3.24e-02,
2024-10-08 23:16:30,429 WARNING [optim.py:503] Scaling gradients by 0.0013495279708877206, model_norm_threshold=1906328884740096.0
2024-10-08 23:16:30,565 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.714e+35, grad_sumsq=1.226e+38, orig_rms_sq=4.660e-03
2024-10-08 23:16:31,286 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=16.48 vs. limit=8.96275
2024-10-08 23:16:32,732 WARNING [optim.py:503] Scaling gradients by 0.00223580002784729, model_norm_threshold=1906328884740096.0
2024-10-08 23:16:32,871 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.613e+35, grad_sumsq=3.461e+37, orig_rms_sq=4.660e-03
2024-10-08 23:16:34,998 WARNING [optim.py:503] Scaling gradients by 0.01846347749233246, model_norm_threshold=1906328884740096.0
2024-10-08 23:16:35,136 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.743e+33, grad_sumsq=5.804e+35, orig_rms_sq=4.725e-03
2024-10-08 23:16:38,346 WARNING [optim.py:503] Scaling gradients by 0.0005118357366882265, model_norm_threshold=1906328884740096.0
2024-10-08 23:16:38,486 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.394e+36, grad_sumsq=1.286e+38, orig_rms_sq=1.861e-02
2024-10-08 23:16:41,296 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=17.96 vs. limit=8.964
2024-10-08 23:16:43,668 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=16.63 vs. limit=8.964
2024-10-08 23:16:44,519 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten.whitening_limit, batch_count=3904.0, ans=8.964
2024-10-08 23:16:44,822 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=14.10 vs. limit=8.964
2024-10-08 23:16:48,690 WARNING [optim.py:503] Scaling gradients by 0.04920747131109238, model_norm_threshold=1906328884740096.0
2024-10-08 23:16:48,828 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.495e+32, grad_sumsq=6.237e+32, orig_rms_sq=4.000e-01
2024-10-08 23:16:53,894 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=18.37 vs. limit=5.976833333333333
2024-10-08 23:16:55,233 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.76 vs. limit=3.5861
2024-10-08 23:16:56,528 WARNING [optim.py:503] Scaling gradients by 0.000944297993555665, model_norm_threshold=1906328884740096.0
2024-10-08 23:16:56,666 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.489e+36, grad_sumsq=3.052e+38, orig_rms_sq=4.878e-03
2024-10-08 23:17:00,076 WARNING [optim.py:503] Scaling gradients by 0.019594870507717133, model_norm_threshold=1906328884740096.0
2024-10-08 23:17:00,213 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.713e+33, grad_sumsq=3.588e+35, orig_rms_sq=4.774e-03
2024-10-08 23:17:03,117 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=512, metric=43.38 vs. limit=10.433
2024-10-08 23:17:03,545 WARNING [optim.py:503] Scaling gradients by 0.00017146112804766744, model_norm_threshold=1906328884740096.0
2024-10-08 23:17:03,684 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.927e+37, grad_sumsq=inf, orig_rms_sq=4.725e-03
2024-10-08 23:17:05,752 WARNING [optim.py:503] Scaling gradients by 0.005851793568581343, model_norm_threshold=1906328884740096.0
2024-10-08 23:17:05,890 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.203e+34, grad_sumsq=4.662e+36, orig_rms_sq=4.725e-03
2024-10-08 23:17:08,196 WARNING [optim.py:503] Scaling gradients by 0.012580008246004581, model_norm_threshold=1906328884740096.0
2024-10-08 23:17:09,133 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.061e+33, grad_sumsq=2.176e+35, orig_rms_sq=1.866e-02
2024-10-08 23:17:16,736 WARNING [optim.py:503] Scaling gradients by 0.0, model_norm_threshold=1906328884740096.0
2024-10-08 23:17:16,767 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out_norm.log_scale with proportion 0.00, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.205e+37, grad_sumsq=inf, orig_rms_sq=1.000e-02
2024-10-08 23:17:20,179 WARNING [optim.py:503] Scaling gradients by 0.08037786185741425, model_norm_threshold=1906328884740096.0
2024-10-08 23:17:20,318 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.112e+32, grad_sumsq=2.500e+34, orig_rms_sq=4.447e-03
2024-10-08 23:17:21,463 WARNING [optim.py:503] Scaling gradients by 0.0054414281621575356, model_norm_threshold=1906328884740096.0
2024-10-08 23:17:21,602 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.073e+34, grad_sumsq=3.655e+33, orig_rms_sq=8.407e+00
2024-10-08 23:17:21,827 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=3914.0, ans=0.31653125
2024-10-08 23:17:23,746 WARNING [optim.py:503] Scaling gradients by 0.016824182122945786, model_norm_threshold=1906328884740096.0
2024-10-08 23:17:23,885 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.613e+33, grad_sumsq=5.941e+35, orig_rms_sq=4.397e-03
2024-10-08 23:17:26,158 INFO [train.py:1154] Epoch 2, batch 4900, loss[loss=1.229, simple_loss=0.6254, pruned_loss=0.7208, ctc_loss=0.9783, over 4846.00 frames. ], tot_loss[loss=1.255, simple_loss=0.6394, pruned_loss=0.7301, ctc_loss=1.027, over 967160.19 frames. ], batch size: 21, lr: 3.24e-02,
2024-10-08 23:17:30,654 WARNING [optim.py:503] Scaling gradients by 0.04585561901330948, model_norm_threshold=1906328884740096.0
2024-10-08 23:17:30,794 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.136e+32, grad_sumsq=1.639e+34, orig_rms_sq=1.914e-02
2024-10-08 23:17:34,091 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.attention_skip_rate, batch_count=3917.3333333333335, ans=0.05309999999999998
2024-10-08 23:17:35,663 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=29.85 vs. limit=10.438
2024-10-08 23:17:47,718 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.min_positive, batch_count=3924.0, ans=0.075475
2024-10-08 23:17:47,928 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=5.87 vs. limit=8.9715
2024-10-08 23:17:55,360 WARNING [optim.py:503] Scaling gradients by 0.05921070650219917, model_norm_threshold=1906328884740096.0
2024-10-08 23:17:55,497 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.57, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.932e+32, grad_sumsq=1.744e+32, orig_rms_sq=3.402e+00
2024-10-08 23:17:56,709 WARNING [optim.py:503] Scaling gradients by 0.07879224419593811, model_norm_threshold=1906328884740096.0
2024-10-08 23:17:56,848 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.880e+32, grad_sumsq=5.525e+31, orig_rms_sq=3.402e+00
2024-10-08 23:17:59,857 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.whiten, num_groups=1, num_channels=512, metric=5.99 vs. limit=5.5709333333333335
2024-10-08 23:18:01,509 WARNING [optim.py:503] Scaling gradients by 0.05354801192879677, model_norm_threshold=1906328884740096.0
2024-10-08 23:18:01,646 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.132e+32, grad_sumsq=1.154e+34, orig_rms_sq=1.847e-02
2024-10-08 23:18:04,960 WARNING [optim.py:503] Scaling gradients by 0.008606292307376862, model_norm_threshold=1906328884740096.0
2024-10-08 23:18:05,097 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.58, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.822e+34, grad_sumsq=8.470e+33, orig_rms_sq=3.332e+00
2024-10-08 23:18:08,269 WARNING [optim.py:503] Scaling gradients by 0.035531215369701385, model_norm_threshold=1906328884740096.0
2024-10-08 23:18:08,408 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.57, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.633e+33, grad_sumsq=4.903e+32, orig_rms_sq=3.332e+00
2024-10-08 23:18:09,055 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=13.75 vs. limit=5.981833333333333
2024-10-08 23:18:14,069 WARNING [optim.py:503] Scaling gradients by 0.011411152780056, model_norm_threshold=1906328884740096.0
2024-10-08 23:18:14,207 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.834e+33, grad_sumsq=3.809e+35, orig_rms_sq=1.794e-02
2024-10-08 23:18:15,295 WARNING [optim.py:503] Scaling gradients by 0.00048243728815577924, model_norm_threshold=1906328884740096.0
2024-10-08 23:18:15,433 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.924e+36, grad_sumsq=1.474e+36, orig_rms_sq=3.341e+00
2024-10-08 23:18:16,459 WARNING [optim.py:503] Scaling gradients by 0.023636415600776672, model_norm_threshold=1906328884740096.0
2024-10-08 23:18:16,598 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.624e+33, grad_sumsq=9.064e+34, orig_rms_sq=1.791e-02
2024-10-08 23:18:16,774 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.bypass.skip_rate, batch_count=3930.6666666666665, ans=0.09899494936611666
2024-10-08 23:18:17,663 WARNING [optim.py:503] Scaling gradients by 0.0007101856172084808, model_norm_threshold=1906328884740096.0
2024-10-08 23:18:17,800 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.446e+36, grad_sumsq=4.320e+35, orig_rms_sq=3.348e+00
2024-10-08 23:18:18,860 WARNING [optim.py:503] Scaling gradients by 0.014654042199254036, model_norm_threshold=1906328884740096.0
2024-10-08 23:18:18,998 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.439e+33, grad_sumsq=1.326e+33, orig_rms_sq=3.348e+00
2024-10-08 23:18:20,051 WARNING [optim.py:503] Scaling gradients by 0.0022906025405973196, model_norm_threshold=1906328884740096.0
2024-10-08 23:18:20,190 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.720e+35, grad_sumsq=1.111e+35, orig_rms_sq=3.348e+00
2024-10-08 23:18:21,511 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.935e+10 3.188e+13 4.530e+14 2.999e+16 inf, threshold=9.061e+14, percent-clipped=37.0
2024-10-08 23:18:22,698 WARNING [optim.py:503] Scaling gradients by 0.00638634292408824, model_norm_threshold=906068179812352.0
2024-10-08 23:18:22,844 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.885e+33, grad_sumsq=2.351e+33, orig_rms_sq=3.354e+00
2024-10-08 23:18:24,113 INFO [train.py:1154] Epoch 2, batch 4950, loss[loss=1.246, simple_loss=0.6525, pruned_loss=0.7071, ctc_loss=1.062, over 4772.00 frames. ], tot_loss[loss=1.257, simple_loss=0.6413, pruned_loss=0.7304, ctc_loss=1.028, over 966954.64 frames. ], batch size: 53, lr: 3.23e-02,
2024-10-08 23:18:29,681 WARNING [optim.py:503] Scaling gradients by 0.00043277270742692053, model_norm_threshold=906068179812352.0
2024-10-08 23:18:29,821 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.750e+36, grad_sumsq=5.137e+35, orig_rms_sq=3.406e+00
2024-10-08 23:18:38,585 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=28.40 vs. limit=10.453
2024-10-08 23:18:43,626 WARNING [optim.py:503] Scaling gradients by 0.01275563146919012, model_norm_threshold=906068179812352.0
2024-10-08 23:18:43,761 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.223e+33, grad_sumsq=6.693e+34, orig_rms_sq=1.827e-02
2024-10-08 23:18:47,385 WARNING [optim.py:503] Scaling gradients by 0.08002765476703644, model_norm_threshold=906068179812352.0
2024-10-08 23:18:47,520 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.47, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.991e+31, grad_sumsq=1.227e+34, orig_rms_sq=4.882e-03
2024-10-08 23:18:55,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.balancer.prob, batch_count=3940.6666666666665, ans=0.31528125
2024-10-08 23:19:01,158 WARNING [optim.py:503] Scaling gradients by 0.02577216923236847, model_norm_threshold=906068179812352.0
2024-10-08 23:19:01,295 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.35, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.302e+32, grad_sumsq=2.306e+34, orig_rms_sq=1.866e-02
2024-10-08 23:19:05,333 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=40.20 vs. limit=8.979
2024-10-08 23:19:11,580 WARNING [optim.py:503] Scaling gradients by 0.0006780748371966183, model_norm_threshold=906068179812352.0
2024-10-08 23:19:11,719 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.971e+35, grad_sumsq=9.983e+37, orig_rms_sq=4.979e-03
2024-10-08 23:19:16,067 WARNING [optim.py:503] Scaling gradients by 0.007194334175437689, model_norm_threshold=906068179812352.0
2024-10-08 23:19:16,203 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.428e+33, grad_sumsq=9.244e+32, orig_rms_sq=3.709e+00
2024-10-08 23:19:20,497 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.85 vs. limit=10.4605
2024-10-08 23:19:21,955 INFO [train.py:1154] Epoch 2, batch 5000, loss[loss=1.273, simple_loss=0.6514, pruned_loss=0.738, ctc_loss=1.047, over 4801.00 frames. ], tot_loss[loss=1.256, simple_loss=0.641, pruned_loss=0.7299, ctc_loss=1.026, over 967809.36 frames. ], batch size: 29, lr: 3.23e-02,
2024-10-08 23:19:28,498 WARNING [optim.py:503] Scaling gradients by 0.06487572938203812, model_norm_threshold=906068179812352.0
2024-10-08 23:19:28,635 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.363e+31, grad_sumsq=1.184e+31, orig_rms_sq=3.685e+00
2024-10-08 23:19:33,010 WARNING [optim.py:503] Scaling gradients by 0.007742816582322121, model_norm_threshold=906068179812352.0
2024-10-08 23:19:33,147 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.470e+33, grad_sumsq=6.715e+32, orig_rms_sq=3.678e+00
2024-10-08 23:19:33,349 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.min_positive, batch_count=3954.0, ans=0.07528750000000001
2024-10-08 23:19:35,666 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.const_attention_rate, batch_count=3954.0, ans=0.027587500000000015
2024-10-08 23:19:37,551 WARNING [optim.py:503] Scaling gradients by 0.001223969622515142, model_norm_threshold=906068179812352.0
2024-10-08 23:19:37,688 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.309e+35, grad_sumsq=7.142e+36, orig_rms_sq=1.833e-02
2024-10-08 23:19:38,301 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=11.10 vs. limit=5.5816
2024-10-08 23:19:39,797 WARNING [optim.py:503] Scaling gradients by 0.025813298299908638, model_norm_threshold=906068179812352.0
2024-10-08 23:19:39,931 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.591e+32, grad_sumsq=1.967e+34, orig_rms_sq=1.825e-02
2024-10-08 23:19:42,695 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=256, metric=13.38 vs. limit=10.4655
2024-10-08 23:19:43,472 WARNING [optim.py:503] Scaling gradients by 0.06949535757303238, model_norm_threshold=906068179812352.0
2024-10-08 23:19:43,608 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.552e+31, grad_sumsq=1.578e+34, orig_rms_sq=4.785e-03
2024-10-08 23:19:52,723 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.bypass_mid.scale_min, batch_count=3957.3333333333335, ans=0.7614933333333334
2024-10-08 23:19:52,763 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.ff2_skip_rate, batch_count=3957.3333333333335, ans=0.010959999999999984
2024-10-08 23:19:53,103 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=17.60 vs. limit=8.984
2024-10-08 23:19:53,844 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=3957.3333333333335, ans=0.26042666666666664
2024-10-08 23:19:54,738 WARNING [optim.py:503] Scaling gradients by 0.011496521532535553, model_norm_threshold=906068179812352.0
2024-10-08 23:19:54,873 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.13, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.893e+32, grad_sumsq=4.321e+34, orig_rms_sq=1.827e-02
2024-10-08 23:19:57,386 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.balancer2.prob, batch_count=3960.6666666666665, ans=0.31434375000000003
2024-10-08 23:19:59,691 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.pos_emb_skip_rate, batch_count=3960.6666666666665, ans=0.00491666666666668
2024-10-08 23:20:01,895 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_module1.balancer1.max_abs, batch_count=3960.6666666666665, ans=7.475416666666666
2024-10-08 23:20:02,397 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.58 vs. limit=10.4705
2024-10-08 23:20:02,711 WARNING [optim.py:503] Scaling gradients by 0.005754144862294197, model_norm_threshold=906068179812352.0
2024-10-08 23:20:02,848 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.989e+33, grad_sumsq=3.307e+35, orig_rms_sq=1.811e-02
2024-10-08 23:20:04,023 WARNING [optim.py:503] Scaling gradients by 0.05607103556394577, model_norm_threshold=906068179812352.0
2024-10-08 23:20:04,159 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.270e+32, grad_sumsq=2.719e+34, orig_rms_sq=4.673e-03
2024-10-08 23:20:06,409 WARNING [optim.py:503] Scaling gradients by 0.09296547621488571, model_norm_threshold=906068179812352.0
2024-10-08 23:20:06,544 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.034e+31, grad_sumsq=1.675e+33, orig_rms_sq=1.811e-02
2024-10-08 23:20:07,834 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.conv_skip_rate, batch_count=3964.0, ans=0.05134999999999998
2024-10-08 23:20:10,919 WARNING [optim.py:503] Scaling gradients by 0.03836747631430626, model_norm_threshold=906068179812352.0
2024-10-08 23:20:11,057 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.952e+31, grad_sumsq=1.945e+34, orig_rms_sq=4.601e-03
2024-10-08 23:20:11,309 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.min_positive, batch_count=3964.0, ans=0.0376125
2024-10-08 23:20:16,889 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.180e+12 4.711e+13 5.912e+14 2.996e+15 2.094e+18, threshold=1.182e+15, percent-clipped=44.0
2024-10-08 23:20:18,217 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.balancer.max_positive, batch_count=3967.3333333333335, ans=0.7896733333333333
2024-10-08 23:20:19,143 INFO [train.py:1154] Epoch 2, batch 5050, loss[loss=1.289, simple_loss=0.6608, pruned_loss=0.76, ctc_loss=0.9938, over 4851.00 frames. ], tot_loss[loss=1.251, simple_loss=0.6387, pruned_loss=0.7279, ctc_loss=1.021, over 968659.83 frames. ], batch size: 19, lr: 3.22e-02,
2024-10-08 23:20:19,248 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.ff3_skip_rate, batch_count=3967.3333333333335, ans=0.010734999999999995
2024-10-08 23:20:20,725 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.68 vs. limit=10.4755
2024-10-08 23:20:24,550 WARNING [optim.py:503] Scaling gradients by 0.0008673117845319211, model_norm_threshold=1182313496969216.0
2024-10-08 23:20:24,685 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.449e+35, grad_sumsq=7.590e+37, orig_rms_sq=4.544e-03
2024-10-08 23:20:24,840 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass.scale_min, batch_count=3967.3333333333335, ans=0.7611433333333334
2024-10-08 23:20:28,123 WARNING [optim.py:503] Scaling gradients by 0.035534244030714035, model_norm_threshold=1182313496969216.0
2024-10-08 23:20:28,261 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.154e+32, grad_sumsq=6.773e+34, orig_rms_sq=4.657e-03
2024-10-08 23:20:30,401 WARNING [optim.py:503] Scaling gradients by 0.010393086820840836, model_norm_threshold=1182313496969216.0
2024-10-08 23:20:30,539 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.61, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.906e+33, grad_sumsq=1.679e+36, orig_rms_sq=4.710e-03
2024-10-08 23:20:31,795 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.bypass_mid.scale_min, batch_count=3970.6666666666665, ans=0.7610266666666667
2024-10-08 23:20:37,093 WARNING [optim.py:503] Scaling gradients by 0.017958099022507668, model_norm_threshold=1182313496969216.0
2024-10-08 23:20:37,231 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.139e+33, grad_sumsq=6.295e+34, orig_rms_sq=1.810e-02
2024-10-08 23:20:40,790 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward2.hidden_balancer.prob, batch_count=3974.0, ans=0.31371875
2024-10-08 23:20:41,875 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=3974.0, ans=0.010584999999999997
2024-10-08 23:20:43,338 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=15.11 vs. limit=8.99025
2024-10-08 23:20:44,964 WARNING [optim.py:503] Scaling gradients by 0.014325758442282677, model_norm_threshold=1182313496969216.0
2024-10-08 23:20:45,100 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.484e+33, grad_sumsq=1.760e+32, orig_rms_sq=8.432e+00
2024-10-08 23:20:47,906 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=30.15 vs. limit=8.99025
2024-10-08 23:20:48,375 WARNING [optim.py:503] Scaling gradients by 0.014776322059333324, model_norm_threshold=1182313496969216.0
2024-10-08 23:20:48,513 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.451e+33, grad_sumsq=7.986e+34, orig_rms_sq=1.817e-02
2024-10-08 23:20:51,221 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=14.94 vs. limit=8.99025
2024-10-08 23:20:51,597 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=4.27 vs. limit=3.5961
2024-10-08 23:20:51,817 WARNING [optim.py:503] Scaling gradients by 0.00012498658907134086, model_norm_threshold=1182313496969216.0
2024-10-08 23:20:51,953 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.886e+37, grad_sumsq=inf, orig_rms_sq=4.561e-03
2024-10-08 23:20:52,621 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.feed_forward2.out_whiten.whitening_limit, batch_count=3977.3333333333335, ans=8.9915
2024-10-08 23:20:55,250 WARNING [optim.py:503] Scaling gradients by 0.03592000901699066, model_norm_threshold=1182313496969216.0
2024-10-08 23:20:55,390 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.bias with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.715e+32, grad_sumsq=7.657e+31, orig_rms_sq=2.240e+00
2024-10-08 23:20:56,844 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=3977.3333333333335, ans=0.31356249999999997
2024-10-08 23:20:59,440 WARNING [optim.py:503] Scaling gradients by 0.09432225674390793, model_norm_threshold=1182313496969216.0
2024-10-08 23:20:59,574 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.42, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.651e+31, grad_sumsq=3.744e+33, orig_rms_sq=1.776e-02
2024-10-08 23:20:59,800 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer1.prob, batch_count=3977.3333333333335, ans=0.31356249999999997
2024-10-08 23:21:01,188 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=20.92 vs. limit=8.9915
2024-10-08 23:21:01,632 WARNING [optim.py:503] Scaling gradients by 0.027300691232085228, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:01,766 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.602e+32, grad_sumsq=1.233e+35, orig_rms_sq=4.543e-03
2024-10-08 23:21:08,761 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass_mid.scale_min, batch_count=3980.6666666666665, ans=0.7606766666666667
2024-10-08 23:21:13,033 WARNING [optim.py:503] Scaling gradients by 0.0006097687873989344, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:13,169 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.214e+35, grad_sumsq=5.452e+37, orig_rms_sq=1.690e-02
2024-10-08 23:21:13,341 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.out_combiner.scale_min, batch_count=3980.6666666666665, ans=0.7606766666666667
2024-10-08 23:21:14,392 WARNING [optim.py:503] Scaling gradients by 0.05466772988438606, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:14,535 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.104e+32, grad_sumsq=6.530e+33, orig_rms_sq=1.690e-02
2024-10-08 23:21:16,718 WARNING [optim.py:503] Scaling gradients by 0.04522209241986275, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:16,857 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.217e+32, grad_sumsq=5.040e+34, orig_rms_sq=4.398e-03
2024-10-08 23:21:16,902 INFO [train.py:1154] Epoch 2, batch 5100, loss[loss=1.192, simple_loss=0.5956, pruned_loss=0.6968, ctc_loss=0.9852, over 4817.00 frames. ], tot_loss[loss=1.252, simple_loss=0.6393, pruned_loss=0.7277, ctc_loss=1.023, over 967844.55 frames. ], batch size: 19, lr: 3.22e-02,
2024-10-08 23:21:19,060 WARNING [optim.py:503] Scaling gradients by 0.07319023460149765, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:19,207 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.170e+32, grad_sumsq=2.660e+34, orig_rms_sq=4.398e-03
2024-10-08 23:21:20,277 WARNING [optim.py:503] Scaling gradients by 0.00019041616178583354, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:20,417 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.917e+36, grad_sumsq=inf, orig_rms_sq=1.676e-02
2024-10-08 23:21:22,712 WARNING [optim.py:503] Scaling gradients by 0.00040113108116202056, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:22,850 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.677e+36, grad_sumsq=1.594e+38, orig_rms_sq=1.679e-02
2024-10-08 23:21:26,371 WARNING [optim.py:503] Scaling gradients by 0.0055398778058588505, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:26,510 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.675e+34, grad_sumsq=3.788e+36, orig_rms_sq=4.422e-03
2024-10-08 23:21:31,038 WARNING [optim.py:503] Scaling gradients by 0.0005258891615085304, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:31,186 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.989e+35, grad_sumsq=2.655e+35, orig_rms_sq=3.386e+00
2024-10-08 23:21:37,327 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=512, metric=36.93 vs. limit=10.4905
2024-10-08 23:21:40,590 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=9.29 vs. limit=5.9976666666666665
2024-10-08 23:21:41,392 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer2.min_abs, batch_count=3990.6666666666665, ans=0.25986
2024-10-08 23:21:41,592 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=29.34 vs. limit=8.9965
2024-10-08 23:21:43,065 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=11.13 vs. limit=8.9965
2024-10-08 23:21:43,377 WARNING [optim.py:503] Scaling gradients by 0.043966297060251236, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:43,517 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.720e+32, grad_sumsq=1.933e+31, orig_rms_sq=8.897e+00
2024-10-08 23:21:46,357 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=17.55 vs. limit=5.9976666666666665
2024-10-08 23:21:47,617 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.42 vs. limit=10.493
2024-10-08 23:21:48,418 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=7.38 vs. limit=5.9976666666666665
2024-10-08 23:21:49,016 WARNING [optim.py:503] Scaling gradients by 0.0, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:49,047 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out_norm.log_scale with proportion 0.00, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.894e+35, grad_sumsq=2.894e+37, orig_rms_sq=1.000e-02
2024-10-08 23:21:52,310 WARNING [optim.py:503] Scaling gradients by 0.0025573011953383684, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:52,448 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.945e+34, grad_sumsq=1.639e+37, orig_rms_sq=4.847e-03
2024-10-08 23:21:53,496 WARNING [optim.py:503] Scaling gradients by 0.008439874276518822, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:53,632 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.363e+33, grad_sumsq=8.799e+35, orig_rms_sq=4.958e-03
2024-10-08 23:21:54,235 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn2.whiten, num_groups=1, num_channels=512, metric=31.49 vs. limit=10.4955
2024-10-08 23:21:54,856 WARNING [optim.py:503] Scaling gradients by 0.03619439899921417, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:54,995 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.462e+32, grad_sumsq=1.410e+34, orig_rms_sq=1.746e-02
2024-10-08 23:21:56,040 WARNING [optim.py:503] Scaling gradients by 0.021086597815155983, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:56,177 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.314e+32, grad_sumsq=8.203e+31, orig_rms_sq=8.916e+00
2024-10-08 23:21:57,142 WARNING [optim.py:503] Scaling gradients by 0.0025320518761873245, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:57,282 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.nonlin_attention.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.612e+34, grad_sumsq=9.136e+34, orig_rms_sq=3.953e-01
2024-10-08 23:21:59,415 WARNING [optim.py:503] Scaling gradients by 0.08246427029371262, model_norm_threshold=1182313496969216.0
2024-10-08 23:21:59,551 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.462e+31, grad_sumsq=8.914e+33, orig_rms_sq=5.006e-03
2024-10-08 23:22:04,866 WARNING [optim.py:503] Scaling gradients by 0.029537474736571312, model_norm_threshold=1182313496969216.0
2024-10-08 23:22:05,002 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.988e+32, grad_sumsq=9.987e+34, orig_rms_sq=4.994e-03
2024-10-08 23:22:06,782 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.62 vs. limit=10.498000000000001
2024-10-08 23:22:08,607 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=21.32 vs. limit=8.999
2024-10-08 23:22:10,340 WARNING [optim.py:503] Scaling gradients by 0.051056865602731705, model_norm_threshold=1182313496969216.0
2024-10-08 23:22:10,477 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.129e+32, grad_sumsq=2.250e+34, orig_rms_sq=5.019e-03
2024-10-08 23:22:10,744 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_True_ctc_True_attdecoder_False_streaming_True/checkpoint-12000.pt
2024-10-08 23:22:12,806 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.839e+11 4.090e+13 5.584e+14 2.316e+16 inf, threshold=1.117e+15, percent-clipped=41.0
2024-10-08 23:22:12,806 WARNING [optim.py:503] Scaling gradients by 0.01124300342053175, model_norm_threshold=1116811487608832.0
2024-10-08 23:22:12,940 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.876e+33, grad_sumsq=5.683e+35, orig_rms_sq=5.060e-03
2024-10-08 23:22:13,076 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.balancer2.prob, batch_count=3997.3333333333335, ans=0.312625
2024-10-08 23:22:14,094 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=4000.6666666666665, ans=0.04999722222222223
2024-10-08 23:22:14,912 WARNING [optim.py:503] Scaling gradients by 0.01329769380390644, model_norm_threshold=1116811487608832.0
2024-10-08 23:22:15,049 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.796e+33, grad_sumsq=1.019e+35, orig_rms_sq=1.763e-02
2024-10-08 23:22:15,093 INFO [train.py:1154] Epoch 2, batch 5150, loss[loss=1.246, simple_loss=0.6493, pruned_loss=0.7151, ctc_loss=1.029, over 4824.00 frames. ], tot_loss[loss=1.252, simple_loss=0.6399, pruned_loss=0.727, ctc_loss=1.023, over 968029.23 frames. ], batch size: 36, lr: 3.21e-02,
2024-10-08 23:22:15,223 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_skip_rate, batch_count=4000.6666666666665, ans=0.04999722222222223
2024-10-08 23:22:19,179 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.33 vs. limit=10.5005
2024-10-08 23:22:22,996 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.bypass_mid.scale_min, batch_count=4000.6666666666665, ans=0.7599766666666667
2024-10-08 23:22:26,022 WARNING [optim.py:503] Scaling gradients by 0.026650521904230118, model_norm_threshold=1116811487608832.0
2024-10-08 23:22:26,160 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.722e+32, grad_sumsq=2.101e+34, orig_rms_sq=1.772e-02
2024-10-08 23:22:27,931 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=512, metric=28.71 vs. limit=10.503
2024-10-08 23:22:28,741 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten.whitening_limit, batch_count=4004.0, ans=10.503
2024-10-08 23:22:30,267 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn2.whiten, num_groups=1, num_channels=192, metric=11.64 vs. limit=10.503
2024-10-08 23:22:30,501 WARNING [optim.py:503] Scaling gradients by 0.0677669495344162, model_norm_threshold=1116811487608832.0
2024-10-08 23:22:30,636 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.522e+31, grad_sumsq=1.091e+34, orig_rms_sq=5.063e-03
2024-10-08 23:22:31,677 WARNING [optim.py:503] Scaling gradients by 0.0008485965663567185, model_norm_threshold=1116811487608832.0
2024-10-08 23:22:31,815 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.604e+35, grad_sumsq=5.144e+37, orig_rms_sq=5.063e-03
2024-10-08 23:22:32,879 WARNING [optim.py:503] Scaling gradients by 0.006781226024031639, model_norm_threshold=1116811487608832.0
2024-10-08 23:22:33,012 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.757e+33, grad_sumsq=3.245e+35, orig_rms_sq=1.774e-02
2024-10-08 23:22:33,769 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.96 vs. limit=10.503
2024-10-08 23:22:34,035 WARNING [optim.py:503] Scaling gradients by 0.0010217053350061178, model_norm_threshold=1116811487608832.0
2024-10-08 23:22:34,169 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.454e+35, grad_sumsq=4.847e+37, orig_rms_sq=5.063e-03
2024-10-08 23:22:36,421 WARNING [optim.py:503] Scaling gradients by 0.008586551994085312, model_norm_threshold=1116811487608832.0
2024-10-08 23:22:36,557 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.254e+33, grad_sumsq=2.364e+35, orig_rms_sq=1.800e-02
2024-10-08 23:22:37,865 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.conv_module1.balancer2.prob, batch_count=4007.3333333333335, ans=0.31215625
2024-10-08 23:22:38,735 WARNING [optim.py:503] Scaling gradients by 0.0002436104987282306, model_norm_threshold=1116811487608832.0
2024-10-08 23:22:38,872 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.872e+36, grad_sumsq=inf, orig_rms_sq=5.062e-03
2024-10-08 23:22:39,101 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.conv_module1.balancer1.prob, batch_count=4007.3333333333335, ans=0.31215625
2024-10-08 23:22:40,092 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=4007.3333333333335, ans=7.504583333333334
2024-10-08 23:22:44,196 WARNING [optim.py:503] Scaling gradients by 0.0006187158869579434, model_norm_threshold=1116811487608832.0
2024-10-08 23:22:44,332 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.13, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.129e+35, grad_sumsq=4.570e+34, orig_rms_sq=9.035e+00
2024-10-08 23:22:44,782 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=35.93 vs. limit=10.5055
2024-10-08 23:22:45,374 WARNING [optim.py:503] Scaling gradients by 0.033653464168310165, model_norm_threshold=1116811487608832.0
2024-10-08 23:22:45,514 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.930e+32, grad_sumsq=1.579e+34, orig_rms_sq=1.856e-02
2024-10-08 23:22:46,093 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=5.29 vs. limit=3.6010999999999997
2024-10-08 23:22:46,114 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=83.50 vs. limit=9.00275
2024-10-08 23:22:48,073 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=4010.6666666666665, ans=0.312
2024-10-08 23:22:49,215 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.feed_forward1.out_proj.dropout_p, batch_count=4010.6666666666665, ans=0.2598933333333333
2024-10-08 23:22:54,524 WARNING [optim.py:503] Scaling gradients by 0.00010033790749730542, model_norm_threshold=1116811487608832.0
2024-10-08 23:22:54,660 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.944e+37, grad_sumsq=inf, orig_rms_sq=1.883e-02
2024-10-08 23:22:55,167 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=8.31 vs. limit=7.005333333333333
2024-10-08 23:22:59,097 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.conv.2.prob, batch_count=4014.0, ans=0.31184375
2024-10-08 23:23:00,308 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.self_attn_weights.pos_emb_skip_rate, batch_count=4014.0, ans=0.0
2024-10-08 23:23:01,163 WARNING [optim.py:503] Scaling gradients by 0.0007147988071665168, model_norm_threshold=1116811487608832.0
2024-10-08 23:23:01,300 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.209e+35, grad_sumsq=1.710e+38, orig_rms_sq=5.385e-03
2024-10-08 23:23:02,209 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=8.47 vs. limit=9.00525
2024-10-08 23:23:02,397 WARNING [optim.py:503] Scaling gradients by 0.031382858753204346, model_norm_threshold=1116811487608832.0
2024-10-08 23:23:02,532 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.856e+32, grad_sumsq=9.041e+34, orig_rms_sq=5.371e-03
2024-10-08 23:23:02,964 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=15.09 vs. limit=7.007
2024-10-08 23:23:03,797 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module2.balancer1.prob, batch_count=4014.0, ans=0.31184375
2024-10-08 23:23:04,354 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=5.52 vs. limit=9.00525
2024-10-08 23:23:05,903 WARNING [optim.py:503] Scaling gradients by 0.0, model_norm_threshold=1116811487608832.0
2024-10-08 23:23:05,934 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out_norm.log_scale with proportion 0.00, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.105e+37, grad_sumsq=inf, orig_rms_sq=1.000e-02
2024-10-08 23:23:07,305 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_proj.dropout_p, batch_count=4014.0, ans=0.25986
2024-10-08 23:23:09,187 WARNING [optim.py:503] Scaling gradients by 0.0009305340936407447, model_norm_threshold=1116811487608832.0
2024-10-08 23:23:09,325 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.012e+35, grad_sumsq=5.636e+37, orig_rms_sq=5.344e-03
2024-10-08 23:23:11,528 INFO [train.py:1154] Epoch 2, batch 5200, loss[loss=1.177, simple_loss=0.6149, pruned_loss=0.6805, ctc_loss=0.9458, over 4774.00 frames. ], tot_loss[loss=1.251, simple_loss=0.6391, pruned_loss=0.727, ctc_loss=1.022, over 967653.16 frames. ], batch size: 29, lr: 3.21e-02,
2024-10-08 23:23:12,562 WARNING [optim.py:503] Scaling gradients by 0.013820231892168522, model_norm_threshold=1116811487608832.0
2024-10-08 23:23:12,701 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.934e+33, grad_sumsq=3.597e+35, orig_rms_sq=5.377e-03
2024-10-08 23:23:18,278 WARNING [optim.py:503] Scaling gradients by 0.014496332965791225, model_norm_threshold=1116811487608832.0
2024-10-08 23:23:18,415 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.182e+33, grad_sumsq=6.548e+34, orig_rms_sq=1.806e-02
2024-10-08 23:23:19,890 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.conv_module2.whiten, num_groups=1, num_channels=384, metric=6.55 vs. limit=9.006499999999999
2024-10-08 23:23:21,529 WARNING [optim.py:503] Scaling gradients by 0.009821039624512196, model_norm_threshold=1116811487608832.0
2024-10-08 23:23:21,666 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.012e+33, grad_sumsq=1.114e+36, orig_rms_sq=5.399e-03
2024-10-08 23:23:23,397 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=512, metric=30.29 vs. limit=10.5155
2024-10-08 23:23:32,092 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff2_skip_rate, batch_count=4020.6666666666665, ans=0.009995507246376812
2024-10-08 23:23:32,416 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=28.69 vs. limit=10.5155
2024-10-08 23:23:34,243 WARNING [optim.py:503] Scaling gradients by 0.08492300659418106, model_norm_threshold=1116811487608832.0
2024-10-08 23:23:34,376 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.430e+31, grad_sumsq=1.911e+33, orig_rms_sq=1.795e-02
2024-10-08 23:23:38,531 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=18.42 vs. limit=9.009
2024-10-08 23:23:39,188 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.conv_module2.balancer1.prob, batch_count=4024.0, ans=0.311375
2024-10-08 23:23:40,018 WARNING [optim.py:503] Scaling gradients by 0.0026458557695150375, model_norm_threshold=1116811487608832.0
2024-10-08 23:23:40,151 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.578e+34, grad_sumsq=1.439e+37, orig_rms_sq=5.265e-03
2024-10-08 23:23:41,217 WARNING [optim.py:503] Scaling gradients by 0.019407350569963455, model_norm_threshold=1116811487608832.0
2024-10-08 23:23:41,351 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.244e+32, grad_sumsq=1.566e+35, orig_rms_sq=5.265e-03
2024-10-08 23:23:44,076 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=11.59 vs. limit=9.009
2024-10-08 23:23:44,758 WARNING [optim.py:503] Scaling gradients by 0.001352310529910028, model_norm_threshold=1116811487608832.0
2024-10-08 23:23:44,893 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.184e+35, grad_sumsq=6.609e+36, orig_rms_sq=1.791e-02
2024-10-08 23:23:49,587 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff3_skip_rate, batch_count=4027.3333333333335, ans=0.009994057971014493
2024-10-08 23:23:52,699 WARNING [optim.py:503] Scaling gradients by 0.001463558990508318, model_norm_threshold=1116811487608832.0
2024-10-08 23:23:52,834 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.247e+35, grad_sumsq=6.862e+36, orig_rms_sq=1.817e-02
2024-10-08 23:23:53,320 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=17.31 vs. limit=9.01025
2024-10-08 23:23:56,361 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.feed_forward1.hidden_balancer.prob, batch_count=4030.6666666666665, ans=0.3110625
2024-10-08 23:23:57,507 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.bypass.scale_min, batch_count=4030.6666666666665, ans=0.7589266666666667
2024-10-08 23:24:01,214 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.conv_module1.whiten, num_groups=1, num_channels=512, metric=3.68 vs. limit=9.0115
2024-10-08 23:24:01,719 WARNING [optim.py:503] Scaling gradients by 0.007628920488059521, model_norm_threshold=1116811487608832.0
2024-10-08 23:24:01,855 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.763e+33, grad_sumsq=1.468e+33, orig_rms_sq=3.925e+00
2024-10-08 23:24:02,367 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=22.81 vs. limit=10.523
2024-10-08 23:24:05,596 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=13.73 vs. limit=7.015333333333333
2024-10-08 23:24:06,360 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.938e+12 6.893e+13 9.510e+14 1.315e+16 inf, threshold=1.902e+15, percent-clipped=47.0
2024-10-08 23:24:06,360 WARNING [optim.py:503] Scaling gradients by 0.008207040838897228, model_norm_threshold=1901988417634304.0
2024-10-08 23:24:06,494 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.076e+34, grad_sumsq=3.965e+36, orig_rms_sq=5.235e-03
2024-10-08 23:24:06,639 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.balancer.max_positive, batch_count=4030.6666666666665, ans=0.7903066666666667
2024-10-08 23:24:07,725 WARNING [optim.py:503] Scaling gradients by 0.09977240860462189, model_norm_threshold=1901988417634304.0
2024-10-08 23:24:07,858 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.811e+31, grad_sumsq=4.833e+33, orig_rms_sq=1.823e-02
2024-10-08 23:24:09,102 INFO [train.py:1154] Epoch 2, batch 5250, loss[loss=1.242, simple_loss=0.6368, pruned_loss=0.731, ctc_loss=0.9644, over 4868.00 frames. ], tot_loss[loss=1.249, simple_loss=0.638, pruned_loss=0.7262, ctc_loss=1.021, over 967732.45 frames. ], batch size: 20, lr: 3.20e-02,
2024-10-08 23:24:12,350 WARNING [optim.py:503] Scaling gradients by 0.024262405931949615, model_norm_threshold=1901988417634304.0
2024-10-08 23:24:12,486 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.405e+33, grad_sumsq=7.679e+34, orig_rms_sq=1.829e-02
2024-10-08 23:24:16,457 WARNING [optim.py:503] Scaling gradients by 0.0057858568616211414, model_norm_threshold=1901988417634304.0
2024-10-08 23:24:16,594 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.868e+34, grad_sumsq=1.571e+36, orig_rms_sq=1.826e-02
2024-10-08 23:24:17,609 WARNING [optim.py:503] Scaling gradients by 0.002375492826104164, model_norm_threshold=1901988417634304.0
2024-10-08 23:24:17,741 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.481e+35, grad_sumsq=4.729e+37, orig_rms_sq=5.245e-03
2024-10-08 23:24:18,383 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=23.99 vs. limit=10.525500000000001
2024-10-08 23:24:23,194 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.ff3_skip_rate, batch_count=4037.3333333333335, ans=0.009991884057971015
2024-10-08 23:24:28,759 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass.scale_min, batch_count=4037.3333333333335, ans=0.7586933333333333
2024-10-08 23:24:29,810 WARNING [optim.py:503] Scaling gradients by 0.05595288425683975, model_norm_threshold=1901988417634304.0
2024-10-08 23:24:29,947 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.705e+32, grad_sumsq=1.481e+34, orig_rms_sq=1.827e-02
2024-10-08 23:24:32,177 WARNING [optim.py:503] Scaling gradients by 0.00287613901309669, model_norm_threshold=1901988417634304.0
2024-10-08 23:24:32,310 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.111e+35, grad_sumsq=6.084e+36, orig_rms_sq=1.827e-02
2024-10-08 23:24:32,688 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2.whitening_limit, batch_count=4040.6666666666665, ans=7.020333333333333
2024-10-08 23:24:33,534 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.const_attention_rate, batch_count=4040.6666666666665, ans=0.025
2024-10-08 23:24:33,568 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.conv_module1.balancer1.max_abs, batch_count=4040.6666666666665, ans=7.525416666666667
2024-10-08 23:24:34,125 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=6.76 vs. limit=5.616266666666666
2024-10-08 23:24:35,468 WARNING [optim.py:503] Scaling gradients by 0.0045960163697600365, model_norm_threshold=1901988417634304.0
2024-10-08 23:24:35,604 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.740e+34, grad_sumsq=3.093e+36, orig_rms_sq=1.856e-02
2024-10-08 23:24:39,843 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=14.58 vs. limit=10.5305
2024-10-08 23:24:40,113 WARNING [optim.py:503] Scaling gradients by 0.009743674658238888, model_norm_threshold=1901988417634304.0
2024-10-08 23:24:40,266 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.41, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.559e+34, grad_sumsq=8.226e+35, orig_rms_sq=1.895e-02
2024-10-08 23:24:41,497 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.min_positive, batch_count=4040.6666666666665, ans=0.03737291666666667
2024-10-08 23:24:43,630 WARNING [optim.py:503] Scaling gradients by 0.0, model_norm_threshold=1901988417634304.0
2024-10-08 23:24:43,662 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out_norm.log_scale with proportion 0.00, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.712e+34, grad_sumsq=5.712e+36, orig_rms_sq=1.000e-02
2024-10-08 23:24:45,819 WARNING [optim.py:503] Scaling gradients by 0.011388387531042099, model_norm_threshold=1901988417634304.0
2024-10-08 23:24:45,955 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.532e+33, grad_sumsq=2.348e+35, orig_rms_sq=1.930e-02
2024-10-08 23:24:48,209 WARNING [optim.py:503] Scaling gradients by 0.0557250902056694, model_norm_threshold=1901988417634304.0
2024-10-08 23:24:48,343 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.117e+32, grad_sumsq=1.584e+34, orig_rms_sq=1.968e-02
2024-10-08 23:24:50,336 WARNING [optim.py:503] Scaling gradients by 0.09020137786865234, model_norm_threshold=1901988417634304.0
2024-10-08 23:24:50,471 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.45, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.994e+32, grad_sumsq=3.743e+34, orig_rms_sq=5.326e-03
2024-10-08 23:24:50,695 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module2.balancer2.prob, batch_count=4044.0, ans=0.31043750000000003
2024-10-08 23:24:52,068 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.conv_module2.whiten, num_groups=1, num_channels=512, metric=76.77 vs. limit=9.0165
2024-10-08 23:24:52,635 WARNING [optim.py:503] Scaling gradients by 0.015623671002686024, model_norm_threshold=1901988417634304.0
2024-10-08 23:24:52,770 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.100e+33, grad_sumsq=7.717e+35, orig_rms_sq=5.314e-03
2024-10-08 23:24:55,225 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.4.encoder.layers.2.self_attn_weights, loss-sum=2.140e+01
2024-10-08 23:24:58,400 WARNING [optim.py:503] Scaling gradients by 0.06901735067367554, model_norm_threshold=1901988417634304.0
2024-10-08 23:24:58,537 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.036e+32, grad_sumsq=1.031e+34, orig_rms_sq=1.974e-02
2024-10-08 23:25:00,727 WARNING [optim.py:503] Scaling gradients by 0.05872160196304321, model_norm_threshold=1901988417634304.0
2024-10-08 23:25:00,861 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.531e+32, grad_sumsq=2.861e+31, orig_rms_sq=8.846e+00
2024-10-08 23:25:04,011 WARNING [optim.py:503] Scaling gradients by 0.027593938633799553, model_norm_threshold=1901988417634304.0
2024-10-08 23:25:04,144 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.766e+33, grad_sumsq=8.872e+34, orig_rms_sq=1.990e-02
2024-10-08 23:25:05,408 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward3.hidden_balancer.prob, batch_count=4050.6666666666665, ans=0.310125
2024-10-08 23:25:06,294 WARNING [optim.py:503] Scaling gradients by 0.037701401859521866, model_norm_threshold=1901988417634304.0
2024-10-08 23:25:06,428 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.34, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.677e+32, grad_sumsq=4.324e+34, orig_rms_sq=2.006e-02
2024-10-08 23:25:06,472 INFO [train.py:1154] Epoch 2, batch 5300, loss[loss=1.299, simple_loss=0.6698, pruned_loss=0.7457, ctc_loss=1.093, over 4823.00 frames. ], tot_loss[loss=1.25, simple_loss=0.6384, pruned_loss=0.7263, ctc_loss=1.022, over 967938.02 frames. ], batch size: 38, lr: 3.20e-02,
2024-10-08 23:25:07,106 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=33.06 vs. limit=9.019
2024-10-08 23:25:07,456 WARNING [optim.py:503] Scaling gradients by 0.003920544404536486, model_norm_threshold=1901988417634304.0
2024-10-08 23:25:07,592 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.850e+34, grad_sumsq=4.411e+36, orig_rms_sq=2.006e-02
2024-10-08 23:25:08,639 WARNING [optim.py:503] Scaling gradients by 0.0015661987708881497, model_norm_threshold=1901988417634304.0
2024-10-08 23:25:08,773 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.199e+35, grad_sumsq=2.093e+37, orig_rms_sq=2.006e-02
2024-10-08 23:25:12,112 WARNING [optim.py:503] Scaling gradients by 0.015193033963441849, model_norm_threshold=1901988417634304.0
2024-10-08 23:25:12,244 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.519e+33, grad_sumsq=4.261e+35, orig_rms_sq=1.999e-02
2024-10-08 23:25:14,847 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=2.83 vs. limit=9.019
2024-10-08 23:25:15,378 WARNING [optim.py:503] Scaling gradients by 0.004742925055325031, model_norm_threshold=1901988417634304.0
2024-10-08 23:25:15,512 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.676e+34, grad_sumsq=2.347e+36, orig_rms_sq=1.992e-02
2024-10-08 23:25:21,041 WARNING [optim.py:503] Scaling gradients by 0.07080032676458359, model_norm_threshold=1901988417634304.0
2024-10-08 23:25:21,178 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.151e+32, grad_sumsq=1.103e+34, orig_rms_sq=1.950e-02
2024-10-08 23:25:22,202 WARNING [optim.py:503] Scaling gradients by 0.024193240329623222, model_norm_threshold=1901988417634304.0
2024-10-08 23:25:22,337 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.072e+32, grad_sumsq=1.815e+35, orig_rms_sq=4.998e-03
2024-10-08 23:25:22,592 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.conv_skip_rate, batch_count=4054.0, ans=0.049775
2024-10-08 23:25:25,816 WARNING [optim.py:503] Scaling gradients by 0.0, model_norm_threshold=1901988417634304.0
2024-10-08 23:25:25,848 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out_norm.log_scale with proportion 0.00, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.457e+37, grad_sumsq=inf, orig_rms_sq=1.000e-02
2024-10-08 23:25:27,117 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module2.balancer2.prob, batch_count=4054.0, ans=0.30996875
2024-10-08 23:25:29,675 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=7.56 vs. limit=6.014333333333333
2024-10-08 23:25:31,465 WARNING [optim.py:503] Scaling gradients by 0.0021039845887571573, model_norm_threshold=1901988417634304.0
2024-10-08 23:25:31,597 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.433e+35, grad_sumsq=3.870e+34, orig_rms_sq=3.702e+00
2024-10-08 23:25:38,382 WARNING [optim.py:503] Scaling gradients by 0.0009159190813079476, model_norm_threshold=1901988417634304.0
2024-10-08 23:25:38,517 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.999e+36, grad_sumsq=inf, orig_rms_sq=4.775e-03
2024-10-08 23:25:39,241 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=11.55 vs. limit=9.0215
2024-10-08 23:25:44,000 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=4060.6666666666665, ans=0.0
2024-10-08 23:25:44,208 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.82 vs. limit=3.6090999999999998
2024-10-08 23:25:44,807 WARNING [optim.py:503] Scaling gradients by 0.0026843473315238953, model_norm_threshold=1901988417634304.0
2024-10-08 23:25:44,941 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.449e+35, grad_sumsq=7.711e+36, orig_rms_sq=1.879e-02
2024-10-08 23:25:47,256 WARNING [optim.py:503] Scaling gradients by 0.020789215341210365, model_norm_threshold=1901988417634304.0
2024-10-08 23:25:47,390 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.201e+33, grad_sumsq=5.703e+32, orig_rms_sq=3.859e+00
2024-10-08 23:25:49,633 WARNING [optim.py:503] Scaling gradients by 0.004227421246469021, model_norm_threshold=1901988417634304.0
2024-10-08 23:25:49,768 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.680e+34, grad_sumsq=9.710e+36, orig_rms_sq=4.820e-03
2024-10-08 23:25:55,414 WARNING [optim.py:503] Scaling gradients by 0.04316885769367218, model_norm_threshold=1901988417634304.0
2024-10-08 23:25:55,551 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.337e+32, grad_sumsq=2.315e+34, orig_rms_sq=1.874e-02
2024-10-08 23:26:01,467 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.701e+11 1.823e+14 1.327e+15 3.399e+16 inf, threshold=2.653e+15, percent-clipped=41.0
2024-10-08 23:26:02,571 WARNING [optim.py:503] Scaling gradients by 0.05837712809443474, model_norm_threshold=2653060891410432.0
2024-10-08 23:26:02,706 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.60, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.231e+33, grad_sumsq=2.412e+35, orig_rms_sq=5.103e-03
2024-10-08 23:26:03,771 WARNING [optim.py:503] Scaling gradients by 0.0, model_norm_threshold=2653060891410432.0
2024-10-08 23:26:03,802 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out_norm.log_scale with proportion 0.00, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.056e+35, grad_sumsq=8.056e+37, orig_rms_sq=1.000e-02
2024-10-08 23:26:03,846 INFO [train.py:1154] Epoch 2, batch 5350, loss[loss=1.29, simple_loss=0.6518, pruned_loss=0.7629, ctc_loss=1.004, over 4978.00 frames. ], tot_loss[loss=1.254, simple_loss=0.6402, pruned_loss=0.729, ctc_loss=1.024, over 967334.95 frames. ], batch size: 19, lr: 3.19e-02,
2024-10-08 23:26:07,076 WARNING [optim.py:503] Scaling gradients by 0.008271670900285244, model_norm_threshold=2653060891410432.0
2024-10-08 23:26:07,211 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.419e+34, grad_sumsq=1.275e+36, orig_rms_sq=1.897e-02
2024-10-08 23:26:09,373 WARNING [optim.py:503] Scaling gradients by 0.003672350663691759, model_norm_threshold=2653060891410432.0
2024-10-08 23:26:09,509 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.926e+35, grad_sumsq=3.736e+37, orig_rms_sq=5.155e-03
2024-10-08 23:26:11,946 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=4067.3333333333335, ans=0.30934375000000003
2024-10-08 23:26:12,750 WARNING [optim.py:503] Scaling gradients by 0.018345868214964867, model_norm_threshold=2653060891410432.0
2024-10-08 23:26:12,884 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.473e+33, grad_sumsq=2.342e+35, orig_rms_sq=1.910e-02
2024-10-08 23:26:15,500 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.whiten, num_groups=1, num_channels=256, metric=10.59 vs. limit=5.628266666666667
2024-10-08 23:26:21,321 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=24.24 vs. limit=9.0265
2024-10-08 23:26:21,671 WARNING [optim.py:503] Scaling gradients by 0.049817997962236404, model_norm_threshold=2653060891410432.0
2024-10-08 23:26:21,804 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.519e+32, grad_sumsq=2.849e+34, orig_rms_sq=1.938e-02
2024-10-08 23:26:23,905 WARNING [optim.py:503] Scaling gradients by 0.0007274933741427958, model_norm_threshold=2653060891410432.0
2024-10-08 23:26:24,039 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.545e+36, grad_sumsq=1.309e+38, orig_rms_sq=1.944e-02
2024-10-08 23:26:26,316 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.out_combiner.scale_min, batch_count=4074.0, ans=0.75741
2024-10-08 23:26:31,556 WARNING [optim.py:503] Scaling gradients by 0.0017714992864057422, model_norm_threshold=2653060891410432.0
2024-10-08 23:26:31,690 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.285e+35, grad_sumsq=1.041e+38, orig_rms_sq=5.077e-03
2024-10-08 23:26:33,906 WARNING [optim.py:503] Scaling gradients by 0.0028847353532910347, model_norm_threshold=2653060891410432.0
2024-10-08 23:26:34,040 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.135e+35, grad_sumsq=5.100e+34, orig_rms_sq=4.185e+00
2024-10-08 23:26:38,428 WARNING [optim.py:503] Scaling gradients by 0.0897604376077652, model_norm_threshold=2653060891410432.0
2024-10-08 23:26:38,560 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.137e+32, grad_sumsq=4.033e+34, orig_rms_sq=5.299e-03
2024-10-08 23:26:40,747 WARNING [optim.py:503] Scaling gradients by 0.004931333940476179, model_norm_threshold=2653060891410432.0
2024-10-08 23:26:40,880 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.15, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.321e+34, grad_sumsq=4.777e+33, orig_rms_sq=9.046e+00
2024-10-08 23:26:42,138 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.0.attention_skip_rate, batch_count=4077.3333333333335, ans=0.04967777777777778
2024-10-08 23:26:43,070 WARNING [optim.py:503] Scaling gradients by 0.0001517317141406238, model_norm_threshold=2653060891410432.0
2024-10-08 23:26:43,205 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.361e+37, grad_sumsq=inf, orig_rms_sq=5.428e-03
2024-10-08 23:26:46,474 WARNING [optim.py:503] Scaling gradients by 0.0901741087436676, model_norm_threshold=2653060891410432.0
2024-10-08 23:26:46,609 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.461e+32, grad_sumsq=1.739e+34, orig_rms_sq=1.990e-02
2024-10-08 23:26:55,524 WARNING [optim.py:503] Scaling gradients by 0.07734976708889008, model_norm_threshold=2653060891410432.0
2024-10-08 23:26:55,658 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.52, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.113e+32, grad_sumsq=3.101e+34, orig_rms_sq=1.971e-02
2024-10-08 23:26:56,736 WARNING [optim.py:503] Scaling gradients by 0.04408407211303711, model_norm_threshold=2653060891410432.0
2024-10-08 23:26:56,869 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.313e+32, grad_sumsq=8.329e+31, orig_rms_sq=8.781e+00
2024-10-08 23:26:58,246 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=9.78 vs. limit=9.03025
2024-10-08 23:27:00,184 INFO [train.py:1154] Epoch 2, batch 5400, loss[loss=1.227, simple_loss=0.6387, pruned_loss=0.6967, ctc_loss=1.056, over 4782.00 frames. ], tot_loss[loss=1.252, simple_loss=0.6393, pruned_loss=0.727, ctc_loss=1.025, over 966509.97 frames. ], batch size: 49, lr: 3.19e-02,
2024-10-08 23:27:00,802 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=7.09 vs. limit=5.6335999999999995
2024-10-08 23:27:04,797 WARNING [optim.py:503] Scaling gradients by 0.023086445406079292, model_norm_threshold=2653060891410432.0
2024-10-08 23:27:04,932 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.348e+33, grad_sumsq=2.687e+32, orig_rms_sq=8.738e+00
2024-10-08 23:27:07,074 WARNING [optim.py:503] Scaling gradients by 0.009928513318300247, model_norm_threshold=2653060891410432.0
2024-10-08 23:27:07,208 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.986e+34, grad_sumsq=1.033e+36, orig_rms_sq=1.922e-02
2024-10-08 23:27:08,223 WARNING [optim.py:503] Scaling gradients by 0.0029181865975260735, model_norm_threshold=2653060891410432.0
2024-10-08 23:27:08,359 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.linear_pos.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.297e+35, grad_sumsq=2.530e+34, orig_rms_sq=9.078e+00
2024-10-08 23:27:10,492 WARNING [optim.py:503] Scaling gradients by 0.004007660783827305, model_norm_threshold=2653060891410432.0
2024-10-08 23:27:10,631 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.567e+34, grad_sumsq=1.097e+34, orig_rms_sq=8.723e+00
2024-10-08 23:27:11,900 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module1.balancer1.min_positive, batch_count=4087.3333333333335, ans=0.037227083333333334
2024-10-08 23:27:14,511 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=15.83 vs. limit=9.03275
2024-10-08 23:27:14,600 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=13.58 vs. limit=6.021833333333333
2024-10-08 23:27:16,163 WARNING [optim.py:503] Scaling gradients by 0.004587369970977306, model_norm_threshold=2653060891410432.0
2024-10-08 23:27:16,299 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.616e+34, grad_sumsq=6.432e+33, orig_rms_sq=8.732e+00
2024-10-08 23:27:23,093 WARNING [optim.py:503] Scaling gradients by 0.016762634739279747, model_norm_threshold=2653060891410432.0
2024-10-08 23:27:23,230 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.292e+33, grad_sumsq=4.366e+35, orig_rms_sq=1.899e-02
2024-10-08 23:27:27,260 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=6.95 vs. limit=6.022666666666667
2024-10-08 23:27:29,651 WARNING [optim.py:503] Scaling gradients by 0.0022678812965750694, model_norm_threshold=2653060891410432.0
2024-10-08 23:27:29,787 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.208e+35, grad_sumsq=2.223e+37, orig_rms_sq=1.893e-02
2024-10-08 23:27:34,201 WARNING [optim.py:503] Scaling gradients by 0.01368028949946165, model_norm_threshold=2653060891410432.0
2024-10-08 23:27:34,336 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.494e+33, grad_sumsq=8.531e+32, orig_rms_sq=8.784e+00
2024-10-08 23:27:36,823 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=3.47 vs. limit=3.6141
2024-10-08 23:27:37,751 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=9.20 vs. limit=6.0235
2024-10-08 23:27:40,572 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.bypass.skip_rate, batch_count=4094.0, ans=0.07
2024-10-08 23:27:42,616 WARNING [optim.py:503] Scaling gradients by 0.004803168121725321, model_norm_threshold=2653060891410432.0
2024-10-08 23:27:42,755 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.546e+34, grad_sumsq=2.993e+36, orig_rms_sq=1.853e-02
2024-10-08 23:27:47,396 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.attention_skip_rate, batch_count=4097.333333333333, ans=0.04959444444444445
2024-10-08 23:27:53,292 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=51.20 vs. limit=9.0365
2024-10-08 23:27:54,388 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=14.77 vs. limit=9.0365
2024-10-08 23:27:54,996 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 2.011e+12 8.910e+13 9.024e+14 2.652e+16 inf, threshold=1.805e+15, percent-clipped=43.0
2024-10-08 23:27:57,352 INFO [train.py:1154] Epoch 2, batch 5450, loss[loss=1.242, simple_loss=0.6364, pruned_loss=0.7299, ctc_loss=0.9695, over 4940.00 frames. ], tot_loss[loss=1.252, simple_loss=0.6388, pruned_loss=0.7275, ctc_loss=1.024, over 967059.66 frames. ], batch size: 19, lr: 3.18e-02,
2024-10-08 23:28:00,281 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=27.41 vs. limit=9.03775
2024-10-08 23:28:00,673 WARNING [optim.py:503] Scaling gradients by 0.00038336197030730546, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:00,809 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.319e+36, grad_sumsq=inf, orig_rms_sq=5.519e-03
2024-10-08 23:28:01,815 WARNING [optim.py:503] Scaling gradients by 0.0010808197548612952, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:01,949 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.034e+36, grad_sumsq=5.490e+37, orig_rms_sq=1.884e-02
2024-10-08 23:28:03,014 WARNING [optim.py:503] Scaling gradients by 0.001067197765223682, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:03,149 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.51, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.446e+36, grad_sumsq=2.620e+38, orig_rms_sq=5.519e-03
2024-10-08 23:28:05,467 INFO [scaling.py:214] ScheduledFloat: name=encoder_embed.convnext.layerdrop_rate, batch_count=4100.666666666667, ans=0.16206883333333333
2024-10-08 23:28:08,172 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=83.41 vs. limit=9.039
2024-10-08 23:28:09,743 WARNING [optim.py:503] Scaling gradients by 0.001904028351418674, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:09,880 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.725e+35, grad_sumsq=9.106e+36, orig_rms_sq=1.894e-02
2024-10-08 23:28:11,982 WARNING [optim.py:503] Scaling gradients by 0.014158892445266247, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:12,119 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.58, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.386e+33, grad_sumsq=1.700e+36, orig_rms_sq=5.521e-03
2024-10-08 23:28:13,678 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.32 vs. limit=10.578
2024-10-08 23:28:14,242 WARNING [optim.py:503] Scaling gradients by 0.0358271598815918, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:14,379 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.577e+32, grad_sumsq=2.422e+34, orig_rms_sq=1.890e-02
2024-10-08 23:28:14,752 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=26.24 vs. limit=9.039
2024-10-08 23:28:15,423 WARNING [optim.py:503] Scaling gradients by 0.007343970704823732, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:15,558 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.173e+34, grad_sumsq=2.138e+36, orig_rms_sq=5.487e-03
2024-10-08 23:28:18,978 WARNING [optim.py:503] Scaling gradients by 0.0844535082578659, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:19,116 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.227e+32, grad_sumsq=4.055e+34, orig_rms_sq=5.491e-03
2024-10-08 23:28:20,204 WARNING [optim.py:503] Scaling gradients by 0.012730281800031662, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:20,338 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.410e+33, grad_sumsq=2.860e+35, orig_rms_sq=1.892e-02
2024-10-08 23:28:20,865 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=27.04 vs. limit=10.5805
2024-10-08 23:28:22,570 WARNING [optim.py:503] Scaling gradients by 0.005468410439789295, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:22,707 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.260e+34, grad_sumsq=2.525e+33, orig_rms_sq=8.952e+00
2024-10-08 23:28:23,795 WARNING [optim.py:503] Scaling gradients by 0.006406770087778568, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:23,933 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.783e+34, grad_sumsq=9.403e+35, orig_rms_sq=1.896e-02
2024-10-08 23:28:24,698 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=256, metric=10.95 vs. limit=9.04025
2024-10-08 23:28:30,707 WARNING [optim.py:503] Scaling gradients by 0.05381140112876892, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:30,842 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.216e+32, grad_sumsq=1.169e+34, orig_rms_sq=1.895e-02
2024-10-08 23:28:32,943 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=192, metric=8.70 vs. limit=7.0553333333333335
2024-10-08 23:28:33,238 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.balancer.min_positive, batch_count=4110.666666666667, ans=0.20889333333333332
2024-10-08 23:28:34,108 WARNING [optim.py:503] Scaling gradients by 0.005720583721995354, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:34,247 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.887e+34, grad_sumsq=1.526e+36, orig_rms_sq=1.892e-02
2024-10-08 23:28:38,938 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.3.encoder.layers.1.self_attn_weights, loss-sum=6.369e+04
2024-10-08 23:28:40,906 WARNING [optim.py:503] Scaling gradients by 0.02648860029876232, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:41,041 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.218e+32, grad_sumsq=1.975e+32, orig_rms_sq=4.161e+00
2024-10-08 23:28:42,038 WARNING [optim.py:503] Scaling gradients by 0.0016699197003617883, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:42,175 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.696e+35, grad_sumsq=1.949e+37, orig_rms_sq=1.896e-02
2024-10-08 23:28:44,895 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=8.99 vs. limit=6.0285
2024-10-08 23:28:47,692 WARNING [optim.py:503] Scaling gradients by 0.07887735962867737, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:47,826 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.320e+32, grad_sumsq=2.473e+34, orig_rms_sq=5.335e-03
2024-10-08 23:28:48,399 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.whiten, num_groups=1, num_channels=512, metric=10.76 vs. limit=5.6456
2024-10-08 23:28:48,858 WARNING [optim.py:503] Scaling gradients by 0.018136320635676384, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:48,995 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.930e+33, grad_sumsq=1.023e+35, orig_rms_sq=1.887e-02
2024-10-08 23:28:51,030 WARNING [optim.py:503] Scaling gradients by 0.0017233936814591289, model_norm_threshold=1804866859040768.0
2024-10-08 23:28:51,169 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.835e+35, grad_sumsq=9.758e+36, orig_rms_sq=1.881e-02
2024-10-08 23:28:54,474 INFO [train.py:1154] Epoch 2, batch 5500, loss[loss=1.212, simple_loss=0.6307, pruned_loss=0.6838, ctc_loss=1.066, over 4791.00 frames. ], tot_loss[loss=1.251, simple_loss=0.6389, pruned_loss=0.7265, ctc_loss=1.024, over 967515.17 frames. ], batch size: 49, lr: 3.18e-02,
2024-10-08 23:28:56,849 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.0.layers.0.self_attn_weights, loss-sum=3.829e+06
2024-10-08 23:29:00,360 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.self_attn_weights.pos_emb_skip_rate, batch_count=4117.333333333333, ans=0.0
2024-10-08 23:29:01,909 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.feed_forward2.out_whiten, num_groups=1, num_channels=384, metric=20.89 vs. limit=9.044
2024-10-08 23:29:04,219 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward3.out_whiten, num_groups=1, num_channels=512, metric=36.28 vs. limit=9.044
2024-10-08 23:29:06,988 WARNING [optim.py:503] Scaling gradients by 0.014872481115162373, model_norm_threshold=1804866859040768.0
2024-10-08 23:29:07,126 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.893e+33, grad_sumsq=1.525e+35, orig_rms_sq=1.897e-02
2024-10-08 23:29:07,849 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=27.28 vs. limit=10.5905
2024-10-08 23:29:08,311 WARNING [optim.py:503] Scaling gradients by 0.06232137605547905, model_norm_threshold=1804866859040768.0
2024-10-08 23:29:08,447 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.934e+32, grad_sumsq=3.642e+34, orig_rms_sq=5.310e-03
2024-10-08 23:29:09,502 WARNING [optim.py:503] Scaling gradients by 0.0036847658921033144, model_norm_threshold=1804866859040768.0
2024-10-08 23:29:09,638 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.308e+34, grad_sumsq=3.825e+36, orig_rms_sq=1.910e-02
2024-10-08 23:29:11,025 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_skip_rate, batch_count=4120.666666666667, ans=0.04949722222222222
2024-10-08 23:29:13,117 WARNING [optim.py:503] Scaling gradients by 0.002181652467697859, model_norm_threshold=1804866859040768.0
2024-10-08 23:29:13,253 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.237e+35, grad_sumsq=4.171e+37, orig_rms_sq=5.363e-03
2024-10-08 23:29:14,366 WARNING [optim.py:503] Scaling gradients by 0.00620149914175272, model_norm_threshold=1804866859040768.0
2024-10-08 23:29:14,502 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.139e+34, grad_sumsq=1.111e+36, orig_rms_sq=1.925e-02
2024-10-08 23:29:22,196 WARNING [optim.py:503] Scaling gradients by 0.03922610729932785, model_norm_threshold=1804866859040768.0
2024-10-08 23:29:22,333 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.40, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.566e+32, grad_sumsq=1.580e+35, orig_rms_sq=5.420e-03
2024-10-08 23:29:24,708 WARNING [optim.py:503] Scaling gradients by 0.049800071865320206, model_norm_threshold=1804866859040768.0
2024-10-08 23:29:24,842 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.102e+32, grad_sumsq=1.634e+34, orig_rms_sq=1.898e-02
2024-10-08 23:29:28,990 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.self_attn2.whiten.whitening_limit, batch_count=4127.333333333333, ans=10.5955
2024-10-08 23:29:31,586 WARNING [optim.py:503] Scaling gradients by 0.01480749249458313, model_norm_threshold=1804866859040768.0
2024-10-08 23:29:31,724 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.956e+33, grad_sumsq=9.599e+32, orig_rms_sq=4.121e+00
2024-10-08 23:29:36,340 WARNING [optim.py:503] Scaling gradients by 0.04440883919596672, model_norm_threshold=1804866859040768.0
2024-10-08 23:29:36,476 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.39, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.366e+32, grad_sumsq=3.382e+34, orig_rms_sq=1.882e-02
2024-10-08 23:29:42,045 WARNING [optim.py:503] Scaling gradients by 0.010808744467794895, model_norm_threshold=1804866859040768.0
2024-10-08 23:29:42,181 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.031e+33, grad_sumsq=1.412e+36, orig_rms_sq=5.686e-03
2024-10-08 23:29:48,940 WARNING [optim.py:503] Scaling gradients by 0.002204930642619729, model_norm_threshold=1804866859040768.0
2024-10-08 23:29:49,077 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.405e+35, grad_sumsq=7.405e+36, orig_rms_sq=1.897e-02
2024-10-08 23:29:49,519 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.conv_module1.balancer1.prob, batch_count=4130.666666666667, ans=0.306375
2024-10-08 23:29:50,381 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 1.433e+12 5.239e+13 8.376e+14 3.624e+16 4.708e+18, threshold=1.675e+15, percent-clipped=38.0
2024-10-08 23:29:51,570 WARNING [optim.py:503] Scaling gradients by 0.07701116800308228, model_norm_threshold=1675100998860800.0
2024-10-08 23:29:51,706 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.297e+31, grad_sumsq=4.364e+33, orig_rms_sq=1.901e-02
2024-10-08 23:29:52,822 INFO [train.py:1154] Epoch 2, batch 5550, loss[loss=1.234, simple_loss=0.6269, pruned_loss=0.7348, ctc_loss=0.9295, over 4800.00 frames. ], tot_loss[loss=1.249, simple_loss=0.6376, pruned_loss=0.7258, ctc_loss=1.021, over 967285.15 frames. ], batch size: 19, lr: 3.17e-02,
2024-10-08 23:29:53,815 WARNING [optim.py:503] Scaling gradients by 0.03143017366528511, model_norm_threshold=1675100998860800.0
2024-10-08 23:29:53,951 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.505e+32, grad_sumsq=1.480e+35, orig_rms_sq=5.748e-03
2024-10-08 23:29:54,467 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=384, metric=13.30 vs. limit=9.05025
2024-10-08 23:29:55,938 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.06 vs. limit=10.6005
2024-10-08 23:30:04,018 WARNING [optim.py:503] Scaling gradients by 0.026934755966067314, model_norm_threshold=1675100998860800.0
2024-10-08 23:30:04,154 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.189e+33, grad_sumsq=6.218e+34, orig_rms_sq=1.912e-02
2024-10-08 23:30:06,324 WARNING [optim.py:503] Scaling gradients by 0.016455452889204025, model_norm_threshold=1675100998860800.0
2024-10-08 23:30:06,464 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.14, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.447e+33, grad_sumsq=7.566e+34, orig_rms_sq=1.912e-02
2024-10-08 23:30:07,124 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=11.24 vs. limit=6.034333333333333
2024-10-08 23:30:07,541 WARNING [optim.py:503] Scaling gradients by 0.074674591422081, model_norm_threshold=1675100998860800.0
2024-10-08 23:30:07,674 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.133e+32, grad_sumsq=5.926e+33, orig_rms_sq=1.912e-02
2024-10-08 23:30:08,755 WARNING [optim.py:503] Scaling gradients by 0.0032120037358254194, model_norm_threshold=1675100998860800.0
2024-10-08 23:30:08,893 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.909e+34, grad_sumsq=3.079e+36, orig_rms_sq=1.919e-02
2024-10-08 23:30:09,876 WARNING [optim.py:503] Scaling gradients by 0.004535384010523558, model_norm_threshold=1675100998860800.0
2024-10-08 23:30:10,012 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.857e+34, grad_sumsq=1.489e+36, orig_rms_sq=1.919e-02
2024-10-08 23:30:10,378 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward3.out_whiten, num_groups=1, num_channels=256, metric=9.47 vs. limit=9.0515
2024-10-08 23:30:13,808 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.03 vs. limit=7.068666666666666
2024-10-08 23:30:15,615 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.const_attention_rate, batch_count=4140.666666666667, ans=0.025
2024-10-08 23:30:17,000 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.feed_forward1.out_whiten, num_groups=1, num_channels=256, metric=11.88 vs. limit=9.05275
2024-10-08 23:30:17,630 WARNING [optim.py:503] Scaling gradients by 0.050962936133146286, model_norm_threshold=1675100998860800.0
2024-10-08 23:30:17,768 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.345e+32, grad_sumsq=1.209e+34, orig_rms_sq=1.940e-02
2024-10-08 23:30:22,228 WARNING [optim.py:503] Scaling gradients by 0.0013966021360829473, model_norm_threshold=1675100998860800.0
2024-10-08 23:30:22,365 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.21, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.071e+35, grad_sumsq=1.578e+37, orig_rms_sq=1.946e-02
2024-10-08 23:30:30,346 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.balancer2.prob, batch_count=4144.0, ans=0.30574999999999997
2024-10-08 23:30:32,950 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=512, metric=24.22 vs. limit=9.054
2024-10-08 23:30:33,461 WARNING [optim.py:503] Scaling gradients by 0.0021240408532321453, model_norm_threshold=1675100998860800.0
2024-10-08 23:30:33,595 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.007e+35, grad_sumsq=4.744e+34, orig_rms_sq=4.230e+00
2024-10-08 23:30:33,776 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.0.conv_module2.balancer2.min_positive, batch_count=4144.0, ans=0.0741
2024-10-08 23:30:34,807 WARNING [optim.py:503] Scaling gradients by 0.0846034586429596, model_norm_threshold=1675100998860800.0
2024-10-08 23:30:34,942 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.008e+32, grad_sumsq=5.134e+33, orig_rms_sq=1.962e-02
2024-10-08 23:30:35,130 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.feed_forward3.hidden_balancer.prob, batch_count=4144.0, ans=0.30574999999999997
2024-10-08 23:30:35,882 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=22.93 vs. limit=9.054
2024-10-08 23:30:41,766 WARNING [optim.py:503] Scaling gradients by 0.01627449505031109, model_norm_threshold=1675100998860800.0
2024-10-08 23:30:41,902 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.053e+33, grad_sumsq=5.571e+35, orig_rms_sq=5.479e-03
2024-10-08 23:30:44,139 WARNING [optim.py:503] Scaling gradients by 0.033502887934446335, model_norm_threshold=1675100998860800.0
2024-10-08 23:30:44,276 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.114e+32, grad_sumsq=3.122e+34, orig_rms_sq=1.959e-02
2024-10-08 23:30:45,377 WARNING [optim.py:503] Scaling gradients by 0.009223912842571735, model_norm_threshold=1675100998860800.0
2024-10-08 23:30:45,513 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.linear_pos.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=8.470e+33, grad_sumsq=1.014e+33, orig_rms_sq=8.355e+00
2024-10-08 23:30:46,532 WARNING [optim.py:503] Scaling gradients by 0.010857236571609974, model_norm_threshold=1675100998860800.0
2024-10-08 23:30:46,672 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.7.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.990e+33, grad_sumsq=2.152e+35, orig_rms_sq=1.854e-02
2024-10-08 23:30:47,976 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=4147.333333333333, ans=0.30559375
2024-10-08 23:30:48,262 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=18.88 vs. limit=7.073666666666666
2024-10-08 23:30:49,865 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.feed_forward2.out_whiten, num_groups=1, num_channels=192, metric=13.80 vs. limit=9.0565
2024-10-08 23:30:50,150 INFO [train.py:1154] Epoch 2, batch 5600, loss[loss=1.272, simple_loss=0.6262, pruned_loss=0.7439, ctc_loss=1.076, over 4864.00 frames. ], tot_loss[loss=1.251, simple_loss=0.6379, pruned_loss=0.7278, ctc_loss=1.021, over 967222.51 frames. ], batch size: 28, lr: 3.17e-02,
2024-10-08 23:30:51,613 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.conv_module1.whiten, num_groups=1, num_channels=384, metric=11.01 vs. limit=9.0565
2024-10-08 23:30:55,525 WARNING [optim.py:503] Scaling gradients by 0.002898578066378832, model_norm_threshold=1675100998860800.0
2024-10-08 23:30:55,659 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.397e+34, grad_sumsq=3.824e+36, orig_rms_sq=1.935e-02
2024-10-08 23:30:59,698 WARNING [optim.py:503] Scaling gradients by 0.00812535360455513, model_norm_threshold=1675100998860800.0
2024-10-08 23:30:59,834 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.420e+34, grad_sumsq=7.333e+35, orig_rms_sq=1.937e-02
2024-10-08 23:31:03,544 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten2, num_groups=1, num_channels=384, metric=7.56 vs. limit=7.077
2024-10-08 23:31:04,171 WARNING [optim.py:503] Scaling gradients by 0.00013947296247351915, model_norm_threshold=1675100998860800.0
2024-10-08 23:31:04,308 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.213e+37, grad_sumsq=inf, orig_rms_sq=1.937e-02
2024-10-08 23:31:04,535 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.feed_forward1.out_proj.dropout_p, batch_count=4154.0, ans=0.25845999999999997
2024-10-08 23:31:04,717 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.conv_module2.whiten, num_groups=1, num_channels=256, metric=8.26 vs. limit=9.05775
2024-10-08 23:31:05,425 WARNING [optim.py:503] Scaling gradients by 0.03436413034796715, model_norm_threshold=1675100998860800.0
2024-10-08 23:31:05,560 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.298e+32, grad_sumsq=2.735e+34, orig_rms_sq=1.937e-02
2024-10-08 23:31:08,922 WARNING [optim.py:503] Scaling gradients by 0.0008470779866911471, model_norm_threshold=1675100998860800.0
2024-10-08 23:31:09,059 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.27, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.073e+36, grad_sumsq=5.579e+37, orig_rms_sq=1.923e-02
2024-10-08 23:31:10,364 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.3.attention_skip_rate, batch_count=4154.0, ans=0.04935833333333334
2024-10-08 23:31:11,225 WARNING [optim.py:503] Scaling gradients by 0.0, model_norm_threshold=1675100998860800.0
2024-10-08 23:31:11,256 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out_norm.log_scale with proportion 0.00, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.788e+36, grad_sumsq=inf, orig_rms_sq=1.000e-02
2024-10-08 23:31:12,341 WARNING [optim.py:503] Scaling gradients by 0.0016321665607392788, model_norm_threshold=1675100998860800.0
2024-10-08 23:31:12,478 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.57, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.989e+35, grad_sumsq=1.169e+38, orig_rms_sq=5.122e-03
2024-10-08 23:31:14,028 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=13.59 vs. limit=10.617999999999999
2024-10-08 23:31:14,229 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.self_attn1.whiten, num_groups=1, num_channels=512, metric=30.45 vs. limit=10.617999999999999
2024-10-08 23:31:15,011 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.2.bypass_mid.scale_min, batch_count=4157.333333333333, ans=0.7544933333333333
2024-10-08 23:31:18,315 WARNING [optim.py:503] Scaling gradients by 0.00019990315195173025, model_norm_threshold=1675100998860800.0
2024-10-08 23:31:18,456 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.out.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.392e+37, grad_sumsq=1.546e+36, orig_rms_sq=8.999e+00
2024-10-08 23:31:20,617 WARNING [optim.py:503] Scaling gradients by 0.0001170215691672638, model_norm_threshold=1675100998860800.0
2024-10-08 23:31:20,757 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.28, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.779e+37, grad_sumsq=inf, orig_rms_sq=5.093e-03
2024-10-08 23:31:20,982 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.prob, batch_count=4157.333333333333, ans=0.305125
2024-10-08 23:31:29,204 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=20.76 vs. limit=10.6205
2024-10-08 23:31:29,729 WARNING [optim.py:503] Scaling gradients by 0.01274876482784748, model_norm_threshold=1675100998860800.0
2024-10-08 23:31:29,876 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.60, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.044e+34, grad_sumsq=2.010e+36, orig_rms_sq=5.195e-03
2024-10-08 23:31:33,223 WARNING [optim.py:503] Scaling gradients by 0.032514024525880814, model_norm_threshold=1675100998860800.0
2024-10-08 23:31:33,360 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.153e+33, grad_sumsq=2.192e+35, orig_rms_sq=5.263e-03
2024-10-08 23:31:37,943 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module1.balancer1.prob, batch_count=4164.0, ans=0.3048125
2024-10-08 23:31:39,001 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.1.bypass_mid.scale_min, batch_count=4164.0, ans=0.75426
2024-10-08 23:31:42,213 WARNING [optim.py:503] Scaling gradients by 0.06914319097995758, model_norm_threshold=1675100998860800.0
2024-10-08 23:31:42,351 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.156e+32, grad_sumsq=5.525e+31, orig_rms_sq=3.902e+00
2024-10-08 23:31:45,652 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 3.513e+12 1.749e+14 8.441e+14 2.243e+16 inf, threshold=1.688e+15, percent-clipped=44.0
2024-10-08 23:31:47,814 INFO [train.py:1154] Epoch 2, batch 5650, loss[loss=1.311, simple_loss=0.6774, pruned_loss=0.7546, ctc_loss=1.091, over 4747.00 frames. ], tot_loss[loss=1.25, simple_loss=0.6359, pruned_loss=0.728, ctc_loss=1.02, over 967197.69 frames. ], batch size: 45, lr: 3.16e-02,
2024-10-08 23:31:48,846 WARNING [optim.py:503] Scaling gradients by 0.047659993171691895, model_norm_threshold=1688171389648896.0
2024-10-08 23:31:48,983 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.975e+32, grad_sumsq=7.635e+31, orig_rms_sq=3.897e+00
2024-10-08 23:31:49,890 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.self_attn1.whiten, num_groups=1, num_channels=192, metric=12.86 vs. limit=10.625499999999999
2024-10-08 23:31:52,805 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=7.60 vs. limit=5.666933333333333
2024-10-08 23:31:54,319 WARNING [optim.py:503] Scaling gradients by 0.013908606953918934, model_norm_threshold=1688171389648896.0
2024-10-08 23:31:54,457 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.333e+33, grad_sumsq=1.120e+33, orig_rms_sq=3.870e+00
2024-10-08 23:32:04,103 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn2.whiten, num_groups=1, num_channels=384, metric=32.15 vs. limit=10.628
2024-10-08 23:32:06,069 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=21.36 vs. limit=10.628
2024-10-08 23:32:06,480 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.0.conv_module1.whiten, num_groups=1, num_channels=192, metric=2.80 vs. limit=9.064
2024-10-08 23:32:07,737 WARNING [optim.py:503] Scaling gradients by 0.020180905237793922, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:07,874 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.16, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.130e+33, grad_sumsq=5.815e+34, orig_rms_sq=1.943e-02
2024-10-08 23:32:11,287 WARNING [optim.py:503] Scaling gradients by 0.006421765778213739, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:11,421 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.706e+34, grad_sumsq=3.213e+36, orig_rms_sq=5.310e-03
2024-10-08 23:32:12,450 WARNING [optim.py:503] Scaling gradients by 0.01839989610016346, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:12,588 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.083e+33, grad_sumsq=1.076e+35, orig_rms_sq=1.936e-02
2024-10-08 23:32:16,938 WARNING [optim.py:503] Scaling gradients by 0.02484286203980446, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:17,075 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.203e+33, grad_sumsq=2.175e+35, orig_rms_sq=5.529e-03
2024-10-08 23:32:19,346 WARNING [optim.py:503] Scaling gradients by 0.03618324175477028, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:19,481 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.139e+32, grad_sumsq=9.294e+34, orig_rms_sq=5.529e-03
2024-10-08 23:32:21,672 WARNING [optim.py:503] Scaling gradients by 0.051392074674367905, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:21,809 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.49, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.286e+32, grad_sumsq=9.316e+34, orig_rms_sq=5.674e-03
2024-10-08 23:32:23,830 WARNING [optim.py:503] Scaling gradients by 0.0013043208746239543, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:23,965 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.44, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.388e+35, grad_sumsq=3.691e+37, orig_rms_sq=2.002e-02
2024-10-08 23:32:25,047 WARNING [optim.py:503] Scaling gradients by 0.00022779253777116537, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:25,182 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.26, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.437e+37, grad_sumsq=inf, orig_rms_sq=2.002e-02
2024-10-08 23:32:30,641 WARNING [optim.py:503] Scaling gradients by 0.08527541905641556, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:30,777 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.243e+32, grad_sumsq=6.151e+33, orig_rms_sq=2.021e-02
2024-10-08 23:32:30,946 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.nonlin_attention.balancer.prob, batch_count=4177.333333333333, ans=0.3041875
2024-10-08 23:32:32,513 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.67 vs. limit=10.6355
2024-10-08 23:32:32,882 WARNING [optim.py:503] Scaling gradients by 0.03266022726893425, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:33,020 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.693e+32, grad_sumsq=1.155e+35, orig_rms_sq=5.795e-03
2024-10-08 23:32:34,829 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=22.95 vs. limit=10.6355
2024-10-08 23:32:39,837 WARNING [optim.py:503] Scaling gradients by 0.025287969037890434, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:39,973 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.17, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.493e+32, grad_sumsq=3.688e+34, orig_rms_sq=2.032e-02
2024-10-08 23:32:40,135 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.1.conv_module1.balancer2.prob, batch_count=4180.666666666667, ans=0.30403125
2024-10-08 23:32:40,617 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=24.52 vs. limit=10.6355
2024-10-08 23:32:41,067 WARNING [optim.py:503] Scaling gradients by 0.0016889609396457672, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:41,205 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.981e+35, grad_sumsq=1.467e+37, orig_rms_sq=2.032e-02
2024-10-08 23:32:44,440 WARNING [optim.py:503] Scaling gradients by 0.0020791920833289623, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:44,574 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.239e+35, grad_sumsq=6.084e+36, orig_rms_sq=2.037e-02
2024-10-08 23:32:44,619 INFO [train.py:1154] Epoch 2, batch 5700, loss[loss=1.358, simple_loss=0.6942, pruned_loss=0.7987, ctc_loss=1.062, over 4891.00 frames. ], tot_loss[loss=1.249, simple_loss=0.6358, pruned_loss=0.7276, ctc_loss=1.019, over 966631.55 frames. ], batch size: 22, lr: 3.16e-02,
2024-10-08 23:32:45,762 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.ff2_skip_rate, batch_count=4184.0, ans=0.00996
2024-10-08 23:32:45,803 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.0.conv_module2.balancer1.max_abs, batch_count=4184.0, ans=7.615
2024-10-08 23:32:47,949 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.encoder.layers.1.balancer1.prob, batch_count=4184.0, ans=0.303875
2024-10-08 23:32:48,807 WARNING [optim.py:503] Scaling gradients by 0.066330187022686, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:48,944 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.19, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.222e+32, grad_sumsq=6.007e+33, orig_rms_sq=2.035e-02
2024-10-08 23:32:50,127 WARNING [optim.py:503] Scaling gradients by 0.0981222540140152, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:50,262 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.817e+31, grad_sumsq=1.055e+34, orig_rms_sq=5.513e-03
2024-10-08 23:32:51,251 WARNING [optim.py:503] Scaling gradients by 0.017674144357442856, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:51,389 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.24, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.209e+33, grad_sumsq=1.086e+35, orig_rms_sq=2.035e-02
2024-10-08 23:32:56,865 WARNING [optim.py:503] Scaling gradients by 0.008662254549562931, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:57,001 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.31, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.195e+34, grad_sumsq=2.186e+36, orig_rms_sq=5.468e-03
2024-10-08 23:32:57,162 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module2.balancer1.prob, batch_count=4187.333333333333, ans=0.30371875000000004
2024-10-08 23:32:59,106 WARNING [optim.py:503] Scaling gradients by 0.07540654391050339, model_norm_threshold=1688171389648896.0
2024-10-08 23:32:59,242 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.25, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.232e+32, grad_sumsq=6.129e+33, orig_rms_sq=2.010e-02
2024-10-08 23:33:03,540 WARNING [optim.py:503] Scaling gradients by 0.0004175715148448944, model_norm_threshold=1688171389648896.0
2024-10-08 23:33:03,677 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder.encoders.0.layers.0.self_attn_weights.in_proj.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=5.274e+36, grad_sumsq=5.807e+35, orig_rms_sq=9.083e+00
2024-10-08 23:33:06,952 WARNING [optim.py:503] Scaling gradients by 0.008326400071382523, model_norm_threshold=1688171389648896.0
2024-10-08 23:33:07,088 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.382e+33, grad_sumsq=4.693e+35, orig_rms_sq=1.999e-02
2024-10-08 23:33:10,104 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn1.whiten, num_groups=1, num_channels=192, metric=11.05 vs. limit=10.643
2024-10-08 23:33:10,961 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=384, metric=17.15 vs. limit=6.047666666666666
2024-10-08 23:33:11,415 WARNING [optim.py:503] Scaling gradients by 0.0010560182854533195, model_norm_threshold=1688171389648896.0
2024-10-08 23:33:11,551 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.37, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.366e+35, grad_sumsq=4.658e+37, orig_rms_sq=2.011e-02
2024-10-08 23:33:12,165 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.whiten, num_groups=1, num_channels=384, metric=7.23 vs. limit=5.676266666666667
2024-10-08 23:33:12,600 WARNING [optim.py:503] Scaling gradients by 0.012144089676439762, model_norm_threshold=1688171389648896.0
2024-10-08 23:33:12,738 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.23, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.391e+33, grad_sumsq=8.261e+35, orig_rms_sq=5.315e-03
2024-10-08 23:33:13,786 WARNING [optim.py:503] Scaling gradients by 0.027891363948583603, model_norm_threshold=1688171389648896.0
2024-10-08 23:33:13,923 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.30, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.087e+33, grad_sumsq=5.405e+34, orig_rms_sq=2.011e-02
2024-10-08 23:33:20,506 WARNING [optim.py:503] Scaling gradients by 0.0013607909204438329, model_norm_threshold=1688171389648896.0
2024-10-08 23:33:20,645 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.18, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.735e+35, grad_sumsq=1.351e+37, orig_rms_sq=2.024e-02
2024-10-08 23:33:22,903 WARNING [optim.py:503] Scaling gradients by 0.009459458291530609, model_norm_threshold=1688171389648896.0
2024-10-08 23:33:23,036 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.32, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.020e+34, grad_sumsq=1.931e+36, orig_rms_sq=5.280e-03
2024-10-08 23:33:23,720 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.1.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=11.88 vs. limit=6.0485
2024-10-08 23:33:25,198 WARNING [optim.py:503] Scaling gradients by 0.016924530267715454, model_norm_threshold=1688171389648896.0
2024-10-08 23:33:25,333 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.50, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.990e+33, grad_sumsq=9.501e+35, orig_rms_sq=5.252e-03
2024-10-08 23:33:34,469 WARNING [optim.py:503] Scaling gradients by 0.002495492808520794, model_norm_threshold=1688171389648896.0
2024-10-08 23:33:34,607 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.46, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.095e+35, grad_sumsq=3.894e+37, orig_rms_sq=5.380e-03
2024-10-08 23:33:35,392 INFO [scaling.py:1024] Whitening: name=encoder.encoders.1.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=256, metric=15.27 vs. limit=10.648
2024-10-08 23:33:37,969 WARNING [optim.py:503] Scaling gradients by 0.05077444761991501, model_norm_threshold=1688171389648896.0
2024-10-08 23:33:38,104 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.64, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=7.034e+32, grad_sumsq=1.307e+35, orig_rms_sq=5.380e-03
2024-10-08 23:33:39,432 WARNING [optim.py:487] Clipping_scale=2.0, grad-norm quartiles 6.007e+11 1.659e+14 9.335e+14 3.325e+16 7.411e+18, threshold=1.867e+15, percent-clipped=45.0
2024-10-08 23:33:39,433 WARNING [optim.py:503] Scaling gradients by 0.0642845630645752, model_norm_threshold=1867005774790656.0
2024-10-08 23:33:39,569 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.416e+32, grad_sumsq=4.467e+34, orig_rms_sq=5.408e-03
2024-10-08 23:33:40,599 WARNING [optim.py:503] Scaling gradients by 0.0002885526046156883, model_norm_threshold=1867005774790656.0
2024-10-08 23:33:40,736 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.22, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=9.066e+36, grad_sumsq=inf, orig_rms_sq=2.068e-02
2024-10-08 23:33:41,839 INFO [train.py:1154] Epoch 2, batch 5750, loss[loss=1.321, simple_loss=0.6915, pruned_loss=0.7591, ctc_loss=1.081, over 4836.00 frames. ], tot_loss[loss=1.253, simple_loss=0.6381, pruned_loss=0.7292, ctc_loss=1.023, over 966899.95 frames. ], batch size: 43, lr: 3.16e-02,
2024-10-08 23:33:44,528 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.nonlin_attention.whiten1, num_groups=1, num_channels=288, metric=6.73 vs. limit=6.050166666666667
2024-10-08 23:33:46,171 WARNING [optim.py:503] Scaling gradients by 0.0010562761453911662, model_norm_threshold=1867005774790656.0
2024-10-08 23:33:46,305 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.300e+35, grad_sumsq=1.629e+35, orig_rms_sq=3.867e+00
2024-10-08 23:33:46,539 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.5.encoder.layers.0.conv_module1.balancer1.max_abs, batch_count=4200.666666666667, ans=7.625416666666667
2024-10-08 23:33:48,434 WARNING [optim.py:503] Scaling gradients by 0.01368738990277052, model_norm_threshold=1867005774790656.0
2024-10-08 23:33:48,568 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.721e+33, grad_sumsq=1.814e+35, orig_rms_sq=2.051e-02
2024-10-08 23:33:49,626 WARNING [optim.py:503] Scaling gradients by 0.001218414749018848, model_norm_threshold=1867005774790656.0
2024-10-08 23:33:49,763 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.4.weight with proportion 0.29, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=6.705e+35, grad_sumsq=1.768e+35, orig_rms_sq=3.793e+00
2024-10-08 23:33:58,506 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.3.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=49.22 vs. limit=9.0765
2024-10-08 23:33:58,657 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.feed_forward3.out_whiten, num_groups=1, num_channels=384, metric=16.48 vs. limit=9.0765
2024-10-08 23:33:59,656 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=26.74 vs. limit=10.653
2024-10-08 23:34:00,817 INFO [scaling.py:1024] Whitening: name=encoder.encoders.2.encoder.layers.0.whiten, num_groups=1, num_channels=384, metric=6.81 vs. limit=5.6815999999999995
2024-10-08 23:34:03,539 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.0.nonlin_attention.balancer.max_positive, batch_count=4207.333333333333, ans=0.7920733333333333
2024-10-08 23:34:03,608 INFO [scaling.py:1120] WithLoss: name=encoder.encoders.2.encoder.layers.2.self_attn_weights, loss-sum=6.781e+05
2024-10-08 23:34:03,790 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.1.nonlin_attention.whiten2, num_groups=1, num_channels=256, metric=13.51 vs. limit=7.103666666666666
2024-10-08 23:34:06,799 WARNING [optim.py:503] Scaling gradients by 0.00300172483548522, model_norm_threshold=1867005774790656.0
2024-10-08 23:34:06,933 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.33, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.267e+35, grad_sumsq=6.466e+36, orig_rms_sq=1.959e-02
2024-10-08 23:34:07,149 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.1.bypass.scale_min, batch_count=4207.333333333333, ans=0.7527433333333333
2024-10-08 23:34:09,062 WARNING [optim.py:503] Scaling gradients by 0.07285542786121368, model_norm_threshold=1867005774790656.0
2024-10-08 23:34:09,198 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.38, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=2.514e+32, grad_sumsq=1.283e+34, orig_rms_sq=1.959e-02
2024-10-08 23:34:14,773 WARNING [optim.py:503] Scaling gradients by 0.0008315186132676899, model_norm_threshold=1867005774790656.0
2024-10-08 23:34:14,908 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.20, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.033e+36, grad_sumsq=2.122e+38, orig_rms_sq=4.870e-03
2024-10-08 23:34:16,000 WARNING [optim.py:503] Scaling gradients by 0.00020766386296600103, model_norm_threshold=1867005774790656.0
2024-10-08 23:34:16,136 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.convnext.depthwise_conv.weight with proportion 0.54, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=4.404e+37, grad_sumsq=inf, orig_rms_sq=1.867e-02
2024-10-08 23:34:19,961 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten.whitening_limit, batch_count=4210.666666666667, ans=10.658000000000001
2024-10-08 23:34:24,013 WARNING [optim.py:503] Scaling gradients by 0.09773197770118713, model_norm_threshold=1867005774790656.0
2024-10-08 23:34:24,147 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.43, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=1.586e+32, grad_sumsq=3.322e+34, orig_rms_sq=4.776e-03
2024-10-08 23:34:25,248 WARNING [optim.py:503] Scaling gradients by 0.06869234144687653, model_norm_threshold=1867005774790656.0
2024-10-08 23:34:25,386 WARNING [optim.py:575] Parameter dominating tot_sumsq encoder_embed.conv.0.weight with proportion 0.52, where dominant_sumsq=(grad_sumsq*orig_rms_sq)=3.813e+32, grad_sumsq=8.049e+34, orig_rms_sq=4.737e-03
2024-10-08 23:34:30,127 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn2.whiten, num_groups=1, num_channels=384, metric=nan vs. limit=10.660499999999999
2024-10-08 23:34:37,208 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.1.encoder.layers.0.conv_module1.balancer2.prob, batch_count=4217.333333333333, ans=0.3023125
2024-10-08 23:34:38,209 INFO [train.py:1154] Epoch 2, batch 5800, loss[loss=nan, simple_loss=inf, pruned_loss=inf, ctc_loss=nan, over 4834.00 frames. ], tot_loss[loss=1.191, simple_loss=0.6067, pruned_loss=0.6929, ctc_loss=0.9733, over 966214.63 frames. ], batch size: 43, lr: 3.15e-02,
2024-10-08 23:34:38,612 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn1.whiten, num_groups=1, num_channels=384, metric=nan vs. limit=10.663
2024-10-08 23:34:50,249 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.self_attn1.whiten, num_groups=1, num_channels=384, metric=nan vs. limit=10.6655
2024-10-08 23:34:59,919 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.0.layers.1.feed_forward1.out_whiten.whitening_limit, batch_count=4224.0, ans=9.084
2024-10-08 23:35:03,626 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.const_attention_rate, batch_count=4224.0, ans=0.025
2024-10-08 23:35:03,888 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=nan vs. limit=10.668
2024-10-08 23:35:05,777 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.2.encoder.layers.2.balancer1.prob, batch_count=4224.0, ans=0.302
2024-10-08 23:35:06,051 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.2.self_attn2.whiten, num_groups=1, num_channels=384, metric=nan vs. limit=10.668
2024-10-08 23:35:08,264 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.2.feed_forward1.out_whiten, num_groups=1, num_channels=512, metric=nan vs. limit=9.084
2024-10-08 23:35:12,728 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.self_attn_weights.whiten_keys, num_groups=4, num_channels=128, metric=nan vs. limit=3.6341
2024-10-08 23:35:13,361 INFO [scaling.py:1024] Whitening: name=encoder.encoders.5.encoder.layers.0.nonlin_attention.whiten1, num_groups=1, num_channels=192, metric=nan vs. limit=6.0568333333333335
2024-10-08 23:35:13,416 INFO [scaling.py:1024] Whitening: name=encoder.encoders.4.encoder.layers.1.conv_module2.whiten, num_groups=1, num_channels=384, metric=nan vs. limit=9.08525
2024-10-08 23:35:14,179 INFO [scaling.py:214] ScheduledFloat: name=encoder.encoders.3.out_combiner.scale_min, batch_count=4227.333333333333, ans=0.7520433333333334
2024-10-08 23:35:23,361 INFO [scaling.py:1024] Whitening: name=encoder.encoders.0.layers.1.whiten, num_groups=1, num_channels=192, metric=nan vs. limit=5.692266666666667
2024-10-08 23:35:26,334 INFO [scaling.py:1024] Whitening: name=encoder.encoders.3.encoder.layers.0.nonlin_attention.whiten2, num_groups=1, num_channels=512, metric=nan vs. limit=7.115333333333334
2024-10-08 23:35:28,937 INFO [train.py:1081] Caught exception: Too many grads were not finite.
2024-10-08 23:35:28,937 INFO [checkpoint.py:75] Saving checkpoint to zipformer/exp_transducer_True_ctc_True_attdecoder_False_streaming_True/bad-model-0.pt
2024-10-08 23:35:29,973 INFO [train.py:1469] Saving batch to zipformer/exp_transducer_True_ctc_True_attdecoder_False_streaming_True/batch-c33f4584-b23b-c1d8-493c-d01609de8895.pt
2024-10-08 23:35:29,982 INFO [train.py:1475] features shape: torch.Size([40, 493, 80])
2024-10-08 23:35:29,983 INFO [train.py:1479] num tokens: 1098
Traceback (most recent call last):
  File "/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/zipformer/train.py", line 1555, in <module>
    main()
  File "/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/zipformer/train.py", line 1545, in main
    run(rank=0, world_size=1, args=args)
  File "/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/zipformer/train.py", line 1412, in run
    train_one_epoch(
  File "/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/zipformer/train.py", line 1077, in train_one_epoch
    scaler.step(optimizer)
  File "/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 315, in step
    return optimizer.step(*args, **kwargs)
  File "/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/mnt/ahogpu_ldisk2/adriang/anaconda3/envs/icefall/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/zipformer/optim.py", line 345, in step
    clipping_scale = self._get_clipping_scale(group, batches)
  File "/mnt/ahogpu_ldisk2/adriang/icefall/egs/basque_parliament_1_gttsehu/ASR/zipformer/optim.py", line 473, in _get_clipping_scale
    raise RuntimeError("Too many grads were not finite")
RuntimeError: Too many grads were not finite
